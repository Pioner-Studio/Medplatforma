    1: """Lite version of scipy.linalg.
    2: 
    3: Notes
    4: -----
    5: This module is a lite version of the linalg.py module in SciPy which
    6: contains high-level Python interface to the LAPACK library.  The lite
    7: version only accesses the following LAPACK functions: dgesv, zgesv,
    8: dgeev, zgeev, dgesdd, zgesdd, dgelsd, zgelsd, dsyevd, zheevd, dgetrf,
    9: zgetrf, dpotrf, zpotrf, dgeqrf, zgeqrf, zungqr, dorgqr.
   10: """
   11: 
   12: __all__ = ['matrix_power', 'solve', 'tensorsolve', 'tensorinv', 'inv',
   13:            'cholesky', 'eigvals', 'eigvalsh', 'pinv', 'slogdet', 'det',
   14:            'svd', 'svdvals', 'eig', 'eigh', 'lstsq', 'norm', 'qr', 'cond',
   15:            'matrix_rank', 'LinAlgError', 'multi_dot', 'trace', 'diagonal',
   16:            'cross', 'outer', 'tensordot', 'matmul', 'matrix_transpose',
   17:            'matrix_norm', 'vector_norm', 'vecdot']
   18: 
   19: import functools
   20: import operator
   21: import warnings
   22: from typing import Any, NamedTuple
   23: 
   24: from numpy._core import (
   25:     abs,
   26:     add,
   27:     all,
   28:     amax,
   29:     amin,
   30:     argsort,
   31:     array,
   32:     asanyarray,
   33:     asarray,
   34:     atleast_2d,
   35:     cdouble,
   36:     complexfloating,
   37:     count_nonzero,
   38:     csingle,
   39:     divide,
   40:     dot,
   41:     double,
   42:     empty,
   43:     empty_like,
   44:     errstate,
   45:     finfo,
   46:     inexact,
   47:     inf,
   48:     intc,
   49:     intp,
   50:     isfinite,
   51:     isnan,
   52:     moveaxis,
   53:     multiply,
   54:     newaxis,
   55:     object_,
   56:     overrides,
   57:     prod,
   58:     reciprocal,
   59:     sign,
   60:     single,
   61:     sort,
   62:     sqrt,
   63:     sum,
   64:     swapaxes,
   65:     zeros,
   66: )
   67: from numpy._core import (
   68:     cross as _core_cross,
   69: )
   70: from numpy._core import (
   71:     diagonal as _core_diagonal,
   72: )
   73: from numpy._core import (
   74:     matmul as _core_matmul,
   75: )
   76: from numpy._core import (
   77:     matrix_transpose as _core_matrix_transpose,
   78: )
   79: from numpy._core import (
   80:     outer as _core_outer,
   81: )
   82: from numpy._core import (
   83:     tensordot as _core_tensordot,
   84: )
   85: from numpy._core import (
   86:     trace as _core_trace,
   87: )
   88: from numpy._core import (
   89:     transpose as _core_transpose,
   90: )
   91: from numpy._core import (
   92:     vecdot as _core_vecdot,
   93: )
   94: from numpy._globals import _NoValue
   95: from numpy._typing import NDArray
   96: from numpy._utils import set_module
   97: from numpy.lib._twodim_base_impl import eye, triu
   98: from numpy.lib.array_utils import normalize_axis_index, normalize_axis_tuple
   99: from numpy.linalg import _umath_linalg
  100: 
  101: 
  102: class EigResult(NamedTuple):
  103:     eigenvalues: NDArray[Any]
  104:     eigenvectors: NDArray[Any]
  105: 
  106: class EighResult(NamedTuple):
  107:     eigenvalues: NDArray[Any]
  108:     eigenvectors: NDArray[Any]
  109: 
  110: class QRResult(NamedTuple):
  111:     Q: NDArray[Any]
  112:     R: NDArray[Any]
  113: 
  114: class SlogdetResult(NamedTuple):
  115:     sign: NDArray[Any]
  116:     logabsdet: NDArray[Any]
  117: 
  118: class SVDResult(NamedTuple):
  119:     U: NDArray[Any]
  120:     S: NDArray[Any]
  121:     Vh: NDArray[Any]
  122: 
  123: 
  124: array_function_dispatch = functools.partial(
  125:     overrides.array_function_dispatch, module='numpy.linalg'
  126: )
  127: 
  128: 
  129: fortran_int = intc
  130: 
  131: 
  132: @set_module('numpy.linalg')
  133: class LinAlgError(ValueError):
  134:     """
  135:     Generic Python-exception-derived object raised by linalg functions.
  136: 
  137:     General purpose exception class, derived from Python's ValueError
  138:     class, programmatically raised in linalg functions when a Linear
  139:     Algebra-related condition would prevent further correct execution of the
  140:     function.
  141: 
  142:     Parameters
  143:     ----------
  144:     None
  145: 
  146:     Examples
  147:     --------
  148:     >>> from numpy import linalg as LA
  149:     >>> LA.inv(np.zeros((2,2)))
  150:     Traceback (most recent call last):
  151:       File "<stdin>", line 1, in <module>
  152:       File "...linalg.py", line 350,
  153:         in inv return wrap(solve(a, identity(a.shape[0], dtype=a.dtype)))
  154:       File "...linalg.py", line 249,
  155:         in solve
  156:         raise LinAlgError('Singular matrix')
  157:     numpy.linalg.LinAlgError: Singular matrix
  158: 
  159:     """
  160: 
  161: 
  162: def _raise_linalgerror_singular(err, flag):
  163:     raise LinAlgError("Singular matrix")
  164: 
  165: def _raise_linalgerror_nonposdef(err, flag):
  166:     raise LinAlgError("Matrix is not positive definite")
  167: 
  168: def _raise_linalgerror_eigenvalues_nonconvergence(err, flag):
  169:     raise LinAlgError("Eigenvalues did not converge")
  170: 
  171: def _raise_linalgerror_svd_nonconvergence(err, flag):
  172:     raise LinAlgError("SVD did not converge")
  173: 
  174: def _raise_linalgerror_lstsq(err, flag):
  175:     raise LinAlgError("SVD did not converge in Linear Least Squares")
  176: 
  177: def _raise_linalgerror_qr(err, flag):
  178:     raise LinAlgError("Incorrect argument found while performing "
  179:                       "QR factorization")
  180: 
  181: 
  182: def _makearray(a):
  183:     new = asarray(a)
  184:     wrap = getattr(a, "__array_wrap__", new.__array_wrap__)
  185:     return new, wrap
  186: 
  187: def isComplexType(t):
  188:     return issubclass(t, complexfloating)
  189: 
  190: 
  191: _real_types_map = {single: single,
  192:                    double: double,
  193:                    csingle: single,
  194:                    cdouble: double}
  195: 
  196: _complex_types_map = {single: csingle,
  197:                       double: cdouble,
  198:                       csingle: csingle,
  199:                       cdouble: cdouble}
  200: 
  201: def _realType(t, default=double):
  202:     return _real_types_map.get(t, default)
  203: 
  204: def _complexType(t, default=cdouble):
  205:     return _complex_types_map.get(t, default)
  206: 
  207: def _commonType(*arrays):
  208:     # in lite version, use higher precision (always double or cdouble)
  209:     result_type = single
  210:     is_complex = False
  211:     for a in arrays:
  212:         type_ = a.dtype.type
  213:         if issubclass(type_, inexact):
  214:             if isComplexType(type_):
  215:                 is_complex = True
  216:             rt = _realType(type_, default=None)
  217:             if rt is double:
  218:                 result_type = double
  219:             elif rt is None:
  220:                 # unsupported inexact scalar
  221:                 raise TypeError(f"array type {a.dtype.name} is unsupported in linalg")
  222:         else:
  223:             result_type = double
  224:     if is_complex:
  225:         result_type = _complex_types_map[result_type]
  226:         return cdouble, result_type
  227:     else:
  228:         return double, result_type
  229: 
  230: 
  231: def _to_native_byte_order(*arrays):
  232:     ret = []
  233:     for arr in arrays:
  234:         if arr.dtype.byteorder not in ('=', '|'):
  235:             ret.append(asarray(arr, dtype=arr.dtype.newbyteorder('=')))
  236:         else:
  237:             ret.append(arr)
  238:     if len(ret) == 1:
  239:         return ret[0]
  240:     else:
  241:         return ret
  242: 
  243: 
  244: def _assert_2d(*arrays):
  245:     for a in arrays:
  246:         if a.ndim != 2:
  247:             raise LinAlgError('%d-dimensional array given. Array must be '
  248:                     'two-dimensional' % a.ndim)
  249: 
  250: def _assert_stacked_2d(*arrays):
  251:     for a in arrays:
  252:         if a.ndim < 2:
  253:             raise LinAlgError('%d-dimensional array given. Array must be '
  254:                     'at least two-dimensional' % a.ndim)
  255: 
  256: def _assert_stacked_square(*arrays):
  257:     for a in arrays:
  258:         try:
  259:             m, n = a.shape[-2:]
  260:         except ValueError:
  261:             raise LinAlgError('%d-dimensional array given. Array must be '
  262:                     'at least two-dimensional' % a.ndim)
  263:         if m != n:
  264:             raise LinAlgError('Last 2 dimensions of the array must be square')
  265: 
  266: def _assert_finite(*arrays):
  267:     for a in arrays:
  268:         if not isfinite(a).all():
  269:             raise LinAlgError("Array must not contain infs or NaNs")
  270: 
  271: def _is_empty_2d(arr):
  272:     # check size first for efficiency
  273:     return arr.size == 0 and prod(arr.shape[-2:]) == 0
  274: 
  275: 
  276: def transpose(a):
  277:     """
  278:     Transpose each matrix in a stack of matrices.
  279: 
  280:     Unlike np.transpose, this only swaps the last two axes, rather than all of
  281:     them
  282: 
  283:     Parameters
  284:     ----------
  285:     a : (...,M,N) array_like
  286: 
  287:     Returns
  288:     -------
  289:     aT : (...,N,M) ndarray
  290:     """
  291:     return swapaxes(a, -1, -2)
  292: 
  293: # Linear equations
  294: 
  295: def _tensorsolve_dispatcher(a, b, axes=None):
  296:     return (a, b)
  297: 
  298: 
  299: @array_function_dispatch(_tensorsolve_dispatcher)
  300: def tensorsolve(a, b, axes=None):
  301:     """
  302:     Solve the tensor equation ``a x = b`` for x.
  303: 
  304:     It is assumed that all indices of `x` are summed over in the product,
  305:     together with the rightmost indices of `a`, as is done in, for example,
  306:     ``tensordot(a, x, axes=x.ndim)``.
  307: 
  308:     Parameters
  309:     ----------
  310:     a : array_like
  311:         Coefficient tensor, of shape ``b.shape + Q``. `Q`, a tuple, equals
  312:         the shape of that sub-tensor of `a` consisting of the appropriate
  313:         number of its rightmost indices, and must be such that
  314:         ``prod(Q) == prod(b.shape)`` (in which sense `a` is said to be
  315:         'square').
  316:     b : array_like
  317:         Right-hand tensor, which can be of any shape.
  318:     axes : tuple of ints, optional
  319:         Axes in `a` to reorder to the right, before inversion.
  320:         If None (default), no reordering is done.
  321: 
  322:     Returns
  323:     -------
  324:     x : ndarray, shape Q
  325: 
  326:     Raises
  327:     ------
  328:     LinAlgError
  329:         If `a` is singular or not 'square' (in the above sense).
  330: 
  331:     See Also
  332:     --------
  333:     numpy.tensordot, tensorinv, numpy.einsum
  334: 
  335:     Examples
  336:     --------
  337:     >>> import numpy as np
  338:     >>> a = np.eye(2*3*4)
  339:     >>> a.shape = (2*3, 4, 2, 3, 4)
  340:     >>> rng = np.random.default_rng()
  341:     >>> b = rng.normal(size=(2*3, 4))
  342:     >>> x = np.linalg.tensorsolve(a, b)
  343:     >>> x.shape
  344:     (2, 3, 4)
  345:     >>> np.allclose(np.tensordot(a, x, axes=3), b)
  346:     True
  347: 
  348:     """
  349:     a, wrap = _makearray(a)
  350:     b = asarray(b)
  351:     an = a.ndim
  352: 
  353:     if axes is not None:
  354:         allaxes = list(range(an))
  355:         for k in axes:
  356:             allaxes.remove(k)
  357:             allaxes.insert(an, k)
  358:         a = a.transpose(allaxes)
  359: 
  360:     oldshape = a.shape[-(an - b.ndim):]
  361:     prod = 1
  362:     for k in oldshape:
  363:         prod *= k
  364: 
  365:     if a.size != prod ** 2:
  366:         raise LinAlgError(
  367:             "Input arrays must satisfy the requirement \
  368:             prod(a.shape[b.ndim:]) == prod(a.shape[:b.ndim])"
  369:         )
  370: 
  371:     a = a.reshape(prod, prod)
  372:     b = b.ravel()
  373:     res = wrap(solve(a, b))
  374:     res.shape = oldshape
  375:     return res
  376: 
  377: 
  378: def _solve_dispatcher(a, b):
  379:     return (a, b)
  380: 
  381: 
  382: @array_function_dispatch(_solve_dispatcher)
  383: def solve(a, b):
  384:     """
  385:     Solve a linear matrix equation, or system of linear scalar equations.
  386: 
  387:     Computes the "exact" solution, `x`, of the well-determined, i.e., full
  388:     rank, linear matrix equation `ax = b`.
  389: 
  390:     Parameters
  391:     ----------
  392:     a : (..., M, M) array_like
  393:         Coefficient matrix.
  394:     b : {(M,), (..., M, K)}, array_like
  395:         Ordinate or "dependent variable" values.
  396: 
  397:     Returns
  398:     -------
  399:     x : {(..., M,), (..., M, K)} ndarray
  400:         Solution to the system a x = b.  Returned shape is (..., M) if b is
  401:         shape (M,) and (..., M, K) if b is (..., M, K), where the "..." part is
  402:         broadcasted between a and b.
  403: 
  404:     Raises
  405:     ------
  406:     LinAlgError
  407:         If `a` is singular or not square.
  408: 
  409:     See Also
  410:     --------
  411:     scipy.linalg.solve : Similar function in SciPy.
  412: 
  413:     Notes
  414:     -----
  415:     Broadcasting rules apply, see the `numpy.linalg` documentation for
  416:     details.
  417: 
  418:     The solutions are computed using LAPACK routine ``_gesv``.
  419: 
  420:     `a` must be square and of full-rank, i.e., all rows (or, equivalently,
  421:     columns) must be linearly independent; if either is not true, use
  422:     `lstsq` for the least-squares best "solution" of the
  423:     system/equation.
  424: 
  425:     .. versionchanged:: 2.0
  426: 
  427:        The b array is only treated as a shape (M,) column vector if it is
  428:        exactly 1-dimensional. In all other instances it is treated as a stack
  429:        of (M, K) matrices. Previously b would be treated as a stack of (M,)
  430:        vectors if b.ndim was equal to a.ndim - 1.
  431: 
  432:     References
  433:     ----------
  434:     .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
  435:            FL, Academic Press, Inc., 1980, pg. 22.
  436: 
  437:     Examples
  438:     --------
  439:     Solve the system of equations:
  440:     ``x0 + 2 * x1 = 1`` and
  441:     ``3 * x0 + 5 * x1 = 2``:
  442: 
  443:     >>> import numpy as np
  444:     >>> a = np.array([[1, 2], [3, 5]])
  445:     >>> b = np.array([1, 2])
  446:     >>> x = np.linalg.solve(a, b)
  447:     >>> x
  448:     array([-1.,  1.])
  449: 
  450:     Check that the solution is correct:
  451: 
  452:     >>> np.allclose(np.dot(a, x), b)
  453:     True
  454: 
  455:     """
  456:     a, _ = _makearray(a)
  457:     _assert_stacked_square(a)
  458:     b, wrap = _makearray(b)
  459:     t, result_t = _commonType(a, b)
  460: 
  461:     # We use the b = (..., M,) logic, only if the number of extra dimensions
  462:     # match exactly
  463:     if b.ndim == 1:
  464:         gufunc = _umath_linalg.solve1
  465:     else:
  466:         gufunc = _umath_linalg.solve
  467: 
  468:     signature = 'DD->D' if isComplexType(t) else 'dd->d'
  469:     with errstate(call=_raise_linalgerror_singular, invalid='call',
  470:                   over='ignore', divide='ignore', under='ignore'):
  471:         r = gufunc(a, b, signature=signature)
  472: 
  473:     return wrap(r.astype(result_t, copy=False))
  474: 
  475: 
  476: def _tensorinv_dispatcher(a, ind=None):
  477:     return (a,)
  478: 
  479: 
  480: @array_function_dispatch(_tensorinv_dispatcher)
  481: def tensorinv(a, ind=2):
  482:     """
  483:     Compute the 'inverse' of an N-dimensional array.
  484: 
  485:     The result is an inverse for `a` relative to the tensordot operation
  486:     ``tensordot(a, b, ind)``, i. e., up to floating-point accuracy,
  487:     ``tensordot(tensorinv(a), a, ind)`` is the "identity" tensor for the
  488:     tensordot operation.
  489: 
  490:     Parameters
  491:     ----------
  492:     a : array_like
  493:         Tensor to 'invert'. Its shape must be 'square', i. e.,
  494:         ``prod(a.shape[:ind]) == prod(a.shape[ind:])``.
  495:     ind : int, optional
  496:         Number of first indices that are involved in the inverse sum.
  497:         Must be a positive integer, default is 2.
  498: 
  499:     Returns
  500:     -------
  501:     b : ndarray
  502:         `a`'s tensordot inverse, shape ``a.shape[ind:] + a.shape[:ind]``.
  503: 
  504:     Raises
  505:     ------
  506:     LinAlgError
  507:         If `a` is singular or not 'square' (in the above sense).
  508: 
  509:     See Also
  510:     --------
  511:     numpy.tensordot, tensorsolve
  512: 
  513:     Examples
  514:     --------
  515:     >>> import numpy as np
  516:     >>> a = np.eye(4*6)
  517:     >>> a.shape = (4, 6, 8, 3)
  518:     >>> ainv = np.linalg.tensorinv(a, ind=2)
  519:     >>> ainv.shape
  520:     (8, 3, 4, 6)
  521:     >>> rng = np.random.default_rng()
  522:     >>> b = rng.normal(size=(4, 6))
  523:     >>> np.allclose(np.tensordot(ainv, b), np.linalg.tensorsolve(a, b))
  524:     True
  525: 
  526:     >>> a = np.eye(4*6)
  527:     >>> a.shape = (24, 8, 3)
  528:     >>> ainv = np.linalg.tensorinv(a, ind=1)
  529:     >>> ainv.shape
  530:     (8, 3, 24)
  531:     >>> rng = np.random.default_rng()
  532:     >>> b = rng.normal(size=24)
  533:     >>> np.allclose(np.tensordot(ainv, b, 1), np.linalg.tensorsolve(a, b))
  534:     True
  535: 
  536:     """
  537:     a = asarray(a)
  538:     oldshape = a.shape
  539:     prod = 1
  540:     if ind > 0:
  541:         invshape = oldshape[ind:] + oldshape[:ind]
  542:         for k in oldshape[ind:]:
  543:             prod *= k
  544:     else:
  545:         raise ValueError("Invalid ind argument.")
  546:     a = a.reshape(prod, -1)
  547:     ia = inv(a)
  548:     return ia.reshape(*invshape)
  549: 
  550: 
  551: # Matrix inversion
  552: 
  553: def _unary_dispatcher(a):
  554:     return (a,)
  555: 
  556: 
  557: @array_function_dispatch(_unary_dispatcher)
  558: def inv(a):
  559:     """
  560:     Compute the inverse of a matrix.
  561: 
  562:     Given a square matrix `a`, return the matrix `ainv` satisfying
  563:     ``a @ ainv = ainv @ a = eye(a.shape[0])``.
  564: 
  565:     Parameters
  566:     ----------
  567:     a : (..., M, M) array_like
  568:         Matrix to be inverted.
  569: 
  570:     Returns
  571:     -------
  572:     ainv : (..., M, M) ndarray or matrix
  573:         Inverse of the matrix `a`.
  574: 
  575:     Raises
  576:     ------
  577:     LinAlgError
  578:         If `a` is not square or inversion fails.
  579: 
  580:     See Also
  581:     --------
  582:     scipy.linalg.inv : Similar function in SciPy.
  583:     numpy.linalg.cond : Compute the condition number of a matrix.
  584:     numpy.linalg.svd : Compute the singular value decomposition of a matrix.
  585: 
  586:     Notes
  587:     -----
  588:     Broadcasting rules apply, see the `numpy.linalg` documentation for
  589:     details.
  590: 
  591:     If `a` is detected to be singular, a `LinAlgError` is raised. If `a` is
  592:     ill-conditioned, a `LinAlgError` may or may not be raised, and results may
  593:     be inaccurate due to floating-point errors.
  594: 
  595:     References
  596:     ----------
  597:     .. [1] Wikipedia, "Condition number",
  598:            https://en.wikipedia.org/wiki/Condition_number
  599: 
  600:     Examples
  601:     --------
  602:     >>> import numpy as np
  603:     >>> from numpy.linalg import inv
  604:     >>> a = np.array([[1., 2.], [3., 4.]])
  605:     >>> ainv = inv(a)
  606:     >>> np.allclose(a @ ainv, np.eye(2))
  607:     True
  608:     >>> np.allclose(ainv @ a, np.eye(2))
  609:     True
  610: 
  611:     If a is a matrix object, then the return value is a matrix as well:
  612: 
  613:     >>> ainv = inv(np.matrix(a))
  614:     >>> ainv
  615:     matrix([[-2. ,  1. ],
  616:             [ 1.5, -0.5]])
  617: 
  618:     Inverses of several matrices can be computed at once:
  619: 
  620:     >>> a = np.array([[[1., 2.], [3., 4.]], [[1, 3], [3, 5]]])
  621:     >>> inv(a)
  622:     array([[[-2.  ,  1.  ],
  623:             [ 1.5 , -0.5 ]],
  624:            [[-1.25,  0.75],
  625:             [ 0.75, -0.25]]])
  626: 
  627:     If a matrix is close to singular, the computed inverse may not satisfy
  628:     ``a @ ainv = ainv @ a = eye(a.shape[0])`` even if a `LinAlgError`
  629:     is not raised:
  630: 
  631:     >>> a = np.array([[2,4,6],[2,0,2],[6,8,14]])
  632:     >>> inv(a)  # No errors raised
  633:     array([[-1.12589991e+15, -5.62949953e+14,  5.62949953e+14],
  634:        [-1.12589991e+15, -5.62949953e+14,  5.62949953e+14],
  635:        [ 1.12589991e+15,  5.62949953e+14, -5.62949953e+14]])
  636:     >>> a @ inv(a)
  637:     array([[ 0.   , -0.5  ,  0.   ],  # may vary
  638:            [-0.5  ,  0.625,  0.25 ],
  639:            [ 0.   ,  0.   ,  1.   ]])
  640: 
  641:     To detect ill-conditioned matrices, you can use `numpy.linalg.cond` to
  642:     compute its *condition number* [1]_. The larger the condition number, the
  643:     more ill-conditioned the matrix is. As a rule of thumb, if the condition
  644:     number ``cond(a) = 10**k``, then you may lose up to ``k`` digits of
  645:     accuracy on top of what would be lost to the numerical method due to loss
  646:     of precision from arithmetic methods.
  647: 
  648:     >>> from numpy.linalg import cond
  649:     >>> cond(a)
  650:     np.float64(8.659885634118668e+17)  # may vary
  651: 
  652:     It is also possible to detect ill-conditioning by inspecting the matrix's
  653:     singular values directly. The ratio between the largest and the smallest
  654:     singular value is the condition number:
  655: 
  656:     >>> from numpy.linalg import svd
  657:     >>> sigma = svd(a, compute_uv=False)  # Do not compute singular vectors
  658:     >>> sigma.max()/sigma.min()
  659:     8.659885634118668e+17  # may vary
  660: 
  661:     """
  662:     a, wrap = _makearray(a)
  663:     _assert_stacked_square(a)
  664:     t, result_t = _commonType(a)
  665: 
  666:     signature = 'D->D' if isComplexType(t) else 'd->d'
  667:     with errstate(call=_raise_linalgerror_singular, invalid='call',
  668:                   over='ignore', divide='ignore', under='ignore'):
  669:         ainv = _umath_linalg.inv(a, signature=signature)
  670:     return wrap(ainv.astype(result_t, copy=False))
  671: 
  672: 
  673: def _matrix_power_dispatcher(a, n):
  674:     return (a,)
  675: 
  676: 
  677: @array_function_dispatch(_matrix_power_dispatcher)
  678: def matrix_power(a, n):
  679:     """
  680:     Raise a square matrix to the (integer) power `n`.
  681: 
  682:     For positive integers `n`, the power is computed by repeated matrix
  683:     squarings and matrix multiplications. If ``n == 0``, the identity matrix
  684:     of the same shape as M is returned. If ``n < 0``, the inverse
  685:     is computed and then raised to the ``abs(n)``.
  686: 
  687:     .. note:: Stacks of object matrices are not currently supported.
  688: 
  689:     Parameters
  690:     ----------
  691:     a : (..., M, M) array_like
  692:         Matrix to be "powered".
  693:     n : int
  694:         The exponent can be any integer or long integer, positive,
  695:         negative, or zero.
  696: 
  697:     Returns
  698:     -------
  699:     a**n : (..., M, M) ndarray or matrix object
  700:         The return value is the same shape and type as `M`;
  701:         if the exponent is positive or zero then the type of the
  702:         elements is the same as those of `M`. If the exponent is
  703:         negative the elements are floating-point.
  704: 
  705:     Raises
  706:     ------
  707:     LinAlgError
  708:         For matrices that are not square or that (for negative powers) cannot
  709:         be inverted numerically.
  710: 
  711:     Examples
  712:     --------
  713:     >>> import numpy as np
  714:     >>> from numpy.linalg import matrix_power
  715:     >>> i = np.array([[0, 1], [-1, 0]]) # matrix equiv. of the imaginary unit
  716:     >>> matrix_power(i, 3) # should = -i
  717:     array([[ 0, -1],
  718:            [ 1,  0]])
  719:     >>> matrix_power(i, 0)
  720:     array([[1, 0],
  721:            [0, 1]])
  722:     >>> matrix_power(i, -3) # should = 1/(-i) = i, but w/ f.p. elements
  723:     array([[ 0.,  1.],
  724:            [-1.,  0.]])
  725: 
  726:     Somewhat more sophisticated example
  727: 
  728:     >>> q = np.zeros((4, 4))
  729:     >>> q[0:2, 0:2] = -i
  730:     >>> q[2:4, 2:4] = i
  731:     >>> q # one of the three quaternion units not equal to 1
  732:     array([[ 0., -1.,  0.,  0.],
  733:            [ 1.,  0.,  0.,  0.],
  734:            [ 0.,  0.,  0.,  1.],
  735:            [ 0.,  0., -1.,  0.]])
  736:     >>> matrix_power(q, 2) # = -np.eye(4)
  737:     array([[-1.,  0.,  0.,  0.],
  738:            [ 0., -1.,  0.,  0.],
  739:            [ 0.,  0., -1.,  0.],
  740:            [ 0.,  0.,  0., -1.]])
  741: 
  742:     """
  743:     a = asanyarray(a)
  744:     _assert_stacked_square(a)
  745: 
  746:     try:
  747:         n = operator.index(n)
  748:     except TypeError as e:
  749:         raise TypeError("exponent must be an integer") from e
  750: 
  751:     # Fall back on dot for object arrays. Object arrays are not supported by
  752:     # the current implementation of matmul using einsum
  753:     if a.dtype != object:
  754:         fmatmul = matmul
  755:     elif a.ndim == 2:
  756:         fmatmul = dot
  757:     else:
  758:         raise NotImplementedError(
  759:             "matrix_power not supported for stacks of object arrays")
  760: 
  761:     if n == 0:
  762:         a = empty_like(a)
  763:         a[...] = eye(a.shape[-2], dtype=a.dtype)
  764:         return a
  765: 
  766:     elif n < 0:
  767:         a = inv(a)
  768:         n = abs(n)
  769: 
  770:     # short-cuts.
  771:     if n == 1:
  772:         return a
  773: 
  774:     elif n == 2:
  775:         return fmatmul(a, a)
  776: 
  777:     elif n == 3:
  778:         return fmatmul(fmatmul(a, a), a)
  779: 
  780:     # Use binary decomposition to reduce the number of matrix multiplications.
  781:     # Here, we iterate over the bits of n, from LSB to MSB, raise `a` to
  782:     # increasing powers of 2, and multiply into the result as needed.
  783:     z = result = None
  784:     while n > 0:
  785:         z = a if z is None else fmatmul(z, z)
  786:         n, bit = divmod(n, 2)
  787:         if bit:
  788:             result = z if result is None else fmatmul(result, z)
  789: 
  790:     return result
  791: 
  792: 
  793: # Cholesky decomposition
  794: 
  795: def _cholesky_dispatcher(a, /, *, upper=None):
  796:     return (a,)
  797: 
  798: 
  799: @array_function_dispatch(_cholesky_dispatcher)
  800: def cholesky(a, /, *, upper=False):
  801:     """
  802:     Cholesky decomposition.
  803: 
  804:     Return the lower or upper Cholesky decomposition, ``L * L.H`` or
  805:     ``U.H * U``, of the square matrix ``a``, where ``L`` is lower-triangular,
  806:     ``U`` is upper-triangular, and ``.H`` is the conjugate transpose operator
  807:     (which is the ordinary transpose if ``a`` is real-valued). ``a`` must be
  808:     Hermitian (symmetric if real-valued) and positive-definite. No checking is
  809:     performed to verify whether ``a`` is Hermitian or not. In addition, only
  810:     the lower or upper-triangular and diagonal elements of ``a`` are used.
  811:     Only ``L`` or ``U`` is actually returned.
  812: 
  813:     Parameters
  814:     ----------
  815:     a : (..., M, M) array_like
  816:         Hermitian (symmetric if all elements are real), positive-definite
  817:         input matrix.
  818:     upper : bool
  819:         If ``True``, the result must be the upper-triangular Cholesky factor.
  820:         If ``False``, the result must be the lower-triangular Cholesky factor.
  821:         Default: ``False``.
  822: 
  823:     Returns
  824:     -------
  825:     L : (..., M, M) array_like
  826:         Lower or upper-triangular Cholesky factor of `a`. Returns a matrix
  827:         object if `a` is a matrix object.
  828: 
  829:     Raises
  830:     ------
  831:     LinAlgError
  832:        If the decomposition fails, for example, if `a` is not
  833:        positive-definite.
  834: 
  835:     See Also
  836:     --------
  837:     scipy.linalg.cholesky : Similar function in SciPy.
  838:     scipy.linalg.cholesky_banded : Cholesky decompose a banded Hermitian
  839:                                    positive-definite matrix.
  840:     scipy.linalg.cho_factor : Cholesky decomposition of a matrix, to use in
  841:                               `scipy.linalg.cho_solve`.
  842: 
  843:     Notes
  844:     -----
  845:     Broadcasting rules apply, see the `numpy.linalg` documentation for
  846:     details.
  847: 
  848:     The Cholesky decomposition is often used as a fast way of solving
  849: 
  850:     .. math:: A \\mathbf{x} = \\mathbf{b}
  851: 
  852:     (when `A` is both Hermitian/symmetric and positive-definite).
  853: 
  854:     First, we solve for :math:`\\mathbf{y}` in
  855: 
  856:     .. math:: L \\mathbf{y} = \\mathbf{b},
  857: 
  858:     and then for :math:`\\mathbf{x}` in
  859: 
  860:     .. math:: L^{H} \\mathbf{x} = \\mathbf{y}.
  861: 
  862:     Examples
  863:     --------
  864:     >>> import numpy as np
  865:     >>> A = np.array([[1,-2j],[2j,5]])
  866:     >>> A
  867:     array([[ 1.+0.j, -0.-2.j],
  868:            [ 0.+2.j,  5.+0.j]])
  869:     >>> L = np.linalg.cholesky(A)
  870:     >>> L
  871:     array([[1.+0.j, 0.+0.j],
  872:            [0.+2.j, 1.+0.j]])
  873:     >>> np.dot(L, L.T.conj()) # verify that L * L.H = A
  874:     array([[1.+0.j, 0.-2.j],
  875:            [0.+2.j, 5.+0.j]])
  876:     >>> A = [[1,-2j],[2j,5]] # what happens if A is only array_like?
  877:     >>> np.linalg.cholesky(A) # an ndarray object is returned
  878:     array([[1.+0.j, 0.+0.j],
  879:            [0.+2.j, 1.+0.j]])
  880:     >>> # But a matrix object is returned if A is a matrix object
  881:     >>> np.linalg.cholesky(np.matrix(A))
  882:     matrix([[ 1.+0.j,  0.+0.j],
  883:             [ 0.+2.j,  1.+0.j]])
  884:     >>> # The upper-triangular Cholesky factor can also be obtained.
  885:     >>> np.linalg.cholesky(A, upper=True)
  886:     array([[1.-0.j, 0.-2.j],
  887:            [0.-0.j, 1.-0.j]])
  888: 
  889:     """
  890:     gufunc = _umath_linalg.cholesky_up if upper else _umath_linalg.cholesky_lo
  891:     a, wrap = _makearray(a)
  892:     _assert_stacked_square(a)
  893:     t, result_t = _commonType(a)
  894:     signature = 'D->D' if isComplexType(t) else 'd->d'
  895:     with errstate(call=_raise_linalgerror_nonposdef, invalid='call',
  896:                   over='ignore', divide='ignore', under='ignore'):
  897:         r = gufunc(a, signature=signature)
  898:     return wrap(r.astype(result_t, copy=False))
  899: 
  900: 
  901: # outer product
  902: 
  903: 
  904: def _outer_dispatcher(x1, x2):
  905:     return (x1, x2)
  906: 
  907: 
  908: @array_function_dispatch(_outer_dispatcher)
  909: def outer(x1, x2, /):
  910:     """
  911:     Compute the outer product of two vectors.
  912: 
  913:     This function is Array API compatible. Compared to ``np.outer``
  914:     it accepts 1-dimensional inputs only.
  915: 
  916:     Parameters
  917:     ----------
  918:     x1 : (M,) array_like
  919:         One-dimensional input array of size ``N``.
  920:         Must have a numeric data type.
  921:     x2 : (N,) array_like
  922:         One-dimensional input array of size ``M``.
  923:         Must have a numeric data type.
  924: 
  925:     Returns
  926:     -------
  927:     out : (M, N) ndarray
  928:         ``out[i, j] = a[i] * b[j]``
  929: 
  930:     See also
  931:     --------
  932:     outer
  933: 
  934:     Examples
  935:     --------
  936:     Make a (*very* coarse) grid for computing a Mandelbrot set:
  937: 
  938:     >>> rl = np.linalg.outer(np.ones((5,)), np.linspace(-2, 2, 5))
  939:     >>> rl
  940:     array([[-2., -1.,  0.,  1.,  2.],
  941:            [-2., -1.,  0.,  1.,  2.],
  942:            [-2., -1.,  0.,  1.,  2.],
  943:            [-2., -1.,  0.,  1.,  2.],
  944:            [-2., -1.,  0.,  1.,  2.]])
  945:     >>> im = np.linalg.outer(1j*np.linspace(2, -2, 5), np.ones((5,)))
  946:     >>> im
  947:     array([[0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j, 0.+2.j],
  948:            [0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j, 0.+1.j],
  949:            [0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j],
  950:            [0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j, 0.-1.j],
  951:            [0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j, 0.-2.j]])
  952:     >>> grid = rl + im
  953:     >>> grid
  954:     array([[-2.+2.j, -1.+2.j,  0.+2.j,  1.+2.j,  2.+2.j],
  955:            [-2.+1.j, -1.+1.j,  0.+1.j,  1.+1.j,  2.+1.j],
  956:            [-2.+0.j, -1.+0.j,  0.+0.j,  1.+0.j,  2.+0.j],
  957:            [-2.-1.j, -1.-1.j,  0.-1.j,  1.-1.j,  2.-1.j],
  958:            [-2.-2.j, -1.-2.j,  0.-2.j,  1.-2.j,  2.-2.j]])
  959: 
  960:     An example using a "vector" of letters:
  961: 
  962:     >>> x = np.array(['a', 'b', 'c'], dtype=object)
  963:     >>> np.linalg.outer(x, [1, 2, 3])
  964:     array([['a', 'aa', 'aaa'],
  965:            ['b', 'bb', 'bbb'],
  966:            ['c', 'cc', 'ccc']], dtype=object)
  967: 
  968:     """
  969:     x1 = asanyarray(x1)
  970:     x2 = asanyarray(x2)
  971:     if x1.ndim != 1 or x2.ndim != 1:
  972:         raise ValueError(
  973:             "Input arrays must be one-dimensional, but they are "
  974:             f"{x1.ndim=} and {x2.ndim=}."
  975:         )
  976:     return _core_outer(x1, x2, out=None)
  977: 
  978: 
  979: # QR decomposition
  980: 
  981: 
  982: def _qr_dispatcher(a, mode=None):
  983:     return (a,)
  984: 
  985: 
  986: @array_function_dispatch(_qr_dispatcher)
  987: def qr(a, mode='reduced'):
  988:     """
  989:     Compute the qr factorization of a matrix.
  990: 
  991:     Factor the matrix `a` as *qr*, where `q` is orthonormal and `r` is
  992:     upper-triangular.
  993: 
  994:     Parameters
  995:     ----------
  996:     a : array_like, shape (..., M, N)
  997:         An array-like object with the dimensionality of at least 2.
  998:     mode : {'reduced', 'complete', 'r', 'raw'}, optional, default: 'reduced'
  999:         If K = min(M, N), then
 1000: 
 1001:         * 'reduced'  : returns Q, R with dimensions (..., M, K), (..., K, N)
 1002:         * 'complete' : returns Q, R with dimensions (..., M, M), (..., M, N)
 1003:         * 'r'        : returns R only with dimensions (..., K, N)
 1004:         * 'raw'      : returns h, tau with dimensions (..., N, M), (..., K,)
 1005: 
 1006:         The options 'reduced', 'complete, and 'raw' are new in numpy 1.8,
 1007:         see the notes for more information. The default is 'reduced', and to
 1008:         maintain backward compatibility with earlier versions of numpy both
 1009:         it and the old default 'full' can be omitted. Note that array h
 1010:         returned in 'raw' mode is transposed for calling Fortran. The
 1011:         'economic' mode is deprecated.  The modes 'full' and 'economic' may
 1012:         be passed using only the first letter for backwards compatibility,
 1013:         but all others must be spelled out. See the Notes for more
 1014:         explanation.
 1015: 
 1016: 
 1017:     Returns
 1018:     -------
 1019:     When mode is 'reduced' or 'complete', the result will be a namedtuple with
 1020:     the attributes `Q` and `R`.
 1021: 
 1022:     Q : ndarray of float or complex, optional
 1023:         A matrix with orthonormal columns. When mode = 'complete' the
 1024:         result is an orthogonal/unitary matrix depending on whether or not
 1025:         a is real/complex. The determinant may be either +/- 1 in that
 1026:         case. In case the number of dimensions in the input array is
 1027:         greater than 2 then a stack of the matrices with above properties
 1028:         is returned.
 1029:     R : ndarray of float or complex, optional
 1030:         The upper-triangular matrix or a stack of upper-triangular
 1031:         matrices if the number of dimensions in the input array is greater
 1032:         than 2.
 1033:     (h, tau) : ndarrays of np.double or np.cdouble, optional
 1034:         The array h contains the Householder reflectors that generate q
 1035:         along with r. The tau array contains scaling factors for the
 1036:         reflectors. In the deprecated  'economic' mode only h is returned.
 1037: 
 1038:     Raises
 1039:     ------
 1040:     LinAlgError
 1041:         If factoring fails.
 1042: 
 1043:     See Also
 1044:     --------
 1045:     scipy.linalg.qr : Similar function in SciPy.
 1046:     scipy.linalg.rq : Compute RQ decomposition of a matrix.
 1047: 
 1048:     Notes
 1049:     -----
 1050:     This is an interface to the LAPACK routines ``dgeqrf``, ``zgeqrf``,
 1051:     ``dorgqr``, and ``zungqr``.
 1052: 
 1053:     For more information on the qr factorization, see for example:
 1054:     https://en.wikipedia.org/wiki/QR_factorization
 1055: 
 1056:     Subclasses of `ndarray` are preserved except for the 'raw' mode. So if
 1057:     `a` is of type `matrix`, all the return values will be matrices too.
 1058: 
 1059:     New 'reduced', 'complete', and 'raw' options for mode were added in
 1060:     NumPy 1.8.0 and the old option 'full' was made an alias of 'reduced'.  In
 1061:     addition the options 'full' and 'economic' were deprecated.  Because
 1062:     'full' was the previous default and 'reduced' is the new default,
 1063:     backward compatibility can be maintained by letting `mode` default.
 1064:     The 'raw' option was added so that LAPACK routines that can multiply
 1065:     arrays by q using the Householder reflectors can be used. Note that in
 1066:     this case the returned arrays are of type np.double or np.cdouble and
 1067:     the h array is transposed to be FORTRAN compatible.  No routines using
 1068:     the 'raw' return are currently exposed by numpy, but some are available
 1069:     in lapack_lite and just await the necessary work.
 1070: 
 1071:     Examples
 1072:     --------
 1073:     >>> import numpy as np
 1074:     >>> rng = np.random.default_rng()
 1075:     >>> a = rng.normal(size=(9, 6))
 1076:     >>> Q, R = np.linalg.qr(a)
 1077:     >>> np.allclose(a, np.dot(Q, R))  # a does equal QR
 1078:     True
 1079:     >>> R2 = np.linalg.qr(a, mode='r')
 1080:     >>> np.allclose(R, R2)  # mode='r' returns the same R as mode='full'
 1081:     True
 1082:     >>> a = np.random.normal(size=(3, 2, 2)) # Stack of 2 x 2 matrices as input
 1083:     >>> Q, R = np.linalg.qr(a)
 1084:     >>> Q.shape
 1085:     (3, 2, 2)
 1086:     >>> R.shape
 1087:     (3, 2, 2)
 1088:     >>> np.allclose(a, np.matmul(Q, R))
 1089:     True
 1090: 
 1091:     Example illustrating a common use of `qr`: solving of least squares
 1092:     problems
 1093: 
 1094:     What are the least-squares-best `m` and `y0` in ``y = y0 + mx`` for
 1095:     the following data: {(0,1), (1,0), (1,2), (2,1)}. (Graph the points
 1096:     and you'll see that it should be y0 = 0, m = 1.)  The answer is provided
 1097:     by solving the over-determined matrix equation ``Ax = b``, where::
 1098: 
 1099:       A = array([[0, 1], [1, 1], [1, 1], [2, 1]])
 1100:       x = array([[y0], [m]])
 1101:       b = array([[1], [0], [2], [1]])
 1102: 
 1103:     If A = QR such that Q is orthonormal (which is always possible via
 1104:     Gram-Schmidt), then ``x = inv(R) * (Q.T) * b``.  (In numpy practice,
 1105:     however, we simply use `lstsq`.)
 1106: 
 1107:     >>> A = np.array([[0, 1], [1, 1], [1, 1], [2, 1]])
 1108:     >>> A
 1109:     array([[0, 1],
 1110:            [1, 1],
 1111:            [1, 1],
 1112:            [2, 1]])
 1113:     >>> b = np.array([1, 2, 2, 3])
 1114:     >>> Q, R = np.linalg.qr(A)
 1115:     >>> p = np.dot(Q.T, b)
 1116:     >>> np.dot(np.linalg.inv(R), p)
 1117:     array([  1.,   1.])
 1118: 
 1119:     """
 1120:     if mode not in ('reduced', 'complete', 'r', 'raw'):
 1121:         if mode in ('f', 'full'):
 1122:             # 2013-04-01, 1.8
 1123:             msg = (
 1124:                 "The 'full' option is deprecated in favor of 'reduced'.\n"
 1125:                 "For backward compatibility let mode default."
 1126:             )
 1127:             warnings.warn(msg, DeprecationWarning, stacklevel=2)
 1128:             mode = 'reduced'
 1129:         elif mode in ('e', 'economic'):
 1130:             # 2013-04-01, 1.8
 1131:             msg = "The 'economic' option is deprecated."
 1132:             warnings.warn(msg, DeprecationWarning, stacklevel=2)
 1133:             mode = 'economic'
 1134:         else:
 1135:             raise ValueError(f"Unrecognized mode '{mode}'")
 1136: 
 1137:     a, wrap = _makearray(a)
 1138:     _assert_stacked_2d(a)
 1139:     m, n = a.shape[-2:]
 1140:     t, result_t = _commonType(a)
 1141:     a = a.astype(t, copy=True)
 1142:     a = _to_native_byte_order(a)
 1143:     mn = min(m, n)
 1144: 
 1145:     signature = 'D->D' if isComplexType(t) else 'd->d'
 1146:     with errstate(call=_raise_linalgerror_qr, invalid='call',
 1147:                   over='ignore', divide='ignore', under='ignore'):
 1148:         tau = _umath_linalg.qr_r_raw(a, signature=signature)
 1149: 
 1150:     # handle modes that don't return q
 1151:     if mode == 'r':
 1152:         r = triu(a[..., :mn, :])
 1153:         r = r.astype(result_t, copy=False)
 1154:         return wrap(r)
 1155: 
 1156:     if mode == 'raw':
 1157:         q = transpose(a)
 1158:         q = q.astype(result_t, copy=False)
 1159:         tau = tau.astype(result_t, copy=False)
 1160:         return wrap(q), tau
 1161: 
 1162:     if mode == 'economic':
 1163:         a = a.astype(result_t, copy=False)
 1164:         return wrap(a)
 1165: 
 1166:     # mc is the number of columns in the resulting q
 1167:     # matrix. If the mode is complete then it is
 1168:     # same as number of rows, and if the mode is reduced,
 1169:     # then it is the minimum of number of rows and columns.
 1170:     if mode == 'complete' and m > n:
 1171:         mc = m
 1172:         gufunc = _umath_linalg.qr_complete
 1173:     else:
 1174:         mc = mn
 1175:         gufunc = _umath_linalg.qr_reduced
 1176: 
 1177:     signature = 'DD->D' if isComplexType(t) else 'dd->d'
 1178:     with errstate(call=_raise_linalgerror_qr, invalid='call',
 1179:                   over='ignore', divide='ignore', under='ignore'):
 1180:         q = gufunc(a, tau, signature=signature)
 1181:     r = triu(a[..., :mc, :])
 1182: 
 1183:     q = q.astype(result_t, copy=False)
 1184:     r = r.astype(result_t, copy=False)
 1185: 
 1186:     return QRResult(wrap(q), wrap(r))
 1187: 
 1188: # Eigenvalues
 1189: 
 1190: 
 1191: @array_function_dispatch(_unary_dispatcher)
 1192: def eigvals(a):
 1193:     """
 1194:     Compute the eigenvalues of a general matrix.
 1195: 
 1196:     Main difference between `eigvals` and `eig`: the eigenvectors aren't
 1197:     returned.
 1198: 
 1199:     Parameters
 1200:     ----------
 1201:     a : (..., M, M) array_like
 1202:         A complex- or real-valued matrix whose eigenvalues will be computed.
 1203: 
 1204:     Returns
 1205:     -------
 1206:     w : (..., M,) ndarray
 1207:         The eigenvalues, each repeated according to its multiplicity.
 1208:         They are not necessarily ordered, nor are they necessarily
 1209:         real for real matrices.
 1210: 
 1211:     Raises
 1212:     ------
 1213:     LinAlgError
 1214:         If the eigenvalue computation does not converge.
 1215: 
 1216:     See Also
 1217:     --------
 1218:     eig : eigenvalues and right eigenvectors of general arrays
 1219:     eigvalsh : eigenvalues of real symmetric or complex Hermitian
 1220:                (conjugate symmetric) arrays.
 1221:     eigh : eigenvalues and eigenvectors of real symmetric or complex
 1222:            Hermitian (conjugate symmetric) arrays.
 1223:     scipy.linalg.eigvals : Similar function in SciPy.
 1224: 
 1225:     Notes
 1226:     -----
 1227:     Broadcasting rules apply, see the `numpy.linalg` documentation for
 1228:     details.
 1229: 
 1230:     This is implemented using the ``_geev`` LAPACK routines which compute
 1231:     the eigenvalues and eigenvectors of general square arrays.
 1232: 
 1233:     Examples
 1234:     --------
 1235:     Illustration, using the fact that the eigenvalues of a diagonal matrix
 1236:     are its diagonal elements, that multiplying a matrix on the left
 1237:     by an orthogonal matrix, `Q`, and on the right by `Q.T` (the transpose
 1238:     of `Q`), preserves the eigenvalues of the "middle" matrix. In other words,
 1239:     if `Q` is orthogonal, then ``Q * A * Q.T`` has the same eigenvalues as
 1240:     ``A``:
 1241: 
 1242:     >>> import numpy as np
 1243:     >>> from numpy import linalg as LA
 1244:     >>> x = np.random.random()
 1245:     >>> Q = np.array([[np.cos(x), -np.sin(x)], [np.sin(x), np.cos(x)]])
 1246:     >>> LA.norm(Q[0, :]), LA.norm(Q[1, :]), np.dot(Q[0, :],Q[1, :])
 1247:     (1.0, 1.0, 0.0)
 1248: 
 1249:     Now multiply a diagonal matrix by ``Q`` on one side and
 1250:     by ``Q.T`` on the other:
 1251: 
 1252:     >>> D = np.diag((-1,1))
 1253:     >>> LA.eigvals(D)
 1254:     array([-1.,  1.])
 1255:     >>> A = np.dot(Q, D)
 1256:     >>> A = np.dot(A, Q.T)
 1257:     >>> LA.eigvals(A)
 1258:     array([ 1., -1.]) # random
 1259: 
 1260:     """
 1261:     a, wrap = _makearray(a)
 1262:     _assert_stacked_square(a)
 1263:     _assert_finite(a)
 1264:     t, result_t = _commonType(a)
 1265: 
 1266:     signature = 'D->D' if isComplexType(t) else 'd->D'
 1267:     with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,
 1268:                   invalid='call', over='ignore', divide='ignore',
 1269:                   under='ignore'):
 1270:         w = _umath_linalg.eigvals(a, signature=signature)
 1271: 
 1272:     if not isComplexType(t):
 1273:         if all(w.imag == 0):
 1274:             w = w.real
 1275:             result_t = _realType(result_t)
 1276:         else:
 1277:             result_t = _complexType(result_t)
 1278: 
 1279:     return w.astype(result_t, copy=False)
 1280: 
 1281: 
 1282: def _eigvalsh_dispatcher(a, UPLO=None):
 1283:     return (a,)
 1284: 
 1285: 
 1286: @array_function_dispatch(_eigvalsh_dispatcher)
 1287: def eigvalsh(a, UPLO='L'):
 1288:     """
 1289:     Compute the eigenvalues of a complex Hermitian or real symmetric matrix.
 1290: 
 1291:     Main difference from eigh: the eigenvectors are not computed.
 1292: 
 1293:     Parameters
 1294:     ----------
 1295:     a : (..., M, M) array_like
 1296:         A complex- or real-valued matrix whose eigenvalues are to be
 1297:         computed.
 1298:     UPLO : {'L', 'U'}, optional
 1299:         Specifies whether the calculation is done with the lower triangular
 1300:         part of `a` ('L', default) or the upper triangular part ('U').
 1301:         Irrespective of this value only the real parts of the diagonal will
 1302:         be considered in the computation to preserve the notion of a Hermitian
 1303:         matrix. It therefore follows that the imaginary part of the diagonal
 1304:         will always be treated as zero.
 1305: 
 1306:     Returns
 1307:     -------
 1308:     w : (..., M,) ndarray
 1309:         The eigenvalues in ascending order, each repeated according to
 1310:         its multiplicity.
 1311: 
 1312:     Raises
 1313:     ------
 1314:     LinAlgError
 1315:         If the eigenvalue computation does not converge.
 1316: 
 1317:     See Also
 1318:     --------
 1319:     eigh : eigenvalues and eigenvectors of real symmetric or complex Hermitian
 1320:            (conjugate symmetric) arrays.
 1321:     eigvals : eigenvalues of general real or complex arrays.
 1322:     eig : eigenvalues and right eigenvectors of general real or complex
 1323:           arrays.
 1324:     scipy.linalg.eigvalsh : Similar function in SciPy.
 1325: 
 1326:     Notes
 1327:     -----
 1328:     Broadcasting rules apply, see the `numpy.linalg` documentation for
 1329:     details.
 1330: 
 1331:     The eigenvalues are computed using LAPACK routines ``_syevd``, ``_heevd``.
 1332: 
 1333:     Examples
 1334:     --------
 1335:     >>> import numpy as np
 1336:     >>> from numpy import linalg as LA
 1337:     >>> a = np.array([[1, -2j], [2j, 5]])
 1338:     >>> LA.eigvalsh(a)
 1339:     array([ 0.17157288,  5.82842712]) # may vary
 1340: 
 1341:     >>> # demonstrate the treatment of the imaginary part of the diagonal
 1342:     >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])
 1343:     >>> a
 1344:     array([[5.+2.j, 9.-2.j],
 1345:            [0.+2.j, 2.-1.j]])
 1346:     >>> # with UPLO='L' this is numerically equivalent to using LA.eigvals()
 1347:     >>> # with:
 1348:     >>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])
 1349:     >>> b
 1350:     array([[5.+0.j, 0.-2.j],
 1351:            [0.+2.j, 2.+0.j]])
 1352:     >>> wa = LA.eigvalsh(a)
 1353:     >>> wb = LA.eigvals(b)
 1354:     >>> wa
 1355:     array([1., 6.])
 1356:     >>> wb
 1357:     array([6.+0.j, 1.+0.j])
 1358: 
 1359:     """
 1360:     UPLO = UPLO.upper()
 1361:     if UPLO not in ('L', 'U'):
 1362:         raise ValueError("UPLO argument must be 'L' or 'U'")
 1363: 
 1364:     if UPLO == 'L':
 1365:         gufunc = _umath_linalg.eigvalsh_lo
 1366:     else:
 1367:         gufunc = _umath_linalg.eigvalsh_up
 1368: 
 1369:     a, wrap = _makearray(a)
 1370:     _assert_stacked_square(a)
 1371:     t, result_t = _commonType(a)
 1372:     signature = 'D->d' if isComplexType(t) else 'd->d'
 1373:     with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,
 1374:                   invalid='call', over='ignore', divide='ignore',
 1375:                   under='ignore'):
 1376:         w = gufunc(a, signature=signature)
 1377:     return w.astype(_realType(result_t), copy=False)
 1378: 
 1379: 
 1380: # Eigenvectors
 1381: 
 1382: 
 1383: @array_function_dispatch(_unary_dispatcher)
 1384: def eig(a):
 1385:     """
 1386:     Compute the eigenvalues and right eigenvectors of a square array.
 1387: 
 1388:     Parameters
 1389:     ----------
 1390:     a : (..., M, M) array
 1391:         Matrices for which the eigenvalues and right eigenvectors will
 1392:         be computed
 1393: 
 1394:     Returns
 1395:     -------
 1396:     A namedtuple with the following attributes:
 1397: 
 1398:     eigenvalues : (..., M) array
 1399:         The eigenvalues, each repeated according to its multiplicity.
 1400:         The eigenvalues are not necessarily ordered. The resulting
 1401:         array will be of complex type, unless the imaginary part is
 1402:         zero in which case it will be cast to a real type. When `a`
 1403:         is real the resulting eigenvalues will be real (0 imaginary
 1404:         part) or occur in conjugate pairs
 1405: 
 1406:     eigenvectors : (..., M, M) array
 1407:         The normalized (unit "length") eigenvectors, such that the
 1408:         column ``eigenvectors[:,i]`` is the eigenvector corresponding to the
 1409:         eigenvalue ``eigenvalues[i]``.
 1410: 
 1411:     Raises
 1412:     ------
 1413:     LinAlgError
 1414:         If the eigenvalue computation does not converge.
 1415: 
 1416:     See Also
 1417:     --------
 1418:     eigvals : eigenvalues of a non-symmetric array.
 1419:     eigh : eigenvalues and eigenvectors of a real symmetric or complex
 1420:            Hermitian (conjugate symmetric) array.
 1421:     eigvalsh : eigenvalues of a real symmetric or complex Hermitian
 1422:                (conjugate symmetric) array.
 1423:     scipy.linalg.eig : Similar function in SciPy that also solves the
 1424:                        generalized eigenvalue problem.
 1425:     scipy.linalg.schur : Best choice for unitary and other non-Hermitian
 1426:                          normal matrices.
 1427: 
 1428:     Notes
 1429:     -----
 1430:     Broadcasting rules apply, see the `numpy.linalg` documentation for
 1431:     details.
 1432: 
 1433:     This is implemented using the ``_geev`` LAPACK routines which compute
 1434:     the eigenvalues and eigenvectors of general square arrays.
 1435: 
 1436:     The number `w` is an eigenvalue of `a` if there exists a vector `v` such
 1437:     that ``a @ v = w * v``. Thus, the arrays `a`, `eigenvalues`, and
 1438:     `eigenvectors` satisfy the equations ``a @ eigenvectors[:,i] =
 1439:     eigenvalues[i] * eigenvectors[:,i]`` for :math:`i \\in \\{0,...,M-1\\}`.
 1440: 
 1441:     The array `eigenvectors` may not be of maximum rank, that is, some of the
 1442:     columns may be linearly dependent, although round-off error may obscure
 1443:     that fact. If the eigenvalues are all different, then theoretically the
 1444:     eigenvectors are linearly independent and `a` can be diagonalized by a
 1445:     similarity transformation using `eigenvectors`, i.e, ``inv(eigenvectors) @
 1446:     a @ eigenvectors`` is diagonal.
 1447: 
 1448:     For non-Hermitian normal matrices the SciPy function `scipy.linalg.schur`
 1449:     is preferred because the matrix `eigenvectors` is guaranteed to be
 1450:     unitary, which is not the case when using `eig`. The Schur factorization
 1451:     produces an upper triangular matrix rather than a diagonal matrix, but for
 1452:     normal matrices only the diagonal of the upper triangular matrix is
 1453:     needed, the rest is roundoff error.
 1454: 
 1455:     Finally, it is emphasized that `eigenvectors` consists of the *right* (as
 1456:     in right-hand side) eigenvectors of `a`. A vector `y` satisfying ``y.T @ a
 1457:     = z * y.T`` for some number `z` is called a *left* eigenvector of `a`,
 1458:     and, in general, the left and right eigenvectors of a matrix are not
 1459:     necessarily the (perhaps conjugate) transposes of each other.
 1460: 
 1461:     References
 1462:     ----------
 1463:     G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando, FL,
 1464:     Academic Press, Inc., 1980, Various pp.
 1465: 
 1466:     Examples
 1467:     --------
 1468:     >>> import numpy as np
 1469:     >>> from numpy import linalg as LA
 1470: 
 1471:     (Almost) trivial example with real eigenvalues and eigenvectors.
 1472: 
 1473:     >>> eigenvalues, eigenvectors = LA.eig(np.diag((1, 2, 3)))
 1474:     >>> eigenvalues
 1475:     array([1., 2., 3.])
 1476:     >>> eigenvectors
 1477:     array([[1., 0., 0.],
 1478:            [0., 1., 0.],
 1479:            [0., 0., 1.]])
 1480: 
 1481:     Real matrix possessing complex eigenvalues and eigenvectors;
 1482:     note that the eigenvalues are complex conjugates of each other.
 1483: 
 1484:     >>> eigenvalues, eigenvectors = LA.eig(np.array([[1, -1], [1, 1]]))
 1485:     >>> eigenvalues
 1486:     array([1.+1.j, 1.-1.j])
 1487:     >>> eigenvectors
 1488:     array([[0.70710678+0.j        , 0.70710678-0.j        ],
 1489:            [0.        -0.70710678j, 0.        +0.70710678j]])
 1490: 
 1491:     Complex-valued matrix with real eigenvalues (but complex-valued
 1492:     eigenvectors); note that ``a.conj().T == a``, i.e., `a` is Hermitian.
 1493: 
 1494:     >>> a = np.array([[1, 1j], [-1j, 1]])
 1495:     >>> eigenvalues, eigenvectors = LA.eig(a)
 1496:     >>> eigenvalues
 1497:     array([2.+0.j, 0.+0.j])
 1498:     >>> eigenvectors
 1499:     array([[ 0.        +0.70710678j,  0.70710678+0.j        ], # may vary
 1500:            [ 0.70710678+0.j        , -0.        +0.70710678j]])
 1501: 
 1502:     Be careful about round-off error!
 1503: 
 1504:     >>> a = np.array([[1 + 1e-9, 0], [0, 1 - 1e-9]])
 1505:     >>> # Theor. eigenvalues are 1 +/- 1e-9
 1506:     >>> eigenvalues, eigenvectors = LA.eig(a)
 1507:     >>> eigenvalues
 1508:     array([1., 1.])
 1509:     >>> eigenvectors
 1510:     array([[1., 0.],
 1511:            [0., 1.]])
 1512: 
 1513:     """
 1514:     a, wrap = _makearray(a)
 1515:     _assert_stacked_square(a)
 1516:     _assert_finite(a)
 1517:     t, result_t = _commonType(a)
 1518: 
 1519:     signature = 'D->DD' if isComplexType(t) else 'd->DD'
 1520:     with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,
 1521:                   invalid='call', over='ignore', divide='ignore',
 1522:                   under='ignore'):
 1523:         w, vt = _umath_linalg.eig(a, signature=signature)
 1524: 
 1525:     if not isComplexType(t) and all(w.imag == 0.0):
 1526:         w = w.real
 1527:         vt = vt.real
 1528:         result_t = _realType(result_t)
 1529:     else:
 1530:         result_t = _complexType(result_t)
 1531: 
 1532:     vt = vt.astype(result_t, copy=False)
 1533:     return EigResult(w.astype(result_t, copy=False), wrap(vt))
 1534: 
 1535: 
 1536: @array_function_dispatch(_eigvalsh_dispatcher)
 1537: def eigh(a, UPLO='L'):
 1538:     """
 1539:     Return the eigenvalues and eigenvectors of a complex Hermitian
 1540:     (conjugate symmetric) or a real symmetric matrix.
 1541: 
 1542:     Returns two objects, a 1-D array containing the eigenvalues of `a`, and
 1543:     a 2-D square array or matrix (depending on the input type) of the
 1544:     corresponding eigenvectors (in columns).
 1545: 
 1546:     Parameters
 1547:     ----------
 1548:     a : (..., M, M) array
 1549:         Hermitian or real symmetric matrices whose eigenvalues and
 1550:         eigenvectors are to be computed.
 1551:     UPLO : {'L', 'U'}, optional
 1552:         Specifies whether the calculation is done with the lower triangular
 1553:         part of `a` ('L', default) or the upper triangular part ('U').
 1554:         Irrespective of this value only the real parts of the diagonal will
 1555:         be considered in the computation to preserve the notion of a Hermitian
 1556:         matrix. It therefore follows that the imaginary part of the diagonal
 1557:         will always be treated as zero.
 1558: 
 1559:     Returns
 1560:     -------
 1561:     A namedtuple with the following attributes:
 1562: 
 1563:     eigenvalues : (..., M) ndarray
 1564:         The eigenvalues in ascending order, each repeated according to
 1565:         its multiplicity.
 1566:     eigenvectors : {(..., M, M) ndarray, (..., M, M) matrix}
 1567:         The column ``eigenvectors[:, i]`` is the normalized eigenvector
 1568:         corresponding to the eigenvalue ``eigenvalues[i]``.  Will return a
 1569:         matrix object if `a` is a matrix object.
 1570: 
 1571:     Raises
 1572:     ------
 1573:     LinAlgError
 1574:         If the eigenvalue computation does not converge.
 1575: 
 1576:     See Also
 1577:     --------
 1578:     eigvalsh : eigenvalues of real symmetric or complex Hermitian
 1579:                (conjugate symmetric) arrays.
 1580:     eig : eigenvalues and right eigenvectors for non-symmetric arrays.
 1581:     eigvals : eigenvalues of non-symmetric arrays.
 1582:     scipy.linalg.eigh : Similar function in SciPy (but also solves the
 1583:                         generalized eigenvalue problem).
 1584: 
 1585:     Notes
 1586:     -----
 1587:     Broadcasting rules apply, see the `numpy.linalg` documentation for
 1588:     details.
 1589: 
 1590:     The eigenvalues/eigenvectors are computed using LAPACK routines ``_syevd``,
 1591:     ``_heevd``.
 1592: 
 1593:     The eigenvalues of real symmetric or complex Hermitian matrices are always
 1594:     real. [1]_ The array `eigenvalues` of (column) eigenvectors is unitary and
 1595:     `a`, `eigenvalues`, and `eigenvectors` satisfy the equations ``dot(a,
 1596:     eigenvectors[:, i]) = eigenvalues[i] * eigenvectors[:, i]``.
 1597: 
 1598:     References
 1599:     ----------
 1600:     .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
 1601:            FL, Academic Press, Inc., 1980, pg. 222.
 1602: 
 1603:     Examples
 1604:     --------
 1605:     >>> import numpy as np
 1606:     >>> from numpy import linalg as LA
 1607:     >>> a = np.array([[1, -2j], [2j, 5]])
 1608:     >>> a
 1609:     array([[ 1.+0.j, -0.-2.j],
 1610:            [ 0.+2.j,  5.+0.j]])
 1611:     >>> eigenvalues, eigenvectors = LA.eigh(a)
 1612:     >>> eigenvalues
 1613:     array([0.17157288, 5.82842712])
 1614:     >>> eigenvectors
 1615:     array([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary
 1616:            [ 0.        +0.38268343j,  0.        -0.92387953j]])
 1617: 
 1618:     >>> (np.dot(a, eigenvectors[:, 0]) -
 1619:     ... eigenvalues[0] * eigenvectors[:, 0])  # verify 1st eigenval/vec pair
 1620:     array([5.55111512e-17+0.0000000e+00j, 0.00000000e+00+1.2490009e-16j])
 1621:     >>> (np.dot(a, eigenvectors[:, 1]) -
 1622:     ... eigenvalues[1] * eigenvectors[:, 1])  # verify 2nd eigenval/vec pair
 1623:     array([0.+0.j, 0.+0.j])
 1624: 
 1625:     >>> A = np.matrix(a) # what happens if input is a matrix object
 1626:     >>> A
 1627:     matrix([[ 1.+0.j, -0.-2.j],
 1628:             [ 0.+2.j,  5.+0.j]])
 1629:     >>> eigenvalues, eigenvectors = LA.eigh(A)
 1630:     >>> eigenvalues
 1631:     array([0.17157288, 5.82842712])
 1632:     >>> eigenvectors
 1633:     matrix([[-0.92387953+0.j        , -0.38268343+0.j        ], # may vary
 1634:             [ 0.        +0.38268343j,  0.        -0.92387953j]])
 1635: 
 1636:     >>> # demonstrate the treatment of the imaginary part of the diagonal
 1637:     >>> a = np.array([[5+2j, 9-2j], [0+2j, 2-1j]])
 1638:     >>> a
 1639:     array([[5.+2.j, 9.-2.j],
 1640:            [0.+2.j, 2.-1.j]])
 1641:     >>> # with UPLO='L' this is numerically equivalent to using LA.eig() with:
 1642:     >>> b = np.array([[5.+0.j, 0.-2.j], [0.+2.j, 2.-0.j]])
 1643:     >>> b
 1644:     array([[5.+0.j, 0.-2.j],
 1645:            [0.+2.j, 2.+0.j]])
 1646:     >>> wa, va = LA.eigh(a)
 1647:     >>> wb, vb = LA.eig(b)
 1648:     >>> wa
 1649:     array([1., 6.])
 1650:     >>> wb
 1651:     array([6.+0.j, 1.+0.j])
 1652:     >>> va
 1653:     array([[-0.4472136 +0.j        , -0.89442719+0.j        ], # may vary
 1654:            [ 0.        +0.89442719j,  0.        -0.4472136j ]])
 1655:     >>> vb
 1656:     array([[ 0.89442719+0.j       , -0.        +0.4472136j],
 1657:            [-0.        +0.4472136j,  0.89442719+0.j       ]])
 1658: 
 1659:     """
 1660:     UPLO = UPLO.upper()
 1661:     if UPLO not in ('L', 'U'):
 1662:         raise ValueError("UPLO argument must be 'L' or 'U'")
 1663: 
 1664:     a, wrap = _makearray(a)
 1665:     _assert_stacked_square(a)
 1666:     t, result_t = _commonType(a)
 1667: 
 1668:     if UPLO == 'L':
 1669:         gufunc = _umath_linalg.eigh_lo
 1670:     else:
 1671:         gufunc = _umath_linalg.eigh_up
 1672: 
 1673:     signature = 'D->dD' if isComplexType(t) else 'd->dd'
 1674:     with errstate(call=_raise_linalgerror_eigenvalues_nonconvergence,
 1675:                   invalid='call', over='ignore', divide='ignore',
 1676:                   under='ignore'):
 1677:         w, vt = gufunc(a, signature=signature)
 1678:     w = w.astype(_realType(result_t), copy=False)
 1679:     vt = vt.astype(result_t, copy=False)
 1680:     return EighResult(w, wrap(vt))
 1681: 
 1682: 
 1683: # Singular value decomposition
 1684: 
 1685: def _svd_dispatcher(a, full_matrices=None, compute_uv=None, hermitian=None):
 1686:     return (a,)
 1687: 
 1688: 
 1689: @array_function_dispatch(_svd_dispatcher)
 1690: def svd(a, full_matrices=True, compute_uv=True, hermitian=False):
 1691:     """
 1692:     Singular Value Decomposition.
 1693: 
 1694:     When `a` is a 2D array, and ``full_matrices=False``, then it is
 1695:     factorized as ``u @ np.diag(s) @ vh = (u * s) @ vh``, where
 1696:     `u` and the Hermitian transpose of `vh` are 2D arrays with
 1697:     orthonormal columns and `s` is a 1D array of `a`'s singular
 1698:     values. When `a` is higher-dimensional, SVD is applied in
 1699:     stacked mode as explained below.
 1700: 
 1701:     Parameters
 1702:     ----------
 1703:     a : (..., M, N) array_like
 1704:         A real or complex array with ``a.ndim >= 2``.
 1705:     full_matrices : bool, optional
 1706:         If True (default), `u` and `vh` have the shapes ``(..., M, M)`` and
 1707:         ``(..., N, N)``, respectively.  Otherwise, the shapes are
 1708:         ``(..., M, K)`` and ``(..., K, N)``, respectively, where
 1709:         ``K = min(M, N)``.
 1710:     compute_uv : bool, optional
 1711:         Whether or not to compute `u` and `vh` in addition to `s`.  True
 1712:         by default.
 1713:     hermitian : bool, optional
 1714:         If True, `a` is assumed to be Hermitian (symmetric if real-valued),
 1715:         enabling a more efficient method for finding singular values.
 1716:         Defaults to False.
 1717: 
 1718:     Returns
 1719:     -------
 1720:     When `compute_uv` is True, the result is a namedtuple with the following
 1721:     attribute names:
 1722: 
 1723:     U : { (..., M, M), (..., M, K) } array
 1724:         Unitary array(s). The first ``a.ndim - 2`` dimensions have the same
 1725:         size as those of the input `a`. The size of the last two dimensions
 1726:         depends on the value of `full_matrices`. Only returned when
 1727:         `compute_uv` is True.
 1728:     S : (..., K) array
 1729:         Vector(s) with the singular values, within each vector sorted in
 1730:         descending order. The first ``a.ndim - 2`` dimensions have the same
 1731:         size as those of the input `a`.
 1732:     Vh : { (..., N, N), (..., K, N) } array
 1733:         Unitary array(s). The first ``a.ndim - 2`` dimensions have the same
 1734:         size as those of the input `a`. The size of the last two dimensions
 1735:         depends on the value of `full_matrices`. Only returned when
 1736:         `compute_uv` is True.
 1737: 
 1738:     Raises
 1739:     ------
 1740:     LinAlgError
 1741:         If SVD computation does not converge.
 1742: 
 1743:     See Also
 1744:     --------
 1745:     scipy.linalg.svd : Similar function in SciPy.
 1746:     scipy.linalg.svdvals : Compute singular values of a matrix.
 1747: 
 1748:     Notes
 1749:     -----
 1750:     The decomposition is performed using LAPACK routine ``_gesdd``.
 1751: 
 1752:     SVD is usually described for the factorization of a 2D matrix :math:`A`.
 1753:     The higher-dimensional case will be discussed below. In the 2D case, SVD is
 1754:     written as :math:`A = U S V^H`, where :math:`A = a`, :math:`U= u`,
 1755:     :math:`S= \\mathtt{np.diag}(s)` and :math:`V^H = vh`. The 1D array `s`
 1756:     contains the singular values of `a` and `u` and `vh` are unitary. The rows
 1757:     of `vh` are the eigenvectors of :math:`A^H A` and the columns of `u` are
 1758:     the eigenvectors of :math:`A A^H`. In both cases the corresponding
 1759:     (possibly non-zero) eigenvalues are given by ``s**2``.
 1760: 
 1761:     If `a` has more than two dimensions, then broadcasting rules apply, as
 1762:     explained in :ref:`routines.linalg-broadcasting`. This means that SVD is
 1763:     working in "stacked" mode: it iterates over all indices of the first
 1764:     ``a.ndim - 2`` dimensions and for each combination SVD is applied to the
 1765:     last two indices. The matrix `a` can be reconstructed from the
 1766:     decomposition with either ``(u * s[..., None, :]) @ vh`` or
 1767:     ``u @ (s[..., None] * vh)``. (The ``@`` operator can be replaced by the
 1768:     function ``np.matmul`` for python versions below 3.5.)
 1769: 
 1770:     If `a` is a ``matrix`` object (as opposed to an ``ndarray``), then so are
 1771:     all the return values.
 1772: 
 1773:     Examples
 1774:     --------
 1775:     >>> import numpy as np
 1776:     >>> rng = np.random.default_rng()
 1777:     >>> a = rng.normal(size=(9, 6)) + 1j*rng.normal(size=(9, 6))
 1778:     >>> b = rng.normal(size=(2, 7, 8, 3)) + 1j*rng.normal(size=(2, 7, 8, 3))
 1779: 
 1780: 
 1781:     Reconstruction based on full SVD, 2D case:
 1782: 
 1783:     >>> U, S, Vh = np.linalg.svd(a, full_matrices=True)
 1784:     >>> U.shape, S.shape, Vh.shape
 1785:     ((9, 9), (6,), (6, 6))
 1786:     >>> np.allclose(a, np.dot(U[:, :6] * S, Vh))
 1787:     True
 1788:     >>> smat = np.zeros((9, 6), dtype=complex)
 1789:     >>> smat[:6, :6] = np.diag(S)
 1790:     >>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))
 1791:     True
 1792: 
 1793:     Reconstruction based on reduced SVD, 2D case:
 1794: 
 1795:     >>> U, S, Vh = np.linalg.svd(a, full_matrices=False)
 1796:     >>> U.shape, S.shape, Vh.shape
 1797:     ((9, 6), (6,), (6, 6))
 1798:     >>> np.allclose(a, np.dot(U * S, Vh))
 1799:     True
 1800:     >>> smat = np.diag(S)
 1801:     >>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))
 1802:     True
 1803: 
 1804:     Reconstruction based on full SVD, 4D case:
 1805: 
 1806:     >>> U, S, Vh = np.linalg.svd(b, full_matrices=True)
 1807:     >>> U.shape, S.shape, Vh.shape
 1808:     ((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))
 1809:     >>> np.allclose(b, np.matmul(U[..., :3] * S[..., None, :], Vh))
 1810:     True
 1811:     >>> np.allclose(b, np.matmul(U[..., :3], S[..., None] * Vh))
 1812:     True
 1813: 
 1814:     Reconstruction based on reduced SVD, 4D case:
 1815: 
 1816:     >>> U, S, Vh = np.linalg.svd(b, full_matrices=False)
 1817:     >>> U.shape, S.shape, Vh.shape
 1818:     ((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))
 1819:     >>> np.allclose(b, np.matmul(U * S[..., None, :], Vh))
 1820:     True
 1821:     >>> np.allclose(b, np.matmul(U, S[..., None] * Vh))
 1822:     True
 1823: 
 1824:     """
 1825:     import numpy as np
 1826:     a, wrap = _makearray(a)
 1827: 
 1828:     if hermitian:
 1829:         # note: lapack svd returns eigenvalues with s ** 2 sorted descending,
 1830:         # but eig returns s sorted ascending, so we re-order the eigenvalues
 1831:         # and related arrays to have the correct order
 1832:         if compute_uv:
 1833:             s, u = eigh(a)
 1834:             sgn = sign(s)
 1835:             s = abs(s)
 1836:             sidx = argsort(s)[..., ::-1]
 1837:             sgn = np.take_along_axis(sgn, sidx, axis=-1)
 1838:             s = np.take_along_axis(s, sidx, axis=-1)
 1839:             u = np.take_along_axis(u, sidx[..., None, :], axis=-1)
 1840:             # singular values are unsigned, move the sign into v
 1841:             vt = transpose(u * sgn[..., None, :]).conjugate()
 1842:             return SVDResult(wrap(u), s, wrap(vt))
 1843:         else:
 1844:             s = eigvalsh(a)
 1845:             s = abs(s)
 1846:             return sort(s)[..., ::-1]
 1847: 
 1848:     _assert_stacked_2d(a)
 1849:     t, result_t = _commonType(a)
 1850: 
 1851:     m, n = a.shape[-2:]
 1852:     if compute_uv:
 1853:         if full_matrices:
 1854:             gufunc = _umath_linalg.svd_f
 1855:         else:
 1856:             gufunc = _umath_linalg.svd_s
 1857: 
 1858:         signature = 'D->DdD' if isComplexType(t) else 'd->ddd'
 1859:         with errstate(call=_raise_linalgerror_svd_nonconvergence,
 1860:                       invalid='call', over='ignore', divide='ignore',
 1861:                       under='ignore'):
 1862:             u, s, vh = gufunc(a, signature=signature)
 1863:         u = u.astype(result_t, copy=False)
 1864:         s = s.astype(_realType(result_t), copy=False)
 1865:         vh = vh.astype(result_t, copy=False)
 1866:         return SVDResult(wrap(u), s, wrap(vh))
 1867:     else:
 1868:         signature = 'D->d' if isComplexType(t) else 'd->d'
 1869:         with errstate(call=_raise_linalgerror_svd_nonconvergence,
 1870:                       invalid='call', over='ignore', divide='ignore',
 1871:                       under='ignore'):
 1872:             s = _umath_linalg.svd(a, signature=signature)
 1873:         s = s.astype(_realType(result_t), copy=False)
 1874:         return s
 1875: 
 1876: 
 1877: def _svdvals_dispatcher(x):
 1878:     return (x,)
 1879: 
 1880: 
 1881: @array_function_dispatch(_svdvals_dispatcher)
 1882: def svdvals(x, /):
 1883:     """
 1884:     Returns the singular values of a matrix (or a stack of matrices) ``x``.
 1885:     When x is a stack of matrices, the function will compute the singular
 1886:     values for each matrix in the stack.
 1887: 
 1888:     This function is Array API compatible.
 1889: 
 1890:     Calling ``np.svdvals(x)`` to get singular values is the same as
 1891:     ``np.svd(x, compute_uv=False, hermitian=False)``.
 1892: 
 1893:     Parameters
 1894:     ----------
 1895:     x : (..., M, N) array_like
 1896:         Input array having shape (..., M, N) and whose last two
 1897:         dimensions form matrices on which to perform singular value
 1898:         decomposition. Should have a floating-point data type.
 1899: 
 1900:     Returns
 1901:     -------
 1902:     out : ndarray
 1903:         An array with shape (..., K) that contains the vector(s)
 1904:         of singular values of length K, where K = min(M, N).
 1905: 
 1906:     See Also
 1907:     --------
 1908:     scipy.linalg.svdvals : Compute singular values of a matrix.
 1909: 
 1910:     Examples
 1911:     --------
 1912: 
 1913:     >>> np.linalg.svdvals([[1, 2, 3, 4, 5],
 1914:     ...                    [1, 4, 9, 16, 25],
 1915:     ...                    [1, 8, 27, 64, 125]])
 1916:     array([146.68862757,   5.57510612,   0.60393245])
 1917: 
 1918:     Determine the rank of a matrix using singular values:
 1919: 
 1920:     >>> s = np.linalg.svdvals([[1, 2, 3],
 1921:     ...                        [2, 4, 6],
 1922:     ...                        [-1, 1, -1]]); s
 1923:     array([8.38434191e+00, 1.64402274e+00, 2.31534378e-16])
 1924:     >>> np.count_nonzero(s > 1e-10)  # Matrix of rank 2
 1925:     2
 1926: 
 1927:     """
 1928:     return svd(x, compute_uv=False, hermitian=False)
 1929: 
 1930: 
 1931: def _cond_dispatcher(x, p=None):
 1932:     return (x,)
 1933: 
 1934: 
 1935: @array_function_dispatch(_cond_dispatcher)
 1936: def cond(x, p=None):
 1937:     """
 1938:     Compute the condition number of a matrix.
 1939: 
 1940:     This function is capable of returning the condition number using
 1941:     one of seven different norms, depending on the value of `p` (see
 1942:     Parameters below).
 1943: 
 1944:     Parameters
 1945:     ----------
 1946:     x : (..., M, N) array_like
 1947:         The matrix whose condition number is sought.
 1948:     p : {None, 1, -1, 2, -2, inf, -inf, 'fro'}, optional
 1949:         Order of the norm used in the condition number computation:
 1950: 
 1951:         =====  ============================
 1952:         p      norm for matrices
 1953:         =====  ============================
 1954:         None   2-norm, computed directly using the ``SVD``
 1955:         'fro'  Frobenius norm
 1956:         inf    max(sum(abs(x), axis=1))
 1957:         -inf   min(sum(abs(x), axis=1))
 1958:         1      max(sum(abs(x), axis=0))
 1959:         -1     min(sum(abs(x), axis=0))
 1960:         2      2-norm (largest sing. value)
 1961:         -2     smallest singular value
 1962:         =====  ============================
 1963: 
 1964:         inf means the `numpy.inf` object, and the Frobenius norm is
 1965:         the root-of-sum-of-squares norm.
 1966: 
 1967:     Returns
 1968:     -------
 1969:     c : {float, inf}
 1970:         The condition number of the matrix. May be infinite.
 1971: 
 1972:     See Also
 1973:     --------
 1974:     numpy.linalg.norm
 1975: 
 1976:     Notes
 1977:     -----
 1978:     The condition number of `x` is defined as the norm of `x` times the
 1979:     norm of the inverse of `x` [1]_; the norm can be the usual L2-norm
 1980:     (root-of-sum-of-squares) or one of a number of other matrix norms.
 1981: 
 1982:     References
 1983:     ----------
 1984:     .. [1] G. Strang, *Linear Algebra and Its Applications*, Orlando, FL,
 1985:            Academic Press, Inc., 1980, pg. 285.
 1986: 
 1987:     Examples
 1988:     --------
 1989:     >>> import numpy as np
 1990:     >>> from numpy import linalg as LA
 1991:     >>> a = np.array([[1, 0, -1], [0, 1, 0], [1, 0, 1]])
 1992:     >>> a
 1993:     array([[ 1,  0, -1],
 1994:            [ 0,  1,  0],
 1995:            [ 1,  0,  1]])
 1996:     >>> LA.cond(a)
 1997:     1.4142135623730951
 1998:     >>> LA.cond(a, 'fro')
 1999:     3.1622776601683795
 2000:     >>> LA.cond(a, np.inf)
 2001:     2.0
 2002:     >>> LA.cond(a, -np.inf)
 2003:     1.0
 2004:     >>> LA.cond(a, 1)
 2005:     2.0
 2006:     >>> LA.cond(a, -1)
 2007:     1.0
 2008:     >>> LA.cond(a, 2)
 2009:     1.4142135623730951
 2010:     >>> LA.cond(a, -2)
 2011:     0.70710678118654746 # may vary
 2012:     >>> (min(LA.svd(a, compute_uv=False)) *
 2013:     ... min(LA.svd(LA.inv(a), compute_uv=False)))
 2014:     0.70710678118654746 # may vary
 2015: 
 2016:     """
 2017:     x = asarray(x)  # in case we have a matrix
 2018:     if _is_empty_2d(x):
 2019:         raise LinAlgError("cond is not defined on empty arrays")
 2020:     if p is None or p in {2, -2}:
 2021:         s = svd(x, compute_uv=False)
 2022:         with errstate(all='ignore'):
 2023:             if p == -2:
 2024:                 r = s[..., -1] / s[..., 0]
 2025:             else:
 2026:                 r = s[..., 0] / s[..., -1]
 2027:     else:
 2028:         # Call inv(x) ignoring errors. The result array will
 2029:         # contain nans in the entries where inversion failed.
 2030:         _assert_stacked_square(x)
 2031:         t, result_t = _commonType(x)
 2032:         signature = 'D->D' if isComplexType(t) else 'd->d'
 2033:         with errstate(all='ignore'):
 2034:             invx = _umath_linalg.inv(x, signature=signature)
 2035:             r = norm(x, p, axis=(-2, -1)) * norm(invx, p, axis=(-2, -1))
 2036:         r = r.astype(result_t, copy=False)
 2037: 
 2038:     # Convert nans to infs unless the original array had nan entries
 2039:     r = asarray(r)
 2040:     nan_mask = isnan(r)
 2041:     if nan_mask.any():
 2042:         nan_mask &= ~isnan(x).any(axis=(-2, -1))
 2043:         if r.ndim > 0:
 2044:             r[nan_mask] = inf
 2045:         elif nan_mask:
 2046:             r[()] = inf
 2047: 
 2048:     # Convention is to return scalars instead of 0d arrays
 2049:     if r.ndim == 0:
 2050:         r = r[()]
 2051: 
 2052:     return r
 2053: 
 2054: 
 2055: def _matrix_rank_dispatcher(A, tol=None, hermitian=None, *, rtol=None):
 2056:     return (A,)
 2057: 
 2058: 
 2059: @array_function_dispatch(_matrix_rank_dispatcher)
 2060: def matrix_rank(A, tol=None, hermitian=False, *, rtol=None):
 2061:     """
 2062:     Return matrix rank of array using SVD method
 2063: 
 2064:     Rank of the array is the number of singular values of the array that are
 2065:     greater than `tol`.
 2066: 
 2067:     Parameters
 2068:     ----------
 2069:     A : {(M,), (..., M, N)} array_like
 2070:         Input vector or stack of matrices.
 2071:     tol : (...) array_like, float, optional
 2072:         Threshold below which SVD values are considered zero. If `tol` is
 2073:         None, and ``S`` is an array with singular values for `M`, and
 2074:         ``eps`` is the epsilon value for datatype of ``S``, then `tol` is
 2075:         set to ``S.max() * max(M, N) * eps``.
 2076:     hermitian : bool, optional
 2077:         If True, `A` is assumed to be Hermitian (symmetric if real-valued),
 2078:         enabling a more efficient method for finding singular values.
 2079:         Defaults to False.
 2080:     rtol : (...) array_like, float, optional
 2081:         Parameter for the relative tolerance component. Only ``tol`` or
 2082:         ``rtol`` can be set at a time. Defaults to ``max(M, N) * eps``.
 2083: 
 2084:         .. versionadded:: 2.0.0
 2085: 
 2086:     Returns
 2087:     -------
 2088:     rank : (...) array_like
 2089:         Rank of A.
 2090: 
 2091:     Notes
 2092:     -----
 2093:     The default threshold to detect rank deficiency is a test on the magnitude
 2094:     of the singular values of `A`.  By default, we identify singular values
 2095:     less than ``S.max() * max(M, N) * eps`` as indicating rank deficiency
 2096:     (with the symbols defined above). This is the algorithm MATLAB uses [1].
 2097:     It also appears in *Numerical recipes* in the discussion of SVD solutions
 2098:     for linear least squares [2].
 2099: 
 2100:     This default threshold is designed to detect rank deficiency accounting
 2101:     for the numerical errors of the SVD computation. Imagine that there
 2102:     is a column in `A` that is an exact (in floating point) linear combination
 2103:     of other columns in `A`. Computing the SVD on `A` will not produce
 2104:     a singular value exactly equal to 0 in general: any difference of
 2105:     the smallest SVD value from 0 will be caused by numerical imprecision
 2106:     in the calculation of the SVD. Our threshold for small SVD values takes
 2107:     this numerical imprecision into account, and the default threshold will
 2108:     detect such numerical rank deficiency. The threshold may declare a matrix
 2109:     `A` rank deficient even if the linear combination of some columns of `A`
 2110:     is not exactly equal to another column of `A` but only numerically very
 2111:     close to another column of `A`.
 2112: 
 2113:     We chose our default threshold because it is in wide use. Other thresholds
 2114:     are possible.  For example, elsewhere in the 2007 edition of *Numerical
 2115:     recipes* there is an alternative threshold of ``S.max() *
 2116:     np.finfo(A.dtype).eps / 2. * np.sqrt(m + n + 1.)``. The authors describe
 2117:     this threshold as being based on "expected roundoff error" (p 71).
 2118: 
 2119:     The thresholds above deal with floating point roundoff error in the
 2120:     calculation of the SVD.  However, you may have more information about
 2121:     the sources of error in `A` that would make you consider other tolerance
 2122:     values to detect *effective* rank deficiency. The most useful measure
 2123:     of the tolerance depends on the operations you intend to use on your
 2124:     matrix. For example, if your data come from uncertain measurements with
 2125:     uncertainties greater than floating point epsilon, choosing a tolerance
 2126:     near that uncertainty may be preferable. The tolerance may be absolute
 2127:     if the uncertainties are absolute rather than relative.
 2128: 
 2129:     References
 2130:     ----------
 2131:     .. [1] MATLAB reference documentation, "Rank"
 2132:            https://www.mathworks.com/help/techdoc/ref/rank.html
 2133:     .. [2] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery,
 2134:            "Numerical Recipes (3rd edition)", Cambridge University Press, 2007,
 2135:            page 795.
 2136: 
 2137:     Examples
 2138:     --------
 2139:     >>> import numpy as np
 2140:     >>> from numpy.linalg import matrix_rank
 2141:     >>> matrix_rank(np.eye(4)) # Full rank matrix
 2142:     4
 2143:     >>> I=np.eye(4); I[-1,-1] = 0. # rank deficient matrix
 2144:     >>> matrix_rank(I)
 2145:     3
 2146:     >>> matrix_rank(np.ones((4,))) # 1 dimension - rank 1 unless all 0
 2147:     1
 2148:     >>> matrix_rank(np.zeros((4,)))
 2149:     0
 2150:     """
 2151:     if rtol is not None and tol is not None:
 2152:         raise ValueError("`tol` and `rtol` can't be both set.")
 2153: 
 2154:     A = asarray(A)
 2155:     if A.ndim < 2:
 2156:         return int(not all(A == 0))
 2157:     S = svd(A, compute_uv=False, hermitian=hermitian)
 2158: 
 2159:     if tol is None:
 2160:         if rtol is None:
 2161:             rtol = max(A.shape[-2:]) * finfo(S.dtype).eps
 2162:         else:
 2163:             rtol = asarray(rtol)[..., newaxis]
 2164:         tol = S.max(axis=-1, keepdims=True) * rtol
 2165:     else:
 2166:         tol = asarray(tol)[..., newaxis]
 2167: 
 2168:     return count_nonzero(S > tol, axis=-1)
 2169: 
 2170: 
 2171: # Generalized inverse
 2172: 
 2173: def _pinv_dispatcher(a, rcond=None, hermitian=None, *, rtol=None):
 2174:     return (a,)
 2175: 
 2176: 
 2177: @array_function_dispatch(_pinv_dispatcher)
 2178: def pinv(a, rcond=None, hermitian=False, *, rtol=_NoValue):
 2179:     """
 2180:     Compute the (Moore-Penrose) pseudo-inverse of a matrix.
 2181: 
 2182:     Calculate the generalized inverse of a matrix using its
 2183:     singular-value decomposition (SVD) and including all
 2184:     *large* singular values.
 2185: 
 2186:     Parameters
 2187:     ----------
 2188:     a : (..., M, N) array_like
 2189:         Matrix or stack of matrices to be pseudo-inverted.
 2190:     rcond : (...) array_like of float, optional
 2191:         Cutoff for small singular values.
 2192:         Singular values less than or equal to
 2193:         ``rcond * largest_singular_value`` are set to zero.
 2194:         Broadcasts against the stack of matrices. Default: ``1e-15``.
 2195:     hermitian : bool, optional
 2196:         If True, `a` is assumed to be Hermitian (symmetric if real-valued),
 2197:         enabling a more efficient method for finding singular values.
 2198:         Defaults to False.
 2199:     rtol : (...) array_like of float, optional
 2200:         Same as `rcond`, but it's an Array API compatible parameter name.
 2201:         Only `rcond` or `rtol` can be set at a time. If none of them are
 2202:         provided then NumPy's ``1e-15`` default is used. If ``rtol=None``
 2203:         is passed then the API standard default is used.
 2204: 
 2205:         .. versionadded:: 2.0.0
 2206: 
 2207:     Returns
 2208:     -------
 2209:     B : (..., N, M) ndarray
 2210:         The pseudo-inverse of `a`. If `a` is a `matrix` instance, then so
 2211:         is `B`.
 2212: 
 2213:     Raises
 2214:     ------
 2215:     LinAlgError
 2216:         If the SVD computation does not converge.
 2217: 
 2218:     See Also
 2219:     --------
 2220:     scipy.linalg.pinv : Similar function in SciPy.
 2221:     scipy.linalg.pinvh : Compute the (Moore-Penrose) pseudo-inverse of a
 2222:                          Hermitian matrix.
 2223: 
 2224:     Notes
 2225:     -----
 2226:     The pseudo-inverse of a matrix A, denoted :math:`A^+`, is
 2227:     defined as: "the matrix that 'solves' [the least-squares problem]
 2228:     :math:`Ax = b`," i.e., if :math:`\\bar{x}` is said solution, then
 2229:     :math:`A^+` is that matrix such that :math:`\\bar{x} = A^+b`.
 2230: 
 2231:     It can be shown that if :math:`Q_1 \\Sigma Q_2^T = A` is the singular
 2232:     value decomposition of A, then
 2233:     :math:`A^+ = Q_2 \\Sigma^+ Q_1^T`, where :math:`Q_{1,2}` are
 2234:     orthogonal matrices, :math:`\\Sigma` is a diagonal matrix consisting
 2235:     of A's so-called singular values, (followed, typically, by
 2236:     zeros), and then :math:`\\Sigma^+` is simply the diagonal matrix
 2237:     consisting of the reciprocals of A's singular values
 2238:     (again, followed by zeros). [1]_
 2239: 
 2240:     References
 2241:     ----------
 2242:     .. [1] G. Strang, *Linear Algebra and Its Applications*, 2nd Ed., Orlando,
 2243:            FL, Academic Press, Inc., 1980, pp. 139-142.
 2244: 
 2245:     Examples
 2246:     --------
 2247:     The following example checks that ``a * a+ * a == a`` and
 2248:     ``a+ * a * a+ == a+``:
 2249: 
 2250:     >>> import numpy as np
 2251:     >>> rng = np.random.default_rng()
 2252:     >>> a = rng.normal(size=(9, 6))
 2253:     >>> B = np.linalg.pinv(a)
 2254:     >>> np.allclose(a, np.dot(a, np.dot(B, a)))
 2255:     True
 2256:     >>> np.allclose(B, np.dot(B, np.dot(a, B)))
 2257:     True
 2258: 
 2259:     """
 2260:     a, wrap = _makearray(a)
 2261:     if rcond is None:
 2262:         if rtol is _NoValue:
 2263:             rcond = 1e-15
 2264:         elif rtol is None:
 2265:             rcond = max(a.shape[-2:]) * finfo(a.dtype).eps
 2266:         else:
 2267:             rcond = rtol
 2268:     elif rtol is not _NoValue:
 2269:         raise ValueError("`rtol` and `rcond` can't be both set.")
 2270:     else:
 2271:         # NOTE: Deprecate `rcond` in a few versions.
 2272:         pass
 2273: 
 2274:     rcond = asarray(rcond)
 2275:     if _is_empty_2d(a):
 2276:         m, n = a.shape[-2:]
 2277:         res = empty(a.shape[:-2] + (n, m), dtype=a.dtype)
 2278:         return wrap(res)
 2279:     a = a.conjugate()
 2280:     u, s, vt = svd(a, full_matrices=False, hermitian=hermitian)
 2281: 
 2282:     # discard small singular values
 2283:     cutoff = rcond[..., newaxis] * amax(s, axis=-1, keepdims=True)
 2284:     large = s > cutoff
 2285:     s = divide(1, s, where=large, out=s)
 2286:     s[~large] = 0
 2287: 
 2288:     res = matmul(transpose(vt), multiply(s[..., newaxis], transpose(u)))
 2289:     return wrap(res)
 2290: 
 2291: 
 2292: # Determinant
 2293: 
 2294: 
 2295: @array_function_dispatch(_unary_dispatcher)
 2296: def slogdet(a):
 2297:     """
 2298:     Compute the sign and (natural) logarithm of the determinant of an array.
 2299: 
 2300:     If an array has a very small or very large determinant, then a call to
 2301:     `det` may overflow or underflow. This routine is more robust against such
 2302:     issues, because it computes the logarithm of the determinant rather than
 2303:     the determinant itself.
 2304: 
 2305:     Parameters
 2306:     ----------
 2307:     a : (..., M, M) array_like
 2308:         Input array, has to be a square 2-D array.
 2309: 
 2310:     Returns
 2311:     -------
 2312:     A namedtuple with the following attributes:
 2313: 
 2314:     sign : (...) array_like
 2315:         A number representing the sign of the determinant. For a real matrix,
 2316:         this is 1, 0, or -1. For a complex matrix, this is a complex number
 2317:         with absolute value 1 (i.e., it is on the unit circle), or else 0.
 2318:     logabsdet : (...) array_like
 2319:         The natural log of the absolute value of the determinant.
 2320: 
 2321:     If the determinant is zero, then `sign` will be 0 and `logabsdet`
 2322:     will be -inf. In all cases, the determinant is equal to
 2323:     ``sign * np.exp(logabsdet)``.
 2324: 
 2325:     See Also
 2326:     --------
 2327:     det
 2328: 
 2329:     Notes
 2330:     -----
 2331:     Broadcasting rules apply, see the `numpy.linalg` documentation for
 2332:     details.
 2333: 
 2334:     The determinant is computed via LU factorization using the LAPACK
 2335:     routine ``z/dgetrf``.
 2336: 
 2337:     Examples
 2338:     --------
 2339:     The determinant of a 2-D array ``[[a, b], [c, d]]`` is ``ad - bc``:
 2340: 
 2341:     >>> import numpy as np
 2342:     >>> a = np.array([[1, 2], [3, 4]])
 2343:     >>> (sign, logabsdet) = np.linalg.slogdet(a)
 2344:     >>> (sign, logabsdet)
 2345:     (-1, 0.69314718055994529) # may vary
 2346:     >>> sign * np.exp(logabsdet)
 2347:     -2.0
 2348: 
 2349:     Computing log-determinants for a stack of matrices:
 2350: 
 2351:     >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
 2352:     >>> a.shape
 2353:     (3, 2, 2)
 2354:     >>> sign, logabsdet = np.linalg.slogdet(a)
 2355:     >>> (sign, logabsdet)
 2356:     (array([-1., -1., -1.]), array([ 0.69314718,  1.09861229,  2.07944154]))
 2357:     >>> sign * np.exp(logabsdet)
 2358:     array([-2., -3., -8.])
 2359: 
 2360:     This routine succeeds where ordinary `det` does not:
 2361: 
 2362:     >>> np.linalg.det(np.eye(500) * 0.1)
 2363:     0.0
 2364:     >>> np.linalg.slogdet(np.eye(500) * 0.1)
 2365:     (1, -1151.2925464970228)
 2366: 
 2367:     """
 2368:     a = asarray(a)
 2369:     _assert_stacked_square(a)
 2370:     t, result_t = _commonType(a)
 2371:     real_t = _realType(result_t)
 2372:     signature = 'D->Dd' if isComplexType(t) else 'd->dd'
 2373:     sign, logdet = _umath_linalg.slogdet(a, signature=signature)
 2374:     sign = sign.astype(result_t, copy=False)
 2375:     logdet = logdet.astype(real_t, copy=False)
 2376:     return SlogdetResult(sign, logdet)
 2377: 
 2378: 
 2379: @array_function_dispatch(_unary_dispatcher)
 2380: def det(a):
 2381:     """
 2382:     Compute the determinant of an array.
 2383: 
 2384:     Parameters
 2385:     ----------
 2386:     a : (..., M, M) array_like
 2387:         Input array to compute determinants for.
 2388: 
 2389:     Returns
 2390:     -------
 2391:     det : (...) array_like
 2392:         Determinant of `a`.
 2393: 
 2394:     See Also
 2395:     --------
 2396:     slogdet : Another way to represent the determinant, more suitable
 2397:       for large matrices where underflow/overflow may occur.
 2398:     scipy.linalg.det : Similar function in SciPy.
 2399: 
 2400:     Notes
 2401:     -----
 2402:     Broadcasting rules apply, see the `numpy.linalg` documentation for
 2403:     details.
 2404: 
 2405:     The determinant is computed via LU factorization using the LAPACK
 2406:     routine ``z/dgetrf``.
 2407: 
 2408:     Examples
 2409:     --------
 2410:     The determinant of a 2-D array [[a, b], [c, d]] is ad - bc:
 2411: 
 2412:     >>> import numpy as np
 2413:     >>> a = np.array([[1, 2], [3, 4]])
 2414:     >>> np.linalg.det(a)
 2415:     -2.0 # may vary
 2416: 
 2417:     Computing determinants for a stack of matrices:
 2418: 
 2419:     >>> a = np.array([ [[1, 2], [3, 4]], [[1, 2], [2, 1]], [[1, 3], [3, 1]] ])
 2420:     >>> a.shape
 2421:     (3, 2, 2)
 2422:     >>> np.linalg.det(a)
 2423:     array([-2., -3., -8.])
 2424: 
 2425:     """
 2426:     a = asarray(a)
 2427:     _assert_stacked_square(a)
 2428:     t, result_t = _commonType(a)
 2429:     signature = 'D->D' if isComplexType(t) else 'd->d'
 2430:     r = _umath_linalg.det(a, signature=signature)
 2431:     r = r.astype(result_t, copy=False)
 2432:     return r
 2433: 
 2434: 
 2435: # Linear Least Squares
 2436: 
 2437: def _lstsq_dispatcher(a, b, rcond=None):
 2438:     return (a, b)
 2439: 
 2440: 
 2441: @array_function_dispatch(_lstsq_dispatcher)
 2442: def lstsq(a, b, rcond=None):
 2443:     r"""
 2444:     Return the least-squares solution to a linear matrix equation.
 2445: 
 2446:     Computes the vector `x` that approximately solves the equation
 2447:     ``a @ x = b``. The equation may be under-, well-, or over-determined
 2448:     (i.e., the number of linearly independent rows of `a` can be less than,
 2449:     equal to, or greater than its number of linearly independent columns).
 2450:     If `a` is square and of full rank, then `x` (but for round-off error)
 2451:     is the "exact" solution of the equation. Else, `x` minimizes the
 2452:     Euclidean 2-norm :math:`||b - ax||`. If there are multiple minimizing
 2453:     solutions, the one with the smallest 2-norm :math:`||x||` is returned.
 2454: 
 2455:     Parameters
 2456:     ----------
 2457:     a : (M, N) array_like
 2458:         "Coefficient" matrix.
 2459:     b : {(M,), (M, K)} array_like
 2460:         Ordinate or "dependent variable" values. If `b` is two-dimensional,
 2461:         the least-squares solution is calculated for each of the `K` columns
 2462:         of `b`.
 2463:     rcond : float, optional
 2464:         Cut-off ratio for small singular values of `a`.
 2465:         For the purposes of rank determination, singular values are treated
 2466:         as zero if they are smaller than `rcond` times the largest singular
 2467:         value of `a`.
 2468:         The default uses the machine precision times ``max(M, N)``.  Passing
 2469:         ``-1`` will use machine precision.
 2470: 
 2471:         .. versionchanged:: 2.0
 2472:             Previously, the default was ``-1``, but a warning was given that
 2473:             this would change.
 2474: 
 2475:     Returns
 2476:     -------
 2477:     x : {(N,), (N, K)} ndarray
 2478:         Least-squares solution. If `b` is two-dimensional,
 2479:         the solutions are in the `K` columns of `x`.
 2480:     residuals : {(1,), (K,), (0,)} ndarray
 2481:         Sums of squared residuals: Squared Euclidean 2-norm for each column in
 2482:         ``b - a @ x``.
 2483:         If the rank of `a` is < N or M <= N, this is an empty array.
 2484:         If `b` is 1-dimensional, this is a (1,) shape array.
 2485:         Otherwise the shape is (K,).
 2486:     rank : int
 2487:         Rank of matrix `a`.
 2488:     s : (min(M, N),) ndarray
 2489:         Singular values of `a`.
 2490: 
 2491:     Raises
 2492:     ------
 2493:     LinAlgError
 2494:         If computation does not converge.
 2495: 
 2496:     See Also
 2497:     --------
 2498:     scipy.linalg.lstsq : Similar function in SciPy.
 2499: 
 2500:     Notes
 2501:     -----
 2502:     If `b` is a matrix, then all array results are returned as matrices.
 2503: 
 2504:     Examples
 2505:     --------
 2506:     Fit a line, ``y = mx + c``, through some noisy data-points:
 2507: 
 2508:     >>> import numpy as np
 2509:     >>> x = np.array([0, 1, 2, 3])
 2510:     >>> y = np.array([-1, 0.2, 0.9, 2.1])
 2511: 
 2512:     By examining the coefficients, we see that the line should have a
 2513:     gradient of roughly 1 and cut the y-axis at, more or less, -1.
 2514: 
 2515:     We can rewrite the line equation as ``y = Ap``, where ``A = [[x 1]]``
 2516:     and ``p = [[m], [c]]``.  Now use `lstsq` to solve for `p`:
 2517: 
 2518:     >>> A = np.vstack([x, np.ones(len(x))]).T
 2519:     >>> A
 2520:     array([[ 0.,  1.],
 2521:            [ 1.,  1.],
 2522:            [ 2.,  1.],
 2523:            [ 3.,  1.]])
 2524: 
 2525:     >>> m, c = np.linalg.lstsq(A, y)[0]
 2526:     >>> m, c
 2527:     (1.0 -0.95) # may vary
 2528: 
 2529:     Plot the data along with the fitted line:
 2530: 
 2531:     >>> import matplotlib.pyplot as plt
 2532:     >>> _ = plt.plot(x, y, 'o', label='Original data', markersize=10)
 2533:     >>> _ = plt.plot(x, m*x + c, 'r', label='Fitted line')
 2534:     >>> _ = plt.legend()
 2535:     >>> plt.show()
 2536: 
 2537:     """
 2538:     a, _ = _makearray(a)
 2539:     b, wrap = _makearray(b)
 2540:     is_1d = b.ndim == 1
 2541:     if is_1d:
 2542:         b = b[:, newaxis]
 2543:     _assert_2d(a, b)
 2544:     m, n = a.shape[-2:]
 2545:     m2, n_rhs = b.shape[-2:]
 2546:     if m != m2:
 2547:         raise LinAlgError('Incompatible dimensions')
 2548: 
 2549:     t, result_t = _commonType(a, b)
 2550:     result_real_t = _realType(result_t)
 2551: 
 2552:     if rcond is None:
 2553:         rcond = finfo(t).eps * max(n, m)
 2554: 
 2555:     signature = 'DDd->Ddid' if isComplexType(t) else 'ddd->ddid'
 2556:     if n_rhs == 0:
 2557:         # lapack can't handle n_rhs = 0 - so allocate
 2558:         # the array one larger in that axis
 2559:         b = zeros(b.shape[:-2] + (m, n_rhs + 1), dtype=b.dtype)
 2560: 
 2561:     with errstate(call=_raise_linalgerror_lstsq, invalid='call',
 2562:                   over='ignore', divide='ignore', under='ignore'):
 2563:         x, resids, rank, s = _umath_linalg.lstsq(a, b, rcond,
 2564:                                                  signature=signature)
 2565:     if m == 0:
 2566:         x[...] = 0
 2567:     if n_rhs == 0:
 2568:         # remove the item we added
 2569:         x = x[..., :n_rhs]
 2570:         resids = resids[..., :n_rhs]
 2571: 
 2572:     # remove the axis we added
 2573:     if is_1d:
 2574:         x = x.squeeze(axis=-1)
 2575:         # we probably should squeeze resids too, but we can't
 2576:         # without breaking compatibility.
 2577: 
 2578:     # as documented
 2579:     if rank != n or m <= n:
 2580:         resids = array([], result_real_t)
 2581: 
 2582:     # coerce output arrays
 2583:     s = s.astype(result_real_t, copy=False)
 2584:     resids = resids.astype(result_real_t, copy=False)
 2585:     # Copying lets the memory in r_parts be freed
 2586:     x = x.astype(result_t, copy=True)
 2587:     return wrap(x), wrap(resids), rank, s
 2588: 
 2589: 
 2590: def _multi_svd_norm(x, row_axis, col_axis, op, initial=None):
 2591:     """Compute a function of the singular values of the 2-D matrices in `x`.
 2592: 
 2593:     This is a private utility function used by `numpy.linalg.norm()`.
 2594: 
 2595:     Parameters
 2596:     ----------
 2597:     x : ndarray
 2598:     row_axis, col_axis : int
 2599:         The axes of `x` that hold the 2-D matrices.
 2600:     op : callable
 2601:         This should be either numpy.amin or `numpy.amax` or `numpy.sum`.
 2602: 
 2603:     Returns
 2604:     -------
 2605:     result : float or ndarray
 2606:         If `x` is 2-D, the return values is a float.
 2607:         Otherwise, it is an array with ``x.ndim - 2`` dimensions.
 2608:         The return values are either the minimum or maximum or sum of the
 2609:         singular values of the matrices, depending on whether `op`
 2610:         is `numpy.amin` or `numpy.amax` or `numpy.sum`.
 2611: 
 2612:     """
 2613:     y = moveaxis(x, (row_axis, col_axis), (-2, -1))
 2614:     result = op(svd(y, compute_uv=False), axis=-1, initial=initial)
 2615:     return result
 2616: 
 2617: 
 2618: def _norm_dispatcher(x, ord=None, axis=None, keepdims=None):
 2619:     return (x,)
 2620: 
 2621: 
 2622: @array_function_dispatch(_norm_dispatcher)
 2623: def norm(x, ord=None, axis=None, keepdims=False):
 2624:     """
 2625:     Matrix or vector norm.
 2626: 
 2627:     This function is able to return one of eight different matrix norms,
 2628:     or one of an infinite number of vector norms (described below), depending
 2629:     on the value of the ``ord`` parameter.
 2630: 
 2631:     Parameters
 2632:     ----------
 2633:     x : array_like
 2634:         Input array.  If `axis` is None, `x` must be 1-D or 2-D, unless `ord`
 2635:         is None. If both `axis` and `ord` are None, the 2-norm of
 2636:         ``x.ravel`` will be returned.
 2637:     ord : {int, float, inf, -inf, 'fro', 'nuc'}, optional
 2638:         Order of the norm (see table under ``Notes`` for what values are
 2639:         supported for matrices and vectors respectively). inf means numpy's
 2640:         `inf` object. The default is None.
 2641:     axis : {None, int, 2-tuple of ints}, optional.
 2642:         If `axis` is an integer, it specifies the axis of `x` along which to
 2643:         compute the vector norms.  If `axis` is a 2-tuple, it specifies the
 2644:         axes that hold 2-D matrices, and the matrix norms of these matrices
 2645:         are computed.  If `axis` is None then either a vector norm (when `x`
 2646:         is 1-D) or a matrix norm (when `x` is 2-D) is returned. The default
 2647:         is None.
 2648: 
 2649:     keepdims : bool, optional
 2650:         If this is set to True, the axes which are normed over are left in the
 2651:         result as dimensions with size one.  With this option the result will
 2652:         broadcast correctly against the original `x`.
 2653: 
 2654:     Returns
 2655:     -------
 2656:     n : float or ndarray
 2657:         Norm of the matrix or vector(s).
 2658: 
 2659:     See Also
 2660:     --------
 2661:     scipy.linalg.norm : Similar function in SciPy.
 2662: 
 2663:     Notes
 2664:     -----
 2665:     For values of ``ord < 1``, the result is, strictly speaking, not a
 2666:     mathematical 'norm', but it may still be useful for various numerical
 2667:     purposes.
 2668: 
 2669:     The following norms can be calculated:
 2670: 
 2671:     =====  ============================  ==========================
 2672:     ord    norm for matrices             norm for vectors
 2673:     =====  ============================  ==========================
 2674:     None   Frobenius norm                2-norm
 2675:     'fro'  Frobenius norm                --
 2676:     'nuc'  nuclear norm                  --
 2677:     inf    max(sum(abs(x), axis=1))      max(abs(x))
 2678:     -inf   min(sum(abs(x), axis=1))      min(abs(x))
 2679:     0      --                            sum(x != 0)
 2680:     1      max(sum(abs(x), axis=0))      as below
 2681:     -1     min(sum(abs(x), axis=0))      as below
 2682:     2      2-norm (largest sing. value)  as below
 2683:     -2     smallest singular value       as below
 2684:     other  --                            sum(abs(x)**ord)**(1./ord)
 2685:     =====  ============================  ==========================
 2686: 
 2687:     The Frobenius norm is given by [1]_:
 2688: 
 2689:     :math:`||A||_F = [\\sum_{i,j} abs(a_{i,j})^2]^{1/2}`
 2690: 
 2691:     The nuclear norm is the sum of the singular values.
 2692: 
 2693:     Both the Frobenius and nuclear norm orders are only defined for
 2694:     matrices and raise a ValueError when ``x.ndim != 2``.
 2695: 
 2696:     References
 2697:     ----------
 2698:     .. [1] G. H. Golub and C. F. Van Loan, *Matrix Computations*,
 2699:            Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15
 2700: 
 2701:     Examples
 2702:     --------
 2703: 
 2704:     >>> import numpy as np
 2705:     >>> from numpy import linalg as LA
 2706:     >>> a = np.arange(9) - 4
 2707:     >>> a
 2708:     array([-4, -3, -2, ...,  2,  3,  4])
 2709:     >>> b = a.reshape((3, 3))
 2710:     >>> b
 2711:     array([[-4, -3, -2],
 2712:            [-1,  0,  1],
 2713:            [ 2,  3,  4]])
 2714: 
 2715:     >>> LA.norm(a)
 2716:     7.745966692414834
 2717:     >>> LA.norm(b)
 2718:     7.745966692414834
 2719:     >>> LA.norm(b, 'fro')
 2720:     7.745966692414834
 2721:     >>> LA.norm(a, np.inf)
 2722:     4.0
 2723:     >>> LA.norm(b, np.inf)
 2724:     9.0
 2725:     >>> LA.norm(a, -np.inf)
 2726:     0.0
 2727:     >>> LA.norm(b, -np.inf)
 2728:     2.0
 2729: 
 2730:     >>> LA.norm(a, 1)
 2731:     20.0
 2732:     >>> LA.norm(b, 1)
 2733:     7.0
 2734:     >>> LA.norm(a, -1)
 2735:     -4.6566128774142013e-010
 2736:     >>> LA.norm(b, -1)
 2737:     6.0
 2738:     >>> LA.norm(a, 2)
 2739:     7.745966692414834
 2740:     >>> LA.norm(b, 2)
 2741:     7.3484692283495345
 2742: 
 2743:     >>> LA.norm(a, -2)
 2744:     0.0
 2745:     >>> LA.norm(b, -2)
 2746:     1.8570331885190563e-016 # may vary
 2747:     >>> LA.norm(a, 3)
 2748:     5.8480354764257312 # may vary
 2749:     >>> LA.norm(a, -3)
 2750:     0.0
 2751: 
 2752:     Using the `axis` argument to compute vector norms:
 2753: 
 2754:     >>> c = np.array([[ 1, 2, 3],
 2755:     ...               [-1, 1, 4]])
 2756:     >>> LA.norm(c, axis=0)
 2757:     array([ 1.41421356,  2.23606798,  5.        ])
 2758:     >>> LA.norm(c, axis=1)
 2759:     array([ 3.74165739,  4.24264069])
 2760:     >>> LA.norm(c, ord=1, axis=1)
 2761:     array([ 6.,  6.])
 2762: 
 2763:     Using the `axis` argument to compute matrix norms:
 2764: 
 2765:     >>> m = np.arange(8).reshape(2,2,2)
 2766:     >>> LA.norm(m, axis=(1,2))
 2767:     array([  3.74165739,  11.22497216])
 2768:     >>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])
 2769:     (3.7416573867739413, 11.224972160321824)
 2770: 
 2771:     """
 2772:     x = asarray(x)
 2773: 
 2774:     if not issubclass(x.dtype.type, (inexact, object_)):
 2775:         x = x.astype(float)
 2776: 
 2777:     # Immediately handle some default, simple, fast, and common cases.
 2778:     if axis is None:
 2779:         ndim = x.ndim
 2780:         if (
 2781:             (ord is None) or
 2782:             (ord in ('f', 'fro') and ndim == 2) or
 2783:             (ord == 2 and ndim == 1)
 2784:         ):
 2785:             x = x.ravel(order='K')
 2786:             if isComplexType(x.dtype.type):
 2787:                 x_real = x.real
 2788:                 x_imag = x.imag
 2789:                 sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)
 2790:             else:
 2791:                 sqnorm = x.dot(x)
 2792:             ret = sqrt(sqnorm)
 2793:             if keepdims:
 2794:                 ret = ret.reshape(ndim * [1])
 2795:             return ret
 2796: 
 2797:     # Normalize the `axis` argument to a tuple.
 2798:     nd = x.ndim
 2799:     if axis is None:
 2800:         axis = tuple(range(nd))
 2801:     elif not isinstance(axis, tuple):
 2802:         try:
 2803:             axis = int(axis)
 2804:         except Exception as e:
 2805:             raise TypeError(
 2806:                 "'axis' must be None, an integer or a tuple of integers"
 2807:             ) from e
 2808:         axis = (axis,)
 2809: 
 2810:     if len(axis) == 1:
 2811:         if ord == inf:
 2812:             return abs(x).max(axis=axis, keepdims=keepdims, initial=0)
 2813:         elif ord == -inf:
 2814:             return abs(x).min(axis=axis, keepdims=keepdims)
 2815:         elif ord == 0:
 2816:             # Zero norm
 2817:             return (
 2818:                 (x != 0)
 2819:                 .astype(x.real.dtype)
 2820:                 .sum(axis=axis, keepdims=keepdims)
 2821:             )
 2822:         elif ord == 1:
 2823:             # special case for speedup
 2824:             return add.reduce(abs(x), axis=axis, keepdims=keepdims)
 2825:         elif ord is None or ord == 2:
 2826:             # special case for speedup
 2827:             s = (x.conj() * x).real
 2828:             return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))
 2829:         # None of the str-type keywords for ord ('fro', 'nuc')
 2830:         # are valid for vectors
 2831:         elif isinstance(ord, str):
 2832:             raise ValueError(f"Invalid norm order '{ord}' for vectors")
 2833:         else:
 2834:             absx = abs(x)
 2835:             absx **= ord
 2836:             ret = add.reduce(absx, axis=axis, keepdims=keepdims)
 2837:             ret **= reciprocal(ord, dtype=ret.dtype)
 2838:             return ret
 2839:     elif len(axis) == 2:
 2840:         row_axis, col_axis = axis
 2841:         row_axis = normalize_axis_index(row_axis, nd)
 2842:         col_axis = normalize_axis_index(col_axis, nd)
 2843:         if row_axis == col_axis:
 2844:             raise ValueError('Duplicate axes given.')
 2845:         if ord == 2:
 2846:             ret = _multi_svd_norm(x, row_axis, col_axis, amax, 0)
 2847:         elif ord == -2:
 2848:             ret = _multi_svd_norm(x, row_axis, col_axis, amin)
 2849:         elif ord == 1:
 2850:             if col_axis > row_axis:
 2851:                 col_axis -= 1
 2852:             ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis, initial=0)
 2853:         elif ord == inf:
 2854:             if row_axis > col_axis:
 2855:                 row_axis -= 1
 2856:             ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis, initial=0)
 2857:         elif ord == -1:
 2858:             if col_axis > row_axis:
 2859:                 col_axis -= 1
 2860:             ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)
 2861:         elif ord == -inf:
 2862:             if row_axis > col_axis:
 2863:                 row_axis -= 1
 2864:             ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)
 2865:         elif ord in [None, 'fro', 'f']:
 2866:             ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))
 2867:         elif ord == 'nuc':
 2868:             ret = _multi_svd_norm(x, row_axis, col_axis, sum, 0)
 2869:         else:
 2870:             raise ValueError("Invalid norm order for matrices.")
 2871:         if keepdims:
 2872:             ret_shape = list(x.shape)
 2873:             ret_shape[axis[0]] = 1
 2874:             ret_shape[axis[1]] = 1
 2875:             ret = ret.reshape(ret_shape)
 2876:         return ret
 2877:     else:
 2878:         raise ValueError("Improper number of dimensions to norm.")
 2879: 
 2880: 
 2881: # multi_dot
 2882: 
 2883: def _multidot_dispatcher(arrays, *, out=None):
 2884:     yield from arrays
 2885:     yield out
 2886: 
 2887: 
 2888: @array_function_dispatch(_multidot_dispatcher)
 2889: def multi_dot(arrays, *, out=None):
 2890:     """
 2891:     Compute the dot product of two or more arrays in a single function call,
 2892:     while automatically selecting the fastest evaluation order.
 2893: 
 2894:     `multi_dot` chains `numpy.dot` and uses optimal parenthesization
 2895:     of the matrices [1]_ [2]_. Depending on the shapes of the matrices,
 2896:     this can speed up the multiplication a lot.
 2897: 
 2898:     If the first argument is 1-D it is treated as a row vector.
 2899:     If the last argument is 1-D it is treated as a column vector.
 2900:     The other arguments must be 2-D.
 2901: 
 2902:     Think of `multi_dot` as::
 2903: 
 2904:         def multi_dot(arrays): return functools.reduce(np.dot, arrays)
 2905: 
 2906: 
 2907:     Parameters
 2908:     ----------
 2909:     arrays : sequence of array_like
 2910:         If the first argument is 1-D it is treated as row vector.
 2911:         If the last argument is 1-D it is treated as column vector.
 2912:         The other arguments must be 2-D.
 2913:     out : ndarray, optional
 2914:         Output argument. This must have the exact kind that would be returned
 2915:         if it was not used. In particular, it must have the right type, must be
 2916:         C-contiguous, and its dtype must be the dtype that would be returned
 2917:         for `dot(a, b)`. This is a performance feature. Therefore, if these
 2918:         conditions are not met, an exception is raised, instead of attempting
 2919:         to be flexible.
 2920: 
 2921:     Returns
 2922:     -------
 2923:     output : ndarray
 2924:         Returns the dot product of the supplied arrays.
 2925: 
 2926:     See Also
 2927:     --------
 2928:     numpy.dot : dot multiplication with two arguments.
 2929: 
 2930:     References
 2931:     ----------
 2932: 
 2933:     .. [1] Cormen, "Introduction to Algorithms", Chapter 15.2, p. 370-378
 2934:     .. [2] https://en.wikipedia.org/wiki/Matrix_chain_multiplication
 2935: 
 2936:     Examples
 2937:     --------
 2938:     `multi_dot` allows you to write::
 2939: 
 2940:     >>> import numpy as np
 2941:     >>> from numpy.linalg import multi_dot
 2942:     >>> # Prepare some data
 2943:     >>> A = np.random.random((10000, 100))
 2944:     >>> B = np.random.random((100, 1000))
 2945:     >>> C = np.random.random((1000, 5))
 2946:     >>> D = np.random.random((5, 333))
 2947:     >>> # the actual dot multiplication
 2948:     >>> _ = multi_dot([A, B, C, D])
 2949: 
 2950:     instead of::
 2951: 
 2952:     >>> _ = np.dot(np.dot(np.dot(A, B), C), D)
 2953:     >>> # or
 2954:     >>> _ = A.dot(B).dot(C).dot(D)
 2955: 
 2956:     Notes
 2957:     -----
 2958:     The cost for a matrix multiplication can be calculated with the
 2959:     following function::
 2960: 
 2961:         def cost(A, B):
 2962:             return A.shape[0] * A.shape[1] * B.shape[1]
 2963: 
 2964:     Assume we have three matrices
 2965:     :math:`A_{10 \times 100}, B_{100 \times 5}, C_{5 \times 50}`.
 2966: 
 2967:     The costs for the two different parenthesizations are as follows::
 2968: 
 2969:         cost((AB)C) = 10*100*5 + 10*5*50   = 5000 + 2500   = 7500
 2970:         cost(A(BC)) = 10*100*50 + 100*5*50 = 50000 + 25000 = 75000
 2971: 
 2972:     """
 2973:     n = len(arrays)
 2974:     # optimization only makes sense for len(arrays) > 2
 2975:     if n < 2:
 2976:         raise ValueError("Expecting at least two arrays.")
 2977:     elif n == 2:
 2978:         return dot(arrays[0], arrays[1], out=out)
 2979: 
 2980:     arrays = [asanyarray(a) for a in arrays]
 2981: 
 2982:     # save original ndim to reshape the result array into the proper form later
 2983:     ndim_first, ndim_last = arrays[0].ndim, arrays[-1].ndim
 2984:     # Explicitly convert vectors to 2D arrays to keep the logic of the internal
 2985:     # _multi_dot_* functions as simple as possible.
 2986:     if arrays[0].ndim == 1:
 2987:         arrays[0] = atleast_2d(arrays[0])
 2988:     if arrays[-1].ndim == 1:
 2989:         arrays[-1] = atleast_2d(arrays[-1]).T
 2990:     _assert_2d(*arrays)
 2991: 
 2992:     # _multi_dot_three is much faster than _multi_dot_matrix_chain_order
 2993:     if n == 3:
 2994:         result = _multi_dot_three(arrays[0], arrays[1], arrays[2], out=out)
 2995:     else:
 2996:         order = _multi_dot_matrix_chain_order(arrays)
 2997:         result = _multi_dot(arrays, order, 0, n - 1, out=out)
 2998: 
 2999:     # return proper shape
 3000:     if ndim_first == 1 and ndim_last == 1:
 3001:         return result[0, 0]  # scalar
 3002:     elif ndim_first == 1 or ndim_last == 1:
 3003:         return result.ravel()  # 1-D
 3004:     else:
 3005:         return result
 3006: 
 3007: 
 3008: def _multi_dot_three(A, B, C, out=None):
 3009:     """
 3010:     Find the best order for three arrays and do the multiplication.
 3011: 
 3012:     For three arguments `_multi_dot_three` is approximately 15 times faster
 3013:     than `_multi_dot_matrix_chain_order`
 3014: 
 3015:     """
 3016:     a0, a1b0 = A.shape
 3017:     b1c0, c1 = C.shape
 3018:     # cost1 = cost((AB)C) = a0*a1b0*b1c0 + a0*b1c0*c1
 3019:     cost1 = a0 * b1c0 * (a1b0 + c1)
 3020:     # cost2 = cost(A(BC)) = a1b0*b1c0*c1 + a0*a1b0*c1
 3021:     cost2 = a1b0 * c1 * (a0 + b1c0)
 3022: 
 3023:     if cost1 < cost2:
 3024:         return dot(dot(A, B), C, out=out)
 3025:     else:
 3026:         return dot(A, dot(B, C), out=out)
 3027: 
 3028: 
 3029: def _multi_dot_matrix_chain_order(arrays, return_costs=False):
 3030:     """
 3031:     Return a np.array that encodes the optimal order of multiplications.
 3032: 
 3033:     The optimal order array is then used by `_multi_dot()` to do the
 3034:     multiplication.
 3035: 
 3036:     Also return the cost matrix if `return_costs` is `True`
 3037: 
 3038:     The implementation CLOSELY follows Cormen, "Introduction to Algorithms",
 3039:     Chapter 15.2, p. 370-378.  Note that Cormen uses 1-based indices.
 3040: 
 3041:         cost[i, j] = min([
 3042:             cost[prefix] + cost[suffix] + cost_mult(prefix, suffix)
 3043:             for k in range(i, j)])
 3044: 
 3045:     """
 3046:     n = len(arrays)
 3047:     # p stores the dimensions of the matrices
 3048:     # Example for p: A_{10x100}, B_{100x5}, C_{5x50} --> p = [10, 100, 5, 50]
 3049:     p = [a.shape[0] for a in arrays] + [arrays[-1].shape[1]]
 3050:     # m is a matrix of costs of the subproblems
 3051:     # m[i,j]: min number of scalar multiplications needed to compute A_{i..j}
 3052:     m = zeros((n, n), dtype=double)
 3053:     # s is the actual ordering
 3054:     # s[i, j] is the value of k at which we split the product A_i..A_j
 3055:     s = empty((n, n), dtype=intp)
 3056: 
 3057:     for l in range(1, n):
 3058:         for i in range(n - l):
 3059:             j = i + l
 3060:             m[i, j] = inf
 3061:             for k in range(i, j):
 3062:                 q = m[i, k] + m[k + 1, j] + p[i] * p[k + 1] * p[j + 1]
 3063:                 if q < m[i, j]:
 3064:                     m[i, j] = q
 3065:                     s[i, j] = k  # Note that Cormen uses 1-based index
 3066: 
 3067:     return (s, m) if return_costs else s
 3068: 
 3069: 
 3070: def _multi_dot(arrays, order, i, j, out=None):
 3071:     """Actually do the multiplication with the given order."""
 3072:     if i == j:
 3073:         # the initial call with non-None out should never get here
 3074:         assert out is None
 3075: 
 3076:         return arrays[i]
 3077:     else:
 3078:         return dot(_multi_dot(arrays, order, i, order[i, j]),
 3079:                    _multi_dot(arrays, order, order[i, j] + 1, j),
 3080:                    out=out)
 3081: 
 3082: 
 3083: # diagonal
 3084: 
 3085: def _diagonal_dispatcher(x, /, *, offset=None):
 3086:     return (x,)
 3087: 
 3088: 
 3089: @array_function_dispatch(_diagonal_dispatcher)
 3090: def diagonal(x, /, *, offset=0):
 3091:     """
 3092:     Returns specified diagonals of a matrix (or a stack of matrices) ``x``.
 3093: 
 3094:     This function is Array API compatible, contrary to
 3095:     :py:func:`numpy.diagonal`, the matrix is assumed
 3096:     to be defined by the last two dimensions.
 3097: 
 3098:     Parameters
 3099:     ----------
 3100:     x : (...,M,N) array_like
 3101:         Input array having shape (..., M, N) and whose innermost two
 3102:         dimensions form MxN matrices.
 3103:     offset : int, optional
 3104:         Offset specifying the off-diagonal relative to the main diagonal,
 3105:         where::
 3106: 
 3107:             * offset = 0: the main diagonal.
 3108:             * offset > 0: off-diagonal above the main diagonal.
 3109:             * offset < 0: off-diagonal below the main diagonal.
 3110: 
 3111:     Returns
 3112:     -------
 3113:     out : (...,min(N,M)) ndarray
 3114:         An array containing the diagonals and whose shape is determined by
 3115:         removing the last two dimensions and appending a dimension equal to
 3116:         the size of the resulting diagonals. The returned array must have
 3117:         the same data type as ``x``.
 3118: 
 3119:     See Also
 3120:     --------
 3121:     numpy.diagonal
 3122: 
 3123:     Examples
 3124:     --------
 3125:     >>> a = np.arange(4).reshape(2, 2); a
 3126:     array([[0, 1],
 3127:            [2, 3]])
 3128:     >>> np.linalg.diagonal(a)
 3129:     array([0, 3])
 3130: 
 3131:     A 3-D example:
 3132: 
 3133:     >>> a = np.arange(8).reshape(2, 2, 2); a
 3134:     array([[[0, 1],
 3135:             [2, 3]],
 3136:            [[4, 5],
 3137:             [6, 7]]])
 3138:     >>> np.linalg.diagonal(a)
 3139:     array([[0, 3],
 3140:            [4, 7]])
 3141: 
 3142:     Diagonals adjacent to the main diagonal can be obtained by using the
 3143:     `offset` argument:
 3144: 
 3145:     >>> a = np.arange(9).reshape(3, 3)
 3146:     >>> a
 3147:     array([[0, 1, 2],
 3148:            [3, 4, 5],
 3149:            [6, 7, 8]])
 3150:     >>> np.linalg.diagonal(a, offset=1)  # First superdiagonal
 3151:     array([1, 5])
 3152:     >>> np.linalg.diagonal(a, offset=2)  # Second superdiagonal
 3153:     array([2])
 3154:     >>> np.linalg.diagonal(a, offset=-1)  # First subdiagonal
 3155:     array([3, 7])
 3156:     >>> np.linalg.diagonal(a, offset=-2)  # Second subdiagonal
 3157:     array([6])
 3158: 
 3159:     The anti-diagonal can be obtained by reversing the order of elements
 3160:     using either `numpy.flipud` or `numpy.fliplr`.
 3161: 
 3162:     >>> a = np.arange(9).reshape(3, 3)
 3163:     >>> a
 3164:     array([[0, 1, 2],
 3165:            [3, 4, 5],
 3166:            [6, 7, 8]])
 3167:     >>> np.linalg.diagonal(np.fliplr(a))  # Horizontal flip
 3168:     array([2, 4, 6])
 3169:     >>> np.linalg.diagonal(np.flipud(a))  # Vertical flip
 3170:     array([6, 4, 2])
 3171: 
 3172:     Note that the order in which the diagonal is retrieved varies depending
 3173:     on the flip function.
 3174: 
 3175:     """
 3176:     return _core_diagonal(x, offset, axis1=-2, axis2=-1)
 3177: 
 3178: 
 3179: # trace
 3180: 
 3181: def _trace_dispatcher(x, /, *, offset=None, dtype=None):
 3182:     return (x,)
 3183: 
 3184: 
 3185: @array_function_dispatch(_trace_dispatcher)
 3186: def trace(x, /, *, offset=0, dtype=None):
 3187:     """
 3188:     Returns the sum along the specified diagonals of a matrix
 3189:     (or a stack of matrices) ``x``.
 3190: 
 3191:     This function is Array API compatible, contrary to
 3192:     :py:func:`numpy.trace`.
 3193: 
 3194:     Parameters
 3195:     ----------
 3196:     x : (...,M,N) array_like
 3197:         Input array having shape (..., M, N) and whose innermost two
 3198:         dimensions form MxN matrices.
 3199:     offset : int, optional
 3200:         Offset specifying the off-diagonal relative to the main diagonal,
 3201:         where::
 3202: 
 3203:             * offset = 0: the main diagonal.
 3204:             * offset > 0: off-diagonal above the main diagonal.
 3205:             * offset < 0: off-diagonal below the main diagonal.
 3206: 
 3207:     dtype : dtype, optional
 3208:         Data type of the returned array.
 3209: 
 3210:     Returns
 3211:     -------
 3212:     out : ndarray
 3213:         An array containing the traces and whose shape is determined by
 3214:         removing the last two dimensions and storing the traces in the last
 3215:         array dimension. For example, if x has rank k and shape:
 3216:         (I, J, K, ..., L, M, N), then an output array has rank k-2 and shape:
 3217:         (I, J, K, ..., L) where::
 3218: 
 3219:             out[i, j, k, ..., l] = trace(a[i, j, k, ..., l, :, :])
 3220: 
 3221:         The returned array must have a data type as described by the dtype
 3222:         parameter above.
 3223: 
 3224:     See Also
 3225:     --------
 3226:     numpy.trace
 3227: 
 3228:     Examples
 3229:     --------
 3230:     >>> np.linalg.trace(np.eye(3))
 3231:     3.0
 3232:     >>> a = np.arange(8).reshape((2, 2, 2))
 3233:     >>> np.linalg.trace(a)
 3234:     array([3, 11])
 3235: 
 3236:     Trace is computed with the last two axes as the 2-d sub-arrays.
 3237:     This behavior differs from :py:func:`numpy.trace` which uses the first two
 3238:     axes by default.
 3239: 
 3240:     >>> a = np.arange(24).reshape((3, 2, 2, 2))
 3241:     >>> np.linalg.trace(a).shape
 3242:     (3, 2)
 3243: 
 3244:     Traces adjacent to the main diagonal can be obtained by using the
 3245:     `offset` argument:
 3246: 
 3247:     >>> a = np.arange(9).reshape((3, 3)); a
 3248:     array([[0, 1, 2],
 3249:            [3, 4, 5],
 3250:            [6, 7, 8]])
 3251:     >>> np.linalg.trace(a, offset=1)  # First superdiagonal
 3252:     6
 3253:     >>> np.linalg.trace(a, offset=2)  # Second superdiagonal
 3254:     2
 3255:     >>> np.linalg.trace(a, offset=-1)  # First subdiagonal
 3256:     10
 3257:     >>> np.linalg.trace(a, offset=-2)  # Second subdiagonal
 3258:     6
 3259: 
 3260:     """
 3261:     return _core_trace(x, offset, axis1=-2, axis2=-1, dtype=dtype)
 3262: 
 3263: 
 3264: # cross
 3265: 
 3266: def _cross_dispatcher(x1, x2, /, *, axis=None):
 3267:     return (x1, x2,)
 3268: 
 3269: 
 3270: @array_function_dispatch(_cross_dispatcher)
 3271: def cross(x1, x2, /, *, axis=-1):
 3272:     """
 3273:     Returns the cross product of 3-element vectors.
 3274: 
 3275:     If ``x1`` and/or ``x2`` are multi-dimensional arrays, then
 3276:     the cross-product of each pair of corresponding 3-element vectors
 3277:     is independently computed.
 3278: 
 3279:     This function is Array API compatible, contrary to
 3280:     :func:`numpy.cross`.
 3281: 
 3282:     Parameters
 3283:     ----------
 3284:     x1 : array_like
 3285:         The first input array.
 3286:     x2 : array_like
 3287:         The second input array. Must be compatible with ``x1`` for all
 3288:         non-compute axes. The size of the axis over which to compute
 3289:         the cross-product must be the same size as the respective axis
 3290:         in ``x1``.
 3291:     axis : int, optional
 3292:         The axis (dimension) of ``x1`` and ``x2`` containing the vectors for
 3293:         which to compute the cross-product. Default: ``-1``.
 3294: 
 3295:     Returns
 3296:     -------
 3297:     out : ndarray
 3298:         An array containing the cross products.
 3299: 
 3300:     See Also
 3301:     --------
 3302:     numpy.cross
 3303: 
 3304:     Examples
 3305:     --------
 3306:     Vector cross-product.
 3307: 
 3308:     >>> x = np.array([1, 2, 3])
 3309:     >>> y = np.array([4, 5, 6])
 3310:     >>> np.linalg.cross(x, y)
 3311:     array([-3,  6, -3])
 3312: 
 3313:     Multiple vector cross-products. Note that the direction of the cross
 3314:     product vector is defined by the *right-hand rule*.
 3315: 
 3316:     >>> x = np.array([[1,2,3], [4,5,6]])
 3317:     >>> y = np.array([[4,5,6], [1,2,3]])
 3318:     >>> np.linalg.cross(x, y)
 3319:     array([[-3,  6, -3],
 3320:            [ 3, -6,  3]])
 3321: 
 3322:     >>> x = np.array([[1, 2], [3, 4], [5, 6]])
 3323:     >>> y = np.array([[4, 5], [6, 1], [2, 3]])
 3324:     >>> np.linalg.cross(x, y, axis=0)
 3325:     array([[-24,  6],
 3326:            [ 18, 24],
 3327:            [-6,  -18]])
 3328: 
 3329:     """
 3330:     x1 = asanyarray(x1)
 3331:     x2 = asanyarray(x2)
 3332: 
 3333:     if x1.shape[axis] != 3 or x2.shape[axis] != 3:
 3334:         raise ValueError(
 3335:             "Both input arrays must be (arrays of) 3-dimensional vectors, "
 3336:             f"but they are {x1.shape[axis]} and {x2.shape[axis]} "
 3337:             "dimensional instead."
 3338:         )
 3339: 
 3340:     return _core_cross(x1, x2, axis=axis)
 3341: 
 3342: 
 3343: # matmul
 3344: 
 3345: def _matmul_dispatcher(x1, x2, /):
 3346:     return (x1, x2)
 3347: 
 3348: 
 3349: @array_function_dispatch(_matmul_dispatcher)
 3350: def matmul(x1, x2, /):
 3351:     """
 3352:     Computes the matrix product.
 3353: 
 3354:     This function is Array API compatible, contrary to
 3355:     :func:`numpy.matmul`.
 3356: 
 3357:     Parameters
 3358:     ----------
 3359:     x1 : array_like
 3360:         The first input array.
 3361:     x2 : array_like
 3362:         The second input array.
 3363: 
 3364:     Returns
 3365:     -------
 3366:     out : ndarray
 3367:         The matrix product of the inputs.
 3368:         This is a scalar only when both ``x1``, ``x2`` are 1-d vectors.
 3369: 
 3370:     Raises
 3371:     ------
 3372:     ValueError
 3373:         If the last dimension of ``x1`` is not the same size as
 3374:         the second-to-last dimension of ``x2``.
 3375: 
 3376:         If a scalar value is passed in.
 3377: 
 3378:     See Also
 3379:     --------
 3380:     numpy.matmul
 3381: 
 3382:     Examples
 3383:     --------
 3384:     For 2-D arrays it is the matrix product:
 3385: 
 3386:     >>> a = np.array([[1, 0],
 3387:     ...               [0, 1]])
 3388:     >>> b = np.array([[4, 1],
 3389:     ...               [2, 2]])
 3390:     >>> np.linalg.matmul(a, b)
 3391:     array([[4, 1],
 3392:            [2, 2]])
 3393: 
 3394:     For 2-D mixed with 1-D, the result is the usual.
 3395: 
 3396:     >>> a = np.array([[1, 0],
 3397:     ...               [0, 1]])
 3398:     >>> b = np.array([1, 2])
 3399:     >>> np.linalg.matmul(a, b)
 3400:     array([1, 2])
 3401:     >>> np.linalg.matmul(b, a)
 3402:     array([1, 2])
 3403: 
 3404: 
 3405:     Broadcasting is conventional for stacks of arrays
 3406: 
 3407:     >>> a = np.arange(2 * 2 * 4).reshape((2, 2, 4))
 3408:     >>> b = np.arange(2 * 2 * 4).reshape((2, 4, 2))
 3409:     >>> np.linalg.matmul(a,b).shape
 3410:     (2, 2, 2)
 3411:     >>> np.linalg.matmul(a, b)[0, 1, 1]
 3412:     98
 3413:     >>> sum(a[0, 1, :] * b[0 , :, 1])
 3414:     98
 3415: 
 3416:     Vector, vector returns the scalar inner product, but neither argument
 3417:     is complex-conjugated:
 3418: 
 3419:     >>> np.linalg.matmul([2j, 3j], [2j, 3j])
 3420:     (-13+0j)
 3421: 
 3422:     Scalar multiplication raises an error.
 3423: 
 3424:     >>> np.linalg.matmul([1,2], 3)
 3425:     Traceback (most recent call last):
 3426:     ...
 3427:     ValueError: matmul: Input operand 1 does not have enough dimensions ...
 3428: 
 3429:     """
 3430:     return _core_matmul(x1, x2)
 3431: 
 3432: 
 3433: # tensordot
 3434: 
 3435: def _tensordot_dispatcher(x1, x2, /, *, axes=None):
 3436:     return (x1, x2)
 3437: 
 3438: 
 3439: @array_function_dispatch(_tensordot_dispatcher)
 3440: def tensordot(x1, x2, /, *, axes=2):
 3441:     return _core_tensordot(x1, x2, axes=axes)
 3442: 
 3443: 
 3444: tensordot.__doc__ = _core_tensordot.__doc__
 3445: 
 3446: 
 3447: # matrix_transpose
 3448: 
 3449: def _matrix_transpose_dispatcher(x):
 3450:     return (x,)
 3451: 
 3452: @array_function_dispatch(_matrix_transpose_dispatcher)
 3453: def matrix_transpose(x, /):
 3454:     return _core_matrix_transpose(x)
 3455: 
 3456: 
 3457: matrix_transpose.__doc__ = f"""{_core_matrix_transpose.__doc__}
 3458: 
 3459:     Notes
 3460:     -----
 3461:     This function is an alias of `numpy.matrix_transpose`.
 3462: """
 3463: 
 3464: 
 3465: # matrix_norm
 3466: 
 3467: def _matrix_norm_dispatcher(x, /, *, keepdims=None, ord=None):
 3468:     return (x,)
 3469: 
 3470: @array_function_dispatch(_matrix_norm_dispatcher)
 3471: def matrix_norm(x, /, *, keepdims=False, ord="fro"):
 3472:     """
 3473:     Computes the matrix norm of a matrix (or a stack of matrices) ``x``.
 3474: 
 3475:     This function is Array API compatible.
 3476: 
 3477:     Parameters
 3478:     ----------
 3479:     x : array_like
 3480:         Input array having shape (..., M, N) and whose two innermost
 3481:         dimensions form ``MxN`` matrices.
 3482:     keepdims : bool, optional
 3483:         If this is set to True, the axes which are normed over are left in
 3484:         the result as dimensions with size one. Default: False.
 3485:     ord : {1, -1, 2, -2, inf, -inf, 'fro', 'nuc'}, optional
 3486:         The order of the norm. For details see the table under ``Notes``
 3487:         in `numpy.linalg.norm`.
 3488: 
 3489:     See Also
 3490:     --------
 3491:     numpy.linalg.norm : Generic norm function
 3492: 
 3493:     Examples
 3494:     --------
 3495:     >>> from numpy import linalg as LA
 3496:     >>> a = np.arange(9) - 4
 3497:     >>> a
 3498:     array([-4, -3, -2, ...,  2,  3,  4])
 3499:     >>> b = a.reshape((3, 3))
 3500:     >>> b
 3501:     array([[-4, -3, -2],
 3502:            [-1,  0,  1],
 3503:            [ 2,  3,  4]])
 3504: 
 3505:     >>> LA.matrix_norm(b)
 3506:     7.745966692414834
 3507:     >>> LA.matrix_norm(b, ord='fro')
 3508:     7.745966692414834
 3509:     >>> LA.matrix_norm(b, ord=np.inf)
 3510:     9.0
 3511:     >>> LA.matrix_norm(b, ord=-np.inf)
 3512:     2.0
 3513: 
 3514:     >>> LA.matrix_norm(b, ord=1)
 3515:     7.0
 3516:     >>> LA.matrix_norm(b, ord=-1)
 3517:     6.0
 3518:     >>> LA.matrix_norm(b, ord=2)
 3519:     7.3484692283495345
 3520:     >>> LA.matrix_norm(b, ord=-2)
 3521:     1.8570331885190563e-016 # may vary
 3522: 
 3523:     """
 3524:     x = asanyarray(x)
 3525:     return norm(x, axis=(-2, -1), keepdims=keepdims, ord=ord)
 3526: 
 3527: 
 3528: # vector_norm
 3529: 
 3530: def _vector_norm_dispatcher(x, /, *, axis=None, keepdims=None, ord=None):
 3531:     return (x,)
 3532: 
 3533: @array_function_dispatch(_vector_norm_dispatcher)
 3534: def vector_norm(x, /, *, axis=None, keepdims=False, ord=2):
 3535:     """
 3536:     Computes the vector norm of a vector (or batch of vectors) ``x``.
 3537: 
 3538:     This function is Array API compatible.
 3539: 
 3540:     Parameters
 3541:     ----------
 3542:     x : array_like
 3543:         Input array.
 3544:     axis : {None, int, 2-tuple of ints}, optional
 3545:         If an integer, ``axis`` specifies the axis (dimension) along which
 3546:         to compute vector norms. If an n-tuple, ``axis`` specifies the axes
 3547:         (dimensions) along which to compute batched vector norms. If ``None``,
 3548:         the vector norm must be computed over all array values (i.e.,
 3549:         equivalent to computing the vector norm of a flattened array).
 3550:         Default: ``None``.
 3551:     keepdims : bool, optional
 3552:         If this is set to True, the axes which are normed over are left in
 3553:         the result as dimensions with size one. Default: False.
 3554:     ord : {int, float, inf, -inf}, optional
 3555:         The order of the norm. For details see the table under ``Notes``
 3556:         in `numpy.linalg.norm`.
 3557: 
 3558:     See Also
 3559:     --------
 3560:     numpy.linalg.norm : Generic norm function
 3561: 
 3562:     Examples
 3563:     --------
 3564:     >>> from numpy import linalg as LA
 3565:     >>> a = np.arange(9) + 1
 3566:     >>> a
 3567:     array([1, 2, 3, 4, 5, 6, 7, 8, 9])
 3568:     >>> b = a.reshape((3, 3))
 3569:     >>> b
 3570:     array([[1, 2, 3],
 3571:            [4, 5, 6],
 3572:            [7, 8, 9]])
 3573: 
 3574:     >>> LA.vector_norm(b)
 3575:     16.881943016134134
 3576:     >>> LA.vector_norm(b, ord=np.inf)
 3577:     9.0
 3578:     >>> LA.vector_norm(b, ord=-np.inf)
 3579:     1.0
 3580: 
 3581:     >>> LA.vector_norm(b, ord=0)
 3582:     9.0
 3583:     >>> LA.vector_norm(b, ord=1)
 3584:     45.0
 3585:     >>> LA.vector_norm(b, ord=-1)
 3586:     0.3534857623790153
 3587:     >>> LA.vector_norm(b, ord=2)
 3588:     16.881943016134134
 3589:     >>> LA.vector_norm(b, ord=-2)
 3590:     0.8058837395885292
 3591: 
 3592:     """
 3593:     x = asanyarray(x)
 3594:     shape = list(x.shape)
 3595:     if axis is None:
 3596:         # Note: np.linalg.norm() doesn't handle 0-D arrays
 3597:         x = x.ravel()
 3598:         _axis = 0
 3599:     elif isinstance(axis, tuple):
 3600:         # Note: The axis argument supports any number of axes, whereas
 3601:         # np.linalg.norm() only supports a single axis for vector norm.
 3602:         normalized_axis = normalize_axis_tuple(axis, x.ndim)
 3603:         rest = tuple(i for i in range(x.ndim) if i not in normalized_axis)
 3604:         newshape = axis + rest
 3605:         x = _core_transpose(x, newshape).reshape(
 3606:             (
 3607:                 prod([x.shape[i] for i in axis], dtype=int),
 3608:                 *[x.shape[i] for i in rest]
 3609:             )
 3610:         )
 3611:         _axis = 0
 3612:     else:
 3613:         _axis = axis
 3614: 
 3615:     res = norm(x, axis=_axis, ord=ord)
 3616: 
 3617:     if keepdims:
 3618:         # We can't reuse np.linalg.norm(keepdims) because of the reshape hacks
 3619:         # above to avoid matrix norm logic.
 3620:         _axis = normalize_axis_tuple(
 3621:             range(len(shape)) if axis is None else axis, len(shape)
 3622:         )
 3623:         for i in _axis:
 3624:             shape[i] = 1
 3625:         res = res.reshape(tuple(shape))
 3626: 
 3627:     return res
 3628: 
 3629: 
 3630: # vecdot
 3631: 
 3632: def _vecdot_dispatcher(x1, x2, /, *, axis=None):
 3633:     return (x1, x2)
 3634: 
 3635: @array_function_dispatch(_vecdot_dispatcher)
 3636: def vecdot(x1, x2, /, *, axis=-1):
 3637:     """
 3638:     Computes the vector dot product.
 3639: 
 3640:     This function is restricted to arguments compatible with the Array API,
 3641:     contrary to :func:`numpy.vecdot`.
 3642: 
 3643:     Let :math:`\\mathbf{a}` be a vector in ``x1`` and :math:`\\mathbf{b}` be
 3644:     a corresponding vector in ``x2``. The dot product is defined as:
 3645: 
 3646:     .. math::
 3647:        \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=0}^{n-1} \\overline{a_i}b_i
 3648: 
 3649:     over the dimension specified by ``axis`` and where :math:`\\overline{a_i}`
 3650:     denotes the complex conjugate if :math:`a_i` is complex and the identity
 3651:     otherwise.
 3652: 
 3653:     Parameters
 3654:     ----------
 3655:     x1 : array_like
 3656:         First input array.
 3657:     x2 : array_like
 3658:         Second input array.
 3659:     axis : int, optional
 3660:         Axis over which to compute the dot product. Default: ``-1``.
 3661: 
 3662:     Returns
 3663:     -------
 3664:     output : ndarray
 3665:         The vector dot product of the input.
 3666: 
 3667:     See Also
 3668:     --------
 3669:     numpy.vecdot
 3670: 
 3671:     Examples
 3672:     --------
 3673:     Get the projected size along a given normal for an array of vectors.
 3674: 
 3675:     >>> v = np.array([[0., 5., 0.], [0., 0., 10.], [0., 6., 8.]])
 3676:     >>> n = np.array([0., 0.6, 0.8])
 3677:     >>> np.linalg.vecdot(v, n)
 3678:     array([ 3.,  8., 10.])
 3679: 
 3680:     """
 3681:     return _core_vecdot(x1, x2, axis=axis)
