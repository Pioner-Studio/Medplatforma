    1: import builtins
    2: import collections.abc
    3: import functools
    4: import re
    5: import sys
    6: import warnings
    7: 
    8: import numpy as np
    9: import numpy._core.numeric as _nx
   10: from numpy._core import overrides, transpose
   11: from numpy._core._multiarray_umath import _array_converter
   12: from numpy._core.fromnumeric import any, mean, nonzero, partition, ravel, sum
   13: from numpy._core.multiarray import _monotonicity, _place, bincount, normalize_axis_index
   14: from numpy._core.multiarray import interp as compiled_interp
   15: from numpy._core.multiarray import interp_complex as compiled_interp_complex
   16: from numpy._core.numeric import (
   17:     absolute,
   18:     arange,
   19:     array,
   20:     asanyarray,
   21:     asarray,
   22:     concatenate,
   23:     dot,
   24:     empty,
   25:     integer,
   26:     intp,
   27:     isscalar,
   28:     ndarray,
   29:     ones,
   30:     take,
   31:     where,
   32:     zeros_like,
   33: )
   34: from numpy._core.numerictypes import typecodes
   35: from numpy._core.umath import (
   36:     add,
   37:     arctan2,
   38:     cos,
   39:     exp,
   40:     frompyfunc,
   41:     less_equal,
   42:     minimum,
   43:     mod,
   44:     not_equal,
   45:     pi,
   46:     sin,
   47:     sqrt,
   48:     subtract,
   49: )
   50: from numpy._utils import set_module
   51: 
   52: # needed in this module for compatibility
   53: from numpy.lib._histograms_impl import histogram, histogramdd  # noqa: F401
   54: from numpy.lib._twodim_base_impl import diag
   55: 
   56: array_function_dispatch = functools.partial(
   57:     overrides.array_function_dispatch, module='numpy')
   58: 
   59: 
   60: __all__ = [
   61:     'select', 'piecewise', 'trim_zeros', 'copy', 'iterable', 'percentile',
   62:     'diff', 'gradient', 'angle', 'unwrap', 'sort_complex', 'flip',
   63:     'rot90', 'extract', 'place', 'vectorize', 'asarray_chkfinite', 'average',
   64:     'bincount', 'digitize', 'cov', 'corrcoef',
   65:     'median', 'sinc', 'hamming', 'hanning', 'bartlett',
   66:     'blackman', 'kaiser', 'trapezoid', 'trapz', 'i0',
   67:     'meshgrid', 'delete', 'insert', 'append', 'interp',
   68:     'quantile'
   69:     ]
   70: 
   71: # _QuantileMethods is a dictionary listing all the supported methods to
   72: # compute quantile/percentile.
   73: #
   74: # Below virtual_index refers to the index of the element where the percentile
   75: # would be found in the sorted sample.
   76: # When the sample contains exactly the percentile wanted, the virtual_index is
   77: # an integer to the index of this element.
   78: # When the percentile wanted is in between two elements, the virtual_index
   79: # is made of a integer part (a.k.a 'i' or 'left') and a fractional part
   80: # (a.k.a 'g' or 'gamma')
   81: #
   82: # Each method in _QuantileMethods has two properties
   83: # get_virtual_index : Callable
   84: #   The function used to compute the virtual_index.
   85: # fix_gamma : Callable
   86: #   A function used for discrete methods to force the index to a specific value.
   87: _QuantileMethods = {
   88:     # --- HYNDMAN and FAN METHODS
   89:     # Discrete methods
   90:     'inverted_cdf': {
   91:         'get_virtual_index': lambda n, quantiles: _inverted_cdf(n, quantiles),  # noqa: PLW0108
   92:         'fix_gamma': None,  # should never be called
   93:     },
   94:     'averaged_inverted_cdf': {
   95:         'get_virtual_index': lambda n, quantiles: (n * quantiles) - 1,
   96:         'fix_gamma': lambda gamma, _: _get_gamma_mask(
   97:             shape=gamma.shape,
   98:             default_value=1.,
   99:             conditioned_value=0.5,
  100:             where=gamma == 0),
  101:     },
  102:     'closest_observation': {
  103:         'get_virtual_index': lambda n, quantiles: _closest_observation(n, quantiles),  # noqa: PLW0108
  104:         'fix_gamma': None,  # should never be called
  105:     },
  106:     # Continuous methods
  107:     'interpolated_inverted_cdf': {
  108:         'get_virtual_index': lambda n, quantiles:
  109:         _compute_virtual_index(n, quantiles, 0, 1),
  110:         'fix_gamma': lambda gamma, _: gamma,
  111:     },
  112:     'hazen': {
  113:         'get_virtual_index': lambda n, quantiles:
  114:         _compute_virtual_index(n, quantiles, 0.5, 0.5),
  115:         'fix_gamma': lambda gamma, _: gamma,
  116:     },
  117:     'weibull': {
  118:         'get_virtual_index': lambda n, quantiles:
  119:         _compute_virtual_index(n, quantiles, 0, 0),
  120:         'fix_gamma': lambda gamma, _: gamma,
  121:     },
  122:     # Default method.
  123:     # To avoid some rounding issues, `(n-1) * quantiles` is preferred to
  124:     # `_compute_virtual_index(n, quantiles, 1, 1)`.
  125:     # They are mathematically equivalent.
  126:     'linear': {
  127:         'get_virtual_index': lambda n, quantiles: (n - 1) * quantiles,
  128:         'fix_gamma': lambda gamma, _: gamma,
  129:     },
  130:     'median_unbiased': {
  131:         'get_virtual_index': lambda n, quantiles:
  132:         _compute_virtual_index(n, quantiles, 1 / 3.0, 1 / 3.0),
  133:         'fix_gamma': lambda gamma, _: gamma,
  134:     },
  135:     'normal_unbiased': {
  136:         'get_virtual_index': lambda n, quantiles:
  137:         _compute_virtual_index(n, quantiles, 3 / 8.0, 3 / 8.0),
  138:         'fix_gamma': lambda gamma, _: gamma,
  139:     },
  140:     # --- OTHER METHODS
  141:     'lower': {
  142:         'get_virtual_index': lambda n, quantiles: np.floor(
  143:             (n - 1) * quantiles).astype(np.intp),
  144:         'fix_gamma': None,  # should never be called, index dtype is int
  145:     },
  146:     'higher': {
  147:         'get_virtual_index': lambda n, quantiles: np.ceil(
  148:             (n - 1) * quantiles).astype(np.intp),
  149:         'fix_gamma': None,  # should never be called, index dtype is int
  150:     },
  151:     'midpoint': {
  152:         'get_virtual_index': lambda n, quantiles: 0.5 * (
  153:                 np.floor((n - 1) * quantiles)
  154:                 + np.ceil((n - 1) * quantiles)),
  155:         'fix_gamma': lambda gamma, index: _get_gamma_mask(
  156:             shape=gamma.shape,
  157:             default_value=0.5,
  158:             conditioned_value=0.,
  159:             where=index % 1 == 0),
  160:     },
  161:     'nearest': {
  162:         'get_virtual_index': lambda n, quantiles: np.around(
  163:             (n - 1) * quantiles).astype(np.intp),
  164:         'fix_gamma': None,
  165:         # should never be called, index dtype is int
  166:     }}
  167: 
  168: 
  169: def _rot90_dispatcher(m, k=None, axes=None):
  170:     return (m,)
  171: 
  172: 
  173: @array_function_dispatch(_rot90_dispatcher)
  174: def rot90(m, k=1, axes=(0, 1)):
  175:     """
  176:     Rotate an array by 90 degrees in the plane specified by axes.
  177: 
  178:     Rotation direction is from the first towards the second axis.
  179:     This means for a 2D array with the default `k` and `axes`, the
  180:     rotation will be counterclockwise.
  181: 
  182:     Parameters
  183:     ----------
  184:     m : array_like
  185:         Array of two or more dimensions.
  186:     k : integer
  187:         Number of times the array is rotated by 90 degrees.
  188:     axes : (2,) array_like
  189:         The array is rotated in the plane defined by the axes.
  190:         Axes must be different.
  191: 
  192:     Returns
  193:     -------
  194:     y : ndarray
  195:         A rotated view of `m`.
  196: 
  197:     See Also
  198:     --------
  199:     flip : Reverse the order of elements in an array along the given axis.
  200:     fliplr : Flip an array horizontally.
  201:     flipud : Flip an array vertically.
  202: 
  203:     Notes
  204:     -----
  205:     ``rot90(m, k=1, axes=(1,0))``  is the reverse of
  206:     ``rot90(m, k=1, axes=(0,1))``
  207: 
  208:     ``rot90(m, k=1, axes=(1,0))`` is equivalent to
  209:     ``rot90(m, k=-1, axes=(0,1))``
  210: 
  211:     Examples
  212:     --------
  213:     >>> import numpy as np
  214:     >>> m = np.array([[1,2],[3,4]], int)
  215:     >>> m
  216:     array([[1, 2],
  217:            [3, 4]])
  218:     >>> np.rot90(m)
  219:     array([[2, 4],
  220:            [1, 3]])
  221:     >>> np.rot90(m, 2)
  222:     array([[4, 3],
  223:            [2, 1]])
  224:     >>> m = np.arange(8).reshape((2,2,2))
  225:     >>> np.rot90(m, 1, (1,2))
  226:     array([[[1, 3],
  227:             [0, 2]],
  228:            [[5, 7],
  229:             [4, 6]]])
  230: 
  231:     """
  232:     axes = tuple(axes)
  233:     if len(axes) != 2:
  234:         raise ValueError("len(axes) must be 2.")
  235: 
  236:     m = asanyarray(m)
  237: 
  238:     if axes[0] == axes[1] or absolute(axes[0] - axes[1]) == m.ndim:
  239:         raise ValueError("Axes must be different.")
  240: 
  241:     if (axes[0] >= m.ndim or axes[0] < -m.ndim
  242:         or axes[1] >= m.ndim or axes[1] < -m.ndim):
  243:         raise ValueError(f"Axes={axes} out of range for array of ndim={m.ndim}.")
  244: 
  245:     k %= 4
  246: 
  247:     if k == 0:
  248:         return m[:]
  249:     if k == 2:
  250:         return flip(flip(m, axes[0]), axes[1])
  251: 
  252:     axes_list = arange(0, m.ndim)
  253:     (axes_list[axes[0]], axes_list[axes[1]]) = (axes_list[axes[1]],
  254:                                                 axes_list[axes[0]])
  255: 
  256:     if k == 1:
  257:         return transpose(flip(m, axes[1]), axes_list)
  258:     else:
  259:         # k == 3
  260:         return flip(transpose(m, axes_list), axes[1])
  261: 
  262: 
  263: def _flip_dispatcher(m, axis=None):
  264:     return (m,)
  265: 
  266: 
  267: @array_function_dispatch(_flip_dispatcher)
  268: def flip(m, axis=None):
  269:     """
  270:     Reverse the order of elements in an array along the given axis.
  271: 
  272:     The shape of the array is preserved, but the elements are reordered.
  273: 
  274:     Parameters
  275:     ----------
  276:     m : array_like
  277:         Input array.
  278:     axis : None or int or tuple of ints, optional
  279:          Axis or axes along which to flip over. The default,
  280:          axis=None, will flip over all of the axes of the input array.
  281:          If axis is negative it counts from the last to the first axis.
  282: 
  283:          If axis is a tuple of ints, flipping is performed on all of the axes
  284:          specified in the tuple.
  285: 
  286:     Returns
  287:     -------
  288:     out : array_like
  289:         A view of `m` with the entries of axis reversed.  Since a view is
  290:         returned, this operation is done in constant time.
  291: 
  292:     See Also
  293:     --------
  294:     flipud : Flip an array vertically (axis=0).
  295:     fliplr : Flip an array horizontally (axis=1).
  296: 
  297:     Notes
  298:     -----
  299:     flip(m, 0) is equivalent to flipud(m).
  300: 
  301:     flip(m, 1) is equivalent to fliplr(m).
  302: 
  303:     flip(m, n) corresponds to ``m[...,::-1,...]`` with ``::-1`` at position n.
  304: 
  305:     flip(m) corresponds to ``m[::-1,::-1,...,::-1]`` with ``::-1`` at all
  306:     positions.
  307: 
  308:     flip(m, (0, 1)) corresponds to ``m[::-1,::-1,...]`` with ``::-1`` at
  309:     position 0 and position 1.
  310: 
  311:     Examples
  312:     --------
  313:     >>> import numpy as np
  314:     >>> A = np.arange(8).reshape((2,2,2))
  315:     >>> A
  316:     array([[[0, 1],
  317:             [2, 3]],
  318:            [[4, 5],
  319:             [6, 7]]])
  320:     >>> np.flip(A, 0)
  321:     array([[[4, 5],
  322:             [6, 7]],
  323:            [[0, 1],
  324:             [2, 3]]])
  325:     >>> np.flip(A, 1)
  326:     array([[[2, 3],
  327:             [0, 1]],
  328:            [[6, 7],
  329:             [4, 5]]])
  330:     >>> np.flip(A)
  331:     array([[[7, 6],
  332:             [5, 4]],
  333:            [[3, 2],
  334:             [1, 0]]])
  335:     >>> np.flip(A, (0, 2))
  336:     array([[[5, 4],
  337:             [7, 6]],
  338:            [[1, 0],
  339:             [3, 2]]])
  340:     >>> rng = np.random.default_rng()
  341:     >>> A = rng.normal(size=(3,4,5))
  342:     >>> np.all(np.flip(A,2) == A[:,:,::-1,...])
  343:     True
  344:     """
  345:     if not hasattr(m, 'ndim'):
  346:         m = asarray(m)
  347:     if axis is None:
  348:         indexer = (np.s_[::-1],) * m.ndim
  349:     else:
  350:         axis = _nx.normalize_axis_tuple(axis, m.ndim)
  351:         indexer = [np.s_[:]] * m.ndim
  352:         for ax in axis:
  353:             indexer[ax] = np.s_[::-1]
  354:         indexer = tuple(indexer)
  355:     return m[indexer]
  356: 
  357: 
  358: @set_module('numpy')
  359: def iterable(y):
  360:     """
  361:     Check whether or not an object can be iterated over.
  362: 
  363:     Parameters
  364:     ----------
  365:     y : object
  366:       Input object.
  367: 
  368:     Returns
  369:     -------
  370:     b : bool
  371:       Return ``True`` if the object has an iterator method or is a
  372:       sequence and ``False`` otherwise.
  373: 
  374: 
  375:     Examples
  376:     --------
  377:     >>> import numpy as np
  378:     >>> np.iterable([1, 2, 3])
  379:     True
  380:     >>> np.iterable(2)
  381:     False
  382: 
  383:     Notes
  384:     -----
  385:     In most cases, the results of ``np.iterable(obj)`` are consistent with
  386:     ``isinstance(obj, collections.abc.Iterable)``. One notable exception is
  387:     the treatment of 0-dimensional arrays::
  388: 
  389:         >>> from collections.abc import Iterable
  390:         >>> a = np.array(1.0)  # 0-dimensional numpy array
  391:         >>> isinstance(a, Iterable)
  392:         True
  393:         >>> np.iterable(a)
  394:         False
  395: 
  396:     """
  397:     try:
  398:         iter(y)
  399:     except TypeError:
  400:         return False
  401:     return True
  402: 
  403: 
  404: def _weights_are_valid(weights, a, axis):
  405:     """Validate weights array.
  406: 
  407:     We assume, weights is not None.
  408:     """
  409:     wgt = np.asanyarray(weights)
  410: 
  411:     # Sanity checks
  412:     if a.shape != wgt.shape:
  413:         if axis is None:
  414:             raise TypeError(
  415:                 "Axis must be specified when shapes of a and weights "
  416:                 "differ.")
  417:         if wgt.shape != tuple(a.shape[ax] for ax in axis):
  418:             raise ValueError(
  419:                 "Shape of weights must be consistent with "
  420:                 "shape of a along specified axis.")
  421: 
  422:         # setup wgt to broadcast along axis
  423:         wgt = wgt.transpose(np.argsort(axis))
  424:         wgt = wgt.reshape(tuple((s if ax in axis else 1)
  425:                                 for ax, s in enumerate(a.shape)))
  426:     return wgt
  427: 
  428: 
  429: def _average_dispatcher(a, axis=None, weights=None, returned=None, *,
  430:                         keepdims=None):
  431:     return (a, weights)
  432: 
  433: 
  434: @array_function_dispatch(_average_dispatcher)
  435: def average(a, axis=None, weights=None, returned=False, *,
  436:             keepdims=np._NoValue):
  437:     """
  438:     Compute the weighted average along the specified axis.
  439: 
  440:     Parameters
  441:     ----------
  442:     a : array_like
  443:         Array containing data to be averaged. If `a` is not an array, a
  444:         conversion is attempted.
  445:     axis : None or int or tuple of ints, optional
  446:         Axis or axes along which to average `a`.  The default,
  447:         `axis=None`, will average over all of the elements of the input array.
  448:         If axis is negative it counts from the last to the first axis.
  449:         If axis is a tuple of ints, averaging is performed on all of the axes
  450:         specified in the tuple instead of a single axis or all the axes as
  451:         before.
  452:     weights : array_like, optional
  453:         An array of weights associated with the values in `a`. Each value in
  454:         `a` contributes to the average according to its associated weight.
  455:         The array of weights must be the same shape as `a` if no axis is
  456:         specified, otherwise the weights must have dimensions and shape
  457:         consistent with `a` along the specified axis.
  458:         If `weights=None`, then all data in `a` are assumed to have a
  459:         weight equal to one.
  460:         The calculation is::
  461: 
  462:             avg = sum(a * weights) / sum(weights)
  463: 
  464:         where the sum is over all included elements.
  465:         The only constraint on the values of `weights` is that `sum(weights)`
  466:         must not be 0.
  467:     returned : bool, optional
  468:         Default is `False`. If `True`, the tuple (`average`, `sum_of_weights`)
  469:         is returned, otherwise only the average is returned.
  470:         If `weights=None`, `sum_of_weights` is equivalent to the number of
  471:         elements over which the average is taken.
  472:     keepdims : bool, optional
  473:         If this is set to True, the axes which are reduced are left
  474:         in the result as dimensions with size one. With this option,
  475:         the result will broadcast correctly against the original `a`.
  476:         *Note:* `keepdims` will not work with instances of `numpy.matrix`
  477:         or other classes whose methods do not support `keepdims`.
  478: 
  479:         .. versionadded:: 1.23.0
  480: 
  481:     Returns
  482:     -------
  483:     retval, [sum_of_weights] : array_type or double
  484:         Return the average along the specified axis. When `returned` is `True`,
  485:         return a tuple with the average as the first element and the sum
  486:         of the weights as the second element. `sum_of_weights` is of the
  487:         same type as `retval`. The result dtype follows a general pattern.
  488:         If `weights` is None, the result dtype will be that of `a` , or ``float64``
  489:         if `a` is integral. Otherwise, if `weights` is not None and `a` is non-
  490:         integral, the result type will be the type of lowest precision capable of
  491:         representing values of both `a` and `weights`. If `a` happens to be
  492:         integral, the previous rules still applies but the result dtype will
  493:         at least be ``float64``.
  494: 
  495:     Raises
  496:     ------
  497:     ZeroDivisionError
  498:         When all weights along axis are zero. See `numpy.ma.average` for a
  499:         version robust to this type of error.
  500:     TypeError
  501:         When `weights` does not have the same shape as `a`, and `axis=None`.
  502:     ValueError
  503:         When `weights` does not have dimensions and shape consistent with `a`
  504:         along specified `axis`.
  505: 
  506:     See Also
  507:     --------
  508:     mean
  509: 
  510:     ma.average : average for masked arrays -- useful if your data contains
  511:                  "missing" values
  512:     numpy.result_type : Returns the type that results from applying the
  513:                         numpy type promotion rules to the arguments.
  514: 
  515:     Examples
  516:     --------
  517:     >>> import numpy as np
  518:     >>> data = np.arange(1, 5)
  519:     >>> data
  520:     array([1, 2, 3, 4])
  521:     >>> np.average(data)
  522:     2.5
  523:     >>> np.average(np.arange(1, 11), weights=np.arange(10, 0, -1))
  524:     4.0
  525: 
  526:     >>> data = np.arange(6).reshape((3, 2))
  527:     >>> data
  528:     array([[0, 1],
  529:            [2, 3],
  530:            [4, 5]])
  531:     >>> np.average(data, axis=1, weights=[1./4, 3./4])
  532:     array([0.75, 2.75, 4.75])
  533:     >>> np.average(data, weights=[1./4, 3./4])
  534:     Traceback (most recent call last):
  535:         ...
  536:     TypeError: Axis must be specified when shapes of a and weights differ.
  537: 
  538:     With ``keepdims=True``, the following result has shape (3, 1).
  539: 
  540:     >>> np.average(data, axis=1, keepdims=True)
  541:     array([[0.5],
  542:            [2.5],
  543:            [4.5]])
  544: 
  545:     >>> data = np.arange(8).reshape((2, 2, 2))
  546:     >>> data
  547:     array([[[0, 1],
  548:             [2, 3]],
  549:            [[4, 5],
  550:             [6, 7]]])
  551:     >>> np.average(data, axis=(0, 1), weights=[[1./4, 3./4], [1., 1./2]])
  552:     array([3.4, 4.4])
  553:     >>> np.average(data, axis=0, weights=[[1./4, 3./4], [1., 1./2]])
  554:     Traceback (most recent call last):
  555:         ...
  556:     ValueError: Shape of weights must be consistent
  557:     with shape of a along specified axis.
  558:     """
  559:     a = np.asanyarray(a)
  560: 
  561:     if axis is not None:
  562:         axis = _nx.normalize_axis_tuple(axis, a.ndim, argname="axis")
  563: 
  564:     if keepdims is np._NoValue:
  565:         # Don't pass on the keepdims argument if one wasn't given.
  566:         keepdims_kw = {}
  567:     else:
  568:         keepdims_kw = {'keepdims': keepdims}
  569: 
  570:     if weights is None:
  571:         avg = a.mean(axis, **keepdims_kw)
  572:         avg_as_array = np.asanyarray(avg)
  573:         scl = avg_as_array.dtype.type(a.size / avg_as_array.size)
  574:     else:
  575:         wgt = _weights_are_valid(weights=weights, a=a, axis=axis)
  576: 
  577:         if issubclass(a.dtype.type, (np.integer, np.bool)):
  578:             result_dtype = np.result_type(a.dtype, wgt.dtype, 'f8')
  579:         else:
  580:             result_dtype = np.result_type(a.dtype, wgt.dtype)
  581: 
  582:         scl = wgt.sum(axis=axis, dtype=result_dtype, **keepdims_kw)
  583:         if np.any(scl == 0.0):
  584:             raise ZeroDivisionError(
  585:                 "Weights sum to zero, can't be normalized")
  586: 
  587:         avg = avg_as_array = np.multiply(a, wgt,
  588:                           dtype=result_dtype).sum(axis, **keepdims_kw) / scl
  589: 
  590:     if returned:
  591:         if scl.shape != avg_as_array.shape:
  592:             scl = np.broadcast_to(scl, avg_as_array.shape).copy()
  593:         return avg, scl
  594:     else:
  595:         return avg
  596: 
  597: 
  598: @set_module('numpy')
  599: def asarray_chkfinite(a, dtype=None, order=None):
  600:     """Convert the input to an array, checking for NaNs or Infs.
  601: 
  602:     Parameters
  603:     ----------
  604:     a : array_like
  605:         Input data, in any form that can be converted to an array.  This
  606:         includes lists, lists of tuples, tuples, tuples of tuples, tuples
  607:         of lists and ndarrays.  Success requires no NaNs or Infs.
  608:     dtype : data-type, optional
  609:         By default, the data-type is inferred from the input data.
  610:     order : {'C', 'F', 'A', 'K'}, optional
  611:         Memory layout.  'A' and 'K' depend on the order of input array a.
  612:         'C' row-major (C-style),
  613:         'F' column-major (Fortran-style) memory representation.
  614:         'A' (any) means 'F' if `a` is Fortran contiguous, 'C' otherwise
  615:         'K' (keep) preserve input order
  616:         Defaults to 'C'.
  617: 
  618:     Returns
  619:     -------
  620:     out : ndarray
  621:         Array interpretation of `a`.  No copy is performed if the input
  622:         is already an ndarray.  If `a` is a subclass of ndarray, a base
  623:         class ndarray is returned.
  624: 
  625:     Raises
  626:     ------
  627:     ValueError
  628:         Raises ValueError if `a` contains NaN (Not a Number) or Inf (Infinity).
  629: 
  630:     See Also
  631:     --------
  632:     asarray : Create and array.
  633:     asanyarray : Similar function which passes through subclasses.
  634:     ascontiguousarray : Convert input to a contiguous array.
  635:     asfortranarray : Convert input to an ndarray with column-major
  636:                      memory order.
  637:     fromiter : Create an array from an iterator.
  638:     fromfunction : Construct an array by executing a function on grid
  639:                    positions.
  640: 
  641:     Examples
  642:     --------
  643:     >>> import numpy as np
  644: 
  645:     Convert a list into an array. If all elements are finite, then
  646:     ``asarray_chkfinite`` is identical to ``asarray``.
  647: 
  648:     >>> a = [1, 2]
  649:     >>> np.asarray_chkfinite(a, dtype=float)
  650:     array([1., 2.])
  651: 
  652:     Raises ValueError if array_like contains Nans or Infs.
  653: 
  654:     >>> a = [1, 2, np.inf]
  655:     >>> try:
  656:     ...     np.asarray_chkfinite(a)
  657:     ... except ValueError:
  658:     ...     print('ValueError')
  659:     ...
  660:     ValueError
  661: 
  662:     """
  663:     a = asarray(a, dtype=dtype, order=order)
  664:     if a.dtype.char in typecodes['AllFloat'] and not np.isfinite(a).all():
  665:         raise ValueError(
  666:             "array must not contain infs or NaNs")
  667:     return a
  668: 
  669: 
  670: def _piecewise_dispatcher(x, condlist, funclist, *args, **kw):
  671:     yield x
  672:     # support the undocumented behavior of allowing scalars
  673:     if np.iterable(condlist):
  674:         yield from condlist
  675: 
  676: 
  677: @array_function_dispatch(_piecewise_dispatcher)
  678: def piecewise(x, condlist, funclist, *args, **kw):
  679:     """
  680:     Evaluate a piecewise-defined function.
  681: 
  682:     Given a set of conditions and corresponding functions, evaluate each
  683:     function on the input data wherever its condition is true.
  684: 
  685:     Parameters
  686:     ----------
  687:     x : ndarray or scalar
  688:         The input domain.
  689:     condlist : list of bool arrays or bool scalars
  690:         Each boolean array corresponds to a function in `funclist`.  Wherever
  691:         `condlist[i]` is True, `funclist[i](x)` is used as the output value.
  692: 
  693:         Each boolean array in `condlist` selects a piece of `x`,
  694:         and should therefore be of the same shape as `x`.
  695: 
  696:         The length of `condlist` must correspond to that of `funclist`.
  697:         If one extra function is given, i.e. if
  698:         ``len(funclist) == len(condlist) + 1``, then that extra function
  699:         is the default value, used wherever all conditions are false.
  700:     funclist : list of callables, f(x,*args,**kw), or scalars
  701:         Each function is evaluated over `x` wherever its corresponding
  702:         condition is True.  It should take a 1d array as input and give an 1d
  703:         array or a scalar value as output.  If, instead of a callable,
  704:         a scalar is provided then a constant function (``lambda x: scalar``) is
  705:         assumed.
  706:     args : tuple, optional
  707:         Any further arguments given to `piecewise` are passed to the functions
  708:         upon execution, i.e., if called ``piecewise(..., ..., 1, 'a')``, then
  709:         each function is called as ``f(x, 1, 'a')``.
  710:     kw : dict, optional
  711:         Keyword arguments used in calling `piecewise` are passed to the
  712:         functions upon execution, i.e., if called
  713:         ``piecewise(..., ..., alpha=1)``, then each function is called as
  714:         ``f(x, alpha=1)``.
  715: 
  716:     Returns
  717:     -------
  718:     out : ndarray
  719:         The output is the same shape and type as x and is found by
  720:         calling the functions in `funclist` on the appropriate portions of `x`,
  721:         as defined by the boolean arrays in `condlist`.  Portions not covered
  722:         by any condition have a default value of 0.
  723: 
  724: 
  725:     See Also
  726:     --------
  727:     choose, select, where
  728: 
  729:     Notes
  730:     -----
  731:     This is similar to choose or select, except that functions are
  732:     evaluated on elements of `x` that satisfy the corresponding condition from
  733:     `condlist`.
  734: 
  735:     The result is::
  736: 
  737:             |--
  738:             |funclist[0](x[condlist[0]])
  739:       out = |funclist[1](x[condlist[1]])
  740:             |...
  741:             |funclist[n2](x[condlist[n2]])
  742:             |--
  743: 
  744:     Examples
  745:     --------
  746:     >>> import numpy as np
  747: 
  748:     Define the signum function, which is -1 for ``x < 0`` and +1 for ``x >= 0``.
  749: 
  750:     >>> x = np.linspace(-2.5, 2.5, 6)
  751:     >>> np.piecewise(x, [x < 0, x >= 0], [-1, 1])
  752:     array([-1., -1., -1.,  1.,  1.,  1.])
  753: 
  754:     Define the absolute value, which is ``-x`` for ``x <0`` and ``x`` for
  755:     ``x >= 0``.
  756: 
  757:     >>> np.piecewise(x, [x < 0, x >= 0], [lambda x: -x, lambda x: x])
  758:     array([2.5,  1.5,  0.5,  0.5,  1.5,  2.5])
  759: 
  760:     Apply the same function to a scalar value.
  761: 
  762:     >>> y = -2
  763:     >>> np.piecewise(y, [y < 0, y >= 0], [lambda x: -x, lambda x: x])
  764:     array(2)
  765: 
  766:     """
  767:     x = asanyarray(x)
  768:     n2 = len(funclist)
  769: 
  770:     # undocumented: single condition is promoted to a list of one condition
  771:     if isscalar(condlist) or (
  772:             not isinstance(condlist[0], (list, ndarray)) and x.ndim != 0):
  773:         condlist = [condlist]
  774: 
  775:     condlist = asarray(condlist, dtype=bool)
  776:     n = len(condlist)
  777: 
  778:     if n == n2 - 1:  # compute the "otherwise" condition.
  779:         condelse = ~np.any(condlist, axis=0, keepdims=True)
  780:         condlist = np.concatenate([condlist, condelse], axis=0)
  781:         n += 1
  782:     elif n != n2:
  783:         raise ValueError(
  784:             f"with {n} condition(s), either {n} or {n + 1} functions are expected"
  785:         )
  786: 
  787:     y = zeros_like(x)
  788:     for cond, func in zip(condlist, funclist):
  789:         if not isinstance(func, collections.abc.Callable):
  790:             y[cond] = func
  791:         else:
  792:             vals = x[cond]
  793:             if vals.size > 0:
  794:                 y[cond] = func(vals, *args, **kw)
  795: 
  796:     return y
  797: 
  798: 
  799: def _select_dispatcher(condlist, choicelist, default=None):
  800:     yield from condlist
  801:     yield from choicelist
  802: 
  803: 
  804: @array_function_dispatch(_select_dispatcher)
  805: def select(condlist, choicelist, default=0):
  806:     """
  807:     Return an array drawn from elements in choicelist, depending on conditions.
  808: 
  809:     Parameters
  810:     ----------
  811:     condlist : list of bool ndarrays
  812:         The list of conditions which determine from which array in `choicelist`
  813:         the output elements are taken. When multiple conditions are satisfied,
  814:         the first one encountered in `condlist` is used.
  815:     choicelist : list of ndarrays
  816:         The list of arrays from which the output elements are taken. It has
  817:         to be of the same length as `condlist`.
  818:     default : scalar, optional
  819:         The element inserted in `output` when all conditions evaluate to False.
  820: 
  821:     Returns
  822:     -------
  823:     output : ndarray
  824:         The output at position m is the m-th element of the array in
  825:         `choicelist` where the m-th element of the corresponding array in
  826:         `condlist` is True.
  827: 
  828:     See Also
  829:     --------
  830:     where : Return elements from one of two arrays depending on condition.
  831:     take, choose, compress, diag, diagonal
  832: 
  833:     Examples
  834:     --------
  835:     >>> import numpy as np
  836: 
  837:     Beginning with an array of integers from 0 to 5 (inclusive),
  838:     elements less than ``3`` are negated, elements greater than ``3``
  839:     are squared, and elements not meeting either of these conditions
  840:     (exactly ``3``) are replaced with a `default` value of ``42``.
  841: 
  842:     >>> x = np.arange(6)
  843:     >>> condlist = [x<3, x>3]
  844:     >>> choicelist = [-x, x**2]
  845:     >>> np.select(condlist, choicelist, 42)
  846:     array([ 0,  -1,  -2, 42, 16, 25])
  847: 
  848:     When multiple conditions are satisfied, the first one encountered in
  849:     `condlist` is used.
  850: 
  851:     >>> condlist = [x<=4, x>3]
  852:     >>> choicelist = [x, x**2]
  853:     >>> np.select(condlist, choicelist, 55)
  854:     array([ 0,  1,  2,  3,  4, 25])
  855: 
  856:     """
  857:     # Check the size of condlist and choicelist are the same, or abort.
  858:     if len(condlist) != len(choicelist):
  859:         raise ValueError(
  860:             'list of cases must be same length as list of conditions')
  861: 
  862:     # Now that the dtype is known, handle the deprecated select([], []) case
  863:     if len(condlist) == 0:
  864:         raise ValueError("select with an empty condition list is not possible")
  865: 
  866:     # TODO: This preserves the Python int, float, complex manually to get the
  867:     #       right `result_type` with NEP 50.  Most likely we will grow a better
  868:     #       way to spell this (and this can be replaced).
  869:     choicelist = [
  870:         choice if type(choice) in (int, float, complex) else np.asarray(choice)
  871:         for choice in choicelist]
  872:     choicelist.append(default if type(default) in (int, float, complex)
  873:                       else np.asarray(default))
  874: 
  875:     try:
  876:         dtype = np.result_type(*choicelist)
  877:     except TypeError as e:
  878:         msg = f'Choicelist and default value do not have a common dtype: {e}'
  879:         raise TypeError(msg) from None
  880: 
  881:     # Convert conditions to arrays and broadcast conditions and choices
  882:     # as the shape is needed for the result. Doing it separately optimizes
  883:     # for example when all choices are scalars.
  884:     condlist = np.broadcast_arrays(*condlist)
  885:     choicelist = np.broadcast_arrays(*choicelist)
  886: 
  887:     # If cond array is not an ndarray in boolean format or scalar bool, abort.
  888:     for i, cond in enumerate(condlist):
  889:         if cond.dtype.type is not np.bool:
  890:             raise TypeError(
  891:                 f'invalid entry {i} in condlist: should be boolean ndarray')
  892: 
  893:     if choicelist[0].ndim == 0:
  894:         # This may be common, so avoid the call.
  895:         result_shape = condlist[0].shape
  896:     else:
  897:         result_shape = np.broadcast_arrays(condlist[0], choicelist[0])[0].shape
  898: 
  899:     result = np.full(result_shape, choicelist[-1], dtype)
  900: 
  901:     # Use np.copyto to burn each choicelist array onto result, using the
  902:     # corresponding condlist as a boolean mask. This is done in reverse
  903:     # order since the first choice should take precedence.
  904:     choicelist = choicelist[-2::-1]
  905:     condlist = condlist[::-1]
  906:     for choice, cond in zip(choicelist, condlist):
  907:         np.copyto(result, choice, where=cond)
  908: 
  909:     return result
  910: 
  911: 
  912: def _copy_dispatcher(a, order=None, subok=None):
  913:     return (a,)
  914: 
  915: 
  916: @array_function_dispatch(_copy_dispatcher)
  917: def copy(a, order='K', subok=False):
  918:     """
  919:     Return an array copy of the given object.
  920: 
  921:     Parameters
  922:     ----------
  923:     a : array_like
  924:         Input data.
  925:     order : {'C', 'F', 'A', 'K'}, optional
  926:         Controls the memory layout of the copy. 'C' means C-order,
  927:         'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous,
  928:         'C' otherwise. 'K' means match the layout of `a` as closely
  929:         as possible. (Note that this function and :meth:`ndarray.copy` are very
  930:         similar, but have different default values for their order=
  931:         arguments.)
  932:     subok : bool, optional
  933:         If True, then sub-classes will be passed-through, otherwise the
  934:         returned array will be forced to be a base-class array (defaults to False).
  935: 
  936:     Returns
  937:     -------
  938:     arr : ndarray
  939:         Array interpretation of `a`.
  940: 
  941:     See Also
  942:     --------
  943:     ndarray.copy : Preferred method for creating an array copy
  944: 
  945:     Notes
  946:     -----
  947:     This is equivalent to:
  948: 
  949:     >>> np.array(a, copy=True)  #doctest: +SKIP
  950: 
  951:     The copy made of the data is shallow, i.e., for arrays with object dtype,
  952:     the new array will point to the same objects.
  953:     See Examples from `ndarray.copy`.
  954: 
  955:     Examples
  956:     --------
  957:     >>> import numpy as np
  958: 
  959:     Create an array x, with a reference y and a copy z:
  960: 
  961:     >>> x = np.array([1, 2, 3])
  962:     >>> y = x
  963:     >>> z = np.copy(x)
  964: 
  965:     Note that, when we modify x, y changes, but not z:
  966: 
  967:     >>> x[0] = 10
  968:     >>> x[0] == y[0]
  969:     True
  970:     >>> x[0] == z[0]
  971:     False
  972: 
  973:     Note that, np.copy clears previously set WRITEABLE=False flag.
  974: 
  975:     >>> a = np.array([1, 2, 3])
  976:     >>> a.flags["WRITEABLE"] = False
  977:     >>> b = np.copy(a)
  978:     >>> b.flags["WRITEABLE"]
  979:     True
  980:     >>> b[0] = 3
  981:     >>> b
  982:     array([3, 2, 3])
  983:     """
  984:     return array(a, order=order, subok=subok, copy=True)
  985: 
  986: # Basic operations
  987: 
  988: 
  989: def _gradient_dispatcher(f, *varargs, axis=None, edge_order=None):
  990:     yield f
  991:     yield from varargs
  992: 
  993: 
  994: @array_function_dispatch(_gradient_dispatcher)
  995: def gradient(f, *varargs, axis=None, edge_order=1):
  996:     """
  997:     Return the gradient of an N-dimensional array.
  998: 
  999:     The gradient is computed using second order accurate central differences
 1000:     in the interior points and either first or second order accurate one-sides
 1001:     (forward or backwards) differences at the boundaries.
 1002:     The returned gradient hence has the same shape as the input array.
 1003: 
 1004:     Parameters
 1005:     ----------
 1006:     f : array_like
 1007:         An N-dimensional array containing samples of a scalar function.
 1008:     varargs : list of scalar or array, optional
 1009:         Spacing between f values. Default unitary spacing for all dimensions.
 1010:         Spacing can be specified using:
 1011: 
 1012:         1. single scalar to specify a sample distance for all dimensions.
 1013:         2. N scalars to specify a constant sample distance for each dimension.
 1014:            i.e. `dx`, `dy`, `dz`, ...
 1015:         3. N arrays to specify the coordinates of the values along each
 1016:            dimension of F. The length of the array must match the size of
 1017:            the corresponding dimension
 1018:         4. Any combination of N scalars/arrays with the meaning of 2. and 3.
 1019: 
 1020:         If `axis` is given, the number of varargs must equal the number of axes
 1021:         specified in the axis parameter.
 1022:         Default: 1. (see Examples below).
 1023: 
 1024:     edge_order : {1, 2}, optional
 1025:         Gradient is calculated using N-th order accurate differences
 1026:         at the boundaries. Default: 1.
 1027:     axis : None or int or tuple of ints, optional
 1028:         Gradient is calculated only along the given axis or axes
 1029:         The default (axis = None) is to calculate the gradient for all the axes
 1030:         of the input array. axis may be negative, in which case it counts from
 1031:         the last to the first axis.
 1032: 
 1033:     Returns
 1034:     -------
 1035:     gradient : ndarray or tuple of ndarray
 1036:         A tuple of ndarrays (or a single ndarray if there is only one
 1037:         dimension) corresponding to the derivatives of f with respect
 1038:         to each dimension. Each derivative has the same shape as f.
 1039: 
 1040:     Examples
 1041:     --------
 1042:     >>> import numpy as np
 1043:     >>> f = np.array([1, 2, 4, 7, 11, 16])
 1044:     >>> np.gradient(f)
 1045:     array([1. , 1.5, 2.5, 3.5, 4.5, 5. ])
 1046:     >>> np.gradient(f, 2)
 1047:     array([0.5 ,  0.75,  1.25,  1.75,  2.25,  2.5 ])
 1048: 
 1049:     Spacing can be also specified with an array that represents the coordinates
 1050:     of the values F along the dimensions.
 1051:     For instance a uniform spacing:
 1052: 
 1053:     >>> x = np.arange(f.size)
 1054:     >>> np.gradient(f, x)
 1055:     array([1. ,  1.5,  2.5,  3.5,  4.5,  5. ])
 1056: 
 1057:     Or a non uniform one:
 1058: 
 1059:     >>> x = np.array([0., 1., 1.5, 3.5, 4., 6.])
 1060:     >>> np.gradient(f, x)
 1061:     array([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])
 1062: 
 1063:     For two dimensional arrays, the return will be two arrays ordered by
 1064:     axis. In this example the first array stands for the gradient in
 1065:     rows and the second one in columns direction:
 1066: 
 1067:     >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]]))
 1068:     (array([[ 2.,  2., -1.],
 1069:             [ 2.,  2., -1.]]),
 1070:      array([[1. , 2.5, 4. ],
 1071:             [1. , 1. , 1. ]]))
 1072: 
 1073:     In this example the spacing is also specified:
 1074:     uniform for axis=0 and non uniform for axis=1
 1075: 
 1076:     >>> dx = 2.
 1077:     >>> y = [1., 1.5, 3.5]
 1078:     >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]]), dx, y)
 1079:     (array([[ 1. ,  1. , -0.5],
 1080:             [ 1. ,  1. , -0.5]]),
 1081:      array([[2. , 2. , 2. ],
 1082:             [2. , 1.7, 0.5]]))
 1083: 
 1084:     It is possible to specify how boundaries are treated using `edge_order`
 1085: 
 1086:     >>> x = np.array([0, 1, 2, 3, 4])
 1087:     >>> f = x**2
 1088:     >>> np.gradient(f, edge_order=1)
 1089:     array([1.,  2.,  4.,  6.,  7.])
 1090:     >>> np.gradient(f, edge_order=2)
 1091:     array([0., 2., 4., 6., 8.])
 1092: 
 1093:     The `axis` keyword can be used to specify a subset of axes of which the
 1094:     gradient is calculated
 1095: 
 1096:     >>> np.gradient(np.array([[1, 2, 6], [3, 4, 5]]), axis=0)
 1097:     array([[ 2.,  2., -1.],
 1098:            [ 2.,  2., -1.]])
 1099: 
 1100:     The `varargs` argument defines the spacing between sample points in the
 1101:     input array. It can take two forms:
 1102: 
 1103:     1. An array, specifying coordinates, which may be unevenly spaced:
 1104: 
 1105:     >>> x = np.array([0., 2., 3., 6., 8.])
 1106:     >>> y = x ** 2
 1107:     >>> np.gradient(y, x, edge_order=2)
 1108:     array([ 0.,  4.,  6., 12., 16.])
 1109: 
 1110:     2. A scalar, representing the fixed sample distance:
 1111: 
 1112:     >>> dx = 2
 1113:     >>> x = np.array([0., 2., 4., 6., 8.])
 1114:     >>> y = x ** 2
 1115:     >>> np.gradient(y, dx, edge_order=2)
 1116:     array([ 0.,  4.,  8., 12., 16.])
 1117: 
 1118:     It's possible to provide different data for spacing along each dimension.
 1119:     The number of arguments must match the number of dimensions in the input
 1120:     data.
 1121: 
 1122:     >>> dx = 2
 1123:     >>> dy = 3
 1124:     >>> x = np.arange(0, 6, dx)
 1125:     >>> y = np.arange(0, 9, dy)
 1126:     >>> xs, ys = np.meshgrid(x, y)
 1127:     >>> zs = xs + 2 * ys
 1128:     >>> np.gradient(zs, dy, dx)  # Passing two scalars
 1129:     (array([[2., 2., 2.],
 1130:             [2., 2., 2.],
 1131:             [2., 2., 2.]]),
 1132:      array([[1., 1., 1.],
 1133:             [1., 1., 1.],
 1134:             [1., 1., 1.]]))
 1135: 
 1136:     Mixing scalars and arrays is also allowed:
 1137: 
 1138:     >>> np.gradient(zs, y, dx)  # Passing one array and one scalar
 1139:     (array([[2., 2., 2.],
 1140:             [2., 2., 2.],
 1141:             [2., 2., 2.]]),
 1142:      array([[1., 1., 1.],
 1143:             [1., 1., 1.],
 1144:             [1., 1., 1.]]))
 1145: 
 1146:     Notes
 1147:     -----
 1148:     Assuming that :math:`f\\in C^{3}` (i.e., :math:`f` has at least 3 continuous
 1149:     derivatives) and let :math:`h_{*}` be a non-homogeneous stepsize, we
 1150:     minimize the "consistency error" :math:`\\eta_{i}` between the true gradient
 1151:     and its estimate from a linear combination of the neighboring grid-points:
 1152: 
 1153:     .. math::
 1154: 
 1155:         \\eta_{i} = f_{i}^{\\left(1\\right)} -
 1156:                     \\left[ \\alpha f\\left(x_{i}\\right) +
 1157:                             \\beta f\\left(x_{i} + h_{d}\\right) +
 1158:                             \\gamma f\\left(x_{i}-h_{s}\\right)
 1159:                     \\right]
 1160: 
 1161:     By substituting :math:`f(x_{i} + h_{d})` and :math:`f(x_{i} - h_{s})`
 1162:     with their Taylor series expansion, this translates into solving
 1163:     the following the linear system:
 1164: 
 1165:     .. math::
 1166: 
 1167:         \\left\\{
 1168:             \\begin{array}{r}
 1169:                 \\alpha+\\beta+\\gamma=0 \\\\
 1170:                 \\beta h_{d}-\\gamma h_{s}=1 \\\\
 1171:                 \\beta h_{d}^{2}+\\gamma h_{s}^{2}=0
 1172:             \\end{array}
 1173:         \\right.
 1174: 
 1175:     The resulting approximation of :math:`f_{i}^{(1)}` is the following:
 1176: 
 1177:     .. math::
 1178: 
 1179:         \\hat f_{i}^{(1)} =
 1180:             \\frac{
 1181:                 h_{s}^{2}f\\left(x_{i} + h_{d}\\right)
 1182:                 + \\left(h_{d}^{2} - h_{s}^{2}\\right)f\\left(x_{i}\\right)
 1183:                 - h_{d}^{2}f\\left(x_{i}-h_{s}\\right)}
 1184:                 { h_{s}h_{d}\\left(h_{d} + h_{s}\\right)}
 1185:             + \\mathcal{O}\\left(\\frac{h_{d}h_{s}^{2}
 1186:                                 + h_{s}h_{d}^{2}}{h_{d}
 1187:                                 + h_{s}}\\right)
 1188: 
 1189:     It is worth noting that if :math:`h_{s}=h_{d}`
 1190:     (i.e., data are evenly spaced)
 1191:     we find the standard second order approximation:
 1192: 
 1193:     .. math::
 1194: 
 1195:         \\hat f_{i}^{(1)}=
 1196:             \\frac{f\\left(x_{i+1}\\right) - f\\left(x_{i-1}\\right)}{2h}
 1197:             + \\mathcal{O}\\left(h^{2}\\right)
 1198: 
 1199:     With a similar procedure the forward/backward approximations used for
 1200:     boundaries can be derived.
 1201: 
 1202:     References
 1203:     ----------
 1204:     .. [1]  Quarteroni A., Sacco R., Saleri F. (2007) Numerical Mathematics
 1205:             (Texts in Applied Mathematics). New York: Springer.
 1206:     .. [2]  Durran D. R. (1999) Numerical Methods for Wave Equations
 1207:             in Geophysical Fluid Dynamics. New York: Springer.
 1208:     .. [3]  Fornberg B. (1988) Generation of Finite Difference Formulas on
 1209:             Arbitrarily Spaced Grids,
 1210:             Mathematics of Computation 51, no. 184 : 699-706.
 1211:             `PDF <https://www.ams.org/journals/mcom/1988-51-184/
 1212:             S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf>`_.
 1213:     """
 1214:     f = np.asanyarray(f)
 1215:     N = f.ndim  # number of dimensions
 1216: 
 1217:     if axis is None:
 1218:         axes = tuple(range(N))
 1219:     else:
 1220:         axes = _nx.normalize_axis_tuple(axis, N)
 1221: 
 1222:     len_axes = len(axes)
 1223:     n = len(varargs)
 1224:     if n == 0:
 1225:         # no spacing argument - use 1 in all axes
 1226:         dx = [1.0] * len_axes
 1227:     elif n == 1 and np.ndim(varargs[0]) == 0:
 1228:         # single scalar for all axes
 1229:         dx = varargs * len_axes
 1230:     elif n == len_axes:
 1231:         # scalar or 1d array for each axis
 1232:         dx = list(varargs)
 1233:         for i, distances in enumerate(dx):
 1234:             distances = np.asanyarray(distances)
 1235:             if distances.ndim == 0:
 1236:                 continue
 1237:             elif distances.ndim != 1:
 1238:                 raise ValueError("distances must be either scalars or 1d")
 1239:             if len(distances) != f.shape[axes[i]]:
 1240:                 raise ValueError("when 1d, distances must match "
 1241:                                  "the length of the corresponding dimension")
 1242:             if np.issubdtype(distances.dtype, np.integer):
 1243:                 # Convert numpy integer types to float64 to avoid modular
 1244:                 # arithmetic in np.diff(distances).
 1245:                 distances = distances.astype(np.float64)
 1246:             diffx = np.diff(distances)
 1247:             # if distances are constant reduce to the scalar case
 1248:             # since it brings a consistent speedup
 1249:             if (diffx == diffx[0]).all():
 1250:                 diffx = diffx[0]
 1251:             dx[i] = diffx
 1252:     else:
 1253:         raise TypeError("invalid number of arguments")
 1254: 
 1255:     if edge_order > 2:
 1256:         raise ValueError("'edge_order' greater than 2 not supported")
 1257: 
 1258:     # use central differences on interior and one-sided differences on the
 1259:     # endpoints. This preserves second order-accuracy over the full domain.
 1260: 
 1261:     outvals = []
 1262: 
 1263:     # create slice objects --- initially all are [:, :, ..., :]
 1264:     slice1 = [slice(None)] * N
 1265:     slice2 = [slice(None)] * N
 1266:     slice3 = [slice(None)] * N
 1267:     slice4 = [slice(None)] * N
 1268: 
 1269:     otype = f.dtype
 1270:     if otype.type is np.datetime64:
 1271:         # the timedelta dtype with the same unit information
 1272:         otype = np.dtype(otype.name.replace('datetime', 'timedelta'))
 1273:         # view as timedelta to allow addition
 1274:         f = f.view(otype)
 1275:     elif otype.type is np.timedelta64:
 1276:         pass
 1277:     elif np.issubdtype(otype, np.inexact):
 1278:         pass
 1279:     else:
 1280:         # All other types convert to floating point.
 1281:         # First check if f is a numpy integer type; if so, convert f to float64
 1282:         # to avoid modular arithmetic when computing the changes in f.
 1283:         if np.issubdtype(otype, np.integer):
 1284:             f = f.astype(np.float64)
 1285:         otype = np.float64
 1286: 
 1287:     for axis, ax_dx in zip(axes, dx):
 1288:         if f.shape[axis] < edge_order + 1:
 1289:             raise ValueError(
 1290:                 "Shape of array too small to calculate a numerical gradient, "
 1291:                 "at least (edge_order + 1) elements are required.")
 1292:         # result allocation
 1293:         out = np.empty_like(f, dtype=otype)
 1294: 
 1295:         # spacing for the current axis
 1296:         uniform_spacing = np.ndim(ax_dx) == 0
 1297: 
 1298:         # Numerical differentiation: 2nd order interior
 1299:         slice1[axis] = slice(1, -1)
 1300:         slice2[axis] = slice(None, -2)
 1301:         slice3[axis] = slice(1, -1)
 1302:         slice4[axis] = slice(2, None)
 1303: 
 1304:         if uniform_spacing:
 1305:             out[tuple(slice1)] = (f[tuple(slice4)] - f[tuple(slice2)]) / (2. * ax_dx)
 1306:         else:
 1307:             dx1 = ax_dx[0:-1]
 1308:             dx2 = ax_dx[1:]
 1309:             a = -(dx2) / (dx1 * (dx1 + dx2))
 1310:             b = (dx2 - dx1) / (dx1 * dx2)
 1311:             c = dx1 / (dx2 * (dx1 + dx2))
 1312:             # fix the shape for broadcasting
 1313:             shape = np.ones(N, dtype=int)
 1314:             shape[axis] = -1
 1315:             a.shape = b.shape = c.shape = shape
 1316:             # 1D equivalent -- out[1:-1] = a * f[:-2] + b * f[1:-1] + c * f[2:]
 1317:             out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \
 1318:                                                 + c * f[tuple(slice4)]
 1319: 
 1320:         # Numerical differentiation: 1st order edges
 1321:         if edge_order == 1:
 1322:             slice1[axis] = 0
 1323:             slice2[axis] = 1
 1324:             slice3[axis] = 0
 1325:             dx_0 = ax_dx if uniform_spacing else ax_dx[0]
 1326:             # 1D equivalent -- out[0] = (f[1] - f[0]) / (x[1] - x[0])
 1327:             out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_0
 1328: 
 1329:             slice1[axis] = -1
 1330:             slice2[axis] = -1
 1331:             slice3[axis] = -2
 1332:             dx_n = ax_dx if uniform_spacing else ax_dx[-1]
 1333:             # 1D equivalent -- out[-1] = (f[-1] - f[-2]) / (x[-1] - x[-2])
 1334:             out[tuple(slice1)] = (f[tuple(slice2)] - f[tuple(slice3)]) / dx_n
 1335: 
 1336:         # Numerical differentiation: 2nd order edges
 1337:         else:
 1338:             slice1[axis] = 0
 1339:             slice2[axis] = 0
 1340:             slice3[axis] = 1
 1341:             slice4[axis] = 2
 1342:             if uniform_spacing:
 1343:                 a = -1.5 / ax_dx
 1344:                 b = 2. / ax_dx
 1345:                 c = -0.5 / ax_dx
 1346:             else:
 1347:                 dx1 = ax_dx[0]
 1348:                 dx2 = ax_dx[1]
 1349:                 a = -(2. * dx1 + dx2) / (dx1 * (dx1 + dx2))
 1350:                 b = (dx1 + dx2) / (dx1 * dx2)
 1351:                 c = - dx1 / (dx2 * (dx1 + dx2))
 1352:             # 1D equivalent -- out[0] = a * f[0] + b * f[1] + c * f[2]
 1353:             out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \
 1354:                                                         + c * f[tuple(slice4)]
 1355: 
 1356:             slice1[axis] = -1
 1357:             slice2[axis] = -3
 1358:             slice3[axis] = -2
 1359:             slice4[axis] = -1
 1360:             if uniform_spacing:
 1361:                 a = 0.5 / ax_dx
 1362:                 b = -2. / ax_dx
 1363:                 c = 1.5 / ax_dx
 1364:             else:
 1365:                 dx1 = ax_dx[-2]
 1366:                 dx2 = ax_dx[-1]
 1367:                 a = (dx2) / (dx1 * (dx1 + dx2))
 1368:                 b = - (dx2 + dx1) / (dx1 * dx2)
 1369:                 c = (2. * dx2 + dx1) / (dx2 * (dx1 + dx2))
 1370:             # 1D equivalent -- out[-1] = a * f[-3] + b * f[-2] + c * f[-1]
 1371:             out[tuple(slice1)] = a * f[tuple(slice2)] + b * f[tuple(slice3)] \
 1372:                                                         + c * f[tuple(slice4)]
 1373: 
 1374:         outvals.append(out)
 1375: 
 1376:         # reset the slice object in this dimension to ":"
 1377:         slice1[axis] = slice(None)
 1378:         slice2[axis] = slice(None)
 1379:         slice3[axis] = slice(None)
 1380:         slice4[axis] = slice(None)
 1381: 
 1382:     if len_axes == 1:
 1383:         return outvals[0]
 1384:     return tuple(outvals)
 1385: 
 1386: 
 1387: def _diff_dispatcher(a, n=None, axis=None, prepend=None, append=None):
 1388:     return (a, prepend, append)
 1389: 
 1390: 
 1391: @array_function_dispatch(_diff_dispatcher)
 1392: def diff(a, n=1, axis=-1, prepend=np._NoValue, append=np._NoValue):
 1393:     """
 1394:     Calculate the n-th discrete difference along the given axis.
 1395: 
 1396:     The first difference is given by ``out[i] = a[i+1] - a[i]`` along
 1397:     the given axis, higher differences are calculated by using `diff`
 1398:     recursively.
 1399: 
 1400:     Parameters
 1401:     ----------
 1402:     a : array_like
 1403:         Input array
 1404:     n : int, optional
 1405:         The number of times values are differenced. If zero, the input
 1406:         is returned as-is.
 1407:     axis : int, optional
 1408:         The axis along which the difference is taken, default is the
 1409:         last axis.
 1410:     prepend, append : array_like, optional
 1411:         Values to prepend or append to `a` along axis prior to
 1412:         performing the difference.  Scalar values are expanded to
 1413:         arrays with length 1 in the direction of axis and the shape
 1414:         of the input array in along all other axes.  Otherwise the
 1415:         dimension and shape must match `a` except along axis.
 1416: 
 1417:     Returns
 1418:     -------
 1419:     diff : ndarray
 1420:         The n-th differences. The shape of the output is the same as `a`
 1421:         except along `axis` where the dimension is smaller by `n`. The
 1422:         type of the output is the same as the type of the difference
 1423:         between any two elements of `a`. This is the same as the type of
 1424:         `a` in most cases. A notable exception is `datetime64`, which
 1425:         results in a `timedelta64` output array.
 1426: 
 1427:     See Also
 1428:     --------
 1429:     gradient, ediff1d, cumsum
 1430: 
 1431:     Notes
 1432:     -----
 1433:     Type is preserved for boolean arrays, so the result will contain
 1434:     `False` when consecutive elements are the same and `True` when they
 1435:     differ.
 1436: 
 1437:     For unsigned integer arrays, the results will also be unsigned. This
 1438:     should not be surprising, as the result is consistent with
 1439:     calculating the difference directly:
 1440: 
 1441:     >>> u8_arr = np.array([1, 0], dtype=np.uint8)
 1442:     >>> np.diff(u8_arr)
 1443:     array([255], dtype=uint8)
 1444:     >>> u8_arr[1,...] - u8_arr[0,...]
 1445:     np.uint8(255)
 1446: 
 1447:     If this is not desirable, then the array should be cast to a larger
 1448:     integer type first:
 1449: 
 1450:     >>> i16_arr = u8_arr.astype(np.int16)
 1451:     >>> np.diff(i16_arr)
 1452:     array([-1], dtype=int16)
 1453: 
 1454:     Examples
 1455:     --------
 1456:     >>> import numpy as np
 1457:     >>> x = np.array([1, 2, 4, 7, 0])
 1458:     >>> np.diff(x)
 1459:     array([ 1,  2,  3, -7])
 1460:     >>> np.diff(x, n=2)
 1461:     array([  1,   1, -10])
 1462: 
 1463:     >>> x = np.array([[1, 3, 6, 10], [0, 5, 6, 8]])
 1464:     >>> np.diff(x)
 1465:     array([[2, 3, 4],
 1466:            [5, 1, 2]])
 1467:     >>> np.diff(x, axis=0)
 1468:     array([[-1,  2,  0, -2]])
 1469: 
 1470:     >>> x = np.arange('1066-10-13', '1066-10-16', dtype=np.datetime64)
 1471:     >>> np.diff(x)
 1472:     array([1, 1], dtype='timedelta64[D]')
 1473: 
 1474:     """
 1475:     if n == 0:
 1476:         return a
 1477:     if n < 0:
 1478:         raise ValueError(
 1479:             "order must be non-negative but got " + repr(n))
 1480: 
 1481:     a = asanyarray(a)
 1482:     nd = a.ndim
 1483:     if nd == 0:
 1484:         raise ValueError("diff requires input that is at least one dimensional")
 1485:     axis = normalize_axis_index(axis, nd)
 1486: 
 1487:     combined = []
 1488:     if prepend is not np._NoValue:
 1489:         prepend = np.asanyarray(prepend)
 1490:         if prepend.ndim == 0:
 1491:             shape = list(a.shape)
 1492:             shape[axis] = 1
 1493:             prepend = np.broadcast_to(prepend, tuple(shape))
 1494:         combined.append(prepend)
 1495: 
 1496:     combined.append(a)
 1497: 
 1498:     if append is not np._NoValue:
 1499:         append = np.asanyarray(append)
 1500:         if append.ndim == 0:
 1501:             shape = list(a.shape)
 1502:             shape[axis] = 1
 1503:             append = np.broadcast_to(append, tuple(shape))
 1504:         combined.append(append)
 1505: 
 1506:     if len(combined) > 1:
 1507:         a = np.concatenate(combined, axis)
 1508: 
 1509:     slice1 = [slice(None)] * nd
 1510:     slice2 = [slice(None)] * nd
 1511:     slice1[axis] = slice(1, None)
 1512:     slice2[axis] = slice(None, -1)
 1513:     slice1 = tuple(slice1)
 1514:     slice2 = tuple(slice2)
 1515: 
 1516:     op = not_equal if a.dtype == np.bool else subtract
 1517:     for _ in range(n):
 1518:         a = op(a[slice1], a[slice2])
 1519: 
 1520:     return a
 1521: 
 1522: 
 1523: def _interp_dispatcher(x, xp, fp, left=None, right=None, period=None):
 1524:     return (x, xp, fp)
 1525: 
 1526: 
 1527: @array_function_dispatch(_interp_dispatcher)
 1528: def interp(x, xp, fp, left=None, right=None, period=None):
 1529:     """
 1530:     One-dimensional linear interpolation for monotonically increasing sample points.
 1531: 
 1532:     Returns the one-dimensional piecewise linear interpolant to a function
 1533:     with given discrete data points (`xp`, `fp`), evaluated at `x`.
 1534: 
 1535:     Parameters
 1536:     ----------
 1537:     x : array_like
 1538:         The x-coordinates at which to evaluate the interpolated values.
 1539: 
 1540:     xp : 1-D sequence of floats
 1541:         The x-coordinates of the data points, must be increasing if argument
 1542:         `period` is not specified. Otherwise, `xp` is internally sorted after
 1543:         normalizing the periodic boundaries with ``xp = xp % period``.
 1544: 
 1545:     fp : 1-D sequence of float or complex
 1546:         The y-coordinates of the data points, same length as `xp`.
 1547: 
 1548:     left : optional float or complex corresponding to fp
 1549:         Value to return for `x < xp[0]`, default is `fp[0]`.
 1550: 
 1551:     right : optional float or complex corresponding to fp
 1552:         Value to return for `x > xp[-1]`, default is `fp[-1]`.
 1553: 
 1554:     period : None or float, optional
 1555:         A period for the x-coordinates. This parameter allows the proper
 1556:         interpolation of angular x-coordinates. Parameters `left` and `right`
 1557:         are ignored if `period` is specified.
 1558: 
 1559:     Returns
 1560:     -------
 1561:     y : float or complex (corresponding to fp) or ndarray
 1562:         The interpolated values, same shape as `x`.
 1563: 
 1564:     Raises
 1565:     ------
 1566:     ValueError
 1567:         If `xp` and `fp` have different length
 1568:         If `xp` or `fp` are not 1-D sequences
 1569:         If `period == 0`
 1570: 
 1571:     See Also
 1572:     --------
 1573:     scipy.interpolate
 1574: 
 1575:     Warnings
 1576:     --------
 1577:     The x-coordinate sequence is expected to be increasing, but this is not
 1578:     explicitly enforced.  However, if the sequence `xp` is non-increasing,
 1579:     interpolation results are meaningless.
 1580: 
 1581:     Note that, since NaN is unsortable, `xp` also cannot contain NaNs.
 1582: 
 1583:     A simple check for `xp` being strictly increasing is::
 1584: 
 1585:         np.all(np.diff(xp) > 0)
 1586: 
 1587:     Examples
 1588:     --------
 1589:     >>> import numpy as np
 1590:     >>> xp = [1, 2, 3]
 1591:     >>> fp = [3, 2, 0]
 1592:     >>> np.interp(2.5, xp, fp)
 1593:     1.0
 1594:     >>> np.interp([0, 1, 1.5, 2.72, 3.14], xp, fp)
 1595:     array([3.  , 3.  , 2.5 , 0.56, 0.  ])
 1596:     >>> UNDEF = -99.0
 1597:     >>> np.interp(3.14, xp, fp, right=UNDEF)
 1598:     -99.0
 1599: 
 1600:     Plot an interpolant to the sine function:
 1601: 
 1602:     >>> x = np.linspace(0, 2*np.pi, 10)
 1603:     >>> y = np.sin(x)
 1604:     >>> xvals = np.linspace(0, 2*np.pi, 50)
 1605:     >>> yinterp = np.interp(xvals, x, y)
 1606:     >>> import matplotlib.pyplot as plt
 1607:     >>> plt.plot(x, y, 'o')
 1608:     [<matplotlib.lines.Line2D object at 0x...>]
 1609:     >>> plt.plot(xvals, yinterp, '-x')
 1610:     [<matplotlib.lines.Line2D object at 0x...>]
 1611:     >>> plt.show()
 1612: 
 1613:     Interpolation with periodic x-coordinates:
 1614: 
 1615:     >>> x = [-180, -170, -185, 185, -10, -5, 0, 365]
 1616:     >>> xp = [190, -190, 350, -350]
 1617:     >>> fp = [5, 10, 3, 4]
 1618:     >>> np.interp(x, xp, fp, period=360)
 1619:     array([7.5 , 5.  , 8.75, 6.25, 3.  , 3.25, 3.5 , 3.75])
 1620: 
 1621:     Complex interpolation:
 1622: 
 1623:     >>> x = [1.5, 4.0]
 1624:     >>> xp = [2,3,5]
 1625:     >>> fp = [1.0j, 0, 2+3j]
 1626:     >>> np.interp(x, xp, fp)
 1627:     array([0.+1.j , 1.+1.5j])
 1628: 
 1629:     """
 1630: 
 1631:     fp = np.asarray(fp)
 1632: 
 1633:     if np.iscomplexobj(fp):
 1634:         interp_func = compiled_interp_complex
 1635:         input_dtype = np.complex128
 1636:     else:
 1637:         interp_func = compiled_interp
 1638:         input_dtype = np.float64
 1639: 
 1640:     if period is not None:
 1641:         if period == 0:
 1642:             raise ValueError("period must be a non-zero value")
 1643:         period = abs(period)
 1644:         left = None
 1645:         right = None
 1646: 
 1647:         x = np.asarray(x, dtype=np.float64)
 1648:         xp = np.asarray(xp, dtype=np.float64)
 1649:         fp = np.asarray(fp, dtype=input_dtype)
 1650: 
 1651:         if xp.ndim != 1 or fp.ndim != 1:
 1652:             raise ValueError("Data points must be 1-D sequences")
 1653:         if xp.shape[0] != fp.shape[0]:
 1654:             raise ValueError("fp and xp are not of the same length")
 1655:         # normalizing periodic boundaries
 1656:         x = x % period
 1657:         xp = xp % period
 1658:         asort_xp = np.argsort(xp)
 1659:         xp = xp[asort_xp]
 1660:         fp = fp[asort_xp]
 1661:         xp = np.concatenate((xp[-1:] - period, xp, xp[0:1] + period))
 1662:         fp = np.concatenate((fp[-1:], fp, fp[0:1]))
 1663: 
 1664:     return interp_func(x, xp, fp, left, right)
 1665: 
 1666: 
 1667: def _angle_dispatcher(z, deg=None):
 1668:     return (z,)
 1669: 
 1670: 
 1671: @array_function_dispatch(_angle_dispatcher)
 1672: def angle(z, deg=False):
 1673:     """
 1674:     Return the angle of the complex argument.
 1675: 
 1676:     Parameters
 1677:     ----------
 1678:     z : array_like
 1679:         A complex number or sequence of complex numbers.
 1680:     deg : bool, optional
 1681:         Return angle in degrees if True, radians if False (default).
 1682: 
 1683:     Returns
 1684:     -------
 1685:     angle : ndarray or scalar
 1686:         The counterclockwise angle from the positive real axis on the complex
 1687:         plane in the range ``(-pi, pi]``, with dtype as numpy.float64.
 1688: 
 1689:     See Also
 1690:     --------
 1691:     arctan2
 1692:     absolute
 1693: 
 1694:     Notes
 1695:     -----
 1696:     This function passes the imaginary and real parts of the argument to
 1697:     `arctan2` to compute the result; consequently, it follows the convention
 1698:     of `arctan2` when the magnitude of the argument is zero. See example.
 1699: 
 1700:     Examples
 1701:     --------
 1702:     >>> import numpy as np
 1703:     >>> np.angle([1.0, 1.0j, 1+1j])               # in radians
 1704:     array([ 0.        ,  1.57079633,  0.78539816]) # may vary
 1705:     >>> np.angle(1+1j, deg=True)                  # in degrees
 1706:     45.0
 1707:     >>> np.angle([0., -0., complex(0., -0.), complex(-0., -0.)])  # convention
 1708:     array([ 0.        ,  3.14159265, -0.        , -3.14159265])
 1709: 
 1710:     """
 1711:     z = asanyarray(z)
 1712:     if issubclass(z.dtype.type, _nx.complexfloating):
 1713:         zimag = z.imag
 1714:         zreal = z.real
 1715:     else:
 1716:         zimag = 0
 1717:         zreal = z
 1718: 
 1719:     a = arctan2(zimag, zreal)
 1720:     if deg:
 1721:         a *= 180 / pi
 1722:     return a
 1723: 
 1724: 
 1725: def _unwrap_dispatcher(p, discont=None, axis=None, *, period=None):
 1726:     return (p,)
 1727: 
 1728: 
 1729: @array_function_dispatch(_unwrap_dispatcher)
 1730: def unwrap(p, discont=None, axis=-1, *, period=2 * pi):
 1731:     r"""
 1732:     Unwrap by taking the complement of large deltas with respect to the period.
 1733: 
 1734:     This unwraps a signal `p` by changing elements which have an absolute
 1735:     difference from their predecessor of more than ``max(discont, period/2)``
 1736:     to their `period`-complementary values.
 1737: 
 1738:     For the default case where `period` is :math:`2\pi` and `discont` is
 1739:     :math:`\pi`, this unwraps a radian phase `p` such that adjacent differences
 1740:     are never greater than :math:`\pi` by adding :math:`2k\pi` for some
 1741:     integer :math:`k`.
 1742: 
 1743:     Parameters
 1744:     ----------
 1745:     p : array_like
 1746:         Input array.
 1747:     discont : float, optional
 1748:         Maximum discontinuity between values, default is ``period/2``.
 1749:         Values below ``period/2`` are treated as if they were ``period/2``.
 1750:         To have an effect different from the default, `discont` should be
 1751:         larger than ``period/2``.
 1752:     axis : int, optional
 1753:         Axis along which unwrap will operate, default is the last axis.
 1754:     period : float, optional
 1755:         Size of the range over which the input wraps. By default, it is
 1756:         ``2 pi``.
 1757: 
 1758:         .. versionadded:: 1.21.0
 1759: 
 1760:     Returns
 1761:     -------
 1762:     out : ndarray
 1763:         Output array.
 1764: 
 1765:     See Also
 1766:     --------
 1767:     rad2deg, deg2rad
 1768: 
 1769:     Notes
 1770:     -----
 1771:     If the discontinuity in `p` is smaller than ``period/2``,
 1772:     but larger than `discont`, no unwrapping is done because taking
 1773:     the complement would only make the discontinuity larger.
 1774: 
 1775:     Examples
 1776:     --------
 1777:     >>> import numpy as np
 1778:     >>> phase = np.linspace(0, np.pi, num=5)
 1779:     >>> phase[3:] += np.pi
 1780:     >>> phase
 1781:     array([ 0.        ,  0.78539816,  1.57079633,  5.49778714,  6.28318531]) # may vary
 1782:     >>> np.unwrap(phase)
 1783:     array([ 0.        ,  0.78539816,  1.57079633, -0.78539816,  0.        ]) # may vary
 1784:     >>> np.unwrap([0, 1, 2, -1, 0], period=4)
 1785:     array([0, 1, 2, 3, 4])
 1786:     >>> np.unwrap([ 1, 2, 3, 4, 5, 6, 1, 2, 3], period=6)
 1787:     array([1, 2, 3, 4, 5, 6, 7, 8, 9])
 1788:     >>> np.unwrap([2, 3, 4, 5, 2, 3, 4, 5], period=4)
 1789:     array([2, 3, 4, 5, 6, 7, 8, 9])
 1790:     >>> phase_deg = np.mod(np.linspace(0 ,720, 19), 360) - 180
 1791:     >>> np.unwrap(phase_deg, period=360)
 1792:     array([-180., -140., -100.,  -60.,  -20.,   20.,   60.,  100.,  140.,
 1793:             180.,  220.,  260.,  300.,  340.,  380.,  420.,  460.,  500.,
 1794:             540.])
 1795:     """
 1796:     p = asarray(p)
 1797:     nd = p.ndim
 1798:     dd = diff(p, axis=axis)
 1799:     if discont is None:
 1800:         discont = period / 2
 1801:     slice1 = [slice(None, None)] * nd     # full slices
 1802:     slice1[axis] = slice(1, None)
 1803:     slice1 = tuple(slice1)
 1804:     dtype = np.result_type(dd, period)
 1805:     if _nx.issubdtype(dtype, _nx.integer):
 1806:         interval_high, rem = divmod(period, 2)
 1807:         boundary_ambiguous = rem == 0
 1808:     else:
 1809:         interval_high = period / 2
 1810:         boundary_ambiguous = True
 1811:     interval_low = -interval_high
 1812:     ddmod = mod(dd - interval_low, period) + interval_low
 1813:     if boundary_ambiguous:
 1814:         # for `mask = (abs(dd) == period/2)`, the above line made
 1815:         # `ddmod[mask] == -period/2`. correct these such that
 1816:         # `ddmod[mask] == sign(dd[mask])*period/2`.
 1817:         _nx.copyto(ddmod, interval_high,
 1818:                    where=(ddmod == interval_low) & (dd > 0))
 1819:     ph_correct = ddmod - dd
 1820:     _nx.copyto(ph_correct, 0, where=abs(dd) < discont)
 1821:     up = array(p, copy=True, dtype=dtype)
 1822:     up[slice1] = p[slice1] + ph_correct.cumsum(axis)
 1823:     return up
 1824: 
 1825: 
 1826: def _sort_complex(a):
 1827:     return (a,)
 1828: 
 1829: 
 1830: @array_function_dispatch(_sort_complex)
 1831: def sort_complex(a):
 1832:     """
 1833:     Sort a complex array using the real part first, then the imaginary part.
 1834: 
 1835:     Parameters
 1836:     ----------
 1837:     a : array_like
 1838:         Input array
 1839: 
 1840:     Returns
 1841:     -------
 1842:     out : complex ndarray
 1843:         Always returns a sorted complex array.
 1844: 
 1845:     Examples
 1846:     --------
 1847:     >>> import numpy as np
 1848:     >>> np.sort_complex([5, 3, 6, 2, 1])
 1849:     array([1.+0.j, 2.+0.j, 3.+0.j, 5.+0.j, 6.+0.j])
 1850: 
 1851:     >>> np.sort_complex([1 + 2j, 2 - 1j, 3 - 2j, 3 - 3j, 3 + 5j])
 1852:     array([1.+2.j,  2.-1.j,  3.-3.j,  3.-2.j,  3.+5.j])
 1853: 
 1854:     """
 1855:     b = array(a, copy=True)
 1856:     b.sort()
 1857:     if not issubclass(b.dtype.type, _nx.complexfloating):
 1858:         if b.dtype.char in 'bhBH':
 1859:             return b.astype('F')
 1860:         elif b.dtype.char == 'g':
 1861:             return b.astype('G')
 1862:         else:
 1863:             return b.astype('D')
 1864:     else:
 1865:         return b
 1866: 
 1867: 
 1868: def _arg_trim_zeros(filt):
 1869:     """Return indices of the first and last non-zero element.
 1870: 
 1871:     Parameters
 1872:     ----------
 1873:     filt : array_like
 1874:         Input array.
 1875: 
 1876:     Returns
 1877:     -------
 1878:     start, stop : ndarray
 1879:         Two arrays containing the indices of the first and last non-zero
 1880:         element in each dimension.
 1881: 
 1882:     See also
 1883:     --------
 1884:     trim_zeros
 1885: 
 1886:     Examples
 1887:     --------
 1888:     >>> import numpy as np
 1889:     >>> _arg_trim_zeros(np.array([0, 0, 1, 1, 0]))
 1890:     (array([2]), array([3]))
 1891:     """
 1892:     nonzero = (
 1893:         np.argwhere(filt)
 1894:         if filt.dtype != np.object_
 1895:         # Historically, `trim_zeros` treats `None` in an object array
 1896:         # as non-zero while argwhere doesn't, account for that
 1897:         else np.argwhere(filt != 0)
 1898:     )
 1899:     if nonzero.size == 0:
 1900:         start = stop = np.array([], dtype=np.intp)
 1901:     else:
 1902:         start = nonzero.min(axis=0)
 1903:         stop = nonzero.max(axis=0)
 1904:     return start, stop
 1905: 
 1906: 
 1907: def _trim_zeros(filt, trim=None, axis=None):
 1908:     return (filt,)
 1909: 
 1910: 
 1911: @array_function_dispatch(_trim_zeros)
 1912: def trim_zeros(filt, trim='fb', axis=None):
 1913:     """Remove values along a dimension which are zero along all other.
 1914: 
 1915:     Parameters
 1916:     ----------
 1917:     filt : array_like
 1918:         Input array.
 1919:     trim : {"fb", "f", "b"}, optional
 1920:         A string with 'f' representing trim from front and 'b' to trim from
 1921:         back. By default, zeros are trimmed on both sides.
 1922:         Front and back refer to the edges of a dimension, with "front" referring
 1923:         to the side with the lowest index 0, and "back" referring to the highest
 1924:         index (or index -1).
 1925:     axis : int or sequence, optional
 1926:         If None, `filt` is cropped such that the smallest bounding box is
 1927:         returned that still contains all values which are not zero.
 1928:         If an axis is specified, `filt` will be sliced in that dimension only
 1929:         on the sides specified by `trim`. The remaining area will be the
 1930:         smallest that still contains all values wich are not zero.
 1931: 
 1932:         .. versionadded:: 2.2.0
 1933: 
 1934:     Returns
 1935:     -------
 1936:     trimmed : ndarray or sequence
 1937:         The result of trimming the input. The number of dimensions and the
 1938:         input data type are preserved.
 1939: 
 1940:     Notes
 1941:     -----
 1942:     For all-zero arrays, the first axis is trimmed first.
 1943: 
 1944:     Examples
 1945:     --------
 1946:     >>> import numpy as np
 1947:     >>> a = np.array((0, 0, 0, 1, 2, 3, 0, 2, 1, 0))
 1948:     >>> np.trim_zeros(a)
 1949:     array([1, 2, 3, 0, 2, 1])
 1950: 
 1951:     >>> np.trim_zeros(a, trim='b')
 1952:     array([0, 0, 0, ..., 0, 2, 1])
 1953: 
 1954:     Multiple dimensions are supported.
 1955: 
 1956:     >>> b = np.array([[0, 0, 2, 3, 0, 0],
 1957:     ...               [0, 1, 0, 3, 0, 0],
 1958:     ...               [0, 0, 0, 0, 0, 0]])
 1959:     >>> np.trim_zeros(b)
 1960:     array([[0, 2, 3],
 1961:            [1, 0, 3]])
 1962: 
 1963:     >>> np.trim_zeros(b, axis=-1)
 1964:     array([[0, 2, 3],
 1965:            [1, 0, 3],
 1966:            [0, 0, 0]])
 1967: 
 1968:     The input data type is preserved, list/tuple in means list/tuple out.
 1969: 
 1970:     >>> np.trim_zeros([0, 1, 2, 0])
 1971:     [1, 2]
 1972: 
 1973:     """
 1974:     filt_ = np.asarray(filt)
 1975: 
 1976:     trim = trim.lower()
 1977:     if trim not in {"fb", "bf", "f", "b"}:
 1978:         raise ValueError(f"unexpected character(s) in `trim`: {trim!r}")
 1979: 
 1980:     start, stop = _arg_trim_zeros(filt_)
 1981:     stop += 1  # Adjust for slicing
 1982: 
 1983:     if start.size == 0:
 1984:         # filt is all-zero -> assign same values to start and stop so that
 1985:         # resulting slice will be empty
 1986:         start = stop = np.zeros(filt_.ndim, dtype=np.intp)
 1987:     else:
 1988:         if 'f' not in trim:
 1989:             start = (None,) * filt_.ndim
 1990:         if 'b' not in trim:
 1991:             stop = (None,) * filt_.ndim
 1992: 
 1993:     if len(start) == 1:
 1994:         # filt is 1D -> don't use multi-dimensional slicing to preserve
 1995:         # non-array input types
 1996:         sl = slice(start[0], stop[0])
 1997:     elif axis is None:
 1998:         # trim all axes
 1999:         sl = tuple(slice(*x) for x in zip(start, stop))
 2000:     else:
 2001:         # only trim single axis
 2002:         axis = normalize_axis_index(axis, filt_.ndim)
 2003:         sl = (slice(None),) * axis + (slice(start[axis], stop[axis]),) + (...,)
 2004: 
 2005:     trimmed = filt[sl]
 2006:     return trimmed
 2007: 
 2008: 
 2009: def _extract_dispatcher(condition, arr):
 2010:     return (condition, arr)
 2011: 
 2012: 
 2013: @array_function_dispatch(_extract_dispatcher)
 2014: def extract(condition, arr):
 2015:     """
 2016:     Return the elements of an array that satisfy some condition.
 2017: 
 2018:     This is equivalent to ``np.compress(ravel(condition), ravel(arr))``.  If
 2019:     `condition` is boolean ``np.extract`` is equivalent to ``arr[condition]``.
 2020: 
 2021:     Note that `place` does the exact opposite of `extract`.
 2022: 
 2023:     Parameters
 2024:     ----------
 2025:     condition : array_like
 2026:         An array whose nonzero or True entries indicate the elements of `arr`
 2027:         to extract.
 2028:     arr : array_like
 2029:         Input array of the same size as `condition`.
 2030: 
 2031:     Returns
 2032:     -------
 2033:     extract : ndarray
 2034:         Rank 1 array of values from `arr` where `condition` is True.
 2035: 
 2036:     See Also
 2037:     --------
 2038:     take, put, copyto, compress, place
 2039: 
 2040:     Examples
 2041:     --------
 2042:     >>> import numpy as np
 2043:     >>> arr = np.arange(12).reshape((3, 4))
 2044:     >>> arr
 2045:     array([[ 0,  1,  2,  3],
 2046:            [ 4,  5,  6,  7],
 2047:            [ 8,  9, 10, 11]])
 2048:     >>> condition = np.mod(arr, 3)==0
 2049:     >>> condition
 2050:     array([[ True, False, False,  True],
 2051:            [False, False,  True, False],
 2052:            [False,  True, False, False]])
 2053:     >>> np.extract(condition, arr)
 2054:     array([0, 3, 6, 9])
 2055: 
 2056: 
 2057:     If `condition` is boolean:
 2058: 
 2059:     >>> arr[condition]
 2060:     array([0, 3, 6, 9])
 2061: 
 2062:     """
 2063:     return _nx.take(ravel(arr), nonzero(ravel(condition))[0])
 2064: 
 2065: 
 2066: def _place_dispatcher(arr, mask, vals):
 2067:     return (arr, mask, vals)
 2068: 
 2069: 
 2070: @array_function_dispatch(_place_dispatcher)
 2071: def place(arr, mask, vals):
 2072:     """
 2073:     Change elements of an array based on conditional and input values.
 2074: 
 2075:     Similar to ``np.copyto(arr, vals, where=mask)``, the difference is that
 2076:     `place` uses the first N elements of `vals`, where N is the number of
 2077:     True values in `mask`, while `copyto` uses the elements where `mask`
 2078:     is True.
 2079: 
 2080:     Note that `extract` does the exact opposite of `place`.
 2081: 
 2082:     Parameters
 2083:     ----------
 2084:     arr : ndarray
 2085:         Array to put data into.
 2086:     mask : array_like
 2087:         Boolean mask array. Must have the same size as `a`.
 2088:     vals : 1-D sequence
 2089:         Values to put into `a`. Only the first N elements are used, where
 2090:         N is the number of True values in `mask`. If `vals` is smaller
 2091:         than N, it will be repeated, and if elements of `a` are to be masked,
 2092:         this sequence must be non-empty.
 2093: 
 2094:     See Also
 2095:     --------
 2096:     copyto, put, take, extract
 2097: 
 2098:     Examples
 2099:     --------
 2100:     >>> import numpy as np
 2101:     >>> arr = np.arange(6).reshape(2, 3)
 2102:     >>> np.place(arr, arr>2, [44, 55])
 2103:     >>> arr
 2104:     array([[ 0,  1,  2],
 2105:            [44, 55, 44]])
 2106: 
 2107:     """
 2108:     return _place(arr, mask, vals)
 2109: 
 2110: 
 2111: def disp(mesg, device=None, linefeed=True):
 2112:     """
 2113:     Display a message on a device.
 2114: 
 2115:     .. deprecated:: 2.0
 2116:         Use your own printing function instead.
 2117: 
 2118:     Parameters
 2119:     ----------
 2120:     mesg : str
 2121:         Message to display.
 2122:     device : object
 2123:         Device to write message. If None, defaults to ``sys.stdout`` which is
 2124:         very similar to ``print``. `device` needs to have ``write()`` and
 2125:         ``flush()`` methods.
 2126:     linefeed : bool, optional
 2127:         Option whether to print a line feed or not. Defaults to True.
 2128: 
 2129:     Raises
 2130:     ------
 2131:     AttributeError
 2132:         If `device` does not have a ``write()`` or ``flush()`` method.
 2133: 
 2134:     Examples
 2135:     --------
 2136:     >>> import numpy as np
 2137: 
 2138:     Besides ``sys.stdout``, a file-like object can also be used as it has
 2139:     both required methods:
 2140: 
 2141:     >>> from io import StringIO
 2142:     >>> buf = StringIO()
 2143:     >>> np.disp('"Display" in a file', device=buf)
 2144:     >>> buf.getvalue()
 2145:     '"Display" in a file\\n'
 2146: 
 2147:     """
 2148: 
 2149:     # Deprecated in NumPy 2.0, 2023-07-11
 2150:     warnings.warn(
 2151:         "`disp` is deprecated, "
 2152:         "use your own printing function instead. "
 2153:         "(deprecated in NumPy 2.0)",
 2154:         DeprecationWarning,
 2155:         stacklevel=2
 2156:     )
 2157: 
 2158:     if device is None:
 2159:         device = sys.stdout
 2160:     if linefeed:
 2161:         device.write(f'{mesg}\n')
 2162:     else:
 2163:         device.write(f'{mesg}')
 2164:     device.flush()
 2165: 
 2166: 
 2167: # See https://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html
 2168: _DIMENSION_NAME = r'\w+'
 2169: _CORE_DIMENSION_LIST = f'(?:{_DIMENSION_NAME}(?:,{_DIMENSION_NAME})*)?'
 2170: _ARGUMENT = fr'\({_CORE_DIMENSION_LIST}\)'
 2171: _ARGUMENT_LIST = f'{_ARGUMENT}(?:,{_ARGUMENT})*'
 2172: _SIGNATURE = f'^{_ARGUMENT_LIST}->{_ARGUMENT_LIST}$'
 2173: 
 2174: 
 2175: def _parse_gufunc_signature(signature):
 2176:     """
 2177:     Parse string signatures for a generalized universal function.
 2178: 
 2179:     Arguments
 2180:     ---------
 2181:     signature : string
 2182:         Generalized universal function signature, e.g., ``(m,n),(n,p)->(m,p)``
 2183:         for ``np.matmul``.
 2184: 
 2185:     Returns
 2186:     -------
 2187:     Tuple of input and output core dimensions parsed from the signature, each
 2188:     of the form List[Tuple[str, ...]].
 2189:     """
 2190:     signature = re.sub(r'\s+', '', signature)
 2191: 
 2192:     if not re.match(_SIGNATURE, signature):
 2193:         raise ValueError(
 2194:             f'not a valid gufunc signature: {signature}')
 2195:     return tuple([tuple(re.findall(_DIMENSION_NAME, arg))
 2196:                   for arg in re.findall(_ARGUMENT, arg_list)]
 2197:                  for arg_list in signature.split('->'))
 2198: 
 2199: 
 2200: def _update_dim_sizes(dim_sizes, arg, core_dims):
 2201:     """
 2202:     Incrementally check and update core dimension sizes for a single argument.
 2203: 
 2204:     Arguments
 2205:     ---------
 2206:     dim_sizes : Dict[str, int]
 2207:         Sizes of existing core dimensions. Will be updated in-place.
 2208:     arg : ndarray
 2209:         Argument to examine.
 2210:     core_dims : Tuple[str, ...]
 2211:         Core dimensions for this argument.
 2212:     """
 2213:     if not core_dims:
 2214:         return
 2215: 
 2216:     num_core_dims = len(core_dims)
 2217:     if arg.ndim < num_core_dims:
 2218:         raise ValueError(
 2219:             '%d-dimensional argument does not have enough '
 2220:             'dimensions for all core dimensions %r'
 2221:             % (arg.ndim, core_dims))
 2222: 
 2223:     core_shape = arg.shape[-num_core_dims:]
 2224:     for dim, size in zip(core_dims, core_shape):
 2225:         if dim in dim_sizes:
 2226:             if size != dim_sizes[dim]:
 2227:                 raise ValueError(
 2228:                     'inconsistent size for core dimension %r: %r vs %r'
 2229:                     % (dim, size, dim_sizes[dim]))
 2230:         else:
 2231:             dim_sizes[dim] = size
 2232: 
 2233: 
 2234: def _parse_input_dimensions(args, input_core_dims):
 2235:     """
 2236:     Parse broadcast and core dimensions for vectorize with a signature.
 2237: 
 2238:     Arguments
 2239:     ---------
 2240:     args : Tuple[ndarray, ...]
 2241:         Tuple of input arguments to examine.
 2242:     input_core_dims : List[Tuple[str, ...]]
 2243:         List of core dimensions corresponding to each input.
 2244: 
 2245:     Returns
 2246:     -------
 2247:     broadcast_shape : Tuple[int, ...]
 2248:         Common shape to broadcast all non-core dimensions to.
 2249:     dim_sizes : Dict[str, int]
 2250:         Common sizes for named core dimensions.
 2251:     """
 2252:     broadcast_args = []
 2253:     dim_sizes = {}
 2254:     for arg, core_dims in zip(args, input_core_dims):
 2255:         _update_dim_sizes(dim_sizes, arg, core_dims)
 2256:         ndim = arg.ndim - len(core_dims)
 2257:         dummy_array = np.lib.stride_tricks.as_strided(0, arg.shape[:ndim])
 2258:         broadcast_args.append(dummy_array)
 2259:     broadcast_shape = np.lib._stride_tricks_impl._broadcast_shape(
 2260:         *broadcast_args
 2261:     )
 2262:     return broadcast_shape, dim_sizes
 2263: 
 2264: 
 2265: def _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims):
 2266:     """Helper for calculating broadcast shapes with core dimensions."""
 2267:     return [broadcast_shape + tuple(dim_sizes[dim] for dim in core_dims)
 2268:             for core_dims in list_of_core_dims]
 2269: 
 2270: 
 2271: def _create_arrays(broadcast_shape, dim_sizes, list_of_core_dims, dtypes,
 2272:                    results=None):
 2273:     """Helper for creating output arrays in vectorize."""
 2274:     shapes = _calculate_shapes(broadcast_shape, dim_sizes, list_of_core_dims)
 2275:     if dtypes is None:
 2276:         dtypes = [None] * len(shapes)
 2277:     if results is None:
 2278:         arrays = tuple(np.empty(shape=shape, dtype=dtype)
 2279:                        for shape, dtype in zip(shapes, dtypes))
 2280:     else:
 2281:         arrays = tuple(np.empty_like(result, shape=shape, dtype=dtype)
 2282:                        for result, shape, dtype
 2283:                        in zip(results, shapes, dtypes))
 2284:     return arrays
 2285: 
 2286: 
 2287: def _get_vectorize_dtype(dtype):
 2288:     if dtype.char in "SU":
 2289:         return dtype.char
 2290:     return dtype
 2291: 
 2292: 
 2293: @set_module('numpy')
 2294: class vectorize:
 2295:     """
 2296:     vectorize(pyfunc=np._NoValue, otypes=None, doc=None, excluded=None,
 2297:     cache=False, signature=None)
 2298: 
 2299:     Returns an object that acts like pyfunc, but takes arrays as input.
 2300: 
 2301:     Define a vectorized function which takes a nested sequence of objects or
 2302:     numpy arrays as inputs and returns a single numpy array or a tuple of numpy
 2303:     arrays. The vectorized function evaluates `pyfunc` over successive tuples
 2304:     of the input arrays like the python map function, except it uses the
 2305:     broadcasting rules of numpy.
 2306: 
 2307:     The data type of the output of `vectorized` is determined by calling
 2308:     the function with the first element of the input.  This can be avoided
 2309:     by specifying the `otypes` argument.
 2310: 
 2311:     Parameters
 2312:     ----------
 2313:     pyfunc : callable, optional
 2314:         A python function or method.
 2315:         Can be omitted to produce a decorator with keyword arguments.
 2316:     otypes : str or list of dtypes, optional
 2317:         The output data type. It must be specified as either a string of
 2318:         typecode characters or a list of data type specifiers. There should
 2319:         be one data type specifier for each output.
 2320:     doc : str, optional
 2321:         The docstring for the function. If None, the docstring will be the
 2322:         ``pyfunc.__doc__``.
 2323:     excluded : set, optional
 2324:         Set of strings or integers representing the positional or keyword
 2325:         arguments for which the function will not be vectorized. These will be
 2326:         passed directly to `pyfunc` unmodified.
 2327: 
 2328:     cache : bool, optional
 2329:         If `True`, then cache the first function call that determines the number
 2330:         of outputs if `otypes` is not provided.
 2331: 
 2332:     signature : string, optional
 2333:         Generalized universal function signature, e.g., ``(m,n),(n)->(m)`` for
 2334:         vectorized matrix-vector multiplication. If provided, ``pyfunc`` will
 2335:         be called with (and expected to return) arrays with shapes given by the
 2336:         size of corresponding core dimensions. By default, ``pyfunc`` is
 2337:         assumed to take scalars as input and output.
 2338: 
 2339:     Returns
 2340:     -------
 2341:     out : callable
 2342:         A vectorized function if ``pyfunc`` was provided,
 2343:         a decorator otherwise.
 2344: 
 2345:     See Also
 2346:     --------
 2347:     frompyfunc : Takes an arbitrary Python function and returns a ufunc
 2348: 
 2349:     Notes
 2350:     -----
 2351:     The `vectorize` function is provided primarily for convenience, not for
 2352:     performance. The implementation is essentially a for loop.
 2353: 
 2354:     If `otypes` is not specified, then a call to the function with the
 2355:     first argument will be used to determine the number of outputs.  The
 2356:     results of this call will be cached if `cache` is `True` to prevent
 2357:     calling the function twice.  However, to implement the cache, the
 2358:     original function must be wrapped which will slow down subsequent
 2359:     calls, so only do this if your function is expensive.
 2360: 
 2361:     The new keyword argument interface and `excluded` argument support
 2362:     further degrades performance.
 2363: 
 2364:     References
 2365:     ----------
 2366:     .. [1] :doc:`/reference/c-api/generalized-ufuncs`
 2367: 
 2368:     Examples
 2369:     --------
 2370:     >>> import numpy as np
 2371:     >>> def myfunc(a, b):
 2372:     ...     "Return a-b if a>b, otherwise return a+b"
 2373:     ...     if a > b:
 2374:     ...         return a - b
 2375:     ...     else:
 2376:     ...         return a + b
 2377: 
 2378:     >>> vfunc = np.vectorize(myfunc)
 2379:     >>> vfunc([1, 2, 3, 4], 2)
 2380:     array([3, 4, 1, 2])
 2381: 
 2382:     The docstring is taken from the input function to `vectorize` unless it
 2383:     is specified:
 2384: 
 2385:     >>> vfunc.__doc__
 2386:     'Return a-b if a>b, otherwise return a+b'
 2387:     >>> vfunc = np.vectorize(myfunc, doc='Vectorized `myfunc`')
 2388:     >>> vfunc.__doc__
 2389:     'Vectorized `myfunc`'
 2390: 
 2391:     The output type is determined by evaluating the first element of the input,
 2392:     unless it is specified:
 2393: 
 2394:     >>> out = vfunc([1, 2, 3, 4], 2)
 2395:     >>> type(out[0])
 2396:     <class 'numpy.int64'>
 2397:     >>> vfunc = np.vectorize(myfunc, otypes=[float])
 2398:     >>> out = vfunc([1, 2, 3, 4], 2)
 2399:     >>> type(out[0])
 2400:     <class 'numpy.float64'>
 2401: 
 2402:     The `excluded` argument can be used to prevent vectorizing over certain
 2403:     arguments.  This can be useful for array-like arguments of a fixed length
 2404:     such as the coefficients for a polynomial as in `polyval`:
 2405: 
 2406:     >>> def mypolyval(p, x):
 2407:     ...     _p = list(p)
 2408:     ...     res = _p.pop(0)
 2409:     ...     while _p:
 2410:     ...         res = res*x + _p.pop(0)
 2411:     ...     return res
 2412: 
 2413:     Here, we exclude the zeroth argument from vectorization whether it is
 2414:     passed by position or keyword.
 2415: 
 2416:     >>> vpolyval = np.vectorize(mypolyval, excluded={0, 'p'})
 2417:     >>> vpolyval([1, 2, 3], x=[0, 1])
 2418:     array([3, 6])
 2419:     >>> vpolyval(p=[1, 2, 3], x=[0, 1])
 2420:     array([3, 6])
 2421: 
 2422:     The `signature` argument allows for vectorizing functions that act on
 2423:     non-scalar arrays of fixed length. For example, you can use it for a
 2424:     vectorized calculation of Pearson correlation coefficient and its p-value:
 2425: 
 2426:     >>> import scipy.stats
 2427:     >>> pearsonr = np.vectorize(scipy.stats.pearsonr,
 2428:     ...                 signature='(n),(n)->(),()')
 2429:     >>> pearsonr([[0, 1, 2, 3]], [[1, 2, 3, 4], [4, 3, 2, 1]])
 2430:     (array([ 1., -1.]), array([ 0.,  0.]))
 2431: 
 2432:     Or for a vectorized convolution:
 2433: 
 2434:     >>> convolve = np.vectorize(np.convolve, signature='(n),(m)->(k)')
 2435:     >>> convolve(np.eye(4), [1, 2, 1])
 2436:     array([[1., 2., 1., 0., 0., 0.],
 2437:            [0., 1., 2., 1., 0., 0.],
 2438:            [0., 0., 1., 2., 1., 0.],
 2439:            [0., 0., 0., 1., 2., 1.]])
 2440: 
 2441:     Decorator syntax is supported.  The decorator can be called as
 2442:     a function to provide keyword arguments:
 2443: 
 2444:     >>> @np.vectorize
 2445:     ... def identity(x):
 2446:     ...     return x
 2447:     ...
 2448:     >>> identity([0, 1, 2])
 2449:     array([0, 1, 2])
 2450:     >>> @np.vectorize(otypes=[float])
 2451:     ... def as_float(x):
 2452:     ...     return x
 2453:     ...
 2454:     >>> as_float([0, 1, 2])
 2455:     array([0., 1., 2.])
 2456:     """
 2457:     def __init__(self, pyfunc=np._NoValue, otypes=None, doc=None,
 2458:                  excluded=None, cache=False, signature=None):
 2459: 
 2460:         if (pyfunc != np._NoValue) and (not callable(pyfunc)):
 2461:             # Splitting the error message to keep
 2462:             # the length below 79 characters.
 2463:             part1 = "When used as a decorator, "
 2464:             part2 = "only accepts keyword arguments."
 2465:             raise TypeError(part1 + part2)
 2466: 
 2467:         self.pyfunc = pyfunc
 2468:         self.cache = cache
 2469:         self.signature = signature
 2470:         if pyfunc != np._NoValue and hasattr(pyfunc, '__name__'):
 2471:             self.__name__ = pyfunc.__name__
 2472: 
 2473:         self._ufunc = {}    # Caching to improve default performance
 2474:         self._doc = None
 2475:         self.__doc__ = doc
 2476:         if doc is None and hasattr(pyfunc, '__doc__'):
 2477:             self.__doc__ = pyfunc.__doc__
 2478:         else:
 2479:             self._doc = doc
 2480: 
 2481:         if isinstance(otypes, str):
 2482:             for char in otypes:
 2483:                 if char not in typecodes['All']:
 2484:                     raise ValueError(f"Invalid otype specified: {char}")
 2485:         elif iterable(otypes):
 2486:             otypes = [_get_vectorize_dtype(_nx.dtype(x)) for x in otypes]
 2487:         elif otypes is not None:
 2488:             raise ValueError("Invalid otype specification")
 2489:         self.otypes = otypes
 2490: 
 2491:         # Excluded variable support
 2492:         if excluded is None:
 2493:             excluded = set()
 2494:         self.excluded = set(excluded)
 2495: 
 2496:         if signature is not None:
 2497:             self._in_and_out_core_dims = _parse_gufunc_signature(signature)
 2498:         else:
 2499:             self._in_and_out_core_dims = None
 2500: 
 2501:     def _init_stage_2(self, pyfunc, *args, **kwargs):
 2502:         self.__name__ = pyfunc.__name__
 2503:         self.pyfunc = pyfunc
 2504:         if self._doc is None:
 2505:             self.__doc__ = pyfunc.__doc__
 2506:         else:
 2507:             self.__doc__ = self._doc
 2508: 
 2509:     def _call_as_normal(self, *args, **kwargs):
 2510:         """
 2511:         Return arrays with the results of `pyfunc` broadcast (vectorized) over
 2512:         `args` and `kwargs` not in `excluded`.
 2513:         """
 2514:         excluded = self.excluded
 2515:         if not kwargs and not excluded:
 2516:             func = self.pyfunc
 2517:             vargs = args
 2518:         else:
 2519:             # The wrapper accepts only positional arguments: we use `names` and
 2520:             # `inds` to mutate `the_args` and `kwargs` to pass to the original
 2521:             # function.
 2522:             nargs = len(args)
 2523: 
 2524:             names = [_n for _n in kwargs if _n not in excluded]
 2525:             inds = [_i for _i in range(nargs) if _i not in excluded]
 2526:             the_args = list(args)
 2527: 
 2528:             def func(*vargs):
 2529:                 for _n, _i in enumerate(inds):
 2530:                     the_args[_i] = vargs[_n]
 2531:                 kwargs.update(zip(names, vargs[len(inds):]))
 2532:                 return self.pyfunc(*the_args, **kwargs)
 2533: 
 2534:             vargs = [args[_i] for _i in inds]
 2535:             vargs.extend([kwargs[_n] for _n in names])
 2536: 
 2537:         return self._vectorize_call(func=func, args=vargs)
 2538: 
 2539:     def __call__(self, *args, **kwargs):
 2540:         if self.pyfunc is np._NoValue:
 2541:             self._init_stage_2(*args, **kwargs)
 2542:             return self
 2543: 
 2544:         return self._call_as_normal(*args, **kwargs)
 2545: 
 2546:     def _get_ufunc_and_otypes(self, func, args):
 2547:         """Return (ufunc, otypes)."""
 2548:         # frompyfunc will fail if args is empty
 2549:         if not args:
 2550:             raise ValueError('args can not be empty')
 2551: 
 2552:         if self.otypes is not None:
 2553:             otypes = self.otypes
 2554: 
 2555:             # self._ufunc is a dictionary whose keys are the number of
 2556:             # arguments (i.e. len(args)) and whose values are ufuncs created
 2557:             # by frompyfunc. len(args) can be different for different calls if
 2558:             # self.pyfunc has parameters with default values.  We only use the
 2559:             # cache when func is self.pyfunc, which occurs when the call uses
 2560:             # only positional arguments and no arguments are excluded.
 2561: 
 2562:             nin = len(args)
 2563:             nout = len(self.otypes)
 2564:             if func is not self.pyfunc or nin not in self._ufunc:
 2565:                 ufunc = frompyfunc(func, nin, nout)
 2566:             else:
 2567:                 ufunc = None  # We'll get it from self._ufunc
 2568:             if func is self.pyfunc:
 2569:                 ufunc = self._ufunc.setdefault(nin, ufunc)
 2570:         else:
 2571:             # Get number of outputs and output types by calling the function on
 2572:             # the first entries of args.  We also cache the result to prevent
 2573:             # the subsequent call when the ufunc is evaluated.
 2574:             # Assumes that ufunc first evaluates the 0th elements in the input
 2575:             # arrays (the input values are not checked to ensure this)
 2576:             args = [asarray(a) for a in args]
 2577:             if builtins.any(arg.size == 0 for arg in args):
 2578:                 raise ValueError('cannot call `vectorize` on size 0 inputs '
 2579:                                  'unless `otypes` is set')
 2580: 
 2581:             inputs = [arg.flat[0] for arg in args]
 2582:             outputs = func(*inputs)
 2583: 
 2584:             # Performance note: profiling indicates that -- for simple
 2585:             # functions at least -- this wrapping can almost double the
 2586:             # execution time.
 2587:             # Hence we make it optional.
 2588:             if self.cache:
 2589:                 _cache = [outputs]
 2590: 
 2591:                 def _func(*vargs):
 2592:                     if _cache:
 2593:                         return _cache.pop()
 2594:                     else:
 2595:                         return func(*vargs)
 2596:             else:
 2597:                 _func = func
 2598: 
 2599:             if isinstance(outputs, tuple):
 2600:                 nout = len(outputs)
 2601:             else:
 2602:                 nout = 1
 2603:                 outputs = (outputs,)
 2604: 
 2605:             otypes = ''.join([asarray(outputs[_k]).dtype.char
 2606:                               for _k in range(nout)])
 2607: 
 2608:             # Performance note: profiling indicates that creating the ufunc is
 2609:             # not a significant cost compared with wrapping so it seems not
 2610:             # worth trying to cache this.
 2611:             ufunc = frompyfunc(_func, len(args), nout)
 2612: 
 2613:         return ufunc, otypes
 2614: 
 2615:     def _vectorize_call(self, func, args):
 2616:         """Vectorized call to `func` over positional `args`."""
 2617:         if self.signature is not None:
 2618:             res = self._vectorize_call_with_signature(func, args)
 2619:         elif not args:
 2620:             res = func()
 2621:         else:
 2622:             ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)
 2623:             # gh-29196: `dtype=object` should eventually be removed
 2624:             args = [asanyarray(a, dtype=object) for a in args]
 2625:             outputs = ufunc(*args, out=...)
 2626: 
 2627:             if ufunc.nout == 1:
 2628:                 res = asanyarray(outputs, dtype=otypes[0])
 2629:             else:
 2630:                 res = tuple(asanyarray(x, dtype=t)
 2631:                             for x, t in zip(outputs, otypes))
 2632:         return res
 2633: 
 2634:     def _vectorize_call_with_signature(self, func, args):
 2635:         """Vectorized call over positional arguments with a signature."""
 2636:         input_core_dims, output_core_dims = self._in_and_out_core_dims
 2637: 
 2638:         if len(args) != len(input_core_dims):
 2639:             raise TypeError('wrong number of positional arguments: '
 2640:                             'expected %r, got %r'
 2641:                             % (len(input_core_dims), len(args)))
 2642:         args = tuple(asanyarray(arg) for arg in args)
 2643: 
 2644:         broadcast_shape, dim_sizes = _parse_input_dimensions(
 2645:             args, input_core_dims)
 2646:         input_shapes = _calculate_shapes(broadcast_shape, dim_sizes,
 2647:                                          input_core_dims)
 2648:         args = [np.broadcast_to(arg, shape, subok=True)
 2649:                 for arg, shape in zip(args, input_shapes)]
 2650: 
 2651:         outputs = None
 2652:         otypes = self.otypes
 2653:         nout = len(output_core_dims)
 2654: 
 2655:         for index in np.ndindex(*broadcast_shape):
 2656:             results = func(*(arg[index] for arg in args))
 2657: 
 2658:             n_results = len(results) if isinstance(results, tuple) else 1
 2659: 
 2660:             if nout != n_results:
 2661:                 raise ValueError(
 2662:                     'wrong number of outputs from pyfunc: expected %r, got %r'
 2663:                     % (nout, n_results))
 2664: 
 2665:             if nout == 1:
 2666:                 results = (results,)
 2667: 
 2668:             if outputs is None:
 2669:                 for result, core_dims in zip(results, output_core_dims):
 2670:                     _update_dim_sizes(dim_sizes, result, core_dims)
 2671: 
 2672:                 outputs = _create_arrays(broadcast_shape, dim_sizes,
 2673:                                          output_core_dims, otypes, results)
 2674: 
 2675:             for output, result in zip(outputs, results):
 2676:                 output[index] = result
 2677: 
 2678:         if outputs is None:
 2679:             # did not call the function even once
 2680:             if otypes is None:
 2681:                 raise ValueError('cannot call `vectorize` on size 0 inputs '
 2682:                                  'unless `otypes` is set')
 2683:             if builtins.any(dim not in dim_sizes
 2684:                             for dims in output_core_dims
 2685:                             for dim in dims):
 2686:                 raise ValueError('cannot call `vectorize` with a signature '
 2687:                                  'including new output dimensions on size 0 '
 2688:                                  'inputs')
 2689:             outputs = _create_arrays(broadcast_shape, dim_sizes,
 2690:                                      output_core_dims, otypes)
 2691: 
 2692:         return outputs[0] if nout == 1 else outputs
 2693: 
 2694: 
 2695: def _cov_dispatcher(m, y=None, rowvar=None, bias=None, ddof=None,
 2696:                     fweights=None, aweights=None, *, dtype=None):
 2697:     return (m, y, fweights, aweights)
 2698: 
 2699: 
 2700: @array_function_dispatch(_cov_dispatcher)
 2701: def cov(m, y=None, rowvar=True, bias=False, ddof=None, fweights=None,
 2702:         aweights=None, *, dtype=None):
 2703:     """
 2704:     Estimate a covariance matrix, given data and weights.
 2705: 
 2706:     Covariance indicates the level to which two variables vary together.
 2707:     If we examine N-dimensional samples, :math:`X = [x_1, x_2, ... x_N]^T`,
 2708:     then the covariance matrix element :math:`C_{ij}` is the covariance of
 2709:     :math:`x_i` and :math:`x_j`. The element :math:`C_{ii}` is the variance
 2710:     of :math:`x_i`.
 2711: 
 2712:     See the notes for an outline of the algorithm.
 2713: 
 2714:     Parameters
 2715:     ----------
 2716:     m : array_like
 2717:         A 1-D or 2-D array containing multiple variables and observations.
 2718:         Each row of `m` represents a variable, and each column a single
 2719:         observation of all those variables. Also see `rowvar` below.
 2720:     y : array_like, optional
 2721:         An additional set of variables and observations. `y` has the same form
 2722:         as that of `m`.
 2723:     rowvar : bool, optional
 2724:         If `rowvar` is True (default), then each row represents a
 2725:         variable, with observations in the columns. Otherwise, the relationship
 2726:         is transposed: each column represents a variable, while the rows
 2727:         contain observations.
 2728:     bias : bool, optional
 2729:         Default normalization (False) is by ``(N - 1)``, where ``N`` is the
 2730:         number of observations given (unbiased estimate). If `bias` is True,
 2731:         then normalization is by ``N``. These values can be overridden by using
 2732:         the keyword ``ddof`` in numpy versions >= 1.5.
 2733:     ddof : int, optional
 2734:         If not ``None`` the default value implied by `bias` is overridden.
 2735:         Note that ``ddof=1`` will return the unbiased estimate, even if both
 2736:         `fweights` and `aweights` are specified, and ``ddof=0`` will return
 2737:         the simple average. See the notes for the details. The default value
 2738:         is ``None``.
 2739:     fweights : array_like, int, optional
 2740:         1-D array of integer frequency weights; the number of times each
 2741:         observation vector should be repeated.
 2742:     aweights : array_like, optional
 2743:         1-D array of observation vector weights. These relative weights are
 2744:         typically large for observations considered "important" and smaller for
 2745:         observations considered less "important". If ``ddof=0`` the array of
 2746:         weights can be used to assign probabilities to observation vectors.
 2747:     dtype : data-type, optional
 2748:         Data-type of the result. By default, the return data-type will have
 2749:         at least `numpy.float64` precision.
 2750: 
 2751:         .. versionadded:: 1.20
 2752: 
 2753:     Returns
 2754:     -------
 2755:     out : ndarray
 2756:         The covariance matrix of the variables.
 2757: 
 2758:     See Also
 2759:     --------
 2760:     corrcoef : Normalized covariance matrix
 2761: 
 2762:     Notes
 2763:     -----
 2764:     Assume that the observations are in the columns of the observation
 2765:     array `m` and let ``f = fweights`` and ``a = aweights`` for brevity. The
 2766:     steps to compute the weighted covariance are as follows::
 2767: 
 2768:         >>> m = np.arange(10, dtype=np.float64)
 2769:         >>> f = np.arange(10) * 2
 2770:         >>> a = np.arange(10) ** 2.
 2771:         >>> ddof = 1
 2772:         >>> w = f * a
 2773:         >>> v1 = np.sum(w)
 2774:         >>> v2 = np.sum(w * a)
 2775:         >>> m -= np.sum(m * w, axis=None, keepdims=True) / v1
 2776:         >>> cov = np.dot(m * w, m.T) * v1 / (v1**2 - ddof * v2)
 2777: 
 2778:     Note that when ``a == 1``, the normalization factor
 2779:     ``v1 / (v1**2 - ddof * v2)`` goes over to ``1 / (np.sum(f) - ddof)``
 2780:     as it should.
 2781: 
 2782:     Examples
 2783:     --------
 2784:     >>> import numpy as np
 2785: 
 2786:     Consider two variables, :math:`x_0` and :math:`x_1`, which
 2787:     correlate perfectly, but in opposite directions:
 2788: 
 2789:     >>> x = np.array([[0, 2], [1, 1], [2, 0]]).T
 2790:     >>> x
 2791:     array([[0, 1, 2],
 2792:            [2, 1, 0]])
 2793: 
 2794:     Note how :math:`x_0` increases while :math:`x_1` decreases. The covariance
 2795:     matrix shows this clearly:
 2796: 
 2797:     >>> np.cov(x)
 2798:     array([[ 1., -1.],
 2799:            [-1.,  1.]])
 2800: 
 2801:     Note that element :math:`C_{0,1}`, which shows the correlation between
 2802:     :math:`x_0` and :math:`x_1`, is negative.
 2803: 
 2804:     Further, note how `x` and `y` are combined:
 2805: 
 2806:     >>> x = [-2.1, -1,  4.3]
 2807:     >>> y = [3,  1.1,  0.12]
 2808:     >>> X = np.stack((x, y), axis=0)
 2809:     >>> np.cov(X)
 2810:     array([[11.71      , -4.286     ], # may vary
 2811:            [-4.286     ,  2.144133]])
 2812:     >>> np.cov(x, y)
 2813:     array([[11.71      , -4.286     ], # may vary
 2814:            [-4.286     ,  2.144133]])
 2815:     >>> np.cov(x)
 2816:     array(11.71)
 2817: 
 2818:     """
 2819:     # Check inputs
 2820:     if ddof is not None and ddof != int(ddof):
 2821:         raise ValueError(
 2822:             "ddof must be integer")
 2823: 
 2824:     # Handles complex arrays too
 2825:     m = np.asarray(m)
 2826:     if m.ndim > 2:
 2827:         raise ValueError("m has more than 2 dimensions")
 2828: 
 2829:     if y is not None:
 2830:         y = np.asarray(y)
 2831:         if y.ndim > 2:
 2832:             raise ValueError("y has more than 2 dimensions")
 2833: 
 2834:     if dtype is None:
 2835:         if y is None:
 2836:             dtype = np.result_type(m, np.float64)
 2837:         else:
 2838:             dtype = np.result_type(m, y, np.float64)
 2839: 
 2840:     X = array(m, ndmin=2, dtype=dtype)
 2841:     if not rowvar and m.ndim != 1:
 2842:         X = X.T
 2843:     if X.shape[0] == 0:
 2844:         return np.array([]).reshape(0, 0)
 2845:     if y is not None:
 2846:         y = array(y, copy=None, ndmin=2, dtype=dtype)
 2847:         if not rowvar and y.shape[0] != 1:
 2848:             y = y.T
 2849:         X = np.concatenate((X, y), axis=0)
 2850: 
 2851:     if ddof is None:
 2852:         if bias == 0:
 2853:             ddof = 1
 2854:         else:
 2855:             ddof = 0
 2856: 
 2857:     # Get the product of frequencies and weights
 2858:     w = None
 2859:     if fweights is not None:
 2860:         fweights = np.asarray(fweights, dtype=float)
 2861:         if not np.all(fweights == np.around(fweights)):
 2862:             raise TypeError(
 2863:                 "fweights must be integer")
 2864:         if fweights.ndim > 1:
 2865:             raise RuntimeError(
 2866:                 "cannot handle multidimensional fweights")
 2867:         if fweights.shape[0] != X.shape[1]:
 2868:             raise RuntimeError(
 2869:                 "incompatible numbers of samples and fweights")
 2870:         if any(fweights < 0):
 2871:             raise ValueError(
 2872:                 "fweights cannot be negative")
 2873:         w = fweights
 2874:     if aweights is not None:
 2875:         aweights = np.asarray(aweights, dtype=float)
 2876:         if aweights.ndim > 1:
 2877:             raise RuntimeError(
 2878:                 "cannot handle multidimensional aweights")
 2879:         if aweights.shape[0] != X.shape[1]:
 2880:             raise RuntimeError(
 2881:                 "incompatible numbers of samples and aweights")
 2882:         if any(aweights < 0):
 2883:             raise ValueError(
 2884:                 "aweights cannot be negative")
 2885:         if w is None:
 2886:             w = aweights
 2887:         else:
 2888:             w *= aweights
 2889: 
 2890:     avg, w_sum = average(X, axis=1, weights=w, returned=True)
 2891:     w_sum = w_sum[0]
 2892: 
 2893:     # Determine the normalization
 2894:     if w is None:
 2895:         fact = X.shape[1] - ddof
 2896:     elif ddof == 0:
 2897:         fact = w_sum
 2898:     elif aweights is None:
 2899:         fact = w_sum - ddof
 2900:     else:
 2901:         fact = w_sum - ddof * sum(w * aweights) / w_sum
 2902: 
 2903:     if fact <= 0:
 2904:         warnings.warn("Degrees of freedom <= 0 for slice",
 2905:                       RuntimeWarning, stacklevel=2)
 2906:         fact = 0.0
 2907: 
 2908:     X -= avg[:, None]
 2909:     if w is None:
 2910:         X_T = X.T
 2911:     else:
 2912:         X_T = (X * w).T
 2913:     c = dot(X, X_T.conj())
 2914:     c *= np.true_divide(1, fact)
 2915:     return c.squeeze()
 2916: 
 2917: 
 2918: def _corrcoef_dispatcher(x, y=None, rowvar=None, bias=None, ddof=None, *,
 2919:                          dtype=None):
 2920:     return (x, y)
 2921: 
 2922: 
 2923: @array_function_dispatch(_corrcoef_dispatcher)
 2924: def corrcoef(x, y=None, rowvar=True, bias=np._NoValue, ddof=np._NoValue, *,
 2925:              dtype=None):
 2926:     """
 2927:     Return Pearson product-moment correlation coefficients.
 2928: 
 2929:     Please refer to the documentation for `cov` for more detail.  The
 2930:     relationship between the correlation coefficient matrix, `R`, and the
 2931:     covariance matrix, `C`, is
 2932: 
 2933:     .. math:: R_{ij} = \\frac{ C_{ij} } { \\sqrt{ C_{ii} C_{jj} } }
 2934: 
 2935:     The values of `R` are between -1 and 1, inclusive.
 2936: 
 2937:     Parameters
 2938:     ----------
 2939:     x : array_like
 2940:         A 1-D or 2-D array containing multiple variables and observations.
 2941:         Each row of `x` represents a variable, and each column a single
 2942:         observation of all those variables. Also see `rowvar` below.
 2943:     y : array_like, optional
 2944:         An additional set of variables and observations. `y` has the same
 2945:         shape as `x`.
 2946:     rowvar : bool, optional
 2947:         If `rowvar` is True (default), then each row represents a
 2948:         variable, with observations in the columns. Otherwise, the relationship
 2949:         is transposed: each column represents a variable, while the rows
 2950:         contain observations.
 2951:     bias : _NoValue, optional
 2952:         Has no effect, do not use.
 2953: 
 2954:         .. deprecated:: 1.10.0
 2955:     ddof : _NoValue, optional
 2956:         Has no effect, do not use.
 2957: 
 2958:         .. deprecated:: 1.10.0
 2959:     dtype : data-type, optional
 2960:         Data-type of the result. By default, the return data-type will have
 2961:         at least `numpy.float64` precision.
 2962: 
 2963:         .. versionadded:: 1.20
 2964: 
 2965:     Returns
 2966:     -------
 2967:     R : ndarray
 2968:         The correlation coefficient matrix of the variables.
 2969: 
 2970:     See Also
 2971:     --------
 2972:     cov : Covariance matrix
 2973: 
 2974:     Notes
 2975:     -----
 2976:     Due to floating point rounding the resulting array may not be Hermitian,
 2977:     the diagonal elements may not be 1, and the elements may not satisfy the
 2978:     inequality abs(a) <= 1. The real and imaginary parts are clipped to the
 2979:     interval [-1,  1] in an attempt to improve on that situation but is not
 2980:     much help in the complex case.
 2981: 
 2982:     This function accepts but discards arguments `bias` and `ddof`.  This is
 2983:     for backwards compatibility with previous versions of this function.  These
 2984:     arguments had no effect on the return values of the function and can be
 2985:     safely ignored in this and previous versions of numpy.
 2986: 
 2987:     Examples
 2988:     --------
 2989:     >>> import numpy as np
 2990: 
 2991:     In this example we generate two random arrays, ``xarr`` and ``yarr``, and
 2992:     compute the row-wise and column-wise Pearson correlation coefficients,
 2993:     ``R``. Since ``rowvar`` is  true by  default, we first find the row-wise
 2994:     Pearson correlation coefficients between the variables of ``xarr``.
 2995: 
 2996:     >>> import numpy as np
 2997:     >>> rng = np.random.default_rng(seed=42)
 2998:     >>> xarr = rng.random((3, 3))
 2999:     >>> xarr
 3000:     array([[0.77395605, 0.43887844, 0.85859792],
 3001:            [0.69736803, 0.09417735, 0.97562235],
 3002:            [0.7611397 , 0.78606431, 0.12811363]])
 3003:     >>> R1 = np.corrcoef(xarr)
 3004:     >>> R1
 3005:     array([[ 1.        ,  0.99256089, -0.68080986],
 3006:            [ 0.99256089,  1.        , -0.76492172],
 3007:            [-0.68080986, -0.76492172,  1.        ]])
 3008: 
 3009:     If we add another set of variables and observations ``yarr``, we can
 3010:     compute the row-wise Pearson correlation coefficients between the
 3011:     variables in ``xarr`` and ``yarr``.
 3012: 
 3013:     >>> yarr = rng.random((3, 3))
 3014:     >>> yarr
 3015:     array([[0.45038594, 0.37079802, 0.92676499],
 3016:            [0.64386512, 0.82276161, 0.4434142 ],
 3017:            [0.22723872, 0.55458479, 0.06381726]])
 3018:     >>> R2 = np.corrcoef(xarr, yarr)
 3019:     >>> R2
 3020:     array([[ 1.        ,  0.99256089, -0.68080986,  0.75008178, -0.934284  ,
 3021:             -0.99004057],
 3022:            [ 0.99256089,  1.        , -0.76492172,  0.82502011, -0.97074098,
 3023:             -0.99981569],
 3024:            [-0.68080986, -0.76492172,  1.        , -0.99507202,  0.89721355,
 3025:              0.77714685],
 3026:            [ 0.75008178,  0.82502011, -0.99507202,  1.        , -0.93657855,
 3027:             -0.83571711],
 3028:            [-0.934284  , -0.97074098,  0.89721355, -0.93657855,  1.        ,
 3029:              0.97517215],
 3030:            [-0.99004057, -0.99981569,  0.77714685, -0.83571711,  0.97517215,
 3031:              1.        ]])
 3032: 
 3033:     Finally if we use the option ``rowvar=False``, the columns are now
 3034:     being treated as the variables and we will find the column-wise Pearson
 3035:     correlation coefficients between variables in ``xarr`` and ``yarr``.
 3036: 
 3037:     >>> R3 = np.corrcoef(xarr, yarr, rowvar=False)
 3038:     >>> R3
 3039:     array([[ 1.        ,  0.77598074, -0.47458546, -0.75078643, -0.9665554 ,
 3040:              0.22423734],
 3041:            [ 0.77598074,  1.        , -0.92346708, -0.99923895, -0.58826587,
 3042:             -0.44069024],
 3043:            [-0.47458546, -0.92346708,  1.        ,  0.93773029,  0.23297648,
 3044:              0.75137473],
 3045:            [-0.75078643, -0.99923895,  0.93773029,  1.        ,  0.55627469,
 3046:              0.47536961],
 3047:            [-0.9665554 , -0.58826587,  0.23297648,  0.55627469,  1.        ,
 3048:             -0.46666491],
 3049:            [ 0.22423734, -0.44069024,  0.75137473,  0.47536961, -0.46666491,
 3050:              1.        ]])
 3051: 
 3052:     """
 3053:     if bias is not np._NoValue or ddof is not np._NoValue:
 3054:         # 2015-03-15, 1.10
 3055:         warnings.warn('bias and ddof have no effect and are deprecated',
 3056:                       DeprecationWarning, stacklevel=2)
 3057:     c = cov(x, y, rowvar, dtype=dtype)
 3058:     try:
 3059:         d = diag(c)
 3060:     except ValueError:
 3061:         # scalar covariance
 3062:         # nan if incorrect value (nan, inf, 0), 1 otherwise
 3063:         return c / c
 3064:     stddev = sqrt(d.real)
 3065:     c /= stddev[:, None]
 3066:     c /= stddev[None, :]
 3067: 
 3068:     # Clip real and imaginary parts to [-1, 1].  This does not guarantee
 3069:     # abs(a[i,j]) <= 1 for complex arrays, but is the best we can do without
 3070:     # excessive work.
 3071:     np.clip(c.real, -1, 1, out=c.real)
 3072:     if np.iscomplexobj(c):
 3073:         np.clip(c.imag, -1, 1, out=c.imag)
 3074: 
 3075:     return c
 3076: 
 3077: 
 3078: @set_module('numpy')
 3079: def blackman(M):
 3080:     """
 3081:     Return the Blackman window.
 3082: 
 3083:     The Blackman window is a taper formed by using the first three
 3084:     terms of a summation of cosines. It was designed to have close to the
 3085:     minimal leakage possible.  It is close to optimal, only slightly worse
 3086:     than a Kaiser window.
 3087: 
 3088:     Parameters
 3089:     ----------
 3090:     M : int
 3091:         Number of points in the output window. If zero or less, an empty
 3092:         array is returned.
 3093: 
 3094:     Returns
 3095:     -------
 3096:     out : ndarray
 3097:         The window, with the maximum value normalized to one (the value one
 3098:         appears only if the number of samples is odd).
 3099: 
 3100:     See Also
 3101:     --------
 3102:     bartlett, hamming, hanning, kaiser
 3103: 
 3104:     Notes
 3105:     -----
 3106:     The Blackman window is defined as
 3107: 
 3108:     .. math::  w(n) = 0.42 - 0.5 \\cos(2\\pi n/M) + 0.08 \\cos(4\\pi n/M)
 3109: 
 3110:     Most references to the Blackman window come from the signal processing
 3111:     literature, where it is used as one of many windowing functions for
 3112:     smoothing values.  It is also known as an apodization (which means
 3113:     "removing the foot", i.e. smoothing discontinuities at the beginning
 3114:     and end of the sampled signal) or tapering function. It is known as a
 3115:     "near optimal" tapering function, almost as good (by some measures)
 3116:     as the kaiser window.
 3117: 
 3118:     References
 3119:     ----------
 3120:     Blackman, R.B. and Tukey, J.W., (1958) The measurement of power spectra,
 3121:     Dover Publications, New York.
 3122: 
 3123:     Oppenheim, A.V., and R.W. Schafer. Discrete-Time Signal Processing.
 3124:     Upper Saddle River, NJ: Prentice-Hall, 1999, pp. 468-471.
 3125: 
 3126:     Examples
 3127:     --------
 3128:     >>> import numpy as np
 3129:     >>> import matplotlib.pyplot as plt
 3130:     >>> np.blackman(12)
 3131:     array([-1.38777878e-17,   3.26064346e-02,   1.59903635e-01, # may vary
 3132:             4.14397981e-01,   7.36045180e-01,   9.67046769e-01,
 3133:             9.67046769e-01,   7.36045180e-01,   4.14397981e-01,
 3134:             1.59903635e-01,   3.26064346e-02,  -1.38777878e-17])
 3135: 
 3136:     Plot the window and the frequency response.
 3137: 
 3138:     .. plot::
 3139:         :include-source:
 3140: 
 3141:         import matplotlib.pyplot as plt
 3142:         from numpy.fft import fft, fftshift
 3143:         window = np.blackman(51)
 3144:         plt.plot(window)
 3145:         plt.title("Blackman window")
 3146:         plt.ylabel("Amplitude")
 3147:         plt.xlabel("Sample")
 3148:         plt.show()  # doctest: +SKIP
 3149: 
 3150:         plt.figure()
 3151:         A = fft(window, 2048) / 25.5
 3152:         mag = np.abs(fftshift(A))
 3153:         freq = np.linspace(-0.5, 0.5, len(A))
 3154:         with np.errstate(divide='ignore', invalid='ignore'):
 3155:             response = 20 * np.log10(mag)
 3156:         response = np.clip(response, -100, 100)
 3157:         plt.plot(freq, response)
 3158:         plt.title("Frequency response of Blackman window")
 3159:         plt.ylabel("Magnitude [dB]")
 3160:         plt.xlabel("Normalized frequency [cycles per sample]")
 3161:         plt.axis('tight')
 3162:         plt.show()
 3163: 
 3164:     """
 3165:     # Ensures at least float64 via 0.0.  M should be an integer, but conversion
 3166:     # to double is safe for a range.
 3167:     values = np.array([0.0, M])
 3168:     M = values[1]
 3169: 
 3170:     if M < 1:
 3171:         return array([], dtype=values.dtype)
 3172:     if M == 1:
 3173:         return ones(1, dtype=values.dtype)
 3174:     n = arange(1 - M, M, 2)
 3175:     return 0.42 + 0.5 * cos(pi * n / (M - 1)) + 0.08 * cos(2.0 * pi * n / (M - 1))
 3176: 
 3177: 
 3178: @set_module('numpy')
 3179: def bartlett(M):
 3180:     """
 3181:     Return the Bartlett window.
 3182: 
 3183:     The Bartlett window is very similar to a triangular window, except
 3184:     that the end points are at zero.  It is often used in signal
 3185:     processing for tapering a signal, without generating too much
 3186:     ripple in the frequency domain.
 3187: 
 3188:     Parameters
 3189:     ----------
 3190:     M : int
 3191:         Number of points in the output window. If zero or less, an
 3192:         empty array is returned.
 3193: 
 3194:     Returns
 3195:     -------
 3196:     out : array
 3197:         The triangular window, with the maximum value normalized to one
 3198:         (the value one appears only if the number of samples is odd), with
 3199:         the first and last samples equal to zero.
 3200: 
 3201:     See Also
 3202:     --------
 3203:     blackman, hamming, hanning, kaiser
 3204: 
 3205:     Notes
 3206:     -----
 3207:     The Bartlett window is defined as
 3208: 
 3209:     .. math:: w(n) = \\frac{2}{M-1} \\left(
 3210:               \\frac{M-1}{2} - \\left|n - \\frac{M-1}{2}\\right|
 3211:               \\right)
 3212: 
 3213:     Most references to the Bartlett window come from the signal processing
 3214:     literature, where it is used as one of many windowing functions for
 3215:     smoothing values.  Note that convolution with this window produces linear
 3216:     interpolation.  It is also known as an apodization (which means "removing
 3217:     the foot", i.e. smoothing discontinuities at the beginning and end of the
 3218:     sampled signal) or tapering function. The Fourier transform of the
 3219:     Bartlett window is the product of two sinc functions. Note the excellent
 3220:     discussion in Kanasewich [2]_.
 3221: 
 3222:     References
 3223:     ----------
 3224:     .. [1] M.S. Bartlett, "Periodogram Analysis and Continuous Spectra",
 3225:            Biometrika 37, 1-16, 1950.
 3226:     .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics",
 3227:            The University of Alberta Press, 1975, pp. 109-110.
 3228:     .. [3] A.V. Oppenheim and R.W. Schafer, "Discrete-Time Signal
 3229:            Processing", Prentice-Hall, 1999, pp. 468-471.
 3230:     .. [4] Wikipedia, "Window function",
 3231:            https://en.wikipedia.org/wiki/Window_function
 3232:     .. [5] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,
 3233:            "Numerical Recipes", Cambridge University Press, 1986, page 429.
 3234: 
 3235:     Examples
 3236:     --------
 3237:     >>> import numpy as np
 3238:     >>> import matplotlib.pyplot as plt
 3239:     >>> np.bartlett(12)
 3240:     array([ 0.        ,  0.18181818,  0.36363636,  0.54545455,  0.72727273, # may vary
 3241:             0.90909091,  0.90909091,  0.72727273,  0.54545455,  0.36363636,
 3242:             0.18181818,  0.        ])
 3243: 
 3244:     Plot the window and its frequency response (requires SciPy and matplotlib).
 3245: 
 3246:     .. plot::
 3247:         :include-source:
 3248: 
 3249:         import matplotlib.pyplot as plt
 3250:         from numpy.fft import fft, fftshift
 3251:         window = np.bartlett(51)
 3252:         plt.plot(window)
 3253:         plt.title("Bartlett window")
 3254:         plt.ylabel("Amplitude")
 3255:         plt.xlabel("Sample")
 3256:         plt.show()
 3257:         plt.figure()
 3258:         A = fft(window, 2048) / 25.5
 3259:         mag = np.abs(fftshift(A))
 3260:         freq = np.linspace(-0.5, 0.5, len(A))
 3261:         with np.errstate(divide='ignore', invalid='ignore'):
 3262:             response = 20 * np.log10(mag)
 3263:         response = np.clip(response, -100, 100)
 3264:         plt.plot(freq, response)
 3265:         plt.title("Frequency response of Bartlett window")
 3266:         plt.ylabel("Magnitude [dB]")
 3267:         plt.xlabel("Normalized frequency [cycles per sample]")
 3268:         plt.axis('tight')
 3269:         plt.show()
 3270: 
 3271:     """
 3272:     # Ensures at least float64 via 0.0.  M should be an integer, but conversion
 3273:     # to double is safe for a range.
 3274:     values = np.array([0.0, M])
 3275:     M = values[1]
 3276: 
 3277:     if M < 1:
 3278:         return array([], dtype=values.dtype)
 3279:     if M == 1:
 3280:         return ones(1, dtype=values.dtype)
 3281:     n = arange(1 - M, M, 2)
 3282:     return where(less_equal(n, 0), 1 + n / (M - 1), 1 - n / (M - 1))
 3283: 
 3284: 
 3285: @set_module('numpy')
 3286: def hanning(M):
 3287:     """
 3288:     Return the Hanning window.
 3289: 
 3290:     The Hanning window is a taper formed by using a weighted cosine.
 3291: 
 3292:     Parameters
 3293:     ----------
 3294:     M : int
 3295:         Number of points in the output window. If zero or less, an
 3296:         empty array is returned.
 3297: 
 3298:     Returns
 3299:     -------
 3300:     out : ndarray, shape(M,)
 3301:         The window, with the maximum value normalized to one (the value
 3302:         one appears only if `M` is odd).
 3303: 
 3304:     See Also
 3305:     --------
 3306:     bartlett, blackman, hamming, kaiser
 3307: 
 3308:     Notes
 3309:     -----
 3310:     The Hanning window is defined as
 3311: 
 3312:     .. math::  w(n) = 0.5 - 0.5\\cos\\left(\\frac{2\\pi{n}}{M-1}\\right)
 3313:                \\qquad 0 \\leq n \\leq M-1
 3314: 
 3315:     The Hanning was named for Julius von Hann, an Austrian meteorologist.
 3316:     It is also known as the Cosine Bell. Some authors prefer that it be
 3317:     called a Hann window, to help avoid confusion with the very similar
 3318:     Hamming window.
 3319: 
 3320:     Most references to the Hanning window come from the signal processing
 3321:     literature, where it is used as one of many windowing functions for
 3322:     smoothing values.  It is also known as an apodization (which means
 3323:     "removing the foot", i.e. smoothing discontinuities at the beginning
 3324:     and end of the sampled signal) or tapering function.
 3325: 
 3326:     References
 3327:     ----------
 3328:     .. [1] Blackman, R.B. and Tukey, J.W., (1958) The measurement of power
 3329:            spectra, Dover Publications, New York.
 3330:     .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics",
 3331:            The University of Alberta Press, 1975, pp. 106-108.
 3332:     .. [3] Wikipedia, "Window function",
 3333:            https://en.wikipedia.org/wiki/Window_function
 3334:     .. [4] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,
 3335:            "Numerical Recipes", Cambridge University Press, 1986, page 425.
 3336: 
 3337:     Examples
 3338:     --------
 3339:     >>> import numpy as np
 3340:     >>> np.hanning(12)
 3341:     array([0.        , 0.07937323, 0.29229249, 0.57115742, 0.82743037,
 3342:            0.97974649, 0.97974649, 0.82743037, 0.57115742, 0.29229249,
 3343:            0.07937323, 0.        ])
 3344: 
 3345:     Plot the window and its frequency response.
 3346: 
 3347:     .. plot::
 3348:         :include-source:
 3349: 
 3350:         import matplotlib.pyplot as plt
 3351:         from numpy.fft import fft, fftshift
 3352:         window = np.hanning(51)
 3353:         plt.plot(window)
 3354:         plt.title("Hann window")
 3355:         plt.ylabel("Amplitude")
 3356:         plt.xlabel("Sample")
 3357:         plt.show()
 3358: 
 3359:         plt.figure()
 3360:         A = fft(window, 2048) / 25.5
 3361:         mag = np.abs(fftshift(A))
 3362:         freq = np.linspace(-0.5, 0.5, len(A))
 3363:         with np.errstate(divide='ignore', invalid='ignore'):
 3364:             response = 20 * np.log10(mag)
 3365:         response = np.clip(response, -100, 100)
 3366:         plt.plot(freq, response)
 3367:         plt.title("Frequency response of the Hann window")
 3368:         plt.ylabel("Magnitude [dB]")
 3369:         plt.xlabel("Normalized frequency [cycles per sample]")
 3370:         plt.axis('tight')
 3371:         plt.show()
 3372: 
 3373:     """
 3374:     # Ensures at least float64 via 0.0.  M should be an integer, but conversion
 3375:     # to double is safe for a range.
 3376:     values = np.array([0.0, M])
 3377:     M = values[1]
 3378: 
 3379:     if M < 1:
 3380:         return array([], dtype=values.dtype)
 3381:     if M == 1:
 3382:         return ones(1, dtype=values.dtype)
 3383:     n = arange(1 - M, M, 2)
 3384:     return 0.5 + 0.5 * cos(pi * n / (M - 1))
 3385: 
 3386: 
 3387: @set_module('numpy')
 3388: def hamming(M):
 3389:     """
 3390:     Return the Hamming window.
 3391: 
 3392:     The Hamming window is a taper formed by using a weighted cosine.
 3393: 
 3394:     Parameters
 3395:     ----------
 3396:     M : int
 3397:         Number of points in the output window. If zero or less, an
 3398:         empty array is returned.
 3399: 
 3400:     Returns
 3401:     -------
 3402:     out : ndarray
 3403:         The window, with the maximum value normalized to one (the value
 3404:         one appears only if the number of samples is odd).
 3405: 
 3406:     See Also
 3407:     --------
 3408:     bartlett, blackman, hanning, kaiser
 3409: 
 3410:     Notes
 3411:     -----
 3412:     The Hamming window is defined as
 3413: 
 3414:     .. math::  w(n) = 0.54 - 0.46\\cos\\left(\\frac{2\\pi{n}}{M-1}\\right)
 3415:                \\qquad 0 \\leq n \\leq M-1
 3416: 
 3417:     The Hamming was named for R. W. Hamming, an associate of J. W. Tukey
 3418:     and is described in Blackman and Tukey. It was recommended for
 3419:     smoothing the truncated autocovariance function in the time domain.
 3420:     Most references to the Hamming window come from the signal processing
 3421:     literature, where it is used as one of many windowing functions for
 3422:     smoothing values.  It is also known as an apodization (which means
 3423:     "removing the foot", i.e. smoothing discontinuities at the beginning
 3424:     and end of the sampled signal) or tapering function.
 3425: 
 3426:     References
 3427:     ----------
 3428:     .. [1] Blackman, R.B. and Tukey, J.W., (1958) The measurement of power
 3429:            spectra, Dover Publications, New York.
 3430:     .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics", The
 3431:            University of Alberta Press, 1975, pp. 109-110.
 3432:     .. [3] Wikipedia, "Window function",
 3433:            https://en.wikipedia.org/wiki/Window_function
 3434:     .. [4] W.H. Press,  B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling,
 3435:            "Numerical Recipes", Cambridge University Press, 1986, page 425.
 3436: 
 3437:     Examples
 3438:     --------
 3439:     >>> import numpy as np
 3440:     >>> np.hamming(12)
 3441:     array([ 0.08      ,  0.15302337,  0.34890909,  0.60546483,  0.84123594, # may vary
 3442:             0.98136677,  0.98136677,  0.84123594,  0.60546483,  0.34890909,
 3443:             0.15302337,  0.08      ])
 3444: 
 3445:     Plot the window and the frequency response.
 3446: 
 3447:     .. plot::
 3448:         :include-source:
 3449: 
 3450:         import matplotlib.pyplot as plt
 3451:         from numpy.fft import fft, fftshift
 3452:         window = np.hamming(51)
 3453:         plt.plot(window)
 3454:         plt.title("Hamming window")
 3455:         plt.ylabel("Amplitude")
 3456:         plt.xlabel("Sample")
 3457:         plt.show()
 3458: 
 3459:         plt.figure()
 3460:         A = fft(window, 2048) / 25.5
 3461:         mag = np.abs(fftshift(A))
 3462:         freq = np.linspace(-0.5, 0.5, len(A))
 3463:         response = 20 * np.log10(mag)
 3464:         response = np.clip(response, -100, 100)
 3465:         plt.plot(freq, response)
 3466:         plt.title("Frequency response of Hamming window")
 3467:         plt.ylabel("Magnitude [dB]")
 3468:         plt.xlabel("Normalized frequency [cycles per sample]")
 3469:         plt.axis('tight')
 3470:         plt.show()
 3471: 
 3472:     """
 3473:     # Ensures at least float64 via 0.0.  M should be an integer, but conversion
 3474:     # to double is safe for a range.
 3475:     values = np.array([0.0, M])
 3476:     M = values[1]
 3477: 
 3478:     if M < 1:
 3479:         return array([], dtype=values.dtype)
 3480:     if M == 1:
 3481:         return ones(1, dtype=values.dtype)
 3482:     n = arange(1 - M, M, 2)
 3483:     return 0.54 + 0.46 * cos(pi * n / (M - 1))
 3484: 
 3485: 
 3486: ## Code from cephes for i0
 3487: 
 3488: _i0A = [
 3489:     -4.41534164647933937950E-18,
 3490:     3.33079451882223809783E-17,
 3491:     -2.43127984654795469359E-16,
 3492:     1.71539128555513303061E-15,
 3493:     -1.16853328779934516808E-14,
 3494:     7.67618549860493561688E-14,
 3495:     -4.85644678311192946090E-13,
 3496:     2.95505266312963983461E-12,
 3497:     -1.72682629144155570723E-11,
 3498:     9.67580903537323691224E-11,
 3499:     -5.18979560163526290666E-10,
 3500:     2.65982372468238665035E-9,
 3501:     -1.30002500998624804212E-8,
 3502:     6.04699502254191894932E-8,
 3503:     -2.67079385394061173391E-7,
 3504:     1.11738753912010371815E-6,
 3505:     -4.41673835845875056359E-6,
 3506:     1.64484480707288970893E-5,
 3507:     -5.75419501008210370398E-5,
 3508:     1.88502885095841655729E-4,
 3509:     -5.76375574538582365885E-4,
 3510:     1.63947561694133579842E-3,
 3511:     -4.32430999505057594430E-3,
 3512:     1.05464603945949983183E-2,
 3513:     -2.37374148058994688156E-2,
 3514:     4.93052842396707084878E-2,
 3515:     -9.49010970480476444210E-2,
 3516:     1.71620901522208775349E-1,
 3517:     -3.04682672343198398683E-1,
 3518:     6.76795274409476084995E-1
 3519:     ]
 3520: 
 3521: _i0B = [
 3522:     -7.23318048787475395456E-18,
 3523:     -4.83050448594418207126E-18,
 3524:     4.46562142029675999901E-17,
 3525:     3.46122286769746109310E-17,
 3526:     -2.82762398051658348494E-16,
 3527:     -3.42548561967721913462E-16,
 3528:     1.77256013305652638360E-15,
 3529:     3.81168066935262242075E-15,
 3530:     -9.55484669882830764870E-15,
 3531:     -4.15056934728722208663E-14,
 3532:     1.54008621752140982691E-14,
 3533:     3.85277838274214270114E-13,
 3534:     7.18012445138366623367E-13,
 3535:     -1.79417853150680611778E-12,
 3536:     -1.32158118404477131188E-11,
 3537:     -3.14991652796324136454E-11,
 3538:     1.18891471078464383424E-11,
 3539:     4.94060238822496958910E-10,
 3540:     3.39623202570838634515E-9,
 3541:     2.26666899049817806459E-8,
 3542:     2.04891858946906374183E-7,
 3543:     2.89137052083475648297E-6,
 3544:     6.88975834691682398426E-5,
 3545:     3.36911647825569408990E-3,
 3546:     8.04490411014108831608E-1
 3547:     ]
 3548: 
 3549: 
 3550: def _chbevl(x, vals):
 3551:     b0 = vals[0]
 3552:     b1 = 0.0
 3553: 
 3554:     for i in range(1, len(vals)):
 3555:         b2 = b1
 3556:         b1 = b0
 3557:         b0 = x * b1 - b2 + vals[i]
 3558: 
 3559:     return 0.5 * (b0 - b2)
 3560: 
 3561: 
 3562: def _i0_1(x):
 3563:     return exp(x) * _chbevl(x / 2.0 - 2, _i0A)
 3564: 
 3565: 
 3566: def _i0_2(x):
 3567:     return exp(x) * _chbevl(32.0 / x - 2.0, _i0B) / sqrt(x)
 3568: 
 3569: 
 3570: def _i0_dispatcher(x):
 3571:     return (x,)
 3572: 
 3573: 
 3574: @array_function_dispatch(_i0_dispatcher)
 3575: def i0(x):
 3576:     """
 3577:     Modified Bessel function of the first kind, order 0.
 3578: 
 3579:     Usually denoted :math:`I_0`.
 3580: 
 3581:     Parameters
 3582:     ----------
 3583:     x : array_like of float
 3584:         Argument of the Bessel function.
 3585: 
 3586:     Returns
 3587:     -------
 3588:     out : ndarray, shape = x.shape, dtype = float
 3589:         The modified Bessel function evaluated at each of the elements of `x`.
 3590: 
 3591:     See Also
 3592:     --------
 3593:     scipy.special.i0, scipy.special.iv, scipy.special.ive
 3594: 
 3595:     Notes
 3596:     -----
 3597:     The scipy implementation is recommended over this function: it is a
 3598:     proper ufunc written in C, and more than an order of magnitude faster.
 3599: 
 3600:     We use the algorithm published by Clenshaw [1]_ and referenced by
 3601:     Abramowitz and Stegun [2]_, for which the function domain is
 3602:     partitioned into the two intervals [0,8] and (8,inf), and Chebyshev
 3603:     polynomial expansions are employed in each interval. Relative error on
 3604:     the domain [0,30] using IEEE arithmetic is documented [3]_ as having a
 3605:     peak of 5.8e-16 with an rms of 1.4e-16 (n = 30000).
 3606: 
 3607:     References
 3608:     ----------
 3609:     .. [1] C. W. Clenshaw, "Chebyshev series for mathematical functions", in
 3610:            *National Physical Laboratory Mathematical Tables*, vol. 5, London:
 3611:            Her Majesty's Stationery Office, 1962.
 3612:     .. [2] M. Abramowitz and I. A. Stegun, *Handbook of Mathematical
 3613:            Functions*, 10th printing, New York: Dover, 1964, pp. 379.
 3614:            https://personal.math.ubc.ca/~cbm/aands/page_379.htm
 3615:     .. [3] https://metacpan.org/pod/distribution/Math-Cephes/lib/Math/Cephes.pod#i0:-Modified-Bessel-function-of-order-zero
 3616: 
 3617:     Examples
 3618:     --------
 3619:     >>> import numpy as np
 3620:     >>> np.i0(0.)
 3621:     array(1.0)
 3622:     >>> np.i0([0, 1, 2, 3])
 3623:     array([1.        , 1.26606588, 2.2795853 , 4.88079259])
 3624: 
 3625:     """
 3626:     x = np.asanyarray(x)
 3627:     if x.dtype.kind == 'c':
 3628:         raise TypeError("i0 not supported for complex values")
 3629:     if x.dtype.kind != 'f':
 3630:         x = x.astype(float)
 3631:     x = np.abs(x)
 3632:     return piecewise(x, [x <= 8.0], [_i0_1, _i0_2])
 3633: 
 3634: ## End of cephes code for i0
 3635: 
 3636: 
 3637: @set_module('numpy')
 3638: def kaiser(M, beta):
 3639:     """
 3640:     Return the Kaiser window.
 3641: 
 3642:     The Kaiser window is a taper formed by using a Bessel function.
 3643: 
 3644:     Parameters
 3645:     ----------
 3646:     M : int
 3647:         Number of points in the output window. If zero or less, an
 3648:         empty array is returned.
 3649:     beta : float
 3650:         Shape parameter for window.
 3651: 
 3652:     Returns
 3653:     -------
 3654:     out : array
 3655:         The window, with the maximum value normalized to one (the value
 3656:         one appears only if the number of samples is odd).
 3657: 
 3658:     See Also
 3659:     --------
 3660:     bartlett, blackman, hamming, hanning
 3661: 
 3662:     Notes
 3663:     -----
 3664:     The Kaiser window is defined as
 3665: 
 3666:     .. math::  w(n) = I_0\\left( \\beta \\sqrt{1-\\frac{4n^2}{(M-1)^2}}
 3667:                \\right)/I_0(\\beta)
 3668: 
 3669:     with
 3670: 
 3671:     .. math:: \\quad -\\frac{M-1}{2} \\leq n \\leq \\frac{M-1}{2},
 3672: 
 3673:     where :math:`I_0` is the modified zeroth-order Bessel function.
 3674: 
 3675:     The Kaiser was named for Jim Kaiser, who discovered a simple
 3676:     approximation to the DPSS window based on Bessel functions.  The Kaiser
 3677:     window is a very good approximation to the Digital Prolate Spheroidal
 3678:     Sequence, or Slepian window, which is the transform which maximizes the
 3679:     energy in the main lobe of the window relative to total energy.
 3680: 
 3681:     The Kaiser can approximate many other windows by varying the beta
 3682:     parameter.
 3683: 
 3684:     ====  =======================
 3685:     beta  Window shape
 3686:     ====  =======================
 3687:     0     Rectangular
 3688:     5     Similar to a Hamming
 3689:     6     Similar to a Hanning
 3690:     8.6   Similar to a Blackman
 3691:     ====  =======================
 3692: 
 3693:     A beta value of 14 is probably a good starting point. Note that as beta
 3694:     gets large, the window narrows, and so the number of samples needs to be
 3695:     large enough to sample the increasingly narrow spike, otherwise NaNs will
 3696:     get returned.
 3697: 
 3698:     Most references to the Kaiser window come from the signal processing
 3699:     literature, where it is used as one of many windowing functions for
 3700:     smoothing values.  It is also known as an apodization (which means
 3701:     "removing the foot", i.e. smoothing discontinuities at the beginning
 3702:     and end of the sampled signal) or tapering function.
 3703: 
 3704:     References
 3705:     ----------
 3706:     .. [1] J. F. Kaiser, "Digital Filters" - Ch 7 in "Systems analysis by
 3707:            digital computer", Editors: F.F. Kuo and J.F. Kaiser, p 218-285.
 3708:            John Wiley and Sons, New York, (1966).
 3709:     .. [2] E.R. Kanasewich, "Time Sequence Analysis in Geophysics", The
 3710:            University of Alberta Press, 1975, pp. 177-178.
 3711:     .. [3] Wikipedia, "Window function",
 3712:            https://en.wikipedia.org/wiki/Window_function
 3713: 
 3714:     Examples
 3715:     --------
 3716:     >>> import numpy as np
 3717:     >>> import matplotlib.pyplot as plt
 3718:     >>> np.kaiser(12, 14)
 3719:      array([7.72686684e-06, 3.46009194e-03, 4.65200189e-02, # may vary
 3720:             2.29737120e-01, 5.99885316e-01, 9.45674898e-01,
 3721:             9.45674898e-01, 5.99885316e-01, 2.29737120e-01,
 3722:             4.65200189e-02, 3.46009194e-03, 7.72686684e-06])
 3723: 
 3724: 
 3725:     Plot the window and the frequency response.
 3726: 
 3727:     .. plot::
 3728:         :include-source:
 3729: 
 3730:         import matplotlib.pyplot as plt
 3731:         from numpy.fft import fft, fftshift
 3732:         window = np.kaiser(51, 14)
 3733:         plt.plot(window)
 3734:         plt.title("Kaiser window")
 3735:         plt.ylabel("Amplitude")
 3736:         plt.xlabel("Sample")
 3737:         plt.show()
 3738: 
 3739:         plt.figure()
 3740:         A = fft(window, 2048) / 25.5
 3741:         mag = np.abs(fftshift(A))
 3742:         freq = np.linspace(-0.5, 0.5, len(A))
 3743:         response = 20 * np.log10(mag)
 3744:         response = np.clip(response, -100, 100)
 3745:         plt.plot(freq, response)
 3746:         plt.title("Frequency response of Kaiser window")
 3747:         plt.ylabel("Magnitude [dB]")
 3748:         plt.xlabel("Normalized frequency [cycles per sample]")
 3749:         plt.axis('tight')
 3750:         plt.show()
 3751: 
 3752:     """
 3753:     # Ensures at least float64 via 0.0.  M should be an integer, but conversion
 3754:     # to double is safe for a range.  (Simplified result_type with 0.0
 3755:     # strongly typed.  result-type is not/less order sensitive, but that mainly
 3756:     # matters for integers anyway.)
 3757:     values = np.array([0.0, M, beta])
 3758:     M = values[1]
 3759:     beta = values[2]
 3760: 
 3761:     if M == 1:
 3762:         return np.ones(1, dtype=values.dtype)
 3763:     n = arange(0, M)
 3764:     alpha = (M - 1) / 2.0
 3765:     return i0(beta * sqrt(1 - ((n - alpha) / alpha)**2.0)) / i0(beta)
 3766: 
 3767: 
 3768: def _sinc_dispatcher(x):
 3769:     return (x,)
 3770: 
 3771: 
 3772: @array_function_dispatch(_sinc_dispatcher)
 3773: def sinc(x):
 3774:     r"""
 3775:     Return the normalized sinc function.
 3776: 
 3777:     The sinc function is equal to :math:`\sin(\pi x)/(\pi x)` for any argument
 3778:     :math:`x\ne 0`. ``sinc(0)`` takes the limit value 1, making ``sinc`` not
 3779:     only everywhere continuous but also infinitely differentiable.
 3780: 
 3781:     .. note::
 3782: 
 3783:         Note the normalization factor of ``pi`` used in the definition.
 3784:         This is the most commonly used definition in signal processing.
 3785:         Use ``sinc(x / np.pi)`` to obtain the unnormalized sinc function
 3786:         :math:`\sin(x)/x` that is more common in mathematics.
 3787: 
 3788:     Parameters
 3789:     ----------
 3790:     x : ndarray
 3791:         Array (possibly multi-dimensional) of values for which to calculate
 3792:         ``sinc(x)``.
 3793: 
 3794:     Returns
 3795:     -------
 3796:     out : ndarray
 3797:         ``sinc(x)``, which has the same shape as the input.
 3798: 
 3799:     Notes
 3800:     -----
 3801:     The name sinc is short for "sine cardinal" or "sinus cardinalis".
 3802: 
 3803:     The sinc function is used in various signal processing applications,
 3804:     including in anti-aliasing, in the construction of a Lanczos resampling
 3805:     filter, and in interpolation.
 3806: 
 3807:     For bandlimited interpolation of discrete-time signals, the ideal
 3808:     interpolation kernel is proportional to the sinc function.
 3809: 
 3810:     References
 3811:     ----------
 3812:     .. [1] Weisstein, Eric W. "Sinc Function." From MathWorld--A Wolfram Web
 3813:            Resource. https://mathworld.wolfram.com/SincFunction.html
 3814:     .. [2] Wikipedia, "Sinc function",
 3815:            https://en.wikipedia.org/wiki/Sinc_function
 3816: 
 3817:     Examples
 3818:     --------
 3819:     >>> import numpy as np
 3820:     >>> import matplotlib.pyplot as plt
 3821:     >>> x = np.linspace(-4, 4, 41)
 3822:     >>> np.sinc(x)
 3823:      array([-3.89804309e-17,  -4.92362781e-02,  -8.40918587e-02, # may vary
 3824:             -8.90384387e-02,  -5.84680802e-02,   3.89804309e-17,
 3825:             6.68206631e-02,   1.16434881e-01,   1.26137788e-01,
 3826:             8.50444803e-02,  -3.89804309e-17,  -1.03943254e-01,
 3827:             -1.89206682e-01,  -2.16236208e-01,  -1.55914881e-01,
 3828:             3.89804309e-17,   2.33872321e-01,   5.04551152e-01,
 3829:             7.56826729e-01,   9.35489284e-01,   1.00000000e+00,
 3830:             9.35489284e-01,   7.56826729e-01,   5.04551152e-01,
 3831:             2.33872321e-01,   3.89804309e-17,  -1.55914881e-01,
 3832:            -2.16236208e-01,  -1.89206682e-01,  -1.03943254e-01,
 3833:            -3.89804309e-17,   8.50444803e-02,   1.26137788e-01,
 3834:             1.16434881e-01,   6.68206631e-02,   3.89804309e-17,
 3835:             -5.84680802e-02,  -8.90384387e-02,  -8.40918587e-02,
 3836:             -4.92362781e-02,  -3.89804309e-17])
 3837: 
 3838:     >>> plt.plot(x, np.sinc(x))
 3839:     [<matplotlib.lines.Line2D object at 0x...>]
 3840:     >>> plt.title("Sinc Function")
 3841:     Text(0.5, 1.0, 'Sinc Function')
 3842:     >>> plt.ylabel("Amplitude")
 3843:     Text(0, 0.5, 'Amplitude')
 3844:     >>> plt.xlabel("X")
 3845:     Text(0.5, 0, 'X')
 3846:     >>> plt.show()
 3847: 
 3848:     """
 3849:     x = np.asanyarray(x)
 3850:     x = pi * x
 3851:     # Hope that 1e-20 is sufficient for objects...
 3852:     eps = np.finfo(x.dtype).eps if x.dtype.kind == "f" else 1e-20
 3853:     y = where(x, x, eps)
 3854:     return sin(y) / y
 3855: 
 3856: 
 3857: def _ureduce(a, func, keepdims=False, **kwargs):
 3858:     """
 3859:     Internal Function.
 3860:     Call `func` with `a` as first argument swapping the axes to use extended
 3861:     axis on functions that don't support it natively.
 3862: 
 3863:     Returns result and a.shape with axis dims set to 1.
 3864: 
 3865:     Parameters
 3866:     ----------
 3867:     a : array_like
 3868:         Input array or object that can be converted to an array.
 3869:     func : callable
 3870:         Reduction function capable of receiving a single axis argument.
 3871:         It is called with `a` as first argument followed by `kwargs`.
 3872:     kwargs : keyword arguments
 3873:         additional keyword arguments to pass to `func`.
 3874: 
 3875:     Returns
 3876:     -------
 3877:     result : tuple
 3878:         Result of func(a, **kwargs) and a.shape with axis dims set to 1
 3879:         which can be used to reshape the result to the same shape a ufunc with
 3880:         keepdims=True would produce.
 3881: 
 3882:     """
 3883:     a = np.asanyarray(a)
 3884:     axis = kwargs.get('axis')
 3885:     out = kwargs.get('out')
 3886: 
 3887:     if keepdims is np._NoValue:
 3888:         keepdims = False
 3889: 
 3890:     nd = a.ndim
 3891:     if axis is not None:
 3892:         axis = _nx.normalize_axis_tuple(axis, nd)
 3893: 
 3894:         if keepdims and out is not None:
 3895:             index_out = tuple(
 3896:                 0 if i in axis else slice(None) for i in range(nd))
 3897:             kwargs['out'] = out[(Ellipsis, ) + index_out]
 3898: 
 3899:         if len(axis) == 1:
 3900:             kwargs['axis'] = axis[0]
 3901:         else:
 3902:             keep = set(range(nd)) - set(axis)
 3903:             nkeep = len(keep)
 3904:             # swap axis that should not be reduced to front
 3905:             for i, s in enumerate(sorted(keep)):
 3906:                 a = a.swapaxes(i, s)
 3907:             # merge reduced axis
 3908:             a = a.reshape(a.shape[:nkeep] + (-1,))
 3909:             kwargs['axis'] = -1
 3910:     elif keepdims and out is not None:
 3911:         index_out = (0, ) * nd
 3912:         kwargs['out'] = out[(Ellipsis, ) + index_out]
 3913: 
 3914:     r = func(a, **kwargs)
 3915: 
 3916:     if out is not None:
 3917:         return out
 3918: 
 3919:     if keepdims:
 3920:         if axis is None:
 3921:             index_r = (np.newaxis, ) * nd
 3922:         else:
 3923:             index_r = tuple(
 3924:                 np.newaxis if i in axis else slice(None)
 3925:                 for i in range(nd))
 3926:         r = r[(Ellipsis, ) + index_r]
 3927: 
 3928:     return r
 3929: 
 3930: 
 3931: def _median_dispatcher(
 3932:         a, axis=None, out=None, overwrite_input=None, keepdims=None):
 3933:     return (a, out)
 3934: 
 3935: 
 3936: @array_function_dispatch(_median_dispatcher)
 3937: def median(a, axis=None, out=None, overwrite_input=False, keepdims=False):
 3938:     """
 3939:     Compute the median along the specified axis.
 3940: 
 3941:     Returns the median of the array elements.
 3942: 
 3943:     Parameters
 3944:     ----------
 3945:     a : array_like
 3946:         Input array or object that can be converted to an array.
 3947:     axis : {int, sequence of int, None}, optional
 3948:         Axis or axes along which the medians are computed. The default,
 3949:         axis=None, will compute the median along a flattened version of
 3950:         the array. If a sequence of axes, the array is first flattened
 3951:         along the given axes, then the median is computed along the
 3952:         resulting flattened axis.
 3953:     out : ndarray, optional
 3954:         Alternative output array in which to place the result. It must
 3955:         have the same shape and buffer length as the expected output,
 3956:         but the type (of the output) will be cast if necessary.
 3957:     overwrite_input : bool, optional
 3958:        If True, then allow use of memory of input array `a` for
 3959:        calculations. The input array will be modified by the call to
 3960:        `median`. This will save memory when you do not need to preserve
 3961:        the contents of the input array. Treat the input as undefined,
 3962:        but it will probably be fully or partially sorted. Default is
 3963:        False. If `overwrite_input` is ``True`` and `a` is not already an
 3964:        `ndarray`, an error will be raised.
 3965:     keepdims : bool, optional
 3966:         If this is set to True, the axes which are reduced are left
 3967:         in the result as dimensions with size one. With this option,
 3968:         the result will broadcast correctly against the original `arr`.
 3969: 
 3970:     Returns
 3971:     -------
 3972:     median : ndarray
 3973:         A new array holding the result. If the input contains integers
 3974:         or floats smaller than ``float64``, then the output data-type is
 3975:         ``np.float64``.  Otherwise, the data-type of the output is the
 3976:         same as that of the input. If `out` is specified, that array is
 3977:         returned instead.
 3978: 
 3979:     See Also
 3980:     --------
 3981:     mean, percentile
 3982: 
 3983:     Notes
 3984:     -----
 3985:     Given a vector ``V`` of length ``N``, the median of ``V`` is the
 3986:     middle value of a sorted copy of ``V``, ``V_sorted`` - i
 3987:     e., ``V_sorted[(N-1)/2]``, when ``N`` is odd, and the average of the
 3988:     two middle values of ``V_sorted`` when ``N`` is even.
 3989: 
 3990:     Examples
 3991:     --------
 3992:     >>> import numpy as np
 3993:     >>> a = np.array([[10, 7, 4], [3, 2, 1]])
 3994:     >>> a
 3995:     array([[10,  7,  4],
 3996:            [ 3,  2,  1]])
 3997:     >>> np.median(a)
 3998:     np.float64(3.5)
 3999:     >>> np.median(a, axis=0)
 4000:     array([6.5, 4.5, 2.5])
 4001:     >>> np.median(a, axis=1)
 4002:     array([7.,  2.])
 4003:     >>> np.median(a, axis=(0, 1))
 4004:     np.float64(3.5)
 4005:     >>> m = np.median(a, axis=0)
 4006:     >>> out = np.zeros_like(m)
 4007:     >>> np.median(a, axis=0, out=m)
 4008:     array([6.5,  4.5,  2.5])
 4009:     >>> m
 4010:     array([6.5,  4.5,  2.5])
 4011:     >>> b = a.copy()
 4012:     >>> np.median(b, axis=1, overwrite_input=True)
 4013:     array([7.,  2.])
 4014:     >>> assert not np.all(a==b)
 4015:     >>> b = a.copy()
 4016:     >>> np.median(b, axis=None, overwrite_input=True)
 4017:     np.float64(3.5)
 4018:     >>> assert not np.all(a==b)
 4019: 
 4020:     """
 4021:     return _ureduce(a, func=_median, keepdims=keepdims, axis=axis, out=out,
 4022:                     overwrite_input=overwrite_input)
 4023: 
 4024: 
 4025: def _median(a, axis=None, out=None, overwrite_input=False):
 4026:     # can't be reasonably be implemented in terms of percentile as we have to
 4027:     # call mean to not break astropy
 4028:     a = np.asanyarray(a)
 4029: 
 4030:     # Set the partition indexes
 4031:     if axis is None:
 4032:         sz = a.size
 4033:     else:
 4034:         sz = a.shape[axis]
 4035:     if sz % 2 == 0:
 4036:         szh = sz // 2
 4037:         kth = [szh - 1, szh]
 4038:     else:
 4039:         kth = [(sz - 1) // 2]
 4040: 
 4041:     # We have to check for NaNs (as of writing 'M' doesn't actually work).
 4042:     supports_nans = np.issubdtype(a.dtype, np.inexact) or a.dtype.kind in 'Mm'
 4043:     if supports_nans:
 4044:         kth.append(-1)
 4045: 
 4046:     if overwrite_input:
 4047:         if axis is None:
 4048:             part = a.ravel()
 4049:             part.partition(kth)
 4050:         else:
 4051:             a.partition(kth, axis=axis)
 4052:             part = a
 4053:     else:
 4054:         part = partition(a, kth, axis=axis)
 4055: 
 4056:     if part.shape == ():
 4057:         # make 0-D arrays work
 4058:         return part.item()
 4059:     if axis is None:
 4060:         axis = 0
 4061: 
 4062:     indexer = [slice(None)] * part.ndim
 4063:     index = part.shape[axis] // 2
 4064:     if part.shape[axis] % 2 == 1:
 4065:         # index with slice to allow mean (below) to work
 4066:         indexer[axis] = slice(index, index + 1)
 4067:     else:
 4068:         indexer[axis] = slice(index - 1, index + 1)
 4069:     indexer = tuple(indexer)
 4070: 
 4071:     # Use mean in both odd and even case to coerce data type,
 4072:     # using out array if needed.
 4073:     rout = mean(part[indexer], axis=axis, out=out)
 4074:     if supports_nans and sz > 0:
 4075:         # If nans are possible, warn and replace by nans like mean would.
 4076:         rout = np.lib._utils_impl._median_nancheck(part, rout, axis)
 4077: 
 4078:     return rout
 4079: 
 4080: 
 4081: def _percentile_dispatcher(a, q, axis=None, out=None, overwrite_input=None,
 4082:                            method=None, keepdims=None, *, weights=None,
 4083:                            interpolation=None):
 4084:     return (a, q, out, weights)
 4085: 
 4086: 
 4087: @array_function_dispatch(_percentile_dispatcher)
 4088: def percentile(a,
 4089:                q,
 4090:                axis=None,
 4091:                out=None,
 4092:                overwrite_input=False,
 4093:                method="linear",
 4094:                keepdims=False,
 4095:                *,
 4096:                weights=None,
 4097:                interpolation=None):
 4098:     """
 4099:     Compute the q-th percentile of the data along the specified axis.
 4100: 
 4101:     Returns the q-th percentile(s) of the array elements.
 4102: 
 4103:     Parameters
 4104:     ----------
 4105:     a : array_like of real numbers
 4106:         Input array or object that can be converted to an array.
 4107:     q : array_like of float
 4108:         Percentage or sequence of percentages for the percentiles to compute.
 4109:         Values must be between 0 and 100 inclusive.
 4110:     axis : {int, tuple of int, None}, optional
 4111:         Axis or axes along which the percentiles are computed. The
 4112:         default is to compute the percentile(s) along a flattened
 4113:         version of the array.
 4114:     out : ndarray, optional
 4115:         Alternative output array in which to place the result. It must
 4116:         have the same shape and buffer length as the expected output,
 4117:         but the type (of the output) will be cast if necessary.
 4118:     overwrite_input : bool, optional
 4119:         If True, then allow the input array `a` to be modified by intermediate
 4120:         calculations, to save memory. In this case, the contents of the input
 4121:         `a` after this function completes is undefined.
 4122:     method : str, optional
 4123:         This parameter specifies the method to use for estimating the
 4124:         percentile.  There are many different methods, some unique to NumPy.
 4125:         See the notes for explanation.  The options sorted by their R type
 4126:         as summarized in the H&F paper [1]_ are:
 4127: 
 4128:         1. 'inverted_cdf'
 4129:         2. 'averaged_inverted_cdf'
 4130:         3. 'closest_observation'
 4131:         4. 'interpolated_inverted_cdf'
 4132:         5. 'hazen'
 4133:         6. 'weibull'
 4134:         7. 'linear'  (default)
 4135:         8. 'median_unbiased'
 4136:         9. 'normal_unbiased'
 4137: 
 4138:         The first three methods are discontinuous.  NumPy further defines the
 4139:         following discontinuous variations of the default 'linear' (7.) option:
 4140: 
 4141:         * 'lower'
 4142:         * 'higher',
 4143:         * 'midpoint'
 4144:         * 'nearest'
 4145: 
 4146:         .. versionchanged:: 1.22.0
 4147:             This argument was previously called "interpolation" and only
 4148:             offered the "linear" default and last four options.
 4149: 
 4150:     keepdims : bool, optional
 4151:         If this is set to True, the axes which are reduced are left in
 4152:         the result as dimensions with size one. With this option, the
 4153:         result will broadcast correctly against the original array `a`.
 4154: 
 4155:      weights : array_like, optional
 4156:         An array of weights associated with the values in `a`. Each value in
 4157:         `a` contributes to the percentile according to its associated weight.
 4158:         The weights array can either be 1-D (in which case its length must be
 4159:         the size of `a` along the given axis) or of the same shape as `a`.
 4160:         If `weights=None`, then all data in `a` are assumed to have a
 4161:         weight equal to one.
 4162:         Only `method="inverted_cdf"` supports weights.
 4163:         See the notes for more details.
 4164: 
 4165:         .. versionadded:: 2.0.0
 4166: 
 4167:     interpolation : str, optional
 4168:         Deprecated name for the method keyword argument.
 4169: 
 4170:         .. deprecated:: 1.22.0
 4171: 
 4172:     Returns
 4173:     -------
 4174:     percentile : scalar or ndarray
 4175:         If `q` is a single percentile and `axis=None`, then the result
 4176:         is a scalar. If multiple percentiles are given, first axis of
 4177:         the result corresponds to the percentiles. The other axes are
 4178:         the axes that remain after the reduction of `a`. If the input
 4179:         contains integers or floats smaller than ``float64``, the output
 4180:         data-type is ``float64``. Otherwise, the output data-type is the
 4181:         same as that of the input. If `out` is specified, that array is
 4182:         returned instead.
 4183: 
 4184:     See Also
 4185:     --------
 4186:     mean
 4187:     median : equivalent to ``percentile(..., 50)``
 4188:     nanpercentile
 4189:     quantile : equivalent to percentile, except q in the range [0, 1].
 4190: 
 4191:     Notes
 4192:     -----
 4193:     The behavior of `numpy.percentile` with percentage `q` is
 4194:     that of `numpy.quantile` with argument ``q/100``.
 4195:     For more information, please see `numpy.quantile`.
 4196: 
 4197:     Examples
 4198:     --------
 4199:     >>> import numpy as np
 4200:     >>> a = np.array([[10, 7, 4], [3, 2, 1]])
 4201:     >>> a
 4202:     array([[10,  7,  4],
 4203:            [ 3,  2,  1]])
 4204:     >>> np.percentile(a, 50)
 4205:     3.5
 4206:     >>> np.percentile(a, 50, axis=0)
 4207:     array([6.5, 4.5, 2.5])
 4208:     >>> np.percentile(a, 50, axis=1)
 4209:     array([7.,  2.])
 4210:     >>> np.percentile(a, 50, axis=1, keepdims=True)
 4211:     array([[7.],
 4212:            [2.]])
 4213: 
 4214:     >>> m = np.percentile(a, 50, axis=0)
 4215:     >>> out = np.zeros_like(m)
 4216:     >>> np.percentile(a, 50, axis=0, out=out)
 4217:     array([6.5, 4.5, 2.5])
 4218:     >>> m
 4219:     array([6.5, 4.5, 2.5])
 4220: 
 4221:     >>> b = a.copy()
 4222:     >>> np.percentile(b, 50, axis=1, overwrite_input=True)
 4223:     array([7.,  2.])
 4224:     >>> assert not np.all(a == b)
 4225: 
 4226:     The different methods can be visualized graphically:
 4227: 
 4228:     .. plot::
 4229: 
 4230:         import matplotlib.pyplot as plt
 4231: 
 4232:         a = np.arange(4)
 4233:         p = np.linspace(0, 100, 6001)
 4234:         ax = plt.gca()
 4235:         lines = [
 4236:             ('linear', '-', 'C0'),
 4237:             ('inverted_cdf', ':', 'C1'),
 4238:             # Almost the same as `inverted_cdf`:
 4239:             ('averaged_inverted_cdf', '-.', 'C1'),
 4240:             ('closest_observation', ':', 'C2'),
 4241:             ('interpolated_inverted_cdf', '--', 'C1'),
 4242:             ('hazen', '--', 'C3'),
 4243:             ('weibull', '-.', 'C4'),
 4244:             ('median_unbiased', '--', 'C5'),
 4245:             ('normal_unbiased', '-.', 'C6'),
 4246:             ]
 4247:         for method, style, color in lines:
 4248:             ax.plot(
 4249:                 p, np.percentile(a, p, method=method),
 4250:                 label=method, linestyle=style, color=color)
 4251:         ax.set(
 4252:             title='Percentiles for different methods and data: ' + str(a),
 4253:             xlabel='Percentile',
 4254:             ylabel='Estimated percentile value',
 4255:             yticks=a)
 4256:         ax.legend(bbox_to_anchor=(1.03, 1))
 4257:         plt.tight_layout()
 4258:         plt.show()
 4259: 
 4260:     References
 4261:     ----------
 4262:     .. [1] R. J. Hyndman and Y. Fan,
 4263:        "Sample quantiles in statistical packages,"
 4264:        The American Statistician, 50(4), pp. 361-365, 1996
 4265: 
 4266:     """
 4267:     if interpolation is not None:
 4268:         method = _check_interpolation_as_method(
 4269:             method, interpolation, "percentile")
 4270: 
 4271:     a = np.asanyarray(a)
 4272:     if a.dtype.kind == "c":
 4273:         raise TypeError("a must be an array of real numbers")
 4274: 
 4275:     # Use dtype of array if possible (e.g., if q is a python int or float)
 4276:     # by making the divisor have the dtype of the data array.
 4277:     q = np.true_divide(q, a.dtype.type(100) if a.dtype.kind == "f" else 100, out=...)
 4278:     if not _quantile_is_valid(q):
 4279:         raise ValueError("Percentiles must be in the range [0, 100]")
 4280: 
 4281:     if weights is not None:
 4282:         if method != "inverted_cdf":
 4283:             msg = ("Only method 'inverted_cdf' supports weights. "
 4284:                    f"Got: {method}.")
 4285:             raise ValueError(msg)
 4286:         if axis is not None:
 4287:             axis = _nx.normalize_axis_tuple(axis, a.ndim, argname="axis")
 4288:         weights = _weights_are_valid(weights=weights, a=a, axis=axis)
 4289:         if np.any(weights < 0):
 4290:             raise ValueError("Weights must be non-negative.")
 4291: 
 4292:     return _quantile_unchecked(
 4293:         a, q, axis, out, overwrite_input, method, keepdims, weights)
 4294: 
 4295: 
 4296: def _quantile_dispatcher(a, q, axis=None, out=None, overwrite_input=None,
 4297:                          method=None, keepdims=None, *, weights=None,
 4298:                          interpolation=None):
 4299:     return (a, q, out, weights)
 4300: 
 4301: 
 4302: @array_function_dispatch(_quantile_dispatcher)
 4303: def quantile(a,
 4304:              q,
 4305:              axis=None,
 4306:              out=None,
 4307:              overwrite_input=False,
 4308:              method="linear",
 4309:              keepdims=False,
 4310:              *,
 4311:              weights=None,
 4312:              interpolation=None):
 4313:     """
 4314:     Compute the q-th quantile of the data along the specified axis.
 4315: 
 4316:     Parameters
 4317:     ----------
 4318:     a : array_like of real numbers
 4319:         Input array or object that can be converted to an array.
 4320:     q : array_like of float
 4321:         Probability or sequence of probabilities of the quantiles to compute.
 4322:         Values must be between 0 and 1 inclusive.
 4323:     axis : {int, tuple of int, None}, optional
 4324:         Axis or axes along which the quantiles are computed. The default is
 4325:         to compute the quantile(s) along a flattened version of the array.
 4326:     out : ndarray, optional
 4327:         Alternative output array in which to place the result. It must have
 4328:         the same shape and buffer length as the expected output, but the
 4329:         type (of the output) will be cast if necessary.
 4330:     overwrite_input : bool, optional
 4331:         If True, then allow the input array `a` to be modified by
 4332:         intermediate calculations, to save memory. In this case, the
 4333:         contents of the input `a` after this function completes is
 4334:         undefined.
 4335:     method : str, optional
 4336:         This parameter specifies the method to use for estimating the
 4337:         quantile.  There are many different methods, some unique to NumPy.
 4338:         The recommended options, numbered as they appear in [1]_, are:
 4339: 
 4340:         1. 'inverted_cdf'
 4341:         2. 'averaged_inverted_cdf'
 4342:         3. 'closest_observation'
 4343:         4. 'interpolated_inverted_cdf'
 4344:         5. 'hazen'
 4345:         6. 'weibull'
 4346:         7. 'linear'  (default)
 4347:         8. 'median_unbiased'
 4348:         9. 'normal_unbiased'
 4349: 
 4350:         The first three methods are discontinuous. For backward compatibility
 4351:         with previous versions of NumPy, the following discontinuous variations
 4352:         of the default 'linear' (7.) option are available:
 4353: 
 4354:         * 'lower'
 4355:         * 'higher',
 4356:         * 'midpoint'
 4357:         * 'nearest'
 4358: 
 4359:         See Notes for details.
 4360: 
 4361:         .. versionchanged:: 1.22.0
 4362:             This argument was previously called "interpolation" and only
 4363:             offered the "linear" default and last four options.
 4364: 
 4365:     keepdims : bool, optional
 4366:         If this is set to True, the axes which are reduced are left in
 4367:         the result as dimensions with size one. With this option, the
 4368:         result will broadcast correctly against the original array `a`.
 4369: 
 4370:     weights : array_like, optional
 4371:         An array of weights associated with the values in `a`. Each value in
 4372:         `a` contributes to the quantile according to its associated weight.
 4373:         The weights array can either be 1-D (in which case its length must be
 4374:         the size of `a` along the given axis) or of the same shape as `a`.
 4375:         If `weights=None`, then all data in `a` are assumed to have a
 4376:         weight equal to one.
 4377:         Only `method="inverted_cdf"` supports weights.
 4378:         See the notes for more details.
 4379: 
 4380:         .. versionadded:: 2.0.0
 4381: 
 4382:     interpolation : str, optional
 4383:         Deprecated name for the method keyword argument.
 4384: 
 4385:         .. deprecated:: 1.22.0
 4386: 
 4387:     Returns
 4388:     -------
 4389:     quantile : scalar or ndarray
 4390:         If `q` is a single probability and `axis=None`, then the result
 4391:         is a scalar. If multiple probability levels are given, first axis
 4392:         of the result corresponds to the quantiles. The other axes are
 4393:         the axes that remain after the reduction of `a`. If the input
 4394:         contains integers or floats smaller than ``float64``, the output
 4395:         data-type is ``float64``. Otherwise, the output data-type is the
 4396:         same as that of the input. If `out` is specified, that array is
 4397:         returned instead.
 4398: 
 4399:     See Also
 4400:     --------
 4401:     mean
 4402:     percentile : equivalent to quantile, but with q in the range [0, 100].
 4403:     median : equivalent to ``quantile(..., 0.5)``
 4404:     nanquantile
 4405: 
 4406:     Notes
 4407:     -----
 4408:     Given a sample `a` from an underlying distribution, `quantile` provides a
 4409:     nonparametric estimate of the inverse cumulative distribution function.
 4410: 
 4411:     By default, this is done by interpolating between adjacent elements in
 4412:     ``y``, a sorted copy of `a`::
 4413: 
 4414:         (1-g)*y[j] + g*y[j+1]
 4415: 
 4416:     where the index ``j`` and coefficient ``g`` are the integral and
 4417:     fractional components of ``q * (n-1)``, and ``n`` is the number of
 4418:     elements in the sample.
 4419: 
 4420:     This is a special case of Equation 1 of H&F [1]_. More generally,
 4421: 
 4422:     - ``j = (q*n + m - 1) // 1``, and
 4423:     - ``g = (q*n + m - 1) % 1``,
 4424: 
 4425:     where ``m`` may be defined according to several different conventions.
 4426:     The preferred convention may be selected using the ``method`` parameter:
 4427: 
 4428:     =============================== =============== ===============
 4429:     ``method``                      number in H&F   ``m``
 4430:     =============================== =============== ===============
 4431:     ``interpolated_inverted_cdf``   4               ``0``
 4432:     ``hazen``                       5               ``1/2``
 4433:     ``weibull``                     6               ``q``
 4434:     ``linear`` (default)            7               ``1 - q``
 4435:     ``median_unbiased``             8               ``q/3 + 1/3``
 4436:     ``normal_unbiased``             9               ``q/4 + 3/8``
 4437:     =============================== =============== ===============
 4438: 
 4439:     Note that indices ``j`` and ``j + 1`` are clipped to the range ``0`` to
 4440:     ``n - 1`` when the results of the formula would be outside the allowed
 4441:     range of non-negative indices. The ``- 1`` in the formulas for ``j`` and
 4442:     ``g`` accounts for Python's 0-based indexing.
 4443: 
 4444:     The table above includes only the estimators from H&F that are continuous
 4445:     functions of probability `q` (estimators 4-9). NumPy also provides the
 4446:     three discontinuous estimators from H&F (estimators 1-3), where ``j`` is
 4447:     defined as above, ``m`` is defined as follows, and ``g`` is a function
 4448:     of the real-valued ``index = q*n + m - 1`` and ``j``.
 4449: 
 4450:     1. ``inverted_cdf``: ``m = 0`` and ``g = int(index - j > 0)``
 4451:     2. ``averaged_inverted_cdf``: ``m = 0`` and
 4452:        ``g = (1 + int(index - j > 0)) / 2``
 4453:     3. ``closest_observation``: ``m = -1/2`` and
 4454:        ``g = 1 - int((index == j) & (j%2 == 1))``
 4455: 
 4456:     For backward compatibility with previous versions of NumPy, `quantile`
 4457:     provides four additional discontinuous estimators. Like
 4458:     ``method='linear'``, all have ``m = 1 - q`` so that ``j = q*(n-1) // 1``,
 4459:     but ``g`` is defined as follows.
 4460: 
 4461:     - ``lower``: ``g = 0``
 4462:     - ``midpoint``: ``g = 0.5``
 4463:     - ``higher``: ``g = 1``
 4464:     - ``nearest``: ``g = (q*(n-1) % 1) > 0.5``
 4465: 
 4466:     **Weighted quantiles:**
 4467:     More formally, the quantile at probability level :math:`q` of a cumulative
 4468:     distribution function :math:`F(y)=P(Y \\leq y)` with probability measure
 4469:     :math:`P` is defined as any number :math:`x` that fulfills the
 4470:     *coverage conditions*
 4471: 
 4472:     .. math:: P(Y < x) \\leq q \\quad\\text{and}\\quad P(Y \\leq x) \\geq q
 4473: 
 4474:     with random variable :math:`Y\\sim P`.
 4475:     Sample quantiles, the result of `quantile`, provide nonparametric
 4476:     estimation of the underlying population counterparts, represented by the
 4477:     unknown :math:`F`, given a data vector `a` of length ``n``.
 4478: 
 4479:     Some of the estimators above arise when one considers :math:`F` as the
 4480:     empirical distribution function of the data, i.e.
 4481:     :math:`F(y) = \\frac{1}{n} \\sum_i 1_{a_i \\leq y}`.
 4482:     Then, different methods correspond to different choices of :math:`x` that
 4483:     fulfill the above coverage conditions. Methods that follow this approach
 4484:     are ``inverted_cdf`` and ``averaged_inverted_cdf``.
 4485: 
 4486:     For weighted quantiles, the coverage conditions still hold. The
 4487:     empirical cumulative distribution is simply replaced by its weighted
 4488:     version, i.e.
 4489:     :math:`P(Y \\leq t) = \\frac{1}{\\sum_i w_i} \\sum_i w_i 1_{x_i \\leq t}`.
 4490:     Only ``method="inverted_cdf"`` supports weights.
 4491: 
 4492:     Examples
 4493:     --------
 4494:     >>> import numpy as np
 4495:     >>> a = np.array([[10, 7, 4], [3, 2, 1]])
 4496:     >>> a
 4497:     array([[10,  7,  4],
 4498:            [ 3,  2,  1]])
 4499:     >>> np.quantile(a, 0.5)
 4500:     3.5
 4501:     >>> np.quantile(a, 0.5, axis=0)
 4502:     array([6.5, 4.5, 2.5])
 4503:     >>> np.quantile(a, 0.5, axis=1)
 4504:     array([7.,  2.])
 4505:     >>> np.quantile(a, 0.5, axis=1, keepdims=True)
 4506:     array([[7.],
 4507:            [2.]])
 4508:     >>> m = np.quantile(a, 0.5, axis=0)
 4509:     >>> out = np.zeros_like(m)
 4510:     >>> np.quantile(a, 0.5, axis=0, out=out)
 4511:     array([6.5, 4.5, 2.5])
 4512:     >>> m
 4513:     array([6.5, 4.5, 2.5])
 4514:     >>> b = a.copy()
 4515:     >>> np.quantile(b, 0.5, axis=1, overwrite_input=True)
 4516:     array([7.,  2.])
 4517:     >>> assert not np.all(a == b)
 4518: 
 4519:     See also `numpy.percentile` for a visualization of most methods.
 4520: 
 4521:     References
 4522:     ----------
 4523:     .. [1] R. J. Hyndman and Y. Fan,
 4524:        "Sample quantiles in statistical packages,"
 4525:        The American Statistician, 50(4), pp. 361-365, 1996
 4526: 
 4527:     """
 4528:     if interpolation is not None:
 4529:         method = _check_interpolation_as_method(
 4530:             method, interpolation, "quantile")
 4531: 
 4532:     a = np.asanyarray(a)
 4533:     if a.dtype.kind == "c":
 4534:         raise TypeError("a must be an array of real numbers")
 4535: 
 4536:     # Use dtype of array if possible (e.g., if q is a python int or float).
 4537:     if isinstance(q, (int, float)) and a.dtype.kind == "f":
 4538:         q = np.asanyarray(q, dtype=a.dtype)
 4539:     else:
 4540:         q = np.asanyarray(q)
 4541: 
 4542:     if not _quantile_is_valid(q):
 4543:         raise ValueError("Quantiles must be in the range [0, 1]")
 4544: 
 4545:     if weights is not None:
 4546:         if method != "inverted_cdf":
 4547:             msg = ("Only method 'inverted_cdf' supports weights. "
 4548:                    f"Got: {method}.")
 4549:             raise ValueError(msg)
 4550:         if axis is not None:
 4551:             axis = _nx.normalize_axis_tuple(axis, a.ndim, argname="axis")
 4552:         weights = _weights_are_valid(weights=weights, a=a, axis=axis)
 4553:         if np.any(weights < 0):
 4554:             raise ValueError("Weights must be non-negative.")
 4555: 
 4556:     return _quantile_unchecked(
 4557:         a, q, axis, out, overwrite_input, method, keepdims, weights)
 4558: 
 4559: 
 4560: def _quantile_unchecked(a,
 4561:                         q,
 4562:                         axis=None,
 4563:                         out=None,
 4564:                         overwrite_input=False,
 4565:                         method="linear",
 4566:                         keepdims=False,
 4567:                         weights=None):
 4568:     """Assumes that q is in [0, 1], and is an ndarray"""
 4569:     return _ureduce(a,
 4570:                     func=_quantile_ureduce_func,
 4571:                     q=q,
 4572:                     weights=weights,
 4573:                     keepdims=keepdims,
 4574:                     axis=axis,
 4575:                     out=out,
 4576:                     overwrite_input=overwrite_input,
 4577:                     method=method)
 4578: 
 4579: 
 4580: def _quantile_is_valid(q):
 4581:     # avoid expensive reductions, relevant for arrays with < O(1000) elements
 4582:     if q.ndim == 1 and q.size < 10:
 4583:         for i in range(q.size):
 4584:             if not (0.0 <= q[i] <= 1.0):
 4585:                 return False
 4586:     elif not (q.min() >= 0 and q.max() <= 1):
 4587:         return False
 4588:     return True
 4589: 
 4590: 
 4591: def _check_interpolation_as_method(method, interpolation, fname):
 4592:     # Deprecated NumPy 1.22, 2021-11-08
 4593:     warnings.warn(
 4594:         f"the `interpolation=` argument to {fname} was renamed to "
 4595:         "`method=`, which has additional options.\n"
 4596:         "Users of the modes 'nearest', 'lower', 'higher', or "
 4597:         "'midpoint' are encouraged to review the method they used. "
 4598:         "(Deprecated NumPy 1.22)",
 4599:         DeprecationWarning, stacklevel=4)
 4600:     if method != "linear":
 4601:         # sanity check, we assume this basically never happens
 4602:         raise TypeError(
 4603:             "You shall not pass both `method` and `interpolation`!\n"
 4604:             "(`interpolation` is Deprecated in favor of `method`)")
 4605:     return interpolation
 4606: 
 4607: 
 4608: def _compute_virtual_index(n, quantiles, alpha: float, beta: float):
 4609:     """
 4610:     Compute the floating point indexes of an array for the linear
 4611:     interpolation of quantiles.
 4612:     n : array_like
 4613:         The sample sizes.
 4614:     quantiles : array_like
 4615:         The quantiles values.
 4616:     alpha : float
 4617:         A constant used to correct the index computed.
 4618:     beta : float
 4619:         A constant used to correct the index computed.
 4620: 
 4621:     alpha and beta values depend on the chosen method
 4622:     (see quantile documentation)
 4623: 
 4624:     Reference:
 4625:     Hyndman&Fan paper "Sample Quantiles in Statistical Packages",
 4626:     DOI: 10.1080/00031305.1996.10473566
 4627:     """
 4628:     return n * quantiles + (
 4629:             alpha + quantiles * (1 - alpha - beta)
 4630:     ) - 1
 4631: 
 4632: 
 4633: def _get_gamma(virtual_indexes, previous_indexes, method):
 4634:     """
 4635:     Compute gamma (a.k.a 'm' or 'weight') for the linear interpolation
 4636:     of quantiles.
 4637: 
 4638:     virtual_indexes : array_like
 4639:         The indexes where the percentile is supposed to be found in the sorted
 4640:         sample.
 4641:     previous_indexes : array_like
 4642:         The floor values of virtual_indexes.
 4643:     interpolation : dict
 4644:         The interpolation method chosen, which may have a specific rule
 4645:         modifying gamma.
 4646: 
 4647:     gamma is usually the fractional part of virtual_indexes but can be modified
 4648:     by the interpolation method.
 4649:     """
 4650:     gamma = np.asanyarray(virtual_indexes - previous_indexes)
 4651:     gamma = method["fix_gamma"](gamma, virtual_indexes)
 4652:     # Ensure both that we have an array, and that we keep the dtype
 4653:     # (which may have been matched to the input array).
 4654:     return np.asanyarray(gamma, dtype=virtual_indexes.dtype)
 4655: 
 4656: 
 4657: def _lerp(a, b, t, out=None):
 4658:     """
 4659:     Compute the linear interpolation weighted by gamma on each point of
 4660:     two same shape array.
 4661: 
 4662:     a : array_like
 4663:         Left bound.
 4664:     b : array_like
 4665:         Right bound.
 4666:     t : array_like
 4667:         The interpolation weight.
 4668:     out : array_like
 4669:         Output array.
 4670:     """
 4671:     diff_b_a = subtract(b, a)
 4672:     # asanyarray is a stop-gap until gh-13105
 4673:     lerp_interpolation = asanyarray(add(a, diff_b_a * t, out=out))
 4674:     subtract(b, diff_b_a * (1 - t), out=lerp_interpolation, where=t >= 0.5,
 4675:              casting='unsafe', dtype=type(lerp_interpolation.dtype))
 4676:     if lerp_interpolation.ndim == 0 and out is None:
 4677:         lerp_interpolation = lerp_interpolation[()]  # unpack 0d arrays
 4678:     return lerp_interpolation
 4679: 
 4680: 
 4681: def _get_gamma_mask(shape, default_value, conditioned_value, where):
 4682:     out = np.full(shape, default_value)
 4683:     np.copyto(out, conditioned_value, where=where, casting="unsafe")
 4684:     return out
 4685: 
 4686: 
 4687: def _discrete_interpolation_to_boundaries(index, gamma_condition_fun):
 4688:     previous = np.floor(index)
 4689:     next = previous + 1
 4690:     gamma = index - previous
 4691:     res = _get_gamma_mask(shape=index.shape,
 4692:                           default_value=next,
 4693:                           conditioned_value=previous,
 4694:                           where=gamma_condition_fun(gamma, index)
 4695:                           ).astype(np.intp)
 4696:     # Some methods can lead to out-of-bound integers, clip them:
 4697:     res[res < 0] = 0
 4698:     return res
 4699: 
 4700: 
 4701: def _closest_observation(n, quantiles):
 4702:     # "choose the nearest even order statistic at g=0" (H&F (1996) pp. 362).
 4703:     # Order is 1-based so for zero-based indexing round to nearest odd index.
 4704:     gamma_fun = lambda gamma, index: (gamma == 0) & (np.floor(index) % 2 == 1)
 4705:     return _discrete_interpolation_to_boundaries((n * quantiles) - 1 - 0.5,
 4706:                                                  gamma_fun)
 4707: 
 4708: 
 4709: def _inverted_cdf(n, quantiles):
 4710:     gamma_fun = lambda gamma, _: (gamma == 0)
 4711:     return _discrete_interpolation_to_boundaries((n * quantiles) - 1,
 4712:                                                  gamma_fun)
 4713: 
 4714: 
 4715: def _quantile_ureduce_func(
 4716:         a: np.array,
 4717:         q: np.array,
 4718:         weights: np.array,
 4719:         axis: int | None = None,
 4720:         out=None,
 4721:         overwrite_input: bool = False,
 4722:         method="linear",
 4723: ) -> np.array:
 4724:     if q.ndim > 2:
 4725:         # The code below works fine for nd, but it might not have useful
 4726:         # semantics. For now, keep the supported dimensions the same as it was
 4727:         # before.
 4728:         raise ValueError("q must be a scalar or 1d")
 4729:     if overwrite_input:
 4730:         if axis is None:
 4731:             axis = 0
 4732:             arr = a.ravel()
 4733:             wgt = None if weights is None else weights.ravel()
 4734:         else:
 4735:             arr = a
 4736:             wgt = weights
 4737:     elif axis is None:
 4738:         axis = 0
 4739:         arr = a.flatten()
 4740:         wgt = None if weights is None else weights.flatten()
 4741:     else:
 4742:         arr = a.copy()
 4743:         wgt = weights
 4744:     result = _quantile(arr,
 4745:                        quantiles=q,
 4746:                        axis=axis,
 4747:                        method=method,
 4748:                        out=out,
 4749:                        weights=wgt)
 4750:     return result
 4751: 
 4752: 
 4753: def _get_indexes(arr, virtual_indexes, valid_values_count):
 4754:     """
 4755:     Get the valid indexes of arr neighbouring virtual_indexes.
 4756:     Note
 4757:     This is a companion function to linear interpolation of
 4758:     Quantiles
 4759: 
 4760:     Returns
 4761:     -------
 4762:     (previous_indexes, next_indexes): Tuple
 4763:         A Tuple of virtual_indexes neighbouring indexes
 4764:     """
 4765:     previous_indexes = np.asanyarray(np.floor(virtual_indexes))
 4766:     next_indexes = np.asanyarray(previous_indexes + 1)
 4767:     indexes_above_bounds = virtual_indexes >= valid_values_count - 1
 4768:     # When indexes is above max index, take the max value of the array
 4769:     if indexes_above_bounds.any():
 4770:         previous_indexes[indexes_above_bounds] = -1
 4771:         next_indexes[indexes_above_bounds] = -1
 4772:     # When indexes is below min index, take the min value of the array
 4773:     indexes_below_bounds = virtual_indexes < 0
 4774:     if indexes_below_bounds.any():
 4775:         previous_indexes[indexes_below_bounds] = 0
 4776:         next_indexes[indexes_below_bounds] = 0
 4777:     if np.issubdtype(arr.dtype, np.inexact):
 4778:         # After the sort, slices having NaNs will have for last element a NaN
 4779:         virtual_indexes_nans = np.isnan(virtual_indexes)
 4780:         if virtual_indexes_nans.any():
 4781:             previous_indexes[virtual_indexes_nans] = -1
 4782:             next_indexes[virtual_indexes_nans] = -1
 4783:     previous_indexes = previous_indexes.astype(np.intp)
 4784:     next_indexes = next_indexes.astype(np.intp)
 4785:     return previous_indexes, next_indexes
 4786: 
 4787: 
 4788: def _quantile(
 4789:         arr: np.array,
 4790:         quantiles: np.array,
 4791:         axis: int = -1,
 4792:         method="linear",
 4793:         out=None,
 4794:         weights=None,
 4795: ):
 4796:     """
 4797:     Private function that doesn't support extended axis or keepdims.
 4798:     These methods are extended to this function using _ureduce
 4799:     See nanpercentile for parameter usage
 4800:     It computes the quantiles of the array for the given axis.
 4801:     A linear interpolation is performed based on the `interpolation`.
 4802: 
 4803:     By default, the method is "linear" where alpha == beta == 1 which
 4804:     performs the 7th method of Hyndman&Fan.
 4805:     With "median_unbiased" we get alpha == beta == 1/3
 4806:     thus the 8th method of Hyndman&Fan.
 4807:     """
 4808:     # --- Setup
 4809:     arr = np.asanyarray(arr)
 4810:     values_count = arr.shape[axis]
 4811:     # The dimensions of `q` are prepended to the output shape, so we need the
 4812:     # axis being sampled from `arr` to be last.
 4813:     if axis != 0:  # But moveaxis is slow, so only call it if necessary.
 4814:         arr = np.moveaxis(arr, axis, destination=0)
 4815:     supports_nans = (
 4816:         np.issubdtype(arr.dtype, np.inexact) or arr.dtype.kind in 'Mm'
 4817:     )
 4818: 
 4819:     if weights is None:
 4820:         # --- Computation of indexes
 4821:         # Index where to find the value in the sorted array.
 4822:         # Virtual because it is a floating point value, not an valid index.
 4823:         # The nearest neighbours are used for interpolation
 4824:         try:
 4825:             method_props = _QuantileMethods[method]
 4826:         except KeyError:
 4827:             raise ValueError(
 4828:                 f"{method!r} is not a valid method. Use one of: "
 4829:                 f"{_QuantileMethods.keys()}") from None
 4830:         virtual_indexes = method_props["get_virtual_index"](values_count,
 4831:                                                             quantiles)
 4832:         virtual_indexes = np.asanyarray(virtual_indexes)
 4833: 
 4834:         if method_props["fix_gamma"] is None:
 4835:             supports_integers = True
 4836:         else:
 4837:             int_virtual_indices = np.issubdtype(virtual_indexes.dtype,
 4838:                                                 np.integer)
 4839:             supports_integers = method == 'linear' and int_virtual_indices
 4840: 
 4841:         if supports_integers:
 4842:             # No interpolation needed, take the points along axis
 4843:             if supports_nans:
 4844:                 # may contain nan, which would sort to the end
 4845:                 arr.partition(
 4846:                     concatenate((virtual_indexes.ravel(), [-1])), axis=0,
 4847:                 )
 4848:                 slices_having_nans = np.isnan(arr[-1, ...])
 4849:             else:
 4850:                 # cannot contain nan
 4851:                 arr.partition(virtual_indexes.ravel(), axis=0)
 4852:                 slices_having_nans = np.array(False, dtype=bool)
 4853:             result = take(arr, virtual_indexes, axis=0, out=out)
 4854:         else:
 4855:             previous_indexes, next_indexes = _get_indexes(arr,
 4856:                                                           virtual_indexes,
 4857:                                                           values_count)
 4858:             # --- Sorting
 4859:             arr.partition(
 4860:                 np.unique(np.concatenate(([0, -1],
 4861:                                           previous_indexes.ravel(),
 4862:                                           next_indexes.ravel(),
 4863:                                           ))),
 4864:                 axis=0)
 4865:             if supports_nans:
 4866:                 slices_having_nans = np.isnan(arr[-1, ...])
 4867:             else:
 4868:                 slices_having_nans = None
 4869:             # --- Get values from indexes
 4870:             previous = arr[previous_indexes]
 4871:             next = arr[next_indexes]
 4872:             # --- Linear interpolation
 4873:             gamma = _get_gamma(virtual_indexes, previous_indexes, method_props)
 4874:             result_shape = virtual_indexes.shape + (1,) * (arr.ndim - 1)
 4875:             gamma = gamma.reshape(result_shape)
 4876:             result = _lerp(previous,
 4877:                         next,
 4878:                         gamma,
 4879:                         out=out)
 4880:     else:
 4881:         # Weighted case
 4882:         # This implements method="inverted_cdf", the only supported weighted
 4883:         # method, which needs to sort anyway.
 4884:         weights = np.asanyarray(weights)
 4885:         if axis != 0:
 4886:             weights = np.moveaxis(weights, axis, destination=0)
 4887:         index_array = np.argsort(arr, axis=0, kind="stable")
 4888: 
 4889:         # arr = arr[index_array, ...]  # but this adds trailing dimensions of
 4890:         # 1.
 4891:         arr = np.take_along_axis(arr, index_array, axis=0)
 4892:         if weights.shape == arr.shape:
 4893:             weights = np.take_along_axis(weights, index_array, axis=0)
 4894:         else:
 4895:             # weights is 1d
 4896:             weights = weights.reshape(-1)[index_array, ...]
 4897: 
 4898:         if supports_nans:
 4899:             # may contain nan, which would sort to the end
 4900:             slices_having_nans = np.isnan(arr[-1, ...])
 4901:         else:
 4902:             # cannot contain nan
 4903:             slices_having_nans = np.array(False, dtype=bool)
 4904: 
 4905:         # We use the weights to calculate the empirical cumulative
 4906:         # distribution function cdf
 4907:         cdf = weights.cumsum(axis=0, dtype=np.float64)
 4908:         cdf /= cdf[-1, ...]  # normalization to 1
 4909:         # Search index i such that
 4910:         #   sum(weights[j], j=0..i-1) < quantile <= sum(weights[j], j=0..i)
 4911:         # is then equivalent to
 4912:         #   cdf[i-1] < quantile <= cdf[i]
 4913:         # Unfortunately, searchsorted only accepts 1-d arrays as first
 4914:         # argument, so we will need to iterate over dimensions.
 4915: 
 4916:         # Without the following cast, searchsorted can return surprising
 4917:         # results, e.g.
 4918:         #   np.searchsorted(np.array([0.2, 0.4, 0.6, 0.8, 1.]),
 4919:         #                   np.array(0.4, dtype=np.float32), side="left")
 4920:         # returns 2 instead of 1 because 0.4 is not binary representable.
 4921:         if quantiles.dtype.kind == "f":
 4922:             cdf = cdf.astype(quantiles.dtype)
 4923:         # Weights must be non-negative, so we might have zero weights at the
 4924:         # beginning leading to some leading zeros in cdf. The call to
 4925:         # np.searchsorted for quantiles=0 will then pick the first element,
 4926:         # but should pick the first one larger than zero. We
 4927:         # therefore simply set 0 values in cdf to -1.
 4928:         if np.any(cdf[0, ...] == 0):
 4929:             cdf[cdf == 0] = -1
 4930: 
 4931:         def find_cdf_1d(arr, cdf):
 4932:             indices = np.searchsorted(cdf, quantiles, side="left")
 4933:             # We might have reached the maximum with i = len(arr), e.g. for
 4934:             # quantiles = 1, and need to cut it to len(arr) - 1.
 4935:             indices = minimum(indices, values_count - 1)
 4936:             result = take(arr, indices, axis=0)
 4937:             return result
 4938: 
 4939:         r_shape = arr.shape[1:]
 4940:         if quantiles.ndim > 0:
 4941:             r_shape = quantiles.shape + r_shape
 4942:         if out is None:
 4943:             result = np.empty_like(arr, shape=r_shape)
 4944:         else:
 4945:             if out.shape != r_shape:
 4946:                 msg = (f"Wrong shape of argument 'out', shape={r_shape} is "
 4947:                        f"required; got shape={out.shape}.")
 4948:                 raise ValueError(msg)
 4949:             result = out
 4950: 
 4951:         # See apply_along_axis, which we do for axis=0. Note that Ni = (,)
 4952:         # always, so we remove it here.
 4953:         Nk = arr.shape[1:]
 4954:         for kk in np.ndindex(Nk):
 4955:             result[(...,) + kk] = find_cdf_1d(
 4956:                 arr[np.s_[:, ] + kk], cdf[np.s_[:, ] + kk]
 4957:             )
 4958: 
 4959:         # Make result the same as in unweighted inverted_cdf.
 4960:         if result.shape == () and result.dtype == np.dtype("O"):
 4961:             result = result.item()
 4962: 
 4963:     if np.any(slices_having_nans):
 4964:         if result.ndim == 0 and out is None:
 4965:             # can't write to a scalar, but indexing will be correct
 4966:             result = arr[-1]
 4967:         else:
 4968:             np.copyto(result, arr[-1, ...], where=slices_having_nans)
 4969:     return result
 4970: 
 4971: 
 4972: def _trapezoid_dispatcher(y, x=None, dx=None, axis=None):
 4973:     return (y, x)
 4974: 
 4975: 
 4976: @array_function_dispatch(_trapezoid_dispatcher)
 4977: def trapezoid(y, x=None, dx=1.0, axis=-1):
 4978:     r"""
 4979:     Integrate along the given axis using the composite trapezoidal rule.
 4980: 
 4981:     If `x` is provided, the integration happens in sequence along its
 4982:     elements - they are not sorted.
 4983: 
 4984:     Integrate `y` (`x`) along each 1d slice on the given axis, compute
 4985:     :math:`\int y(x) dx`.
 4986:     When `x` is specified, this integrates along the parametric curve,
 4987:     computing :math:`\int_t y(t) dt =
 4988:     \int_t y(t) \left.\frac{dx}{dt}\right|_{x=x(t)} dt`.
 4989: 
 4990:     .. versionadded:: 2.0.0
 4991: 
 4992:     Parameters
 4993:     ----------
 4994:     y : array_like
 4995:         Input array to integrate.
 4996:     x : array_like, optional
 4997:         The sample points corresponding to the `y` values. If `x` is None,
 4998:         the sample points are assumed to be evenly spaced `dx` apart. The
 4999:         default is None.
 5000:     dx : scalar, optional
 5001:         The spacing between sample points when `x` is None. The default is 1.
 5002:     axis : int, optional
 5003:         The axis along which to integrate.
 5004: 
 5005:     Returns
 5006:     -------
 5007:     trapezoid : float or ndarray
 5008:         Definite integral of `y` = n-dimensional array as approximated along
 5009:         a single axis by the trapezoidal rule. If `y` is a 1-dimensional array,
 5010:         then the result is a float. If `n` is greater than 1, then the result
 5011:         is an `n`-1 dimensional array.
 5012: 
 5013:     See Also
 5014:     --------
 5015:     sum, cumsum
 5016: 
 5017:     Notes
 5018:     -----
 5019:     Image [2]_ illustrates trapezoidal rule -- y-axis locations of points
 5020:     will be taken from `y` array, by default x-axis distances between
 5021:     points will be 1.0, alternatively they can be provided with `x` array
 5022:     or with `dx` scalar.  Return value will be equal to combined area under
 5023:     the red lines.
 5024: 
 5025: 
 5026:     References
 5027:     ----------
 5028:     .. [1] Wikipedia page: https://en.wikipedia.org/wiki/Trapezoidal_rule
 5029: 
 5030:     .. [2] Illustration image:
 5031:            https://en.wikipedia.org/wiki/File:Composite_trapezoidal_rule_illustration.png
 5032: 
 5033:     Examples
 5034:     --------
 5035:     >>> import numpy as np
 5036: 
 5037:     Use the trapezoidal rule on evenly spaced points:
 5038: 
 5039:     >>> np.trapezoid([1, 2, 3])
 5040:     4.0
 5041: 
 5042:     The spacing between sample points can be selected by either the
 5043:     ``x`` or ``dx`` arguments:
 5044: 
 5045:     >>> np.trapezoid([1, 2, 3], x=[4, 6, 8])
 5046:     8.0
 5047:     >>> np.trapezoid([1, 2, 3], dx=2)
 5048:     8.0
 5049: 
 5050:     Using a decreasing ``x`` corresponds to integrating in reverse:
 5051: 
 5052:     >>> np.trapezoid([1, 2, 3], x=[8, 6, 4])
 5053:     -8.0
 5054: 
 5055:     More generally ``x`` is used to integrate along a parametric curve. We can
 5056:     estimate the integral :math:`\int_0^1 x^2 = 1/3` using:
 5057: 
 5058:     >>> x = np.linspace(0, 1, num=50)
 5059:     >>> y = x**2
 5060:     >>> np.trapezoid(y, x)
 5061:     0.33340274885464394
 5062: 
 5063:     Or estimate the area of a circle, noting we repeat the sample which closes
 5064:     the curve:
 5065: 
 5066:     >>> theta = np.linspace(0, 2 * np.pi, num=1000, endpoint=True)
 5067:     >>> np.trapezoid(np.cos(theta), x=np.sin(theta))
 5068:     3.141571941375841
 5069: 
 5070:     ``np.trapezoid`` can be applied along a specified axis to do multiple
 5071:     computations in one call:
 5072: 
 5073:     >>> a = np.arange(6).reshape(2, 3)
 5074:     >>> a
 5075:     array([[0, 1, 2],
 5076:            [3, 4, 5]])
 5077:     >>> np.trapezoid(a, axis=0)
 5078:     array([1.5, 2.5, 3.5])
 5079:     >>> np.trapezoid(a, axis=1)
 5080:     array([2.,  8.])
 5081:     """
 5082: 
 5083:     y = asanyarray(y)
 5084:     if x is None:
 5085:         d = dx
 5086:     else:
 5087:         x = asanyarray(x)
 5088:         if x.ndim == 1:
 5089:             d = diff(x)
 5090:             # reshape to correct shape
 5091:             shape = [1] * y.ndim
 5092:             shape[axis] = d.shape[0]
 5093:             d = d.reshape(shape)
 5094:         else:
 5095:             d = diff(x, axis=axis)
 5096:     nd = y.ndim
 5097:     slice1 = [slice(None)] * nd
 5098:     slice2 = [slice(None)] * nd
 5099:     slice1[axis] = slice(1, None)
 5100:     slice2[axis] = slice(None, -1)
 5101:     try:
 5102:         ret = (d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0).sum(axis)
 5103:     except ValueError:
 5104:         # Operations didn't work, cast to ndarray
 5105:         d = np.asarray(d)
 5106:         y = np.asarray(y)
 5107:         ret = add.reduce(d * (y[tuple(slice1)] + y[tuple(slice2)]) / 2.0, axis)
 5108:     return ret
 5109: 
 5110: 
 5111: @set_module('numpy')
 5112: def trapz(y, x=None, dx=1.0, axis=-1):
 5113:     """
 5114:     `trapz` is deprecated in NumPy 2.0.
 5115: 
 5116:     Please use `trapezoid` instead, or one of the numerical integration
 5117:     functions in `scipy.integrate`.
 5118:     """
 5119:     # Deprecated in NumPy 2.0, 2023-08-18
 5120:     warnings.warn(
 5121:         "`trapz` is deprecated. Use `trapezoid` instead, or one of the "
 5122:         "numerical integration functions in `scipy.integrate`.",
 5123:         DeprecationWarning,
 5124:         stacklevel=2
 5125:     )
 5126:     return trapezoid(y, x=x, dx=dx, axis=axis)
 5127: 
 5128: 
 5129: def _meshgrid_dispatcher(*xi, copy=None, sparse=None, indexing=None):
 5130:     return xi
 5131: 
 5132: 
 5133: # Based on scitools meshgrid
 5134: @array_function_dispatch(_meshgrid_dispatcher)
 5135: def meshgrid(*xi, copy=True, sparse=False, indexing='xy'):
 5136:     """
 5137:     Return a tuple of coordinate matrices from coordinate vectors.
 5138: 
 5139:     Make N-D coordinate arrays for vectorized evaluations of
 5140:     N-D scalar/vector fields over N-D grids, given
 5141:     one-dimensional coordinate arrays x1, x2,..., xn.
 5142: 
 5143:     Parameters
 5144:     ----------
 5145:     x1, x2,..., xn : array_like
 5146:         1-D arrays representing the coordinates of a grid.
 5147:     indexing : {'xy', 'ij'}, optional
 5148:         Cartesian ('xy', default) or matrix ('ij') indexing of output.
 5149:         See Notes for more details.
 5150:     sparse : bool, optional
 5151:         If True the shape of the returned coordinate array for dimension *i*
 5152:         is reduced from ``(N1, ..., Ni, ... Nn)`` to
 5153:         ``(1, ..., 1, Ni, 1, ..., 1)``.  These sparse coordinate grids are
 5154:         intended to be used with :ref:`basics.broadcasting`.  When all
 5155:         coordinates are used in an expression, broadcasting still leads to a
 5156:         fully-dimensonal result array.
 5157: 
 5158:         Default is False.
 5159: 
 5160:     copy : bool, optional
 5161:         If False, a view into the original arrays are returned in order to
 5162:         conserve memory.  Default is True.  Please note that
 5163:         ``sparse=False, copy=False`` will likely return non-contiguous
 5164:         arrays.  Furthermore, more than one element of a broadcast array
 5165:         may refer to a single memory location.  If you need to write to the
 5166:         arrays, make copies first.
 5167: 
 5168:     Returns
 5169:     -------
 5170:     X1, X2,..., XN : tuple of ndarrays
 5171:         For vectors `x1`, `x2`,..., `xn` with lengths ``Ni=len(xi)``,
 5172:         returns ``(N1, N2, N3,..., Nn)`` shaped arrays if indexing='ij'
 5173:         or ``(N2, N1, N3,..., Nn)`` shaped arrays if indexing='xy'
 5174:         with the elements of `xi` repeated to fill the matrix along
 5175:         the first dimension for `x1`, the second for `x2` and so on.
 5176: 
 5177:     Notes
 5178:     -----
 5179:     This function supports both indexing conventions through the indexing
 5180:     keyword argument.  Giving the string 'ij' returns a meshgrid with
 5181:     matrix indexing, while 'xy' returns a meshgrid with Cartesian indexing.
 5182:     In the 2-D case with inputs of length M and N, the outputs are of shape
 5183:     (N, M) for 'xy' indexing and (M, N) for 'ij' indexing.  In the 3-D case
 5184:     with inputs of length M, N and P, outputs are of shape (N, M, P) for
 5185:     'xy' indexing and (M, N, P) for 'ij' indexing.  The difference is
 5186:     illustrated by the following code snippet::
 5187: 
 5188:         xv, yv = np.meshgrid(x, y, indexing='ij')
 5189:         for i in range(nx):
 5190:             for j in range(ny):
 5191:                 # treat xv[i,j], yv[i,j]
 5192: 
 5193:         xv, yv = np.meshgrid(x, y, indexing='xy')
 5194:         for i in range(nx):
 5195:             for j in range(ny):
 5196:                 # treat xv[j,i], yv[j,i]
 5197: 
 5198:     In the 1-D and 0-D case, the indexing and sparse keywords have no effect.
 5199: 
 5200:     See Also
 5201:     --------
 5202:     mgrid : Construct a multi-dimensional "meshgrid" using indexing notation.
 5203:     ogrid : Construct an open multi-dimensional "meshgrid" using indexing
 5204:             notation.
 5205:     :ref:`how-to-index`
 5206: 
 5207:     Examples
 5208:     --------
 5209:     >>> import numpy as np
 5210:     >>> nx, ny = (3, 2)
 5211:     >>> x = np.linspace(0, 1, nx)
 5212:     >>> y = np.linspace(0, 1, ny)
 5213:     >>> xv, yv = np.meshgrid(x, y)
 5214:     >>> xv
 5215:     array([[0. , 0.5, 1. ],
 5216:            [0. , 0.5, 1. ]])
 5217:     >>> yv
 5218:     array([[0.,  0.,  0.],
 5219:            [1.,  1.,  1.]])
 5220: 
 5221:     The result of `meshgrid` is a coordinate grid:
 5222: 
 5223:     >>> import matplotlib.pyplot as plt
 5224:     >>> plt.plot(xv, yv, marker='o', color='k', linestyle='none')
 5225:     >>> plt.show()
 5226: 
 5227:     You can create sparse output arrays to save memory and computation time.
 5228: 
 5229:     >>> xv, yv = np.meshgrid(x, y, sparse=True)
 5230:     >>> xv
 5231:     array([[0. ,  0.5,  1. ]])
 5232:     >>> yv
 5233:     array([[0.],
 5234:            [1.]])
 5235: 
 5236:     `meshgrid` is very useful to evaluate functions on a grid. If the
 5237:     function depends on all coordinates, both dense and sparse outputs can be
 5238:     used.
 5239: 
 5240:     >>> x = np.linspace(-5, 5, 101)
 5241:     >>> y = np.linspace(-5, 5, 101)
 5242:     >>> # full coordinate arrays
 5243:     >>> xx, yy = np.meshgrid(x, y)
 5244:     >>> zz = np.sqrt(xx**2 + yy**2)
 5245:     >>> xx.shape, yy.shape, zz.shape
 5246:     ((101, 101), (101, 101), (101, 101))
 5247:     >>> # sparse coordinate arrays
 5248:     >>> xs, ys = np.meshgrid(x, y, sparse=True)
 5249:     >>> zs = np.sqrt(xs**2 + ys**2)
 5250:     >>> xs.shape, ys.shape, zs.shape
 5251:     ((1, 101), (101, 1), (101, 101))
 5252:     >>> np.array_equal(zz, zs)
 5253:     True
 5254: 
 5255:     >>> h = plt.contourf(x, y, zs)
 5256:     >>> plt.axis('scaled')
 5257:     >>> plt.colorbar()
 5258:     >>> plt.show()
 5259:     """
 5260:     ndim = len(xi)
 5261: 
 5262:     if indexing not in ['xy', 'ij']:
 5263:         raise ValueError(
 5264:             "Valid values for `indexing` are 'xy' and 'ij'.")
 5265: 
 5266:     s0 = (1,) * ndim
 5267:     output = [np.asanyarray(x).reshape(s0[:i] + (-1,) + s0[i + 1:])
 5268:               for i, x in enumerate(xi)]
 5269: 
 5270:     if indexing == 'xy' and ndim > 1:
 5271:         # switch first and second axis
 5272:         output[0].shape = (1, -1) + s0[2:]
 5273:         output[1].shape = (-1, 1) + s0[2:]
 5274: 
 5275:     if not sparse:
 5276:         # Return the full N-D matrix (not only the 1-D vector)
 5277:         output = np.broadcast_arrays(*output, subok=True)
 5278: 
 5279:     if copy:
 5280:         output = tuple(x.copy() for x in output)
 5281: 
 5282:     return output
 5283: 
 5284: 
 5285: def _delete_dispatcher(arr, obj, axis=None):
 5286:     return (arr, obj)
 5287: 
 5288: 
 5289: @array_function_dispatch(_delete_dispatcher)
 5290: def delete(arr, obj, axis=None):
 5291:     """
 5292:     Return a new array with sub-arrays along an axis deleted. For a one
 5293:     dimensional array, this returns those entries not returned by
 5294:     `arr[obj]`.
 5295: 
 5296:     Parameters
 5297:     ----------
 5298:     arr : array_like
 5299:         Input array.
 5300:     obj : slice, int, array-like of ints or bools
 5301:         Indicate indices of sub-arrays to remove along the specified axis.
 5302: 
 5303:         .. versionchanged:: 1.19.0
 5304:             Boolean indices are now treated as a mask of elements to remove,
 5305:             rather than being cast to the integers 0 and 1.
 5306: 
 5307:     axis : int, optional
 5308:         The axis along which to delete the subarray defined by `obj`.
 5309:         If `axis` is None, `obj` is applied to the flattened array.
 5310: 
 5311:     Returns
 5312:     -------
 5313:     out : ndarray
 5314:         A copy of `arr` with the elements specified by `obj` removed. Note
 5315:         that `delete` does not occur in-place. If `axis` is None, `out` is
 5316:         a flattened array.
 5317: 
 5318:     See Also
 5319:     --------
 5320:     insert : Insert elements into an array.
 5321:     append : Append elements at the end of an array.
 5322: 
 5323:     Notes
 5324:     -----
 5325:     Often it is preferable to use a boolean mask. For example:
 5326: 
 5327:     >>> arr = np.arange(12) + 1
 5328:     >>> mask = np.ones(len(arr), dtype=bool)
 5329:     >>> mask[[0,2,4]] = False
 5330:     >>> result = arr[mask,...]
 5331: 
 5332:     Is equivalent to ``np.delete(arr, [0,2,4], axis=0)``, but allows further
 5333:     use of `mask`.
 5334: 
 5335:     Examples
 5336:     --------
 5337:     >>> import numpy as np
 5338:     >>> arr = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])
 5339:     >>> arr
 5340:     array([[ 1,  2,  3,  4],
 5341:            [ 5,  6,  7,  8],
 5342:            [ 9, 10, 11, 12]])
 5343:     >>> np.delete(arr, 1, 0)
 5344:     array([[ 1,  2,  3,  4],
 5345:            [ 9, 10, 11, 12]])
 5346: 
 5347:     >>> np.delete(arr, np.s_[::2], 1)
 5348:     array([[ 2,  4],
 5349:            [ 6,  8],
 5350:            [10, 12]])
 5351:     >>> np.delete(arr, [1,3,5], None)
 5352:     array([ 1,  3,  5,  7,  8,  9, 10, 11, 12])
 5353: 
 5354:     """
 5355:     conv = _array_converter(arr)
 5356:     arr, = conv.as_arrays(subok=False)
 5357: 
 5358:     ndim = arr.ndim
 5359:     arrorder = 'F' if arr.flags.fnc else 'C'
 5360:     if axis is None:
 5361:         if ndim != 1:
 5362:             arr = arr.ravel()
 5363:         # needed for np.matrix, which is still not 1d after being ravelled
 5364:         ndim = arr.ndim
 5365:         axis = ndim - 1
 5366:     else:
 5367:         axis = normalize_axis_index(axis, ndim)
 5368: 
 5369:     slobj = [slice(None)] * ndim
 5370:     N = arr.shape[axis]
 5371:     newshape = list(arr.shape)
 5372: 
 5373:     if isinstance(obj, slice):
 5374:         start, stop, step = obj.indices(N)
 5375:         xr = range(start, stop, step)
 5376:         numtodel = len(xr)
 5377: 
 5378:         if numtodel <= 0:
 5379:             return conv.wrap(arr.copy(order=arrorder), to_scalar=False)
 5380: 
 5381:         # Invert if step is negative:
 5382:         if step < 0:
 5383:             step = -step
 5384:             start = xr[-1]
 5385:             stop = xr[0] + 1
 5386: 
 5387:         newshape[axis] -= numtodel
 5388:         new = empty(newshape, arr.dtype, arrorder)
 5389:         # copy initial chunk
 5390:         if start == 0:
 5391:             pass
 5392:         else:
 5393:             slobj[axis] = slice(None, start)
 5394:             new[tuple(slobj)] = arr[tuple(slobj)]
 5395:         # copy end chunk
 5396:         if stop == N:
 5397:             pass
 5398:         else:
 5399:             slobj[axis] = slice(stop - numtodel, None)
 5400:             slobj2 = [slice(None)] * ndim
 5401:             slobj2[axis] = slice(stop, None)
 5402:             new[tuple(slobj)] = arr[tuple(slobj2)]
 5403:         # copy middle pieces
 5404:         if step == 1:
 5405:             pass
 5406:         else:  # use array indexing.
 5407:             keep = ones(stop - start, dtype=bool)
 5408:             keep[:stop - start:step] = False
 5409:             slobj[axis] = slice(start, stop - numtodel)
 5410:             slobj2 = [slice(None)] * ndim
 5411:             slobj2[axis] = slice(start, stop)
 5412:             arr = arr[tuple(slobj2)]
 5413:             slobj2[axis] = keep
 5414:             new[tuple(slobj)] = arr[tuple(slobj2)]
 5415: 
 5416:         return conv.wrap(new, to_scalar=False)
 5417: 
 5418:     if isinstance(obj, (int, integer)) and not isinstance(obj, bool):
 5419:         single_value = True
 5420:     else:
 5421:         single_value = False
 5422:         _obj = obj
 5423:         obj = np.asarray(obj)
 5424:         # `size == 0` to allow empty lists similar to indexing, but (as there)
 5425:         # is really too generic:
 5426:         if obj.size == 0 and not isinstance(_obj, np.ndarray):
 5427:             obj = obj.astype(intp)
 5428:         elif obj.size == 1 and obj.dtype.kind in "ui":
 5429:             # For a size 1 integer array we can use the single-value path
 5430:             # (most dtypes, except boolean, should just fail later).
 5431:             obj = obj.item()
 5432:             single_value = True
 5433: 
 5434:     if single_value:
 5435:         # optimization for a single value
 5436:         if (obj < -N or obj >= N):
 5437:             raise IndexError(
 5438:                 f"index {obj} is out of bounds for axis {axis} with "
 5439:                 f"size {N}")
 5440:         if (obj < 0):
 5441:             obj += N
 5442:         newshape[axis] -= 1
 5443:         new = empty(newshape, arr.dtype, arrorder)
 5444:         slobj[axis] = slice(None, obj)
 5445:         new[tuple(slobj)] = arr[tuple(slobj)]
 5446:         slobj[axis] = slice(obj, None)
 5447:         slobj2 = [slice(None)] * ndim
 5448:         slobj2[axis] = slice(obj + 1, None)
 5449:         new[tuple(slobj)] = arr[tuple(slobj2)]
 5450:     else:
 5451:         if obj.dtype == bool:
 5452:             if obj.shape != (N,):
 5453:                 raise ValueError('boolean array argument obj to delete '
 5454:                                  'must be one dimensional and match the axis '
 5455:                                  f'length of {N}')
 5456: 
 5457:             # optimization, the other branch is slower
 5458:             keep = ~obj
 5459:         else:
 5460:             keep = ones(N, dtype=bool)
 5461:             keep[obj,] = False
 5462: 
 5463:         slobj[axis] = keep
 5464:         new = arr[tuple(slobj)]
 5465: 
 5466:     return conv.wrap(new, to_scalar=False)
 5467: 
 5468: 
 5469: def _insert_dispatcher(arr, obj, values, axis=None):
 5470:     return (arr, obj, values)
 5471: 
 5472: 
 5473: @array_function_dispatch(_insert_dispatcher)
 5474: def insert(arr, obj, values, axis=None):
 5475:     """
 5476:     Insert values along the given axis before the given indices.
 5477: 
 5478:     Parameters
 5479:     ----------
 5480:     arr : array_like
 5481:         Input array.
 5482:     obj : slice, int, array-like of ints or bools
 5483:         Object that defines the index or indices before which `values` is
 5484:         inserted.
 5485: 
 5486:         .. versionchanged:: 2.1.2
 5487:             Boolean indices are now treated as a mask of elements to insert,
 5488:             rather than being cast to the integers 0 and 1.
 5489: 
 5490:         Support for multiple insertions when `obj` is a single scalar or a
 5491:         sequence with one element (similar to calling insert multiple
 5492:         times).
 5493:     values : array_like
 5494:         Values to insert into `arr`. If the type of `values` is different
 5495:         from that of `arr`, `values` is converted to the type of `arr`.
 5496:         `values` should be shaped so that ``arr[...,obj,...] = values``
 5497:         is legal.
 5498:     axis : int, optional
 5499:         Axis along which to insert `values`.  If `axis` is None then `arr`
 5500:         is flattened first.
 5501: 
 5502:     Returns
 5503:     -------
 5504:     out : ndarray
 5505:         A copy of `arr` with `values` inserted.  Note that `insert`
 5506:         does not occur in-place: a new array is returned. If
 5507:         `axis` is None, `out` is a flattened array.
 5508: 
 5509:     See Also
 5510:     --------
 5511:     append : Append elements at the end of an array.
 5512:     concatenate : Join a sequence of arrays along an existing axis.
 5513:     delete : Delete elements from an array.
 5514: 
 5515:     Notes
 5516:     -----
 5517:     Note that for higher dimensional inserts ``obj=0`` behaves very different
 5518:     from ``obj=[0]`` just like ``arr[:,0,:] = values`` is different from
 5519:     ``arr[:,[0],:] = values``. This is because of the difference between basic
 5520:     and advanced :ref:`indexing <basics.indexing>`.
 5521: 
 5522:     Examples
 5523:     --------
 5524:     >>> import numpy as np
 5525:     >>> a = np.arange(6).reshape(3, 2)
 5526:     >>> a
 5527:     array([[0, 1],
 5528:            [2, 3],
 5529:            [4, 5]])
 5530:     >>> np.insert(a, 1, 6)
 5531:     array([0, 6, 1, 2, 3, 4, 5])
 5532:     >>> np.insert(a, 1, 6, axis=1)
 5533:     array([[0, 6, 1],
 5534:            [2, 6, 3],
 5535:            [4, 6, 5]])
 5536: 
 5537:     Difference between sequence and scalars,
 5538:     showing how ``obj=[1]`` behaves different from ``obj=1``:
 5539: 
 5540:     >>> np.insert(a, [1], [[7],[8],[9]], axis=1)
 5541:     array([[0, 7, 1],
 5542:            [2, 8, 3],
 5543:            [4, 9, 5]])
 5544:     >>> np.insert(a, 1, [[7],[8],[9]], axis=1)
 5545:     array([[0, 7, 8, 9, 1],
 5546:            [2, 7, 8, 9, 3],
 5547:            [4, 7, 8, 9, 5]])
 5548:     >>> np.array_equal(np.insert(a, 1, [7, 8, 9], axis=1),
 5549:     ...                np.insert(a, [1], [[7],[8],[9]], axis=1))
 5550:     True
 5551: 
 5552:     >>> b = a.flatten()
 5553:     >>> b
 5554:     array([0, 1, 2, 3, 4, 5])
 5555:     >>> np.insert(b, [2, 2], [6, 7])
 5556:     array([0, 1, 6, 7, 2, 3, 4, 5])
 5557: 
 5558:     >>> np.insert(b, slice(2, 4), [7, 8])
 5559:     array([0, 1, 7, 2, 8, 3, 4, 5])
 5560: 
 5561:     >>> np.insert(b, [2, 2], [7.13, False]) # type casting
 5562:     array([0, 1, 7, 0, 2, 3, 4, 5])
 5563: 
 5564:     >>> x = np.arange(8).reshape(2, 4)
 5565:     >>> idx = (1, 3)
 5566:     >>> np.insert(x, idx, 999, axis=1)
 5567:     array([[  0, 999,   1,   2, 999,   3],
 5568:            [  4, 999,   5,   6, 999,   7]])
 5569: 
 5570:     """
 5571:     conv = _array_converter(arr)
 5572:     arr, = conv.as_arrays(subok=False)
 5573: 
 5574:     ndim = arr.ndim
 5575:     arrorder = 'F' if arr.flags.fnc else 'C'
 5576:     if axis is None:
 5577:         if ndim != 1:
 5578:             arr = arr.ravel()
 5579:         # needed for np.matrix, which is still not 1d after being ravelled
 5580:         ndim = arr.ndim
 5581:         axis = ndim - 1
 5582:     else:
 5583:         axis = normalize_axis_index(axis, ndim)
 5584:     slobj = [slice(None)] * ndim
 5585:     N = arr.shape[axis]
 5586:     newshape = list(arr.shape)
 5587: 
 5588:     if isinstance(obj, slice):
 5589:         # turn it into a range object
 5590:         indices = arange(*obj.indices(N), dtype=intp)
 5591:     else:
 5592:         # need to copy obj, because indices will be changed in-place
 5593:         indices = np.array(obj)
 5594:         if indices.dtype == bool:
 5595:             if obj.ndim != 1:
 5596:                 raise ValueError('boolean array argument obj to insert '
 5597:                                 'must be one dimensional')
 5598:             indices = np.flatnonzero(obj)
 5599:         elif indices.ndim > 1:
 5600:             raise ValueError(
 5601:                 "index array argument obj to insert must be one dimensional "
 5602:                 "or scalar")
 5603:     if indices.size == 1:
 5604:         index = indices.item()
 5605:         if index < -N or index > N:
 5606:             raise IndexError(f"index {obj} is out of bounds for axis {axis} "
 5607:                              f"with size {N}")
 5608:         if (index < 0):
 5609:             index += N
 5610: 
 5611:         # There are some object array corner cases here, but we cannot avoid
 5612:         # that:
 5613:         values = array(values, copy=None, ndmin=arr.ndim, dtype=arr.dtype)
 5614:         if indices.ndim == 0:
 5615:             # broadcasting is very different here, since a[:,0,:] = ... behaves
 5616:             # very different from a[:,[0],:] = ...! This changes values so that
 5617:             # it works likes the second case. (here a[:,0:1,:])
 5618:             values = np.moveaxis(values, 0, axis)
 5619:         numnew = values.shape[axis]
 5620:         newshape[axis] += numnew
 5621:         new = empty(newshape, arr.dtype, arrorder)
 5622:         slobj[axis] = slice(None, index)
 5623:         new[tuple(slobj)] = arr[tuple(slobj)]
 5624:         slobj[axis] = slice(index, index + numnew)
 5625:         new[tuple(slobj)] = values
 5626:         slobj[axis] = slice(index + numnew, None)
 5627:         slobj2 = [slice(None)] * ndim
 5628:         slobj2[axis] = slice(index, None)
 5629:         new[tuple(slobj)] = arr[tuple(slobj2)]
 5630: 
 5631:         return conv.wrap(new, to_scalar=False)
 5632: 
 5633:     elif indices.size == 0 and not isinstance(obj, np.ndarray):
 5634:         # Can safely cast the empty list to intp
 5635:         indices = indices.astype(intp)
 5636: 
 5637:     indices[indices < 0] += N
 5638: 
 5639:     numnew = len(indices)
 5640:     order = indices.argsort(kind='mergesort')   # stable sort
 5641:     indices[order] += np.arange(numnew)
 5642: 
 5643:     newshape[axis] += numnew
 5644:     old_mask = ones(newshape[axis], dtype=bool)
 5645:     old_mask[indices] = False
 5646: 
 5647:     new = empty(newshape, arr.dtype, arrorder)
 5648:     slobj2 = [slice(None)] * ndim
 5649:     slobj[axis] = indices
 5650:     slobj2[axis] = old_mask
 5651:     new[tuple(slobj)] = values
 5652:     new[tuple(slobj2)] = arr
 5653: 
 5654:     return conv.wrap(new, to_scalar=False)
 5655: 
 5656: 
 5657: def _append_dispatcher(arr, values, axis=None):
 5658:     return (arr, values)
 5659: 
 5660: 
 5661: @array_function_dispatch(_append_dispatcher)
 5662: def append(arr, values, axis=None):
 5663:     """
 5664:     Append values to the end of an array.
 5665: 
 5666:     Parameters
 5667:     ----------
 5668:     arr : array_like
 5669:         Values are appended to a copy of this array.
 5670:     values : array_like
 5671:         These values are appended to a copy of `arr`.  It must be of the
 5672:         correct shape (the same shape as `arr`, excluding `axis`).  If
 5673:         `axis` is not specified, `values` can be any shape and will be
 5674:         flattened before use.
 5675:     axis : int, optional
 5676:         The axis along which `values` are appended.  If `axis` is not
 5677:         given, both `arr` and `values` are flattened before use.
 5678: 
 5679:     Returns
 5680:     -------
 5681:     append : ndarray
 5682:         A copy of `arr` with `values` appended to `axis`.  Note that
 5683:         `append` does not occur in-place: a new array is allocated and
 5684:         filled.  If `axis` is None, `out` is a flattened array.
 5685: 
 5686:     See Also
 5687:     --------
 5688:     insert : Insert elements into an array.
 5689:     delete : Delete elements from an array.
 5690: 
 5691:     Examples
 5692:     --------
 5693:     >>> import numpy as np
 5694:     >>> np.append([1, 2, 3], [[4, 5, 6], [7, 8, 9]])
 5695:     array([1, 2, 3, ..., 7, 8, 9])
 5696: 
 5697:     When `axis` is specified, `values` must have the correct shape.
 5698: 
 5699:     >>> np.append([[1, 2, 3], [4, 5, 6]], [[7, 8, 9]], axis=0)
 5700:     array([[1, 2, 3],
 5701:            [4, 5, 6],
 5702:            [7, 8, 9]])
 5703: 
 5704:     >>> np.append([[1, 2, 3], [4, 5, 6]], [7, 8, 9], axis=0)
 5705:     Traceback (most recent call last):
 5706:         ...
 5707:     ValueError: all the input arrays must have same number of dimensions, but
 5708:     the array at index 0 has 2 dimension(s) and the array at index 1 has 1
 5709:     dimension(s)
 5710: 
 5711:     >>> a = np.array([1, 2], dtype=int)
 5712:     >>> c = np.append(a, [])
 5713:     >>> c
 5714:     array([1., 2.])
 5715:     >>> c.dtype
 5716:     float64
 5717: 
 5718:     Default dtype for empty ndarrays is `float64` thus making the output of dtype
 5719:     `float64` when appended with dtype `int64`
 5720: 
 5721:     """
 5722:     arr = asanyarray(arr)
 5723:     if axis is None:
 5724:         if arr.ndim != 1:
 5725:             arr = arr.ravel()
 5726:         values = ravel(values)
 5727:         axis = arr.ndim - 1
 5728:     return concatenate((arr, values), axis=axis)
 5729: 
 5730: 
 5731: def _digitize_dispatcher(x, bins, right=None):
 5732:     return (x, bins)
 5733: 
 5734: 
 5735: @array_function_dispatch(_digitize_dispatcher)
 5736: def digitize(x, bins, right=False):
 5737:     """
 5738:     Return the indices of the bins to which each value in input array belongs.
 5739: 
 5740:     =========  =============  ============================
 5741:     `right`    order of bins  returned index `i` satisfies
 5742:     =========  =============  ============================
 5743:     ``False``  increasing     ``bins[i-1] <= x < bins[i]``
 5744:     ``True``   increasing     ``bins[i-1] < x <= bins[i]``
 5745:     ``False``  decreasing     ``bins[i-1] > x >= bins[i]``
 5746:     ``True``   decreasing     ``bins[i-1] >= x > bins[i]``
 5747:     =========  =============  ============================
 5748: 
 5749:     If values in `x` are beyond the bounds of `bins`, 0 or ``len(bins)`` is
 5750:     returned as appropriate.
 5751: 
 5752:     Parameters
 5753:     ----------
 5754:     x : array_like
 5755:         Input array to be binned. Prior to NumPy 1.10.0, this array had to
 5756:         be 1-dimensional, but can now have any shape.
 5757:     bins : array_like
 5758:         Array of bins. It has to be 1-dimensional and monotonic.
 5759:     right : bool, optional
 5760:         Indicating whether the intervals include the right or the left bin
 5761:         edge. Default behavior is (right==False) indicating that the interval
 5762:         does not include the right edge. The left bin end is open in this
 5763:         case, i.e., bins[i-1] <= x < bins[i] is the default behavior for
 5764:         monotonically increasing bins.
 5765: 
 5766:     Returns
 5767:     -------
 5768:     indices : ndarray of ints
 5769:         Output array of indices, of same shape as `x`.
 5770: 
 5771:     Raises
 5772:     ------
 5773:     ValueError
 5774:         If `bins` is not monotonic.
 5775:     TypeError
 5776:         If the type of the input is complex.
 5777: 
 5778:     See Also
 5779:     --------
 5780:     bincount, histogram, unique, searchsorted
 5781: 
 5782:     Notes
 5783:     -----
 5784:     If values in `x` are such that they fall outside the bin range,
 5785:     attempting to index `bins` with the indices that `digitize` returns
 5786:     will result in an IndexError.
 5787: 
 5788:     .. versionadded:: 1.10.0
 5789: 
 5790:     `numpy.digitize` is  implemented in terms of `numpy.searchsorted`.
 5791:     This means that a binary search is used to bin the values, which scales
 5792:     much better for larger number of bins than the previous linear search.
 5793:     It also removes the requirement for the input array to be 1-dimensional.
 5794: 
 5795:     For monotonically *increasing* `bins`, the following are equivalent::
 5796: 
 5797:         np.digitize(x, bins, right=True)
 5798:         np.searchsorted(bins, x, side='left')
 5799: 
 5800:     Note that as the order of the arguments are reversed, the side must be too.
 5801:     The `searchsorted` call is marginally faster, as it does not do any
 5802:     monotonicity checks. Perhaps more importantly, it supports all dtypes.
 5803: 
 5804:     Examples
 5805:     --------
 5806:     >>> import numpy as np
 5807:     >>> x = np.array([0.2, 6.4, 3.0, 1.6])
 5808:     >>> bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0])
 5809:     >>> inds = np.digitize(x, bins)
 5810:     >>> inds
 5811:     array([1, 4, 3, 2])
 5812:     >>> for n in range(x.size):
 5813:     ...   print(bins[inds[n]-1], "<=", x[n], "<", bins[inds[n]])
 5814:     ...
 5815:     0.0 <= 0.2 < 1.0
 5816:     4.0 <= 6.4 < 10.0
 5817:     2.5 <= 3.0 < 4.0
 5818:     1.0 <= 1.6 < 2.5
 5819: 
 5820:     >>> x = np.array([1.2, 10.0, 12.4, 15.5, 20.])
 5821:     >>> bins = np.array([0, 5, 10, 15, 20])
 5822:     >>> np.digitize(x,bins,right=True)
 5823:     array([1, 2, 3, 4, 4])
 5824:     >>> np.digitize(x,bins,right=False)
 5825:     array([1, 3, 3, 4, 5])
 5826:     """
 5827:     x = _nx.asarray(x)
 5828:     bins = _nx.asarray(bins)
 5829: 
 5830:     # here for compatibility, searchsorted below is happy to take this
 5831:     if np.issubdtype(x.dtype, _nx.complexfloating):
 5832:         raise TypeError("x may not be complex")
 5833: 
 5834:     mono = _monotonicity(bins)
 5835:     if mono == 0:
 5836:         raise ValueError("bins must be monotonically increasing or decreasing")
 5837: 
 5838:     # this is backwards because the arguments below are swapped
 5839:     side = 'left' if right else 'right'
 5840:     if mono == -1:
 5841:         # reverse the bins, and invert the results
 5842:         return len(bins) - _nx.searchsorted(bins[::-1], x, side=side)
 5843:     else:
 5844:         return _nx.searchsorted(bins, x, side=side)
