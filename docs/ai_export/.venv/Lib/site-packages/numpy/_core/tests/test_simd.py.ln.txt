    1: # NOTE: Please avoid the use of numpy.testing since NPYV intrinsics
    2: # may be involved in their functionality.
    3: import itertools
    4: import math
    5: import operator
    6: import re
    7: 
    8: import pytest
    9: from numpy._core._multiarray_umath import __cpu_baseline__
   10: 
   11: from numpy._core._simd import clear_floatstatus, get_floatstatus, targets
   12: 
   13: 
   14: def check_floatstatus(divbyzero=False, overflow=False,
   15:                       underflow=False, invalid=False,
   16:                       all=False):
   17:     #define NPY_FPE_DIVIDEBYZERO  1
   18:     #define NPY_FPE_OVERFLOW      2
   19:     #define NPY_FPE_UNDERFLOW     4
   20:     #define NPY_FPE_INVALID       8
   21:     err = get_floatstatus()
   22:     ret = (all or divbyzero) and (err & 1) != 0
   23:     ret |= (all or overflow) and (err & 2) != 0
   24:     ret |= (all or underflow) and (err & 4) != 0
   25:     ret |= (all or invalid) and (err & 8) != 0
   26:     return ret
   27: 
   28: class _Test_Utility:
   29:     # submodule of the desired SIMD extension, e.g. targets["AVX512F"]
   30:     npyv = None
   31:     # the current data type suffix e.g. 's8'
   32:     sfx = None
   33:     # target name can be 'baseline' or one or more of CPU features
   34:     target_name = None
   35: 
   36:     def __getattr__(self, attr):
   37:         """
   38:         To call NPV intrinsics without the attribute 'npyv' and
   39:         auto suffixing intrinsics according to class attribute 'sfx'
   40:         """
   41:         return getattr(self.npyv, attr + "_" + self.sfx)
   42: 
   43:     def _x2(self, intrin_name):
   44:         return getattr(self.npyv, f"{intrin_name}_{self.sfx}x2")
   45: 
   46:     def _data(self, start=None, count=None, reverse=False):
   47:         """
   48:         Create list of consecutive numbers according to number of vector's lanes.
   49:         """
   50:         if start is None:
   51:             start = 1
   52:         if count is None:
   53:             count = self.nlanes
   54:         rng = range(start, start + count)
   55:         if reverse:
   56:             rng = reversed(rng)
   57:         if self._is_fp():
   58:             return [x / 1.0 for x in rng]
   59:         return list(rng)
   60: 
   61:     def _is_unsigned(self):
   62:         return self.sfx[0] == 'u'
   63: 
   64:     def _is_signed(self):
   65:         return self.sfx[0] == 's'
   66: 
   67:     def _is_fp(self):
   68:         return self.sfx[0] == 'f'
   69: 
   70:     def _scalar_size(self):
   71:         return int(self.sfx[1:])
   72: 
   73:     def _int_clip(self, seq):
   74:         if self._is_fp():
   75:             return seq
   76:         max_int = self._int_max()
   77:         min_int = self._int_min()
   78:         return [min(max(v, min_int), max_int) for v in seq]
   79: 
   80:     def _int_max(self):
   81:         if self._is_fp():
   82:             return None
   83:         max_u = self._to_unsigned(self.setall(-1))[0]
   84:         if self._is_signed():
   85:             return max_u // 2
   86:         return max_u
   87: 
   88:     def _int_min(self):
   89:         if self._is_fp():
   90:             return None
   91:         if self._is_unsigned():
   92:             return 0
   93:         return -(self._int_max() + 1)
   94: 
   95:     def _true_mask(self):
   96:         max_unsig = getattr(self.npyv, "setall_u" + self.sfx[1:])(-1)
   97:         return max_unsig[0]
   98: 
   99:     def _to_unsigned(self, vector):
  100:         if isinstance(vector, (list, tuple)):
  101:             return getattr(self.npyv, "load_u" + self.sfx[1:])(vector)
  102:         else:
  103:             sfx = vector.__name__.replace("npyv_", "")
  104:             if sfx[0] == "b":
  105:                 cvt_intrin = "cvt_u{0}_b{0}"
  106:             else:
  107:                 cvt_intrin = "reinterpret_u{0}_{1}"
  108:             return getattr(self.npyv, cvt_intrin.format(sfx[1:], sfx))(vector)
  109: 
  110:     def _pinfinity(self):
  111:         return float("inf")
  112: 
  113:     def _ninfinity(self):
  114:         return -float("inf")
  115: 
  116:     def _nan(self):
  117:         return float("nan")
  118: 
  119:     def _cpu_features(self):
  120:         target = self.target_name
  121:         if target == "baseline":
  122:             target = __cpu_baseline__
  123:         else:
  124:             target = target.split('__')  # multi-target separator
  125:         return ' '.join(target)
  126: 
  127: class _SIMD_BOOL(_Test_Utility):
  128:     """
  129:     To test all boolean vector types at once
  130:     """
  131:     def _nlanes(self):
  132:         return getattr(self.npyv, "nlanes_u" + self.sfx[1:])
  133: 
  134:     def _data(self, start=None, count=None, reverse=False):
  135:         true_mask = self._true_mask()
  136:         rng = range(self._nlanes())
  137:         if reverse:
  138:             rng = reversed(rng)
  139:         return [true_mask if x % 2 else 0 for x in rng]
  140: 
  141:     def _load_b(self, data):
  142:         len_str = self.sfx[1:]
  143:         load = getattr(self.npyv, "load_u" + len_str)
  144:         cvt = getattr(self.npyv, f"cvt_b{len_str}_u{len_str}")
  145:         return cvt(load(data))
  146: 
  147:     def test_operators_logical(self):
  148:         """
  149:         Logical operations for boolean types.
  150:         Test intrinsics:
  151:             npyv_xor_##SFX, npyv_and_##SFX, npyv_or_##SFX, npyv_not_##SFX,
  152:             npyv_andc_b8, npvy_orc_b8, nvpy_xnor_b8
  153:         """
  154:         data_a = self._data()
  155:         data_b = self._data(reverse=True)
  156:         vdata_a = self._load_b(data_a)
  157:         vdata_b = self._load_b(data_b)
  158: 
  159:         data_and = [a & b for a, b in zip(data_a, data_b)]
  160:         vand = getattr(self, "and")(vdata_a, vdata_b)
  161:         assert vand == data_and
  162: 
  163:         data_or = [a | b for a, b in zip(data_a, data_b)]
  164:         vor = getattr(self, "or")(vdata_a, vdata_b)
  165:         assert vor == data_or
  166: 
  167:         data_xor = [a ^ b for a, b in zip(data_a, data_b)]
  168:         vxor = self.xor(vdata_a, vdata_b)
  169:         assert vxor == data_xor
  170: 
  171:         vnot = getattr(self, "not")(vdata_a)
  172:         assert vnot == data_b
  173: 
  174:         # among the boolean types, andc, orc and xnor only support b8
  175:         if self.sfx not in ("b8"):
  176:             return
  177: 
  178:         data_andc = [(a & ~b) & 0xFF for a, b in zip(data_a, data_b)]
  179:         vandc = self.andc(vdata_a, vdata_b)
  180:         assert data_andc == vandc
  181: 
  182:         data_orc = [(a | ~b) & 0xFF for a, b in zip(data_a, data_b)]
  183:         vorc = self.orc(vdata_a, vdata_b)
  184:         assert data_orc == vorc
  185: 
  186:         data_xnor = [~(a ^ b) & 0xFF for a, b in zip(data_a, data_b)]
  187:         vxnor = self.xnor(vdata_a, vdata_b)
  188:         assert data_xnor == vxnor
  189: 
  190:     def test_tobits(self):
  191:         data2bits = lambda data: sum(int(x != 0) << i for i, x in enumerate(data, 0))
  192:         for data in (self._data(), self._data(reverse=True)):
  193:             vdata = self._load_b(data)
  194:             data_bits = data2bits(data)
  195:             tobits = self.tobits(vdata)
  196:             bin_tobits = bin(tobits)
  197:             assert bin_tobits == bin(data_bits)
  198: 
  199:     def test_pack(self):
  200:         """
  201:         Pack multiple vectors into one
  202:         Test intrinsics:
  203:             npyv_pack_b8_b16
  204:             npyv_pack_b8_b32
  205:             npyv_pack_b8_b64
  206:         """
  207:         if self.sfx not in ("b16", "b32", "b64"):
  208:             return
  209:         # create the vectors
  210:         data = self._data()
  211:         rdata = self._data(reverse=True)
  212:         vdata = self._load_b(data)
  213:         vrdata = self._load_b(rdata)
  214:         pack_simd = getattr(self.npyv, f"pack_b8_{self.sfx}")
  215:         # for scalar execution, concatenate the elements of the multiple lists
  216:         # into a single list (spack) and then iterate over the elements of
  217:         # the created list applying a mask to capture the first byte of them.
  218:         if self.sfx == "b16":
  219:             spack = [(i & 0xFF) for i in (list(rdata) + list(data))]
  220:             vpack = pack_simd(vrdata, vdata)
  221:         elif self.sfx == "b32":
  222:             spack = [(i & 0xFF) for i in (2 * list(rdata) + 2 * list(data))]
  223:             vpack = pack_simd(vrdata, vrdata, vdata, vdata)
  224:         elif self.sfx == "b64":
  225:             spack = [(i & 0xFF) for i in (4 * list(rdata) + 4 * list(data))]
  226:             vpack = pack_simd(vrdata, vrdata, vrdata, vrdata,
  227:                                vdata,  vdata,  vdata,  vdata)
  228:         assert vpack == spack
  229: 
  230:     @pytest.mark.parametrize("intrin", ["any", "all"])
  231:     @pytest.mark.parametrize("data", (
  232:         [-1, 0],
  233:         [0, -1],
  234:         [-1],
  235:         [0]
  236:     ))
  237:     def test_operators_crosstest(self, intrin, data):
  238:         """
  239:         Test intrinsics:
  240:             npyv_any_##SFX
  241:             npyv_all_##SFX
  242:         """
  243:         data_a = self._load_b(data * self._nlanes())
  244:         func = eval(intrin)
  245:         intrin = getattr(self, intrin)
  246:         desired = func(data_a)
  247:         simd = intrin(data_a)
  248:         assert not not simd == desired
  249: 
  250: class _SIMD_INT(_Test_Utility):
  251:     """
  252:     To test all integer vector types at once
  253:     """
  254:     def test_operators_shift(self):
  255:         if self.sfx in ("u8", "s8"):
  256:             return
  257: 
  258:         data_a = self._data(self._int_max() - self.nlanes)
  259:         data_b = self._data(self._int_min(), reverse=True)
  260:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
  261: 
  262:         for count in range(self._scalar_size()):
  263:             # load to cast
  264:             data_shl_a = self.load([a << count for a in data_a])
  265:             # left shift
  266:             shl = self.shl(vdata_a, count)
  267:             assert shl == data_shl_a
  268:             # load to cast
  269:             data_shr_a = self.load([a >> count for a in data_a])
  270:             # right shift
  271:             shr = self.shr(vdata_a, count)
  272:             assert shr == data_shr_a
  273: 
  274:         # shift by zero or max or out-range immediate constant is not applicable and illogical
  275:         for count in range(1, self._scalar_size()):
  276:             # load to cast
  277:             data_shl_a = self.load([a << count for a in data_a])
  278:             # left shift by an immediate constant
  279:             shli = self.shli(vdata_a, count)
  280:             assert shli == data_shl_a
  281:             # load to cast
  282:             data_shr_a = self.load([a >> count for a in data_a])
  283:             # right shift by an immediate constant
  284:             shri = self.shri(vdata_a, count)
  285:             assert shri == data_shr_a
  286: 
  287:     def test_arithmetic_subadd_saturated(self):
  288:         if self.sfx in ("u32", "s32", "u64", "s64"):
  289:             return
  290: 
  291:         data_a = self._data(self._int_max() - self.nlanes)
  292:         data_b = self._data(self._int_min(), reverse=True)
  293:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
  294: 
  295:         data_adds = self._int_clip([a + b for a, b in zip(data_a, data_b)])
  296:         adds = self.adds(vdata_a, vdata_b)
  297:         assert adds == data_adds
  298: 
  299:         data_subs = self._int_clip([a - b for a, b in zip(data_a, data_b)])
  300:         subs = self.subs(vdata_a, vdata_b)
  301:         assert subs == data_subs
  302: 
  303:     def test_math_max_min(self):
  304:         data_a = self._data()
  305:         data_b = self._data(self.nlanes)
  306:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
  307: 
  308:         data_max = [max(a, b) for a, b in zip(data_a, data_b)]
  309:         simd_max = self.max(vdata_a, vdata_b)
  310:         assert simd_max == data_max
  311: 
  312:         data_min = [min(a, b) for a, b in zip(data_a, data_b)]
  313:         simd_min = self.min(vdata_a, vdata_b)
  314:         assert simd_min == data_min
  315: 
  316:     @pytest.mark.parametrize("start", [-100, -10000, 0, 100, 10000])
  317:     def test_reduce_max_min(self, start):
  318:         """
  319:         Test intrinsics:
  320:             npyv_reduce_max_##sfx
  321:             npyv_reduce_min_##sfx
  322:         """
  323:         vdata_a = self.load(self._data(start))
  324:         assert self.reduce_max(vdata_a) == max(vdata_a)
  325:         assert self.reduce_min(vdata_a) == min(vdata_a)
  326: 
  327: 
  328: class _SIMD_FP32(_Test_Utility):
  329:     """
  330:     To only test single precision
  331:     """
  332:     def test_conversions(self):
  333:         """
  334:         Round to nearest even integer, assume CPU control register is set to rounding.
  335:         Test intrinsics:
  336:             npyv_round_s32_##SFX
  337:         """
  338:         features = self._cpu_features()
  339:         if not self.npyv.simd_f64 and re.match(r".*(NEON|ASIMD)", features):
  340:             # very costly to emulate nearest even on Armv7
  341:             # instead we round halves to up. e.g. 0.5 -> 1, -0.5 -> -1
  342:             _round = lambda v: int(v + (0.5 if v >= 0 else -0.5))
  343:         else:
  344:             _round = round
  345:         vdata_a = self.load(self._data())
  346:         vdata_a = self.sub(vdata_a, self.setall(0.5))
  347:         data_round = [_round(x) for x in vdata_a]
  348:         vround = self.round_s32(vdata_a)
  349:         assert vround == data_round
  350: 
  351: class _SIMD_FP64(_Test_Utility):
  352:     """
  353:     To only test double precision
  354:     """
  355:     def test_conversions(self):
  356:         """
  357:         Round to nearest even integer, assume CPU control register is set to rounding.
  358:         Test intrinsics:
  359:             npyv_round_s32_##SFX
  360:         """
  361:         vdata_a = self.load(self._data())
  362:         vdata_a = self.sub(vdata_a, self.setall(0.5))
  363:         vdata_b = self.mul(vdata_a, self.setall(-1.5))
  364:         data_round = [round(x) for x in list(vdata_a) + list(vdata_b)]
  365:         vround = self.round_s32(vdata_a, vdata_b)
  366:         assert vround == data_round
  367: 
  368: class _SIMD_FP(_Test_Utility):
  369:     """
  370:     To test all float vector types at once
  371:     """
  372:     def test_arithmetic_fused(self):
  373:         vdata_a, vdata_b, vdata_c = [self.load(self._data())] * 3
  374:         vdata_cx2 = self.add(vdata_c, vdata_c)
  375:         # multiply and add, a*b + c
  376:         data_fma = self.load([a * b + c for a, b, c in zip(vdata_a, vdata_b, vdata_c)])
  377:         fma = self.muladd(vdata_a, vdata_b, vdata_c)
  378:         assert fma == data_fma
  379:         # multiply and subtract, a*b - c
  380:         fms = self.mulsub(vdata_a, vdata_b, vdata_c)
  381:         data_fms = self.sub(data_fma, vdata_cx2)
  382:         assert fms == data_fms
  383:         # negate multiply and add, -(a*b) + c
  384:         nfma = self.nmuladd(vdata_a, vdata_b, vdata_c)
  385:         data_nfma = self.sub(vdata_cx2, data_fma)
  386:         assert nfma == data_nfma
  387:         # negate multiply and subtract, -(a*b) - c
  388:         nfms = self.nmulsub(vdata_a, vdata_b, vdata_c)
  389:         data_nfms = self.mul(data_fma, self.setall(-1))
  390:         assert nfms == data_nfms
  391:         # multiply, add for odd elements and subtract even elements.
  392:         # (a * b) -+ c
  393:         fmas = list(self.muladdsub(vdata_a, vdata_b, vdata_c))
  394:         assert fmas[0::2] == list(data_fms)[0::2]
  395:         assert fmas[1::2] == list(data_fma)[1::2]
  396: 
  397:     def test_abs(self):
  398:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  399:         data = self._data()
  400:         vdata = self.load(self._data())
  401: 
  402:         abs_cases = ((-0, 0), (ninf, pinf), (pinf, pinf), (nan, nan))
  403:         for case, desired in abs_cases:
  404:             data_abs = [desired] * self.nlanes
  405:             vabs = self.abs(self.setall(case))
  406:             assert vabs == pytest.approx(data_abs, nan_ok=True)
  407: 
  408:         vabs = self.abs(self.mul(vdata, self.setall(-1)))
  409:         assert vabs == data
  410: 
  411:     def test_sqrt(self):
  412:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  413:         data = self._data()
  414:         vdata = self.load(self._data())
  415: 
  416:         sqrt_cases = ((-0.0, -0.0), (0.0, 0.0), (-1.0, nan), (ninf, nan), (pinf, pinf))
  417:         for case, desired in sqrt_cases:
  418:             data_sqrt = [desired] * self.nlanes
  419:             sqrt = self.sqrt(self.setall(case))
  420:             assert sqrt == pytest.approx(data_sqrt, nan_ok=True)
  421: 
  422:         data_sqrt = self.load([math.sqrt(x) for x in data])  # load to truncate precision
  423:         sqrt = self.sqrt(vdata)
  424:         assert sqrt == data_sqrt
  425: 
  426:     def test_square(self):
  427:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  428:         data = self._data()
  429:         vdata = self.load(self._data())
  430:         # square
  431:         square_cases = ((nan, nan), (pinf, pinf), (ninf, pinf))
  432:         for case, desired in square_cases:
  433:             data_square = [desired] * self.nlanes
  434:             square = self.square(self.setall(case))
  435:             assert square == pytest.approx(data_square, nan_ok=True)
  436: 
  437:         data_square = [x * x for x in data]
  438:         square = self.square(vdata)
  439:         assert square == data_square
  440: 
  441:     @pytest.mark.parametrize("intrin, func", [("ceil", math.ceil),
  442:     ("trunc", math.trunc), ("floor", math.floor), ("rint", round)])
  443:     def test_rounding(self, intrin, func):
  444:         """
  445:         Test intrinsics:
  446:             npyv_rint_##SFX
  447:             npyv_ceil_##SFX
  448:             npyv_trunc_##SFX
  449:             npyv_floor##SFX
  450:         """
  451:         intrin_name = intrin
  452:         intrin = getattr(self, intrin)
  453:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  454:         # special cases
  455:         round_cases = ((nan, nan), (pinf, pinf), (ninf, ninf))
  456:         for case, desired in round_cases:
  457:             data_round = [desired] * self.nlanes
  458:             _round = intrin(self.setall(case))
  459:             assert _round == pytest.approx(data_round, nan_ok=True)
  460: 
  461:         for x in range(0, 2**20, 256**2):
  462:             for w in (-1.05, -1.10, -1.15, 1.05, 1.10, 1.15):
  463:                 data = self.load([(x + a) * w for a in range(self.nlanes)])
  464:                 data_round = [func(x) for x in data]
  465:                 _round = intrin(data)
  466:                 assert _round == data_round
  467: 
  468:         # test large numbers
  469:         for i in (
  470:             1.1529215045988576e+18, 4.6116860183954304e+18,
  471:             5.902958103546122e+20, 2.3611832414184488e+21
  472:         ):
  473:             x = self.setall(i)
  474:             y = intrin(x)
  475:             data_round = [func(n) for n in x]
  476:             assert y == data_round
  477: 
  478:         # signed zero
  479:         if intrin_name == "floor":
  480:             data_szero = (-0.0,)
  481:         else:
  482:             data_szero = (-0.0, -0.25, -0.30, -0.45, -0.5)
  483: 
  484:         for w in data_szero:
  485:             _round = self._to_unsigned(intrin(self.setall(w)))
  486:             data_round = self._to_unsigned(self.setall(-0.0))
  487:             assert _round == data_round
  488: 
  489:     @pytest.mark.parametrize("intrin", [
  490:         "max", "maxp", "maxn", "min", "minp", "minn"
  491:     ])
  492:     def test_max_min(self, intrin):
  493:         """
  494:         Test intrinsics:
  495:             npyv_max_##sfx
  496:             npyv_maxp_##sfx
  497:             npyv_maxn_##sfx
  498:             npyv_min_##sfx
  499:             npyv_minp_##sfx
  500:             npyv_minn_##sfx
  501:             npyv_reduce_max_##sfx
  502:             npyv_reduce_maxp_##sfx
  503:             npyv_reduce_maxn_##sfx
  504:             npyv_reduce_min_##sfx
  505:             npyv_reduce_minp_##sfx
  506:             npyv_reduce_minn_##sfx
  507:         """
  508:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  509:         chk_nan = {"xp": 1, "np": 1, "nn": 2, "xn": 2}.get(intrin[-2:], 0)
  510:         func = eval(intrin[:3])
  511:         reduce_intrin = getattr(self, "reduce_" + intrin)
  512:         intrin = getattr(self, intrin)
  513:         hf_nlanes = self.nlanes // 2
  514: 
  515:         cases = (
  516:             ([0.0, -0.0], [-0.0, 0.0]),
  517:             ([10, -10],  [10, -10]),
  518:             ([pinf, 10], [10, ninf]),
  519:             ([10, pinf], [ninf, 10]),
  520:             ([10, -10], [10, -10]),
  521:             ([-10, 10], [-10, 10])
  522:         )
  523:         for op1, op2 in cases:
  524:             vdata_a = self.load(op1 * hf_nlanes)
  525:             vdata_b = self.load(op2 * hf_nlanes)
  526:             data = func(vdata_a, vdata_b)
  527:             simd = intrin(vdata_a, vdata_b)
  528:             assert simd == data
  529:             data = func(vdata_a)
  530:             simd = reduce_intrin(vdata_a)
  531:             assert simd == data
  532: 
  533:         if not chk_nan:
  534:             return
  535:         if chk_nan == 1:
  536:             test_nan = lambda a, b: (
  537:                 b if math.isnan(a) else a if math.isnan(b) else b
  538:             )
  539:         else:
  540:             test_nan = lambda a, b: (
  541:                 nan if math.isnan(a) or math.isnan(b) else b
  542:             )
  543:         cases = (
  544:             (nan, 10),
  545:             (10, nan),
  546:             (nan, pinf),
  547:             (pinf, nan),
  548:             (nan, nan)
  549:         )
  550:         for op1, op2 in cases:
  551:             vdata_ab = self.load([op1, op2] * hf_nlanes)
  552:             data = test_nan(op1, op2)
  553:             simd = reduce_intrin(vdata_ab)
  554:             assert simd == pytest.approx(data, nan_ok=True)
  555:             vdata_a = self.setall(op1)
  556:             vdata_b = self.setall(op2)
  557:             data = [data] * self.nlanes
  558:             simd = intrin(vdata_a, vdata_b)
  559:             assert simd == pytest.approx(data, nan_ok=True)
  560: 
  561:     def test_reciprocal(self):
  562:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  563:         data = self._data()
  564:         vdata = self.load(self._data())
  565: 
  566:         recip_cases = ((nan, nan), (pinf, 0.0), (ninf, -0.0), (0.0, pinf), (-0.0, ninf))
  567:         for case, desired in recip_cases:
  568:             data_recip = [desired] * self.nlanes
  569:             recip = self.recip(self.setall(case))
  570:             assert recip == pytest.approx(data_recip, nan_ok=True)
  571: 
  572:         data_recip = self.load([1 / x for x in data])  # load to truncate precision
  573:         recip = self.recip(vdata)
  574:         assert recip == data_recip
  575: 
  576:     def test_special_cases(self):
  577:         """
  578:         Compare Not NaN. Test intrinsics:
  579:             npyv_notnan_##SFX
  580:         """
  581:         nnan = self.notnan(self.setall(self._nan()))
  582:         assert nnan == [0] * self.nlanes
  583: 
  584:     @pytest.mark.parametrize("intrin_name", [
  585:         "rint", "trunc", "ceil", "floor"
  586:     ])
  587:     def test_unary_invalid_fpexception(self, intrin_name):
  588:         intrin = getattr(self, intrin_name)
  589:         for d in [float("nan"), float("inf"), -float("inf")]:
  590:             v = self.setall(d)
  591:             clear_floatstatus()
  592:             intrin(v)
  593:             assert check_floatstatus(invalid=True) is False
  594: 
  595:     @pytest.mark.parametrize('py_comp,np_comp', [
  596:         (operator.lt, "cmplt"),
  597:         (operator.le, "cmple"),
  598:         (operator.gt, "cmpgt"),
  599:         (operator.ge, "cmpge"),
  600:         (operator.eq, "cmpeq"),
  601:         (operator.ne, "cmpneq")
  602:     ])
  603:     def test_comparison_with_nan(self, py_comp, np_comp):
  604:         pinf, ninf, nan = self._pinfinity(), self._ninfinity(), self._nan()
  605:         mask_true = self._true_mask()
  606: 
  607:         def to_bool(vector):
  608:             return [lane == mask_true for lane in vector]
  609: 
  610:         intrin = getattr(self, np_comp)
  611:         cmp_cases = ((0, nan), (nan, 0), (nan, nan), (pinf, nan),
  612:                      (ninf, nan), (-0.0, +0.0))
  613:         for case_operand1, case_operand2 in cmp_cases:
  614:             data_a = [case_operand1] * self.nlanes
  615:             data_b = [case_operand2] * self.nlanes
  616:             vdata_a = self.setall(case_operand1)
  617:             vdata_b = self.setall(case_operand2)
  618:             vcmp = to_bool(intrin(vdata_a, vdata_b))
  619:             data_cmp = [py_comp(a, b) for a, b in zip(data_a, data_b)]
  620:             assert vcmp == data_cmp
  621: 
  622:     @pytest.mark.parametrize("intrin", ["any", "all"])
  623:     @pytest.mark.parametrize("data", (
  624:         [float("nan"), 0],
  625:         [0, float("nan")],
  626:         [float("nan"), 1],
  627:         [1, float("nan")],
  628:         [float("nan"), float("nan")],
  629:         [0.0, -0.0],
  630:         [-0.0, 0.0],
  631:         [1.0, -0.0]
  632:     ))
  633:     def test_operators_crosstest(self, intrin, data):
  634:         """
  635:         Test intrinsics:
  636:             npyv_any_##SFX
  637:             npyv_all_##SFX
  638:         """
  639:         data_a = self.load(data * self.nlanes)
  640:         func = eval(intrin)
  641:         intrin = getattr(self, intrin)
  642:         desired = func(data_a)
  643:         simd = intrin(data_a)
  644:         assert not not simd == desired
  645: 
  646: class _SIMD_ALL(_Test_Utility):
  647:     """
  648:     To test all vector types at once
  649:     """
  650:     def test_memory_load(self):
  651:         data = self._data()
  652:         # unaligned load
  653:         load_data = self.load(data)
  654:         assert load_data == data
  655:         # aligned load
  656:         loada_data = self.loada(data)
  657:         assert loada_data == data
  658:         # stream load
  659:         loads_data = self.loads(data)
  660:         assert loads_data == data
  661:         # load lower part
  662:         loadl = self.loadl(data)
  663:         loadl_half = list(loadl)[:self.nlanes // 2]
  664:         data_half = data[:self.nlanes // 2]
  665:         assert loadl_half == data_half
  666:         assert loadl != data  # detect overflow
  667: 
  668:     def test_memory_store(self):
  669:         data = self._data()
  670:         vdata = self.load(data)
  671:         # unaligned store
  672:         store = [0] * self.nlanes
  673:         self.store(store, vdata)
  674:         assert store == data
  675:         # aligned store
  676:         store_a = [0] * self.nlanes
  677:         self.storea(store_a, vdata)
  678:         assert store_a == data
  679:         # stream store
  680:         store_s = [0] * self.nlanes
  681:         self.stores(store_s, vdata)
  682:         assert store_s == data
  683:         # store lower part
  684:         store_l = [0] * self.nlanes
  685:         self.storel(store_l, vdata)
  686:         assert store_l[:self.nlanes // 2] == data[:self.nlanes // 2]
  687:         assert store_l != vdata  # detect overflow
  688:         # store higher part
  689:         store_h = [0] * self.nlanes
  690:         self.storeh(store_h, vdata)
  691:         assert store_h[:self.nlanes // 2] == data[self.nlanes // 2:]
  692:         assert store_h != vdata  # detect overflow
  693: 
  694:     @pytest.mark.parametrize("intrin, elsizes, scale, fill", [
  695:         ("self.load_tillz, self.load_till", (32, 64), 1, [0xffff]),
  696:         ("self.load2_tillz, self.load2_till", (32, 64), 2, [0xffff, 0x7fff]),
  697:     ])
  698:     def test_memory_partial_load(self, intrin, elsizes, scale, fill):
  699:         if self._scalar_size() not in elsizes:
  700:             return
  701:         npyv_load_tillz, npyv_load_till = eval(intrin)
  702:         data = self._data()
  703:         lanes = list(range(1, self.nlanes + 1))
  704:         lanes += [self.nlanes**2, self.nlanes**4]  # test out of range
  705:         for n in lanes:
  706:             load_till = npyv_load_till(data, n, *fill)
  707:             load_tillz = npyv_load_tillz(data, n)
  708:             n *= scale
  709:             data_till = data[:n] + fill * ((self.nlanes - n) // scale)
  710:             assert load_till == data_till
  711:             data_tillz = data[:n] + [0] * (self.nlanes - n)
  712:             assert load_tillz == data_tillz
  713: 
  714:     @pytest.mark.parametrize("intrin, elsizes, scale", [
  715:         ("self.store_till", (32, 64), 1),
  716:         ("self.store2_till", (32, 64), 2),
  717:     ])
  718:     def test_memory_partial_store(self, intrin, elsizes, scale):
  719:         if self._scalar_size() not in elsizes:
  720:             return
  721:         npyv_store_till = eval(intrin)
  722:         data = self._data()
  723:         data_rev = self._data(reverse=True)
  724:         vdata = self.load(data)
  725:         lanes = list(range(1, self.nlanes + 1))
  726:         lanes += [self.nlanes**2, self.nlanes**4]
  727:         for n in lanes:
  728:             data_till = data_rev.copy()
  729:             data_till[:n * scale] = data[:n * scale]
  730:             store_till = self._data(reverse=True)
  731:             npyv_store_till(store_till, n, vdata)
  732:             assert store_till == data_till
  733: 
  734:     @pytest.mark.parametrize("intrin, elsizes, scale", [
  735:         ("self.loadn", (32, 64), 1),
  736:         ("self.loadn2", (32, 64), 2),
  737:     ])
  738:     def test_memory_noncont_load(self, intrin, elsizes, scale):
  739:         if self._scalar_size() not in elsizes:
  740:             return
  741:         npyv_loadn = eval(intrin)
  742:         for stride in range(-64, 64):
  743:             if stride < 0:
  744:                 data = self._data(stride, -stride * self.nlanes)
  745:                 data_stride = list(itertools.chain(
  746:                     *zip(*[data[-i::stride] for i in range(scale, 0, -1)])
  747:                 ))
  748:             elif stride == 0:
  749:                 data = self._data()
  750:                 data_stride = data[0:scale] * (self.nlanes // scale)
  751:             else:
  752:                 data = self._data(count=stride * self.nlanes)
  753:                 data_stride = list(itertools.chain(
  754:                     *zip(*[data[i::stride] for i in range(scale)]))
  755:                 )
  756:             data_stride = self.load(data_stride)  # cast unsigned
  757:             loadn = npyv_loadn(data, stride)
  758:             assert loadn == data_stride
  759: 
  760:     @pytest.mark.parametrize("intrin, elsizes, scale, fill", [
  761:         ("self.loadn_tillz, self.loadn_till", (32, 64), 1, [0xffff]),
  762:         ("self.loadn2_tillz, self.loadn2_till", (32, 64), 2, [0xffff, 0x7fff]),
  763:     ])
  764:     def test_memory_noncont_partial_load(self, intrin, elsizes, scale, fill):
  765:         if self._scalar_size() not in elsizes:
  766:             return
  767:         npyv_loadn_tillz, npyv_loadn_till = eval(intrin)
  768:         lanes = list(range(1, self.nlanes + 1))
  769:         lanes += [self.nlanes**2, self.nlanes**4]
  770:         for stride in range(-64, 64):
  771:             if stride < 0:
  772:                 data = self._data(stride, -stride * self.nlanes)
  773:                 data_stride = list(itertools.chain(
  774:                     *zip(*[data[-i::stride] for i in range(scale, 0, -1)])
  775:                 ))
  776:             elif stride == 0:
  777:                 data = self._data()
  778:                 data_stride = data[0:scale] * (self.nlanes // scale)
  779:             else:
  780:                 data = self._data(count=stride * self.nlanes)
  781:                 data_stride = list(itertools.chain(
  782:                     *zip(*[data[i::stride] for i in range(scale)])
  783:                 ))
  784:             data_stride = list(self.load(data_stride))  # cast unsigned
  785:             for n in lanes:
  786:                 nscale = n * scale
  787:                 llanes = self.nlanes - nscale
  788:                 data_stride_till = (
  789:                     data_stride[:nscale] + fill * (llanes // scale)
  790:                 )
  791:                 loadn_till = npyv_loadn_till(data, stride, n, *fill)
  792:                 assert loadn_till == data_stride_till
  793:                 data_stride_tillz = data_stride[:nscale] + [0] * llanes
  794:                 loadn_tillz = npyv_loadn_tillz(data, stride, n)
  795:                 assert loadn_tillz == data_stride_tillz
  796: 
  797:     @pytest.mark.parametrize("intrin, elsizes, scale", [
  798:         ("self.storen", (32, 64), 1),
  799:         ("self.storen2", (32, 64), 2),
  800:     ])
  801:     def test_memory_noncont_store(self, intrin, elsizes, scale):
  802:         if self._scalar_size() not in elsizes:
  803:             return
  804:         npyv_storen = eval(intrin)
  805:         data = self._data()
  806:         vdata = self.load(data)
  807:         hlanes = self.nlanes // scale
  808:         for stride in range(1, 64):
  809:             data_storen = [0xff] * stride * self.nlanes
  810:             for s in range(0, hlanes * stride, stride):
  811:                 i = (s // stride) * scale
  812:                 data_storen[s:s + scale] = data[i:i + scale]
  813:             storen = [0xff] * stride * self.nlanes
  814:             storen += [0x7f] * 64
  815:             npyv_storen(storen, stride, vdata)
  816:             assert storen[:-64] == data_storen
  817:             assert storen[-64:] == [0x7f] * 64  # detect overflow
  818: 
  819:         for stride in range(-64, 0):
  820:             data_storen = [0xff] * -stride * self.nlanes
  821:             for s in range(0, hlanes * stride, stride):
  822:                 i = (s // stride) * scale
  823:                 data_storen[s - scale:s or None] = data[i:i + scale]
  824:             storen = [0x7f] * 64
  825:             storen += [0xff] * -stride * self.nlanes
  826:             npyv_storen(storen, stride, vdata)
  827:             assert storen[64:] == data_storen
  828:             assert storen[:64] == [0x7f] * 64  # detect overflow
  829:         # stride 0
  830:         data_storen = [0x7f] * self.nlanes
  831:         storen = data_storen.copy()
  832:         data_storen[0:scale] = data[-scale:]
  833:         npyv_storen(storen, 0, vdata)
  834:         assert storen == data_storen
  835: 
  836:     @pytest.mark.parametrize("intrin, elsizes, scale", [
  837:         ("self.storen_till", (32, 64), 1),
  838:         ("self.storen2_till", (32, 64), 2),
  839:     ])
  840:     def test_memory_noncont_partial_store(self, intrin, elsizes, scale):
  841:         if self._scalar_size() not in elsizes:
  842:             return
  843:         npyv_storen_till = eval(intrin)
  844:         data = self._data()
  845:         vdata = self.load(data)
  846:         lanes = list(range(1, self.nlanes + 1))
  847:         lanes += [self.nlanes**2, self.nlanes**4]
  848:         hlanes = self.nlanes // scale
  849:         for stride in range(1, 64):
  850:             for n in lanes:
  851:                 data_till = [0xff] * stride * self.nlanes
  852:                 tdata = data[:n * scale] + [0xff] * (self.nlanes - n * scale)
  853:                 for s in range(0, hlanes * stride, stride)[:n]:
  854:                     i = (s // stride) * scale
  855:                     data_till[s:s + scale] = tdata[i:i + scale]
  856:                 storen_till = [0xff] * stride * self.nlanes
  857:                 storen_till += [0x7f] * 64
  858:                 npyv_storen_till(storen_till, stride, n, vdata)
  859:                 assert storen_till[:-64] == data_till
  860:                 assert storen_till[-64:] == [0x7f] * 64  # detect overflow
  861: 
  862:         for stride in range(-64, 0):
  863:             for n in lanes:
  864:                 data_till = [0xff] * -stride * self.nlanes
  865:                 tdata = data[:n * scale] + [0xff] * (self.nlanes - n * scale)
  866:                 for s in range(0, hlanes * stride, stride)[:n]:
  867:                     i = (s // stride) * scale
  868:                     data_till[s - scale:s or None] = tdata[i:i + scale]
  869:                 storen_till = [0x7f] * 64
  870:                 storen_till += [0xff] * -stride * self.nlanes
  871:                 npyv_storen_till(storen_till, stride, n, vdata)
  872:                 assert storen_till[64:] == data_till
  873:                 assert storen_till[:64] == [0x7f] * 64  # detect overflow
  874: 
  875:         # stride 0
  876:         for n in lanes:
  877:             data_till = [0x7f] * self.nlanes
  878:             storen_till = data_till.copy()
  879:             data_till[0:scale] = data[:n * scale][-scale:]
  880:             npyv_storen_till(storen_till, 0, n, vdata)
  881:             assert storen_till == data_till
  882: 
  883:     @pytest.mark.parametrize("intrin, table_size, elsize", [
  884:         ("self.lut32", 32, 32),
  885:         ("self.lut16", 16, 64)
  886:     ])
  887:     def test_lut(self, intrin, table_size, elsize):
  888:         """
  889:         Test lookup table intrinsics:
  890:             npyv_lut32_##sfx
  891:             npyv_lut16_##sfx
  892:         """
  893:         if elsize != self._scalar_size():
  894:             return
  895:         intrin = eval(intrin)
  896:         idx_itrin = getattr(self.npyv, f"setall_u{elsize}")
  897:         table = range(table_size)
  898:         for i in table:
  899:             broadi = self.setall(i)
  900:             idx = idx_itrin(i)
  901:             lut = intrin(table, idx)
  902:             assert lut == broadi
  903: 
  904:     def test_misc(self):
  905:         broadcast_zero = self.zero()
  906:         assert broadcast_zero == [0] * self.nlanes
  907:         for i in range(1, 10):
  908:             broadcasti = self.setall(i)
  909:             assert broadcasti == [i] * self.nlanes
  910: 
  911:         data_a, data_b = self._data(), self._data(reverse=True)
  912:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
  913: 
  914:         # py level of npyv_set_* don't support ignoring the extra specified lanes or
  915:         # fill non-specified lanes with zero.
  916:         vset = self.set(*data_a)
  917:         assert vset == data_a
  918:         # py level of npyv_setf_* don't support ignoring the extra specified lanes or
  919:         # fill non-specified lanes with the specified scalar.
  920:         vsetf = self.setf(10, *data_a)
  921:         assert vsetf == data_a
  922: 
  923:         # We're testing the sanity of _simd's type-vector,
  924:         # reinterpret* intrinsics itself are tested via compiler
  925:         # during the build of _simd module
  926:         sfxes = ["u8", "s8", "u16", "s16", "u32", "s32", "u64", "s64"]
  927:         if self.npyv.simd_f64:
  928:             sfxes.append("f64")
  929:         if self.npyv.simd_f32:
  930:             sfxes.append("f32")
  931:         for sfx in sfxes:
  932:             vec_name = getattr(self, "reinterpret_" + sfx)(vdata_a).__name__
  933:             assert vec_name == "npyv_" + sfx
  934: 
  935:         # select & mask operations
  936:         select_a = self.select(self.cmpeq(self.zero(), self.zero()), vdata_a, vdata_b)
  937:         assert select_a == data_a
  938:         select_b = self.select(self.cmpneq(self.zero(), self.zero()), vdata_a, vdata_b)
  939:         assert select_b == data_b
  940: 
  941:         # test extract elements
  942:         assert self.extract0(vdata_b) == vdata_b[0]
  943: 
  944:         # cleanup intrinsic is only used with AVX for
  945:         # zeroing registers to avoid the AVX-SSE transition penalty,
  946:         # so nothing to test here
  947:         self.npyv.cleanup()
  948: 
  949:     def test_reorder(self):
  950:         data_a, data_b = self._data(), self._data(reverse=True)
  951:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
  952:         # lower half part
  953:         data_a_lo = data_a[:self.nlanes // 2]
  954:         data_b_lo = data_b[:self.nlanes // 2]
  955:         # higher half part
  956:         data_a_hi = data_a[self.nlanes // 2:]
  957:         data_b_hi = data_b[self.nlanes // 2:]
  958:         # combine two lower parts
  959:         combinel = self.combinel(vdata_a, vdata_b)
  960:         assert combinel == data_a_lo + data_b_lo
  961:         # combine two higher parts
  962:         combineh = self.combineh(vdata_a, vdata_b)
  963:         assert combineh == data_a_hi + data_b_hi
  964:         # combine x2
  965:         combine = self.combine(vdata_a, vdata_b)
  966:         assert combine == (data_a_lo + data_b_lo, data_a_hi + data_b_hi)
  967: 
  968:         # zip(interleave)
  969:         data_zipl = self.load([
  970:             v for p in zip(data_a_lo, data_b_lo) for v in p
  971:         ])
  972:         data_ziph = self.load([
  973:             v for p in zip(data_a_hi, data_b_hi) for v in p
  974:         ])
  975:         vzip = self.zip(vdata_a, vdata_b)
  976:         assert vzip == (data_zipl, data_ziph)
  977:         vzip = [0] * self.nlanes * 2
  978:         self._x2("store")(vzip, (vdata_a, vdata_b))
  979:         assert vzip == list(data_zipl) + list(data_ziph)
  980: 
  981:         # unzip(deinterleave)
  982:         unzip = self.unzip(data_zipl, data_ziph)
  983:         assert unzip == (data_a, data_b)
  984:         unzip = self._x2("load")(list(data_zipl) + list(data_ziph))
  985:         assert unzip == (data_a, data_b)
  986: 
  987:     def test_reorder_rev64(self):
  988:         # Reverse elements of each 64-bit lane
  989:         ssize = self._scalar_size()
  990:         if ssize == 64:
  991:             return
  992:         data_rev64 = [
  993:             y for x in range(0, self.nlanes, 64 // ssize)
  994:               for y in reversed(range(x, x + 64 // ssize))
  995:         ]
  996:         rev64 = self.rev64(self.load(range(self.nlanes)))
  997:         assert rev64 == data_rev64
  998: 
  999:     def test_reorder_permi128(self):
 1000:         """
 1001:         Test permuting elements for each 128-bit lane.
 1002:         npyv_permi128_##sfx
 1003:         """
 1004:         ssize = self._scalar_size()
 1005:         if ssize < 32:
 1006:             return
 1007:         data = self.load(self._data())
 1008:         permn = 128 // ssize
 1009:         permd = permn - 1
 1010:         nlane128 = self.nlanes // permn
 1011:         shfl = [0, 1] if ssize == 64 else [0, 2, 4, 6]
 1012:         for i in range(permn):
 1013:             indices = [(i >> shf) & permd for shf in shfl]
 1014:             vperm = self.permi128(data, *indices)
 1015:             data_vperm = [
 1016:                 data[j + (e & -permn)]
 1017:                 for e, j in enumerate(indices * nlane128)
 1018:             ]
 1019:             assert vperm == data_vperm
 1020: 
 1021:     @pytest.mark.parametrize('func, intrin', [
 1022:         (operator.lt, "cmplt"),
 1023:         (operator.le, "cmple"),
 1024:         (operator.gt, "cmpgt"),
 1025:         (operator.ge, "cmpge"),
 1026:         (operator.eq, "cmpeq")
 1027:     ])
 1028:     def test_operators_comparison(self, func, intrin):
 1029:         if self._is_fp():
 1030:             data_a = self._data()
 1031:         else:
 1032:             data_a = self._data(self._int_max() - self.nlanes)
 1033:         data_b = self._data(self._int_min(), reverse=True)
 1034:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
 1035:         intrin = getattr(self, intrin)
 1036: 
 1037:         mask_true = self._true_mask()
 1038: 
 1039:         def to_bool(vector):
 1040:             return [lane == mask_true for lane in vector]
 1041: 
 1042:         data_cmp = [func(a, b) for a, b in zip(data_a, data_b)]
 1043:         cmp = to_bool(intrin(vdata_a, vdata_b))
 1044:         assert cmp == data_cmp
 1045: 
 1046:     def test_operators_logical(self):
 1047:         if self._is_fp():
 1048:             data_a = self._data()
 1049:         else:
 1050:             data_a = self._data(self._int_max() - self.nlanes)
 1051:         data_b = self._data(self._int_min(), reverse=True)
 1052:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
 1053: 
 1054:         if self._is_fp():
 1055:             data_cast_a = self._to_unsigned(vdata_a)
 1056:             data_cast_b = self._to_unsigned(vdata_b)
 1057:             cast, cast_data = self._to_unsigned, self._to_unsigned
 1058:         else:
 1059:             data_cast_a, data_cast_b = data_a, data_b
 1060:             cast, cast_data = lambda a: a, self.load
 1061: 
 1062:         data_xor = cast_data([a ^ b for a, b in zip(data_cast_a, data_cast_b)])
 1063:         vxor = cast(self.xor(vdata_a, vdata_b))
 1064:         assert vxor == data_xor
 1065: 
 1066:         data_or = cast_data([a | b for a, b in zip(data_cast_a, data_cast_b)])
 1067:         vor = cast(getattr(self, "or")(vdata_a, vdata_b))
 1068:         assert vor == data_or
 1069: 
 1070:         data_and = cast_data([a & b for a, b in zip(data_cast_a, data_cast_b)])
 1071:         vand = cast(getattr(self, "and")(vdata_a, vdata_b))
 1072:         assert vand == data_and
 1073: 
 1074:         data_not = cast_data([~a for a in data_cast_a])
 1075:         vnot = cast(getattr(self, "not")(vdata_a))
 1076:         assert vnot == data_not
 1077: 
 1078:         if self.sfx not in ("u8"):
 1079:             return
 1080:         data_andc = [a & ~b for a, b in zip(data_cast_a, data_cast_b)]
 1081:         vandc = cast(self.andc(vdata_a, vdata_b))
 1082:         assert vandc == data_andc
 1083: 
 1084:     @pytest.mark.parametrize("intrin", ["any", "all"])
 1085:     @pytest.mark.parametrize("data", (
 1086:         [1, 2, 3, 4],
 1087:         [-1, -2, -3, -4],
 1088:         [0, 1, 2, 3, 4],
 1089:         [0x7f, 0x7fff, 0x7fffffff, 0x7fffffffffffffff],
 1090:         [0, -1, -2, -3, 4],
 1091:         [0],
 1092:         [1],
 1093:         [-1]
 1094:     ))
 1095:     def test_operators_crosstest(self, intrin, data):
 1096:         """
 1097:         Test intrinsics:
 1098:             npyv_any_##SFX
 1099:             npyv_all_##SFX
 1100:         """
 1101:         data_a = self.load(data * self.nlanes)
 1102:         func = eval(intrin)
 1103:         intrin = getattr(self, intrin)
 1104:         desired = func(data_a)
 1105:         simd = intrin(data_a)
 1106:         assert not not simd == desired
 1107: 
 1108:     def test_conversion_boolean(self):
 1109:         bsfx = "b" + self.sfx[1:]
 1110:         to_boolean = getattr(self.npyv, f"cvt_{bsfx}_{self.sfx}")
 1111:         from_boolean = getattr(self.npyv, f"cvt_{self.sfx}_{bsfx}")
 1112: 
 1113:         false_vb = to_boolean(self.setall(0))
 1114:         true_vb = self.cmpeq(self.setall(0), self.setall(0))
 1115:         assert false_vb != true_vb
 1116: 
 1117:         false_vsfx = from_boolean(false_vb)
 1118:         true_vsfx = from_boolean(true_vb)
 1119:         assert false_vsfx != true_vsfx
 1120: 
 1121:     def test_conversion_expand(self):
 1122:         """
 1123:         Test expand intrinsics:
 1124:             npyv_expand_u16_u8
 1125:             npyv_expand_u32_u16
 1126:         """
 1127:         if self.sfx not in ("u8", "u16"):
 1128:             return
 1129:         totype = self.sfx[0] + str(int(self.sfx[1:]) * 2)
 1130:         expand = getattr(self.npyv, f"expand_{totype}_{self.sfx}")
 1131:         # close enough from the edge to detect any deviation
 1132:         data = self._data(self._int_max() - self.nlanes)
 1133:         vdata = self.load(data)
 1134:         edata = expand(vdata)
 1135:         # lower half part
 1136:         data_lo = data[:self.nlanes // 2]
 1137:         # higher half part
 1138:         data_hi = data[self.nlanes // 2:]
 1139:         assert edata == (data_lo, data_hi)
 1140: 
 1141:     def test_arithmetic_subadd(self):
 1142:         if self._is_fp():
 1143:             data_a = self._data()
 1144:         else:
 1145:             data_a = self._data(self._int_max() - self.nlanes)
 1146:         data_b = self._data(self._int_min(), reverse=True)
 1147:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
 1148: 
 1149:         # non-saturated
 1150:         data_add = self.load([a + b for a, b in zip(data_a, data_b)])  # load to cast
 1151:         add = self.add(vdata_a, vdata_b)
 1152:         assert add == data_add
 1153:         data_sub = self.load([a - b for a, b in zip(data_a, data_b)])
 1154:         sub = self.sub(vdata_a, vdata_b)
 1155:         assert sub == data_sub
 1156: 
 1157:     def test_arithmetic_mul(self):
 1158:         if self.sfx in ("u64", "s64"):
 1159:             return
 1160: 
 1161:         if self._is_fp():
 1162:             data_a = self._data()
 1163:         else:
 1164:             data_a = self._data(self._int_max() - self.nlanes)
 1165:         data_b = self._data(self._int_min(), reverse=True)
 1166:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
 1167: 
 1168:         data_mul = self.load([a * b for a, b in zip(data_a, data_b)])
 1169:         mul = self.mul(vdata_a, vdata_b)
 1170:         assert mul == data_mul
 1171: 
 1172:     def test_arithmetic_div(self):
 1173:         if not self._is_fp():
 1174:             return
 1175: 
 1176:         data_a, data_b = self._data(), self._data(reverse=True)
 1177:         vdata_a, vdata_b = self.load(data_a), self.load(data_b)
 1178: 
 1179:         # load to truncate f64 to precision of f32
 1180:         data_div = self.load([a / b for a, b in zip(data_a, data_b)])
 1181:         div = self.div(vdata_a, vdata_b)
 1182:         assert div == data_div
 1183: 
 1184:     def test_arithmetic_intdiv(self):
 1185:         """
 1186:         Test integer division intrinsics:
 1187:             npyv_divisor_##sfx
 1188:             npyv_divc_##sfx
 1189:         """
 1190:         if self._is_fp():
 1191:             return
 1192: 
 1193:         int_min = self._int_min()
 1194: 
 1195:         def trunc_div(a, d):
 1196:             """
 1197:             Divide towards zero works with large integers > 2^53,
 1198:             and wrap around overflow similar to what C does.
 1199:             """
 1200:             if d == -1 and a == int_min:
 1201:                 return a
 1202:             sign_a, sign_d = a < 0, d < 0
 1203:             if a == 0 or sign_a == sign_d:
 1204:                 return a // d
 1205:             return (a + sign_d - sign_a) // d + 1
 1206: 
 1207:         data = [1, -int_min]  # to test overflow
 1208:         data += range(0, 2**8, 2**5)
 1209:         data += range(0, 2**8, 2**5 - 1)
 1210:         bsize = self._scalar_size()
 1211:         if bsize > 8:
 1212:             data += range(2**8, 2**16, 2**13)
 1213:             data += range(2**8, 2**16, 2**13 - 1)
 1214:         if bsize > 16:
 1215:             data += range(2**16, 2**32, 2**29)
 1216:             data += range(2**16, 2**32, 2**29 - 1)
 1217:         if bsize > 32:
 1218:             data += range(2**32, 2**64, 2**61)
 1219:             data += range(2**32, 2**64, 2**61 - 1)
 1220:         # negate
 1221:         data += [-x for x in data]
 1222:         for dividend, divisor in itertools.product(data, data):
 1223:             divisor = self.setall(divisor)[0]  # cast
 1224:             if divisor == 0:
 1225:                 continue
 1226:             dividend = self.load(self._data(dividend))
 1227:             data_divc = [trunc_div(a, divisor) for a in dividend]
 1228:             divisor_parms = self.divisor(divisor)
 1229:             divc = self.divc(dividend, divisor_parms)
 1230:             assert divc == data_divc
 1231: 
 1232:     def test_arithmetic_reduce_sum(self):
 1233:         """
 1234:         Test reduce sum intrinsics:
 1235:             npyv_sum_##sfx
 1236:         """
 1237:         if self.sfx not in ("u32", "u64", "f32", "f64"):
 1238:             return
 1239:         # reduce sum
 1240:         data = self._data()
 1241:         vdata = self.load(data)
 1242: 
 1243:         data_sum = sum(data)
 1244:         vsum = self.sum(vdata)
 1245:         assert vsum == data_sum
 1246: 
 1247:     def test_arithmetic_reduce_sumup(self):
 1248:         """
 1249:         Test extend reduce sum intrinsics:
 1250:             npyv_sumup_##sfx
 1251:         """
 1252:         if self.sfx not in ("u8", "u16"):
 1253:             return
 1254:         rdata = (0, self.nlanes, self._int_min(), self._int_max() - self.nlanes)
 1255:         for r in rdata:
 1256:             data = self._data(r)
 1257:             vdata = self.load(data)
 1258:             data_sum = sum(data)
 1259:             vsum = self.sumup(vdata)
 1260:             assert vsum == data_sum
 1261: 
 1262:     def test_mask_conditional(self):
 1263:         """
 1264:         Conditional addition and subtraction for all supported data types.
 1265:         Test intrinsics:
 1266:             npyv_ifadd_##SFX, npyv_ifsub_##SFX
 1267:         """
 1268:         vdata_a = self.load(self._data())
 1269:         vdata_b = self.load(self._data(reverse=True))
 1270:         true_mask = self.cmpeq(self.zero(), self.zero())
 1271:         false_mask = self.cmpneq(self.zero(), self.zero())
 1272: 
 1273:         data_sub = self.sub(vdata_b, vdata_a)
 1274:         ifsub = self.ifsub(true_mask, vdata_b, vdata_a, vdata_b)
 1275:         assert ifsub == data_sub
 1276:         ifsub = self.ifsub(false_mask, vdata_a, vdata_b, vdata_b)
 1277:         assert ifsub == vdata_b
 1278: 
 1279:         data_add = self.add(vdata_b, vdata_a)
 1280:         ifadd = self.ifadd(true_mask, vdata_b, vdata_a, vdata_b)
 1281:         assert ifadd == data_add
 1282:         ifadd = self.ifadd(false_mask, vdata_a, vdata_b, vdata_b)
 1283:         assert ifadd == vdata_b
 1284: 
 1285:         if not self._is_fp():
 1286:             return
 1287:         data_div = self.div(vdata_b, vdata_a)
 1288:         ifdiv = self.ifdiv(true_mask, vdata_b, vdata_a, vdata_b)
 1289:         assert ifdiv == data_div
 1290:         ifdivz = self.ifdivz(true_mask, vdata_b, vdata_a)
 1291:         assert ifdivz == data_div
 1292:         ifdiv = self.ifdiv(false_mask, vdata_a, vdata_b, vdata_b)
 1293:         assert ifdiv == vdata_b
 1294:         ifdivz = self.ifdivz(false_mask, vdata_a, vdata_b)
 1295:         assert ifdivz == self.zero()
 1296: 
 1297: 
 1298: bool_sfx = ("b8", "b16", "b32", "b64")
 1299: int_sfx = ("u8", "s8", "u16", "s16", "u32", "s32", "u64", "s64")
 1300: fp_sfx = ("f32", "f64")
 1301: all_sfx = int_sfx + fp_sfx
 1302: tests_registry = {
 1303:     bool_sfx: _SIMD_BOOL,
 1304:     int_sfx:  _SIMD_INT,
 1305:     fp_sfx:   _SIMD_FP,
 1306:     ("f32",): _SIMD_FP32,
 1307:     ("f64",): _SIMD_FP64,
 1308:     all_sfx:  _SIMD_ALL
 1309: }
 1310: for target_name, npyv in targets.items():
 1311:     simd_width = npyv.simd if npyv else ''
 1312:     pretty_name = target_name.split('__')  # multi-target separator
 1313:     if len(pretty_name) > 1:
 1314:         # multi-target
 1315:         pretty_name = f"({' '.join(pretty_name)})"
 1316:     else:
 1317:         pretty_name = pretty_name[0]
 1318: 
 1319:     skip = ""
 1320:     skip_sfx = {}
 1321:     if not npyv:
 1322:         skip = f"target '{pretty_name}' isn't supported by current machine"
 1323:     elif not npyv.simd:
 1324:         skip = f"target '{pretty_name}' isn't supported by NPYV"
 1325:     else:
 1326:         if not npyv.simd_f32:
 1327:             skip_sfx["f32"] = f"target '{pretty_name}' "\
 1328:                                "doesn't support single-precision"
 1329:         if not npyv.simd_f64:
 1330:             skip_sfx["f64"] = f"target '{pretty_name}' doesn't"\
 1331:                                "support double-precision"
 1332: 
 1333:     for sfxes, cls in tests_registry.items():
 1334:         for sfx in sfxes:
 1335:             skip_m = skip_sfx.get(sfx, skip)
 1336:             inhr = (cls,)
 1337:             attr = {"npyv": targets[target_name], "sfx": sfx, "target_name": target_name}
 1338:             tcls = type(f"Test{cls.__name__}_{simd_width}_{target_name}_{sfx}", inhr, attr)
 1339:             if skip_m:
 1340:                 pytest.mark.skip(reason=skip_m)(tcls)
 1341:             globals()[tcls.__name__] = tcls
