    1: """
    2: Tests for array coercion, mainly through testing `np.array` results directly.
    3: Note that other such tests exist, e.g., in `test_api.py` and many corner-cases
    4: are tested (sometimes indirectly) elsewhere.
    5: """
    6: 
    7: from itertools import permutations, product
    8: 
    9: import numpy._core._multiarray_umath as ncu
   10: import pytest
   11: from numpy._core._rational_tests import rational
   12: from pytest import param
   13: 
   14: import numpy as np
   15: from numpy.testing import IS_64BIT, IS_PYPY, assert_array_equal
   16: 
   17: 
   18: def arraylikes():
   19:     """
   20:     Generator for functions converting an array into various array-likes.
   21:     If full is True (default) it includes array-likes not capable of handling
   22:     all dtypes.
   23:     """
   24:     # base array:
   25:     def ndarray(a):
   26:         return a
   27: 
   28:     yield param(ndarray, id="ndarray")
   29: 
   30:     # subclass:
   31:     class MyArr(np.ndarray):
   32:         pass
   33: 
   34:     def subclass(a):
   35:         return a.view(MyArr)
   36: 
   37:     yield subclass
   38: 
   39:     class _SequenceLike:
   40:         # Older NumPy versions, sometimes cared whether a protocol array was
   41:         # also _SequenceLike.  This shouldn't matter, but keep it for now
   42:         # for __array__ and not the others.
   43:         def __len__(self):
   44:             raise TypeError
   45: 
   46:         def __getitem__(self, _, /):
   47:             raise TypeError
   48: 
   49:     # Array-interface
   50:     class ArrayDunder(_SequenceLike):
   51:         def __init__(self, a):
   52:             self.a = a
   53: 
   54:         def __array__(self, dtype=None, copy=None):
   55:             if dtype is None:
   56:                 return self.a
   57:             return self.a.astype(dtype)
   58: 
   59:     yield param(ArrayDunder, id="__array__")
   60: 
   61:     # memory-view
   62:     yield param(memoryview, id="memoryview")
   63: 
   64:     # Array-interface
   65:     class ArrayInterface:
   66:         def __init__(self, a):
   67:             self.a = a  # need to hold on to keep interface valid
   68:             self.__array_interface__ = a.__array_interface__
   69: 
   70:     yield param(ArrayInterface, id="__array_interface__")
   71: 
   72:     # Array-Struct
   73:     class ArrayStruct:
   74:         def __init__(self, a):
   75:             self.a = a  # need to hold on to keep struct valid
   76:             self.__array_struct__ = a.__array_struct__
   77: 
   78:     yield param(ArrayStruct, id="__array_struct__")
   79: 
   80: 
   81: def scalar_instances(times=True, extended_precision=True, user_dtype=True):
   82:     # Hard-coded list of scalar instances.
   83:     # Floats:
   84:     yield param(np.sqrt(np.float16(5)), id="float16")
   85:     yield param(np.sqrt(np.float32(5)), id="float32")
   86:     yield param(np.sqrt(np.float64(5)), id="float64")
   87:     if extended_precision:
   88:         yield param(np.sqrt(np.longdouble(5)), id="longdouble")
   89: 
   90:     # Complex:
   91:     yield param(np.sqrt(np.complex64(2 + 3j)), id="complex64")
   92:     yield param(np.sqrt(np.complex128(2 + 3j)), id="complex128")
   93:     if extended_precision:
   94:         yield param(np.sqrt(np.clongdouble(2 + 3j)), id="clongdouble")
   95: 
   96:     # Bool:
   97:     # XFAIL: Bool should be added, but has some bad properties when it
   98:     # comes to strings, see also gh-9875
   99:     # yield param(np.bool(0), id="bool")
  100: 
  101:     # Integers:
  102:     yield param(np.int8(2), id="int8")
  103:     yield param(np.int16(2), id="int16")
  104:     yield param(np.int32(2), id="int32")
  105:     yield param(np.int64(2), id="int64")
  106: 
  107:     yield param(np.uint8(2), id="uint8")
  108:     yield param(np.uint16(2), id="uint16")
  109:     yield param(np.uint32(2), id="uint32")
  110:     yield param(np.uint64(2), id="uint64")
  111: 
  112:     # Rational:
  113:     if user_dtype:
  114:         yield param(rational(1, 2), id="rational")
  115: 
  116:     # Cannot create a structured void scalar directly:
  117:     structured = np.array([(1, 3)], "i,i")[0]
  118:     assert isinstance(structured, np.void)
  119:     assert structured.dtype == np.dtype("i,i")
  120:     yield param(structured, id="structured")
  121: 
  122:     if times:
  123:         # Datetimes and timedelta
  124:         yield param(np.timedelta64(2), id="timedelta64[generic]")
  125:         yield param(np.timedelta64(23, "s"), id="timedelta64[s]")
  126:         yield param(np.timedelta64("NaT", "s"), id="timedelta64[s](NaT)")
  127: 
  128:         yield param(np.datetime64("NaT"), id="datetime64[generic](NaT)")
  129:         yield param(np.datetime64("2020-06-07 12:43", "ms"), id="datetime64[ms]")
  130: 
  131:     # Strings and unstructured void:
  132:     yield param(np.bytes_(b"1234"), id="bytes")
  133:     yield param(np.str_("2345"), id="unicode")
  134:     yield param(np.void(b"4321"), id="unstructured_void")
  135: 
  136: 
  137: def is_parametric_dtype(dtype):
  138:     """Returns True if the dtype is a parametric legacy dtype (itemsize
  139:     is 0, or a datetime without units)
  140:     """
  141:     if dtype.itemsize == 0:
  142:         return True
  143:     if issubclass(dtype.type, (np.datetime64, np.timedelta64)):
  144:         if dtype.name.endswith("64"):
  145:             # Generic time units
  146:             return True
  147:     return False
  148: 
  149: 
  150: class TestStringDiscovery:
  151:     @pytest.mark.parametrize("obj",
  152:             [object(), 1.2, 10**43, None, "string"],
  153:             ids=["object", "1.2", "10**43", "None", "string"])
  154:     def test_basic_stringlength(self, obj):
  155:         length = len(str(obj))
  156:         expected = np.dtype(f"S{length}")
  157: 
  158:         assert np.array(obj, dtype="S").dtype == expected
  159:         assert np.array([obj], dtype="S").dtype == expected
  160: 
  161:         # A nested array is also discovered correctly
  162:         arr = np.array(obj, dtype="O")
  163:         assert np.array(arr, dtype="S").dtype == expected
  164:         # Also if we use the dtype class
  165:         assert np.array(arr, dtype=type(expected)).dtype == expected
  166:         # Check that .astype() behaves identical
  167:         assert arr.astype("S").dtype == expected
  168:         # The DType class is accepted by `.astype()`
  169:         assert arr.astype(type(np.dtype("S"))).dtype == expected
  170: 
  171:     @pytest.mark.parametrize("obj",
  172:             [object(), 1.2, 10**43, None, "string"],
  173:             ids=["object", "1.2", "10**43", "None", "string"])
  174:     def test_nested_arrays_stringlength(self, obj):
  175:         length = len(str(obj))
  176:         expected = np.dtype(f"S{length}")
  177:         arr = np.array(obj, dtype="O")
  178:         assert np.array([arr, arr], dtype="S").dtype == expected
  179: 
  180:     @pytest.mark.parametrize("arraylike", arraylikes())
  181:     def test_unpack_first_level(self, arraylike):
  182:         # We unpack exactly one level of array likes
  183:         obj = np.array([None])
  184:         obj[0] = np.array(1.2)
  185:         # the length of the included item, not of the float dtype
  186:         length = len(str(obj[0]))
  187:         expected = np.dtype(f"S{length}")
  188: 
  189:         obj = arraylike(obj)
  190:         # casting to string usually calls str(obj)
  191:         arr = np.array([obj], dtype="S")
  192:         assert arr.shape == (1, 1)
  193:         assert arr.dtype == expected
  194: 
  195: 
  196: class TestScalarDiscovery:
  197:     def test_void_special_case(self):
  198:         # Void dtypes with structures discover tuples as elements
  199:         arr = np.array((1, 2, 3), dtype="i,i,i")
  200:         assert arr.shape == ()
  201:         arr = np.array([(1, 2, 3)], dtype="i,i,i")
  202:         assert arr.shape == (1,)
  203: 
  204:     def test_char_special_case(self):
  205:         arr = np.array("string", dtype="c")
  206:         assert arr.shape == (6,)
  207:         assert arr.dtype.char == "c"
  208:         arr = np.array(["string"], dtype="c")
  209:         assert arr.shape == (1, 6)
  210:         assert arr.dtype.char == "c"
  211: 
  212:     def test_char_special_case_deep(self):
  213:         # Check that the character special case errors correctly if the
  214:         # array is too deep:
  215:         nested = ["string"]  # 2 dimensions (due to string being sequence)
  216:         for i in range(ncu.MAXDIMS - 2):
  217:             nested = [nested]
  218: 
  219:         arr = np.array(nested, dtype='c')
  220:         assert arr.shape == (1,) * (ncu.MAXDIMS - 1) + (6,)
  221:         with pytest.raises(ValueError):
  222:             np.array([nested], dtype="c")
  223: 
  224:     def test_unknown_object(self):
  225:         arr = np.array(object())
  226:         assert arr.shape == ()
  227:         assert arr.dtype == np.dtype("O")
  228: 
  229:     @pytest.mark.parametrize("scalar", scalar_instances())
  230:     def test_scalar(self, scalar):
  231:         arr = np.array(scalar)
  232:         assert arr.shape == ()
  233:         assert arr.dtype == scalar.dtype
  234: 
  235:         arr = np.array([[scalar, scalar]])
  236:         assert arr.shape == (1, 2)
  237:         assert arr.dtype == scalar.dtype
  238: 
  239:     # Additionally to string this test also runs into a corner case
  240:     # with datetime promotion (the difference is the promotion order).
  241:     @pytest.mark.filterwarnings("ignore:Promotion of numbers:FutureWarning")
  242:     def test_scalar_promotion(self):
  243:         for sc1, sc2 in product(scalar_instances(), scalar_instances()):
  244:             sc1, sc2 = sc1.values[0], sc2.values[0]
  245:             # test all combinations:
  246:             try:
  247:                 arr = np.array([sc1, sc2])
  248:             except (TypeError, ValueError):
  249:                 # The promotion between two times can fail
  250:                 # XFAIL (ValueError): Some object casts are currently undefined
  251:                 continue
  252:             assert arr.shape == (2,)
  253:             try:
  254:                 dt1, dt2 = sc1.dtype, sc2.dtype
  255:                 expected_dtype = np.promote_types(dt1, dt2)
  256:                 assert arr.dtype == expected_dtype
  257:             except TypeError as e:
  258:                 # Will currently always go to object dtype
  259:                 assert arr.dtype == np.dtype("O")
  260: 
  261:     @pytest.mark.parametrize("scalar", scalar_instances())
  262:     def test_scalar_coercion(self, scalar):
  263:         # This tests various scalar coercion paths, mainly for the numerical
  264:         # types. It includes some paths not directly related to `np.array`.
  265:         if isinstance(scalar, np.inexact):
  266:             # Ensure we have a full-precision number if available
  267:             scalar = type(scalar)((scalar * 2)**0.5)
  268: 
  269:         if type(scalar) is rational:
  270:             # Rational generally fails due to a missing cast. In the future
  271:             # object casts should automatically be defined based on `setitem`.
  272:             pytest.xfail("Rational to object cast is undefined currently.")
  273: 
  274:         # Use casting from object:
  275:         arr = np.array(scalar, dtype=object).astype(scalar.dtype)
  276: 
  277:         # Test various ways to create an array containing this scalar:
  278:         arr1 = np.array(scalar).reshape(1)
  279:         arr2 = np.array([scalar])
  280:         arr3 = np.empty(1, dtype=scalar.dtype)
  281:         arr3[0] = scalar
  282:         arr4 = np.empty(1, dtype=scalar.dtype)
  283:         arr4[:] = [scalar]
  284:         # All of these methods should yield the same results
  285:         assert_array_equal(arr, arr1)
  286:         assert_array_equal(arr, arr2)
  287:         assert_array_equal(arr, arr3)
  288:         assert_array_equal(arr, arr4)
  289: 
  290:     @pytest.mark.xfail(IS_PYPY, reason="`int(np.complex128(3))` fails on PyPy")
  291:     @pytest.mark.filterwarnings("ignore::numpy.exceptions.ComplexWarning")
  292:     @pytest.mark.parametrize("cast_to", scalar_instances())
  293:     def test_scalar_coercion_same_as_cast_and_assignment(self, cast_to):
  294:         """
  295:         Test that in most cases:
  296:            * `np.array(scalar, dtype=dtype)`
  297:            * `np.empty((), dtype=dtype)[()] = scalar`
  298:            * `np.array(scalar).astype(dtype)`
  299:         should behave the same.  The only exceptions are parametric dtypes
  300:         (mainly datetime/timedelta without unit) and void without fields.
  301:         """
  302:         dtype = cast_to.dtype  # use to parametrize only the target dtype
  303: 
  304:         for scalar in scalar_instances(times=False):
  305:             scalar = scalar.values[0]
  306: 
  307:             if dtype.type == np.void:
  308:                 if scalar.dtype.fields is not None and dtype.fields is None:
  309:                     # Here, coercion to "V6" works, but the cast fails.
  310:                     # Since the types are identical, SETITEM takes care of
  311:                     # this, but has different rules than the cast.
  312:                     with pytest.raises(TypeError):
  313:                         np.array(scalar).astype(dtype)
  314:                     np.array(scalar, dtype=dtype)
  315:                     np.array([scalar], dtype=dtype)
  316:                     continue
  317: 
  318:             # The main test, we first try to use casting and if it succeeds
  319:             # continue below testing that things are the same, otherwise
  320:             # test that the alternative paths at least also fail.
  321:             try:
  322:                 cast = np.array(scalar).astype(dtype)
  323:             except (TypeError, ValueError, RuntimeError):
  324:                 # coercion should also raise (error type may change)
  325:                 with pytest.raises(Exception):  # noqa: B017
  326:                     np.array(scalar, dtype=dtype)
  327: 
  328:                 if (isinstance(scalar, rational) and
  329:                         np.issubdtype(dtype, np.signedinteger)):
  330:                     return
  331: 
  332:                 with pytest.raises(Exception):  # noqa: B017
  333:                     np.array([scalar], dtype=dtype)
  334:                 # assignment should also raise
  335:                 res = np.zeros((), dtype=dtype)
  336:                 with pytest.raises(Exception):  # noqa: B017
  337:                     res[()] = scalar
  338: 
  339:                 return
  340: 
  341:             # Non error path:
  342:             arr = np.array(scalar, dtype=dtype)
  343:             assert_array_equal(arr, cast)
  344:             # assignment behaves the same
  345:             ass = np.zeros((), dtype=dtype)
  346:             ass[()] = scalar
  347:             assert_array_equal(ass, cast)
  348: 
  349:     @pytest.mark.parametrize("pyscalar", [10, 10.32, 10.14j, 10**100])
  350:     def test_pyscalar_subclasses(self, pyscalar):
  351:         """NumPy arrays are read/write which means that anything but invariant
  352:         behaviour is on thin ice.  However, we currently are happy to discover
  353:         subclasses of Python float, int, complex the same as the base classes.
  354:         This should potentially be deprecated.
  355:         """
  356:         class MyScalar(type(pyscalar)):
  357:             pass
  358: 
  359:         res = np.array(MyScalar(pyscalar))
  360:         expected = np.array(pyscalar)
  361:         assert_array_equal(res, expected)
  362: 
  363:     @pytest.mark.parametrize("dtype_char", np.typecodes["All"])
  364:     def test_default_dtype_instance(self, dtype_char):
  365:         if dtype_char in "SU":
  366:             dtype = np.dtype(dtype_char + "1")
  367:         elif dtype_char == "V":
  368:             # Legacy behaviour was to use V8. The reason was float64 being the
  369:             # default dtype and that having 8 bytes.
  370:             dtype = np.dtype("V8")
  371:         else:
  372:             dtype = np.dtype(dtype_char)
  373: 
  374:         discovered_dtype, _ = ncu._discover_array_parameters([], type(dtype))
  375: 
  376:         assert discovered_dtype == dtype
  377:         assert discovered_dtype.itemsize == dtype.itemsize
  378: 
  379:     @pytest.mark.parametrize("dtype", np.typecodes["Integer"])
  380:     @pytest.mark.parametrize(["scalar", "error"],
  381:             [(np.float64(np.nan), ValueError),
  382:              (np.array(-1).astype(np.ulonglong)[()], OverflowError)])
  383:     def test_scalar_to_int_coerce_does_not_cast(self, dtype, scalar, error):
  384:         """
  385:         Signed integers are currently different in that they do not cast other
  386:         NumPy scalar, but instead use scalar.__int__(). The hardcoded
  387:         exception to this rule is `np.array(scalar, dtype=integer)`.
  388:         """
  389:         dtype = np.dtype(dtype)
  390: 
  391:         # This is a special case using casting logic. It warns for the NaN
  392:         # but allows the cast (giving undefined behaviour).
  393:         with np.errstate(invalid="ignore"):
  394:             coerced = np.array(scalar, dtype=dtype)
  395:             cast = np.array(scalar).astype(dtype)
  396:         assert_array_equal(coerced, cast)
  397: 
  398:         # However these fail:
  399:         with pytest.raises(error):
  400:             np.array([scalar], dtype=dtype)
  401:         with pytest.raises(error):
  402:             cast[()] = scalar
  403: 
  404: 
  405: class TestTimeScalars:
  406:     @pytest.mark.parametrize("dtype", [np.int64, np.float32])
  407:     @pytest.mark.parametrize("scalar",
  408:             [param(np.timedelta64("NaT", "s"), id="timedelta64[s](NaT)"),
  409:              param(np.timedelta64(123, "s"), id="timedelta64[s]"),
  410:              param(np.datetime64("NaT", "generic"), id="datetime64[generic](NaT)"),
  411:              param(np.datetime64(1, "D"), id="datetime64[D]")],)
  412:     def test_coercion_basic(self, dtype, scalar):
  413:         # Note the `[scalar]` is there because np.array(scalar) uses stricter
  414:         # `scalar.__int__()` rules for backward compatibility right now.
  415:         arr = np.array(scalar, dtype=dtype)
  416:         cast = np.array(scalar).astype(dtype)
  417:         assert_array_equal(arr, cast)
  418: 
  419:         ass = np.ones((), dtype=dtype)
  420:         if issubclass(dtype, np.integer):
  421:             with pytest.raises(TypeError):
  422:                 # raises, as would np.array([scalar], dtype=dtype), this is
  423:                 # conversion from times, but behaviour of integers.
  424:                 ass[()] = scalar
  425:         else:
  426:             ass[()] = scalar
  427:             assert_array_equal(ass, cast)
  428: 
  429:     @pytest.mark.parametrize("dtype", [np.int64, np.float32])
  430:     @pytest.mark.parametrize("scalar",
  431:             [param(np.timedelta64(123, "ns"), id="timedelta64[ns]"),
  432:              param(np.timedelta64(12, "generic"), id="timedelta64[generic]")])
  433:     def test_coercion_timedelta_convert_to_number(self, dtype, scalar):
  434:         # Only "ns" and "generic" timedeltas can be converted to numbers
  435:         # so these are slightly special.
  436:         arr = np.array(scalar, dtype=dtype)
  437:         cast = np.array(scalar).astype(dtype)
  438:         ass = np.ones((), dtype=dtype)
  439:         ass[()] = scalar  # raises, as would np.array([scalar], dtype=dtype)
  440: 
  441:         assert_array_equal(arr, cast)
  442:         assert_array_equal(cast, cast)
  443: 
  444:     @pytest.mark.parametrize("dtype", ["S6", "U6"])
  445:     @pytest.mark.parametrize(["val", "unit"],
  446:             [param(123, "s", id="[s]"), param(123, "D", id="[D]")])
  447:     def test_coercion_assignment_datetime(self, val, unit, dtype):
  448:         # String from datetime64 assignment is currently special cased to
  449:         # never use casting.  This is because casting will error in this
  450:         # case, and traditionally in most cases the behaviour is maintained
  451:         # like this.  (`np.array(scalar, dtype="U6")` would have failed before)
  452:         # TODO: This discrepancy _should_ be resolved, either by relaxing the
  453:         #       cast, or by deprecating the first part.
  454:         scalar = np.datetime64(val, unit)
  455:         dtype = np.dtype(dtype)
  456:         cut_string = dtype.type(str(scalar)[:6])
  457: 
  458:         arr = np.array(scalar, dtype=dtype)
  459:         assert arr[()] == cut_string
  460:         ass = np.ones((), dtype=dtype)
  461:         ass[()] = scalar
  462:         assert ass[()] == cut_string
  463: 
  464:         with pytest.raises(RuntimeError):
  465:             # However, unlike the above assignment using `str(scalar)[:6]`
  466:             # due to being handled by the string DType and not be casting
  467:             # the explicit cast fails:
  468:             np.array(scalar).astype(dtype)
  469: 
  470:     @pytest.mark.parametrize(["val", "unit"],
  471:             [param(123, "s", id="[s]"), param(123, "D", id="[D]")])
  472:     def test_coercion_assignment_timedelta(self, val, unit):
  473:         scalar = np.timedelta64(val, unit)
  474: 
  475:         # Unlike datetime64, timedelta allows the unsafe cast:
  476:         np.array(scalar, dtype="S6")
  477:         cast = np.array(scalar).astype("S6")
  478:         ass = np.ones((), dtype="S6")
  479:         ass[()] = scalar
  480:         expected = scalar.astype("S")[:6]
  481:         assert cast[()] == expected
  482:         assert ass[()] == expected
  483: 
  484: class TestNested:
  485:     def test_nested_simple(self):
  486:         initial = [1.2]
  487:         nested = initial
  488:         for i in range(ncu.MAXDIMS - 1):
  489:             nested = [nested]
  490: 
  491:         arr = np.array(nested, dtype="float64")
  492:         assert arr.shape == (1,) * ncu.MAXDIMS
  493:         with pytest.raises(ValueError):
  494:             np.array([nested], dtype="float64")
  495: 
  496:         with pytest.raises(ValueError, match=".*would exceed the maximum"):
  497:             np.array([nested])  # user must ask for `object` explicitly
  498: 
  499:         arr = np.array([nested], dtype=object)
  500:         assert arr.dtype == np.dtype("O")
  501:         assert arr.shape == (1,) * ncu.MAXDIMS
  502:         assert arr.item() is initial
  503: 
  504:     def test_pathological_self_containing(self):
  505:         # Test that this also works for two nested sequences
  506:         l = []
  507:         l.append(l)
  508:         arr = np.array([l, l, l], dtype=object)
  509:         assert arr.shape == (3,) + (1,) * (ncu.MAXDIMS - 1)
  510: 
  511:         # Also check a ragged case:
  512:         arr = np.array([l, [None], l], dtype=object)
  513:         assert arr.shape == (3, 1)
  514: 
  515:     @pytest.mark.parametrize("arraylike", arraylikes())
  516:     def test_nested_arraylikes(self, arraylike):
  517:         # We try storing an array like into an array, but the array-like
  518:         # will have too many dimensions.  This means the shape discovery
  519:         # decides that the array-like must be treated as an object (a special
  520:         # case of ragged discovery).  The result will be an array with one
  521:         # dimension less than the maximum dimensions, and the array being
  522:         # assigned to it (which does work for object or if `float(arraylike)`
  523:         # works).
  524:         initial = arraylike(np.ones((1, 1)))
  525: 
  526:         nested = initial
  527:         for i in range(ncu.MAXDIMS - 1):
  528:             nested = [nested]
  529: 
  530:         with pytest.raises(ValueError, match=".*would exceed the maximum"):
  531:             # It will refuse to assign the array into
  532:             np.array(nested, dtype="float64")
  533: 
  534:         # If this is object, we end up assigning a (1, 1) array into (1,)
  535:         # (due to running out of dimensions), this is currently supported but
  536:         # a special case which is not ideal.
  537:         arr = np.array(nested, dtype=object)
  538:         assert arr.shape == (1,) * ncu.MAXDIMS
  539:         assert arr.item() == np.array(initial).item()
  540: 
  541:     @pytest.mark.parametrize("arraylike", arraylikes())
  542:     def test_uneven_depth_ragged(self, arraylike):
  543:         arr = np.arange(4).reshape((2, 2))
  544:         arr = arraylike(arr)
  545: 
  546:         # Array is ragged in the second dimension already:
  547:         out = np.array([arr, [arr]], dtype=object)
  548:         assert out.shape == (2,)
  549:         assert out[0] is arr
  550:         assert type(out[1]) is list
  551: 
  552:         # Array is ragged in the third dimension:
  553:         with pytest.raises(ValueError):
  554:             # This is a broadcast error during assignment, because
  555:             # the array shape would be (2, 2, 2) but `arr[0, 0] = arr` fails.
  556:             np.array([arr, [arr, arr]], dtype=object)
  557: 
  558:     def test_empty_sequence(self):
  559:         arr = np.array([[], [1], [[1]]], dtype=object)
  560:         assert arr.shape == (3,)
  561: 
  562:         # The empty sequence stops further dimension discovery, so the
  563:         # result shape will be (0,) which leads to an error during:
  564:         with pytest.raises(ValueError):
  565:             np.array([[], np.empty((0, 1))], dtype=object)
  566: 
  567:     def test_array_of_different_depths(self):
  568:         # When multiple arrays (or array-likes) are included in a
  569:         # sequences and have different depth, we currently discover
  570:         # as many dimensions as they share. (see also gh-17224)
  571:         arr = np.zeros((3, 2))
  572:         mismatch_first_dim = np.zeros((1, 2))
  573:         mismatch_second_dim = np.zeros((3, 3))
  574: 
  575:         dtype, shape = ncu._discover_array_parameters(
  576:             [arr, mismatch_second_dim], dtype=np.dtype("O"))
  577:         assert shape == (2, 3)
  578: 
  579:         dtype, shape = ncu._discover_array_parameters(
  580:             [arr, mismatch_first_dim], dtype=np.dtype("O"))
  581:         assert shape == (2,)
  582:         # The second case is currently supported because the arrays
  583:         # can be stored as objects:
  584:         res = np.asarray([arr, mismatch_first_dim], dtype=np.dtype("O"))
  585:         assert res[0] is arr
  586:         assert res[1] is mismatch_first_dim
  587: 
  588: 
  589: class TestBadSequences:
  590:     # These are tests for bad objects passed into `np.array`, in general
  591:     # these have undefined behaviour.  In the old code they partially worked
  592:     # when now they will fail.  We could (and maybe should) create a copy
  593:     # of all sequences to be safe against bad-actors.
  594: 
  595:     def test_growing_list(self):
  596:         # List to coerce, `mylist` will append to it during coercion
  597:         obj = []
  598: 
  599:         class mylist(list):
  600:             def __len__(self):
  601:                 obj.append([1, 2])
  602:                 return super().__len__()
  603: 
  604:         obj.append(mylist([1, 2]))
  605: 
  606:         with pytest.raises(RuntimeError):
  607:             np.array(obj)
  608: 
  609:     # Note: We do not test a shrinking list.  These do very evil things
  610:     #       and the only way to fix them would be to copy all sequences.
  611:     #       (which may be a real option in the future).
  612: 
  613:     def test_mutated_list(self):
  614:         # List to coerce, `mylist` will mutate the first element
  615:         obj = []
  616: 
  617:         class mylist(list):
  618:             def __len__(self):
  619:                 obj[0] = [2, 3]  # replace with a different list.
  620:                 return super().__len__()
  621: 
  622:         obj.append([2, 3])
  623:         obj.append(mylist([1, 2]))
  624:         # Does not crash:
  625:         np.array(obj)
  626: 
  627:     def test_replace_0d_array(self):
  628:         # List to coerce, `mylist` will mutate the first element
  629:         obj = []
  630: 
  631:         class baditem:
  632:             def __len__(self):
  633:                 obj[0][0] = 2  # replace with a different list.
  634:                 raise ValueError("not actually a sequence!")
  635: 
  636:             def __getitem__(self, _, /):
  637:                 pass
  638: 
  639:         # Runs into a corner case in the new code, the `array(2)` is cached
  640:         # so replacing it invalidates the cache.
  641:         obj.append([np.array(2), baditem()])
  642:         with pytest.raises(RuntimeError):
  643:             np.array(obj)
  644: 
  645: 
  646: class TestArrayLikes:
  647:     @pytest.mark.parametrize("arraylike", arraylikes())
  648:     def test_0d_object_special_case(self, arraylike):
  649:         arr = np.array(0.)
  650:         obj = arraylike(arr)
  651:         # A single array-like is always converted:
  652:         res = np.array(obj, dtype=object)
  653:         assert_array_equal(arr, res)
  654: 
  655:         # But a single 0-D nested array-like never:
  656:         res = np.array([obj], dtype=object)
  657:         assert res[0] is obj
  658: 
  659:     @pytest.mark.parametrize("arraylike", arraylikes())
  660:     @pytest.mark.parametrize("arr", [np.array(0.), np.arange(4)])
  661:     def test_object_assignment_special_case(self, arraylike, arr):
  662:         obj = arraylike(arr)
  663:         empty = np.arange(1, dtype=object)
  664:         empty[:] = [obj]
  665:         assert empty[0] is obj
  666: 
  667:     def test_0d_generic_special_case(self):
  668:         class ArraySubclass(np.ndarray):
  669:             def __float__(self):
  670:                 raise TypeError("e.g. quantities raise on this")
  671: 
  672:         arr = np.array(0.)
  673:         obj = arr.view(ArraySubclass)
  674:         res = np.array(obj)
  675:         # The subclass is simply cast:
  676:         assert_array_equal(arr, res)
  677: 
  678:         # If the 0-D array-like is included, __float__ is currently
  679:         # guaranteed to be used.  We may want to change that, quantities
  680:         # and masked arrays half make use of this.
  681:         with pytest.raises(TypeError):
  682:             np.array([obj])
  683: 
  684:         # The same holds for memoryview:
  685:         obj = memoryview(arr)
  686:         res = np.array(obj)
  687:         assert_array_equal(arr, res)
  688:         with pytest.raises(ValueError):
  689:             # The error type does not matter much here.
  690:             np.array([obj])
  691: 
  692:     def test_arraylike_classes(self):
  693:         # The classes of array-likes should generally be acceptable to be
  694:         # stored inside a numpy (object) array.  This tests all of the
  695:         # special attributes (since all are checked during coercion).
  696:         arr = np.array(np.int64)
  697:         assert arr[()] is np.int64
  698:         arr = np.array([np.int64])
  699:         assert arr[0] is np.int64
  700: 
  701:         # This also works for properties/unbound methods:
  702:         class ArrayLike:
  703:             @property
  704:             def __array_interface__(self):
  705:                 pass
  706: 
  707:             @property
  708:             def __array_struct__(self):
  709:                 pass
  710: 
  711:             def __array__(self, dtype=None, copy=None):
  712:                 pass
  713: 
  714:         arr = np.array(ArrayLike)
  715:         assert arr[()] is ArrayLike
  716:         arr = np.array([ArrayLike])
  717:         assert arr[0] is ArrayLike
  718: 
  719:     @pytest.mark.skipif(not IS_64BIT, reason="Needs 64bit platform")
  720:     def test_too_large_array_error_paths(self):
  721:         """Test the error paths, including for memory leaks"""
  722:         arr = np.array(0, dtype="uint8")
  723:         # Guarantees that a contiguous copy won't work:
  724:         arr = np.broadcast_to(arr, 2**62)
  725: 
  726:         for i in range(5):
  727:             # repeat, to ensure caching cannot have an effect:
  728:             with pytest.raises(MemoryError):
  729:                 np.array(arr)
  730:             with pytest.raises(MemoryError):
  731:                 np.array([arr])
  732: 
  733:     @pytest.mark.parametrize("attribute",
  734:         ["__array_interface__", "__array__", "__array_struct__"])
  735:     @pytest.mark.parametrize("error", [RecursionError, MemoryError])
  736:     def test_bad_array_like_attributes(self, attribute, error):
  737:         # RecursionError and MemoryError are considered fatal. All errors
  738:         # (except AttributeError) should probably be raised in the future,
  739:         # but shapely made use of it, so it will require a deprecation.
  740: 
  741:         class BadInterface:
  742:             def __getattr__(self, attr):
  743:                 if attr == attribute:
  744:                     raise error
  745:                 super().__getattr__(attr)
  746: 
  747:         with pytest.raises(error):
  748:             np.array(BadInterface())
  749: 
  750:     @pytest.mark.parametrize("error", [RecursionError, MemoryError])
  751:     def test_bad_array_like_bad_length(self, error):
  752:         # RecursionError and MemoryError are considered "critical" in
  753:         # sequences. We could expand this more generally though. (NumPy 1.20)
  754:         class BadSequence:
  755:             def __len__(self):
  756:                 raise error
  757: 
  758:             def __getitem__(self, _, /):
  759:                 # must have getitem to be a Sequence
  760:                 return 1
  761: 
  762:         with pytest.raises(error):
  763:             np.array(BadSequence())
  764: 
  765:     def test_array_interface_descr_optional(self):
  766:         # The descr should be optional regression test for gh-27249
  767:         arr = np.ones(10, dtype="V10")
  768:         iface = arr.__array_interface__
  769:         iface.pop("descr")
  770: 
  771:         class MyClass:
  772:             __array_interface__ = iface
  773: 
  774:         assert_array_equal(np.asarray(MyClass), arr)
  775: 
  776: 
  777: class TestAsArray:
  778:     """Test expected behaviors of ``asarray``."""
  779: 
  780:     def test_dtype_identity(self):
  781:         """Confirm the intended behavior for *dtype* kwarg.
  782: 
  783:         The result of ``asarray()`` should have the dtype provided through the
  784:         keyword argument, when used. This forces unique array handles to be
  785:         produced for unique np.dtype objects, but (for equivalent dtypes), the
  786:         underlying data (the base object) is shared with the original array
  787:         object.
  788: 
  789:         Ref https://github.com/numpy/numpy/issues/1468
  790:         """
  791:         int_array = np.array([1, 2, 3], dtype='i')
  792:         assert np.asarray(int_array) is int_array
  793: 
  794:         # The character code resolves to the singleton dtype object provided
  795:         # by the numpy package.
  796:         assert np.asarray(int_array, dtype='i') is int_array
  797: 
  798:         # Derive a dtype from n.dtype('i'), but add a metadata object to force
  799:         # the dtype to be distinct.
  800:         unequal_type = np.dtype('i', metadata={'spam': True})
  801:         annotated_int_array = np.asarray(int_array, dtype=unequal_type)
  802:         assert annotated_int_array is not int_array
  803:         assert annotated_int_array.base is int_array
  804:         # Create an equivalent descriptor with a new and distinct dtype
  805:         # instance.
  806:         equivalent_requirement = np.dtype('i', metadata={'spam': True})
  807:         annotated_int_array_alt = np.asarray(annotated_int_array,
  808:                                              dtype=equivalent_requirement)
  809:         assert unequal_type == equivalent_requirement
  810:         assert unequal_type is not equivalent_requirement
  811:         assert annotated_int_array_alt is not annotated_int_array
  812:         assert annotated_int_array_alt.dtype is equivalent_requirement
  813: 
  814:         # Check the same logic for a pair of C types whose equivalence may vary
  815:         # between computing environments.
  816:         # Find an equivalent pair.
  817:         integer_type_codes = ('i', 'l', 'q')
  818:         integer_dtypes = [np.dtype(code) for code in integer_type_codes]
  819:         typeA = None
  820:         typeB = None
  821:         for typeA, typeB in permutations(integer_dtypes, r=2):
  822:             if typeA == typeB:
  823:                 assert typeA is not typeB
  824:                 break
  825:         assert isinstance(typeA, np.dtype) and isinstance(typeB, np.dtype)
  826: 
  827:         # These ``asarray()`` calls may produce a new view or a copy,
  828:         # but never the same object.
  829:         long_int_array = np.asarray(int_array, dtype='l')
  830:         long_long_int_array = np.asarray(int_array, dtype='q')
  831:         assert long_int_array is not int_array
  832:         assert long_long_int_array is not int_array
  833:         assert np.asarray(long_int_array, dtype='q') is not long_int_array
  834:         array_a = np.asarray(int_array, dtype=typeA)
  835:         assert typeA == typeB
  836:         assert typeA is not typeB
  837:         assert array_a.dtype is typeA
  838:         assert array_a is not np.asarray(array_a, dtype=typeB)
  839:         assert np.asarray(array_a, dtype=typeB).dtype is typeB
  840:         assert array_a is np.asarray(array_a, dtype=typeB).base
  841: 
  842: 
  843: class TestSpecialAttributeLookupFailure:
  844:     # An exception was raised while fetching the attribute
  845: 
  846:     class WeirdArrayLike:
  847:         @property
  848:         def __array__(self, dtype=None, copy=None):  # noqa: PLR0206
  849:             raise RuntimeError("oops!")
  850: 
  851:     class WeirdArrayInterface:
  852:         @property
  853:         def __array_interface__(self):
  854:             raise RuntimeError("oops!")
  855: 
  856:     def test_deprecated(self):
  857:         with pytest.raises(RuntimeError):
  858:             np.array(self.WeirdArrayLike())
  859:         with pytest.raises(RuntimeError):
  860:             np.array(self.WeirdArrayInterface())
  861: 
  862: 
  863: def test_subarray_from_array_construction():
  864:     # Arrays are more complex, since they "broadcast" on success:
  865:     arr = np.array([1, 2])
  866: 
  867:     res = arr.astype("2i")
  868:     assert_array_equal(res, [[1, 1], [2, 2]])
  869: 
  870:     res = np.array(arr, dtype="(2,)i")
  871: 
  872:     assert_array_equal(res, [[1, 1], [2, 2]])
  873: 
  874:     res = np.array([[(1,), (2,)], arr], dtype="2i")
  875:     assert_array_equal(res, [[[1, 1], [2, 2]], [[1, 1], [2, 2]]])
  876: 
  877:     # Also try a multi-dimensional example:
  878:     arr = np.arange(5 * 2).reshape(5, 2)
  879:     expected = np.broadcast_to(arr[:, :, np.newaxis, np.newaxis], (5, 2, 2, 2))
  880: 
  881:     res = arr.astype("(2,2)f")
  882:     assert_array_equal(res, expected)
  883: 
  884:     res = np.array(arr, dtype="(2,2)f")
  885:     assert_array_equal(res, expected)
  886: 
  887: 
  888: def test_empty_string():
  889:     # Empty strings are unfortunately often converted to S1 and we need to
  890:     # make sure we are filling the S1 and not the (possibly) detected S0
  891:     # result.  This should likely just return S0 and if not maybe the decision
  892:     # to return S1 should be moved.
  893:     res = np.array([""] * 10, dtype="S")
  894:     assert_array_equal(res, np.array("\0", "S1"))
  895:     assert res.dtype == "S1"
  896: 
  897:     arr = np.array([""] * 10, dtype=object)
  898: 
  899:     res = arr.astype("S")
  900:     assert_array_equal(res, b"")
  901:     assert res.dtype == "S1"
  902: 
  903:     res = np.array(arr, dtype="S")
  904:     assert_array_equal(res, b"")
  905:     # TODO: This is arguably weird/wrong, but seems old:
  906:     assert res.dtype == f"S{np.dtype('O').itemsize}"
  907: 
  908:     res = np.array([[""] * 10, arr], dtype="S")
  909:     assert_array_equal(res, b"")
  910:     assert res.shape == (2, 10)
  911:     assert res.dtype == "S1"
