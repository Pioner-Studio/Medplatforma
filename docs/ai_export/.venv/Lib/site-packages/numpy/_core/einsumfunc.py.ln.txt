    1: """
    2: Implementation of optimized einsum.
    3: 
    4: """
    5: import itertools
    6: import operator
    7: 
    8: from numpy._core.multiarray import c_einsum
    9: from numpy._core.numeric import asanyarray, tensordot
   10: from numpy._core.overrides import array_function_dispatch
   11: 
   12: __all__ = ['einsum', 'einsum_path']
   13: 
   14: # importing string for string.ascii_letters would be too slow
   15: # the first import before caching has been measured to take 800 Вµs (#23777)
   16: # imports begin with uppercase to mimic ASCII values to avoid sorting issues
   17: einsum_symbols = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
   18: einsum_symbols_set = set(einsum_symbols)
   19: 
   20: 
   21: def _flop_count(idx_contraction, inner, num_terms, size_dictionary):
   22:     """
   23:     Computes the number of FLOPS in the contraction.
   24: 
   25:     Parameters
   26:     ----------
   27:     idx_contraction : iterable
   28:         The indices involved in the contraction
   29:     inner : bool
   30:         Does this contraction require an inner product?
   31:     num_terms : int
   32:         The number of terms in a contraction
   33:     size_dictionary : dict
   34:         The size of each of the indices in idx_contraction
   35: 
   36:     Returns
   37:     -------
   38:     flop_count : int
   39:         The total number of FLOPS required for the contraction.
   40: 
   41:     Examples
   42:     --------
   43: 
   44:     >>> _flop_count('abc', False, 1, {'a': 2, 'b':3, 'c':5})
   45:     30
   46: 
   47:     >>> _flop_count('abc', True, 2, {'a': 2, 'b':3, 'c':5})
   48:     60
   49: 
   50:     """
   51: 
   52:     overall_size = _compute_size_by_dict(idx_contraction, size_dictionary)
   53:     op_factor = max(1, num_terms - 1)
   54:     if inner:
   55:         op_factor += 1
   56: 
   57:     return overall_size * op_factor
   58: 
   59: def _compute_size_by_dict(indices, idx_dict):
   60:     """
   61:     Computes the product of the elements in indices based on the dictionary
   62:     idx_dict.
   63: 
   64:     Parameters
   65:     ----------
   66:     indices : iterable
   67:         Indices to base the product on.
   68:     idx_dict : dictionary
   69:         Dictionary of index sizes
   70: 
   71:     Returns
   72:     -------
   73:     ret : int
   74:         The resulting product.
   75: 
   76:     Examples
   77:     --------
   78:     >>> _compute_size_by_dict('abbc', {'a': 2, 'b':3, 'c':5})
   79:     90
   80: 
   81:     """
   82:     ret = 1
   83:     for i in indices:
   84:         ret *= idx_dict[i]
   85:     return ret
   86: 
   87: 
   88: def _find_contraction(positions, input_sets, output_set):
   89:     """
   90:     Finds the contraction for a given set of input and output sets.
   91: 
   92:     Parameters
   93:     ----------
   94:     positions : iterable
   95:         Integer positions of terms used in the contraction.
   96:     input_sets : list
   97:         List of sets that represent the lhs side of the einsum subscript
   98:     output_set : set
   99:         Set that represents the rhs side of the overall einsum subscript
  100: 
  101:     Returns
  102:     -------
  103:     new_result : set
  104:         The indices of the resulting contraction
  105:     remaining : list
  106:         List of sets that have not been contracted, the new set is appended to
  107:         the end of this list
  108:     idx_removed : set
  109:         Indices removed from the entire contraction
  110:     idx_contraction : set
  111:         The indices used in the current contraction
  112: 
  113:     Examples
  114:     --------
  115: 
  116:     # A simple dot product test case
  117:     >>> pos = (0, 1)
  118:     >>> isets = [set('ab'), set('bc')]
  119:     >>> oset = set('ac')
  120:     >>> _find_contraction(pos, isets, oset)
  121:     ({'a', 'c'}, [{'a', 'c'}], {'b'}, {'a', 'b', 'c'})
  122: 
  123:     # A more complex case with additional terms in the contraction
  124:     >>> pos = (0, 2)
  125:     >>> isets = [set('abd'), set('ac'), set('bdc')]
  126:     >>> oset = set('ac')
  127:     >>> _find_contraction(pos, isets, oset)
  128:     ({'a', 'c'}, [{'a', 'c'}, {'a', 'c'}], {'b', 'd'}, {'a', 'b', 'c', 'd'})
  129:     """
  130: 
  131:     idx_contract = set()
  132:     idx_remain = output_set.copy()
  133:     remaining = []
  134:     for ind, value in enumerate(input_sets):
  135:         if ind in positions:
  136:             idx_contract |= value
  137:         else:
  138:             remaining.append(value)
  139:             idx_remain |= value
  140: 
  141:     new_result = idx_remain & idx_contract
  142:     idx_removed = (idx_contract - new_result)
  143:     remaining.append(new_result)
  144: 
  145:     return (new_result, remaining, idx_removed, idx_contract)
  146: 
  147: 
  148: def _optimal_path(input_sets, output_set, idx_dict, memory_limit):
  149:     """
  150:     Computes all possible pair contractions, sieves the results based
  151:     on ``memory_limit`` and returns the lowest cost path. This algorithm
  152:     scales factorial with respect to the elements in the list ``input_sets``.
  153: 
  154:     Parameters
  155:     ----------
  156:     input_sets : list
  157:         List of sets that represent the lhs side of the einsum subscript
  158:     output_set : set
  159:         Set that represents the rhs side of the overall einsum subscript
  160:     idx_dict : dictionary
  161:         Dictionary of index sizes
  162:     memory_limit : int
  163:         The maximum number of elements in a temporary array
  164: 
  165:     Returns
  166:     -------
  167:     path : list
  168:         The optimal contraction order within the memory limit constraint.
  169: 
  170:     Examples
  171:     --------
  172:     >>> isets = [set('abd'), set('ac'), set('bdc')]
  173:     >>> oset = set()
  174:     >>> idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}
  175:     >>> _optimal_path(isets, oset, idx_sizes, 5000)
  176:     [(0, 2), (0, 1)]
  177:     """
  178: 
  179:     full_results = [(0, [], input_sets)]
  180:     for iteration in range(len(input_sets) - 1):
  181:         iter_results = []
  182: 
  183:         # Compute all unique pairs
  184:         for curr in full_results:
  185:             cost, positions, remaining = curr
  186:             for con in itertools.combinations(
  187:                 range(len(input_sets) - iteration), 2
  188:             ):
  189: 
  190:                 # Find the contraction
  191:                 cont = _find_contraction(con, remaining, output_set)
  192:                 new_result, new_input_sets, idx_removed, idx_contract = cont
  193: 
  194:                 # Sieve the results based on memory_limit
  195:                 new_size = _compute_size_by_dict(new_result, idx_dict)
  196:                 if new_size > memory_limit:
  197:                     continue
  198: 
  199:                 # Build (total_cost, positions, indices_remaining)
  200:                 total_cost = cost + _flop_count(
  201:                     idx_contract, idx_removed, len(con), idx_dict
  202:                 )
  203:                 new_pos = positions + [con]
  204:                 iter_results.append((total_cost, new_pos, new_input_sets))
  205: 
  206:         # Update combinatorial list, if we did not find anything return best
  207:         # path + remaining contractions
  208:         if iter_results:
  209:             full_results = iter_results
  210:         else:
  211:             path = min(full_results, key=lambda x: x[0])[1]
  212:             path += [tuple(range(len(input_sets) - iteration))]
  213:             return path
  214: 
  215:     # If we have not found anything return single einsum contraction
  216:     if len(full_results) == 0:
  217:         return [tuple(range(len(input_sets)))]
  218: 
  219:     path = min(full_results, key=lambda x: x[0])[1]
  220:     return path
  221: 
  222: def _parse_possible_contraction(
  223:         positions, input_sets, output_set, idx_dict,
  224:         memory_limit, path_cost, naive_cost
  225:     ):
  226:     """Compute the cost (removed size + flops) and resultant indices for
  227:     performing the contraction specified by ``positions``.
  228: 
  229:     Parameters
  230:     ----------
  231:     positions : tuple of int
  232:         The locations of the proposed tensors to contract.
  233:     input_sets : list of sets
  234:         The indices found on each tensors.
  235:     output_set : set
  236:         The output indices of the expression.
  237:     idx_dict : dict
  238:         Mapping of each index to its size.
  239:     memory_limit : int
  240:         The total allowed size for an intermediary tensor.
  241:     path_cost : int
  242:         The contraction cost so far.
  243:     naive_cost : int
  244:         The cost of the unoptimized expression.
  245: 
  246:     Returns
  247:     -------
  248:     cost : (int, int)
  249:         A tuple containing the size of any indices removed, and the flop cost.
  250:     positions : tuple of int
  251:         The locations of the proposed tensors to contract.
  252:     new_input_sets : list of sets
  253:         The resulting new list of indices if this proposed contraction
  254:         is performed.
  255: 
  256:     """
  257: 
  258:     # Find the contraction
  259:     contract = _find_contraction(positions, input_sets, output_set)
  260:     idx_result, new_input_sets, idx_removed, idx_contract = contract
  261: 
  262:     # Sieve the results based on memory_limit
  263:     new_size = _compute_size_by_dict(idx_result, idx_dict)
  264:     if new_size > memory_limit:
  265:         return None
  266: 
  267:     # Build sort tuple
  268:     old_sizes = (
  269:         _compute_size_by_dict(input_sets[p], idx_dict) for p in positions
  270:     )
  271:     removed_size = sum(old_sizes) - new_size
  272: 
  273:     # NB: removed_size used to be just the size of any removed indices i.e.:
  274:     #     helpers.compute_size_by_dict(idx_removed, idx_dict)
  275:     cost = _flop_count(idx_contract, idx_removed, len(positions), idx_dict)
  276:     sort = (-removed_size, cost)
  277: 
  278:     # Sieve based on total cost as well
  279:     if (path_cost + cost) > naive_cost:
  280:         return None
  281: 
  282:     # Add contraction to possible choices
  283:     return [sort, positions, new_input_sets]
  284: 
  285: 
  286: def _update_other_results(results, best):
  287:     """Update the positions and provisional input_sets of ``results``
  288:     based on performing the contraction result ``best``. Remove any
  289:     involving the tensors contracted.
  290: 
  291:     Parameters
  292:     ----------
  293:     results : list
  294:         List of contraction results produced by
  295:         ``_parse_possible_contraction``.
  296:     best : list
  297:         The best contraction of ``results`` i.e. the one that
  298:         will be performed.
  299: 
  300:     Returns
  301:     -------
  302:     mod_results : list
  303:         The list of modified results, updated with outcome of
  304:         ``best`` contraction.
  305:     """
  306: 
  307:     best_con = best[1]
  308:     bx, by = best_con
  309:     mod_results = []
  310: 
  311:     for cost, (x, y), con_sets in results:
  312: 
  313:         # Ignore results involving tensors just contracted
  314:         if x in best_con or y in best_con:
  315:             continue
  316: 
  317:         # Update the input_sets
  318:         del con_sets[by - int(by > x) - int(by > y)]
  319:         del con_sets[bx - int(bx > x) - int(bx > y)]
  320:         con_sets.insert(-1, best[2][-1])
  321: 
  322:         # Update the position indices
  323:         mod_con = x - int(x > bx) - int(x > by), y - int(y > bx) - int(y > by)
  324:         mod_results.append((cost, mod_con, con_sets))
  325: 
  326:     return mod_results
  327: 
  328: def _greedy_path(input_sets, output_set, idx_dict, memory_limit):
  329:     """
  330:     Finds the path by contracting the best pair until the input list is
  331:     exhausted. The best pair is found by minimizing the tuple
  332:     ``(-prod(indices_removed), cost)``.  What this amounts to is prioritizing
  333:     matrix multiplication or inner product operations, then Hadamard like
  334:     operations, and finally outer operations. Outer products are limited by
  335:     ``memory_limit``. This algorithm scales cubically with respect to the
  336:     number of elements in the list ``input_sets``.
  337: 
  338:     Parameters
  339:     ----------
  340:     input_sets : list
  341:         List of sets that represent the lhs side of the einsum subscript
  342:     output_set : set
  343:         Set that represents the rhs side of the overall einsum subscript
  344:     idx_dict : dictionary
  345:         Dictionary of index sizes
  346:     memory_limit : int
  347:         The maximum number of elements in a temporary array
  348: 
  349:     Returns
  350:     -------
  351:     path : list
  352:         The greedy contraction order within the memory limit constraint.
  353: 
  354:     Examples
  355:     --------
  356:     >>> isets = [set('abd'), set('ac'), set('bdc')]
  357:     >>> oset = set()
  358:     >>> idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}
  359:     >>> _greedy_path(isets, oset, idx_sizes, 5000)
  360:     [(0, 2), (0, 1)]
  361:     """
  362: 
  363:     # Handle trivial cases that leaked through
  364:     if len(input_sets) == 1:
  365:         return [(0,)]
  366:     elif len(input_sets) == 2:
  367:         return [(0, 1)]
  368: 
  369:     # Build up a naive cost
  370:     contract = _find_contraction(
  371:         range(len(input_sets)), input_sets, output_set
  372:     )
  373:     idx_result, new_input_sets, idx_removed, idx_contract = contract
  374:     naive_cost = _flop_count(
  375:         idx_contract, idx_removed, len(input_sets), idx_dict
  376:     )
  377: 
  378:     # Initially iterate over all pairs
  379:     comb_iter = itertools.combinations(range(len(input_sets)), 2)
  380:     known_contractions = []
  381: 
  382:     path_cost = 0
  383:     path = []
  384: 
  385:     for iteration in range(len(input_sets) - 1):
  386: 
  387:         # Iterate over all pairs on the first step, only previously
  388:         # found pairs on subsequent steps
  389:         for positions in comb_iter:
  390: 
  391:             # Always initially ignore outer products
  392:             if input_sets[positions[0]].isdisjoint(input_sets[positions[1]]):
  393:                 continue
  394: 
  395:             result = _parse_possible_contraction(
  396:                 positions, input_sets, output_set, idx_dict,
  397:                 memory_limit, path_cost, naive_cost
  398:             )
  399:             if result is not None:
  400:                 known_contractions.append(result)
  401: 
  402:         # If we do not have a inner contraction, rescan pairs
  403:         # including outer products
  404:         if len(known_contractions) == 0:
  405: 
  406:             # Then check the outer products
  407:             for positions in itertools.combinations(
  408:                 range(len(input_sets)), 2
  409:             ):
  410:                 result = _parse_possible_contraction(
  411:                     positions, input_sets, output_set, idx_dict,
  412:                     memory_limit, path_cost, naive_cost
  413:                 )
  414:                 if result is not None:
  415:                     known_contractions.append(result)
  416: 
  417:             # If we still did not find any remaining contractions,
  418:             # default back to einsum like behavior
  419:             if len(known_contractions) == 0:
  420:                 path.append(tuple(range(len(input_sets))))
  421:                 break
  422: 
  423:         # Sort based on first index
  424:         best = min(known_contractions, key=lambda x: x[0])
  425: 
  426:         # Now propagate as many unused contractions as possible
  427:         # to the next iteration
  428:         known_contractions = _update_other_results(known_contractions, best)
  429: 
  430:         # Next iteration only compute contractions with the new tensor
  431:         # All other contractions have been accounted for
  432:         input_sets = best[2]
  433:         new_tensor_pos = len(input_sets) - 1
  434:         comb_iter = ((i, new_tensor_pos) for i in range(new_tensor_pos))
  435: 
  436:         # Update path and total cost
  437:         path.append(best[1])
  438:         path_cost += best[0][1]
  439: 
  440:     return path
  441: 
  442: 
  443: def _can_dot(inputs, result, idx_removed):
  444:     """
  445:     Checks if we can use BLAS (np.tensordot) call and its beneficial to do so.
  446: 
  447:     Parameters
  448:     ----------
  449:     inputs : list of str
  450:         Specifies the subscripts for summation.
  451:     result : str
  452:         Resulting summation.
  453:     idx_removed : set
  454:         Indices that are removed in the summation
  455: 
  456: 
  457:     Returns
  458:     -------
  459:     type : bool
  460:         Returns true if BLAS should and can be used, else False
  461: 
  462:     Notes
  463:     -----
  464:     If the operations is BLAS level 1 or 2 and is not already aligned
  465:     we default back to einsum as the memory movement to copy is more
  466:     costly than the operation itself.
  467: 
  468: 
  469:     Examples
  470:     --------
  471: 
  472:     # Standard GEMM operation
  473:     >>> _can_dot(['ij', 'jk'], 'ik', set('j'))
  474:     True
  475: 
  476:     # Can use the standard BLAS, but requires odd data movement
  477:     >>> _can_dot(['ijj', 'jk'], 'ik', set('j'))
  478:     False
  479: 
  480:     # DDOT where the memory is not aligned
  481:     >>> _can_dot(['ijk', 'ikj'], '', set('ijk'))
  482:     False
  483: 
  484:     """
  485: 
  486:     # All `dot` calls remove indices
  487:     if len(idx_removed) == 0:
  488:         return False
  489: 
  490:     # BLAS can only handle two operands
  491:     if len(inputs) != 2:
  492:         return False
  493: 
  494:     input_left, input_right = inputs
  495: 
  496:     for c in set(input_left + input_right):
  497:         # can't deal with repeated indices on same input or more than 2 total
  498:         nl, nr = input_left.count(c), input_right.count(c)
  499:         if (nl > 1) or (nr > 1) or (nl + nr > 2):
  500:             return False
  501: 
  502:         # can't do implicit summation or dimension collapse e.g.
  503:         #     "ab,bc->c" (implicitly sum over 'a')
  504:         #     "ab,ca->ca" (take diagonal of 'a')
  505:         if nl + nr - 1 == int(c in result):
  506:             return False
  507: 
  508:     # Build a few temporaries
  509:     set_left = set(input_left)
  510:     set_right = set(input_right)
  511:     keep_left = set_left - idx_removed
  512:     keep_right = set_right - idx_removed
  513:     rs = len(idx_removed)
  514: 
  515:     # At this point we are a DOT, GEMV, or GEMM operation
  516: 
  517:     # Handle inner products
  518: 
  519:     # DDOT with aligned data
  520:     if input_left == input_right:
  521:         return True
  522: 
  523:     # DDOT without aligned data (better to use einsum)
  524:     if set_left == set_right:
  525:         return False
  526: 
  527:     # Handle the 4 possible (aligned) GEMV or GEMM cases
  528: 
  529:     # GEMM or GEMV no transpose
  530:     if input_left[-rs:] == input_right[:rs]:
  531:         return True
  532: 
  533:     # GEMM or GEMV transpose both
  534:     if input_left[:rs] == input_right[-rs:]:
  535:         return True
  536: 
  537:     # GEMM or GEMV transpose right
  538:     if input_left[-rs:] == input_right[-rs:]:
  539:         return True
  540: 
  541:     # GEMM or GEMV transpose left
  542:     if input_left[:rs] == input_right[:rs]:
  543:         return True
  544: 
  545:     # Einsum is faster than GEMV if we have to copy data
  546:     if not keep_left or not keep_right:
  547:         return False
  548: 
  549:     # We are a matrix-matrix product, but we need to copy data
  550:     return True
  551: 
  552: 
  553: def _parse_einsum_input(operands):
  554:     """
  555:     A reproduction of einsum c side einsum parsing in python.
  556: 
  557:     Returns
  558:     -------
  559:     input_strings : str
  560:         Parsed input strings
  561:     output_string : str
  562:         Parsed output string
  563:     operands : list of array_like
  564:         The operands to use in the numpy contraction
  565: 
  566:     Examples
  567:     --------
  568:     The operand list is simplified to reduce printing:
  569: 
  570:     >>> np.random.seed(123)
  571:     >>> a = np.random.rand(4, 4)
  572:     >>> b = np.random.rand(4, 4, 4)
  573:     >>> _parse_einsum_input(('...a,...a->...', a, b))
  574:     ('za,xza', 'xz', [a, b]) # may vary
  575: 
  576:     >>> _parse_einsum_input((a, [Ellipsis, 0], b, [Ellipsis, 0]))
  577:     ('za,xza', 'xz', [a, b]) # may vary
  578:     """
  579: 
  580:     if len(operands) == 0:
  581:         raise ValueError("No input operands")
  582: 
  583:     if isinstance(operands[0], str):
  584:         subscripts = operands[0].replace(" ", "")
  585:         operands = [asanyarray(v) for v in operands[1:]]
  586: 
  587:         # Ensure all characters are valid
  588:         for s in subscripts:
  589:             if s in '.,->':
  590:                 continue
  591:             if s not in einsum_symbols:
  592:                 raise ValueError(f"Character {s} is not a valid symbol.")
  593: 
  594:     else:
  595:         tmp_operands = list(operands)
  596:         operand_list = []
  597:         subscript_list = []
  598:         for p in range(len(operands) // 2):
  599:             operand_list.append(tmp_operands.pop(0))
  600:             subscript_list.append(tmp_operands.pop(0))
  601: 
  602:         output_list = tmp_operands[-1] if len(tmp_operands) else None
  603:         operands = [asanyarray(v) for v in operand_list]
  604:         subscripts = ""
  605:         last = len(subscript_list) - 1
  606:         for num, sub in enumerate(subscript_list):
  607:             for s in sub:
  608:                 if s is Ellipsis:
  609:                     subscripts += "..."
  610:                 else:
  611:                     try:
  612:                         s = operator.index(s)
  613:                     except TypeError as e:
  614:                         raise TypeError(
  615:                             "For this input type lists must contain "
  616:                             "either int or Ellipsis"
  617:                         ) from e
  618:                     subscripts += einsum_symbols[s]
  619:             if num != last:
  620:                 subscripts += ","
  621: 
  622:         if output_list is not None:
  623:             subscripts += "->"
  624:             for s in output_list:
  625:                 if s is Ellipsis:
  626:                     subscripts += "..."
  627:                 else:
  628:                     try:
  629:                         s = operator.index(s)
  630:                     except TypeError as e:
  631:                         raise TypeError(
  632:                             "For this input type lists must contain "
  633:                             "either int or Ellipsis"
  634:                         ) from e
  635:                     subscripts += einsum_symbols[s]
  636:     # Check for proper "->"
  637:     if ("-" in subscripts) or (">" in subscripts):
  638:         invalid = (subscripts.count("-") > 1) or (subscripts.count(">") > 1)
  639:         if invalid or (subscripts.count("->") != 1):
  640:             raise ValueError("Subscripts can only contain one '->'.")
  641: 
  642:     # Parse ellipses
  643:     if "." in subscripts:
  644:         used = subscripts.replace(".", "").replace(",", "").replace("->", "")
  645:         unused = list(einsum_symbols_set - set(used))
  646:         ellipse_inds = "".join(unused)
  647:         longest = 0
  648: 
  649:         if "->" in subscripts:
  650:             input_tmp, output_sub = subscripts.split("->")
  651:             split_subscripts = input_tmp.split(",")
  652:             out_sub = True
  653:         else:
  654:             split_subscripts = subscripts.split(',')
  655:             out_sub = False
  656: 
  657:         for num, sub in enumerate(split_subscripts):
  658:             if "." in sub:
  659:                 if (sub.count(".") != 3) or (sub.count("...") != 1):
  660:                     raise ValueError("Invalid Ellipses.")
  661: 
  662:                 # Take into account numerical values
  663:                 if operands[num].shape == ():
  664:                     ellipse_count = 0
  665:                 else:
  666:                     ellipse_count = max(operands[num].ndim, 1)
  667:                     ellipse_count -= (len(sub) - 3)
  668: 
  669:                 if ellipse_count > longest:
  670:                     longest = ellipse_count
  671: 
  672:                 if ellipse_count < 0:
  673:                     raise ValueError("Ellipses lengths do not match.")
  674:                 elif ellipse_count == 0:
  675:                     split_subscripts[num] = sub.replace('...', '')
  676:                 else:
  677:                     rep_inds = ellipse_inds[-ellipse_count:]
  678:                     split_subscripts[num] = sub.replace('...', rep_inds)
  679: 
  680:         subscripts = ",".join(split_subscripts)
  681:         if longest == 0:
  682:             out_ellipse = ""
  683:         else:
  684:             out_ellipse = ellipse_inds[-longest:]
  685: 
  686:         if out_sub:
  687:             subscripts += "->" + output_sub.replace("...", out_ellipse)
  688:         else:
  689:             # Special care for outputless ellipses
  690:             output_subscript = ""
  691:             tmp_subscripts = subscripts.replace(",", "")
  692:             for s in sorted(set(tmp_subscripts)):
  693:                 if s not in (einsum_symbols):
  694:                     raise ValueError(f"Character {s} is not a valid symbol.")
  695:                 if tmp_subscripts.count(s) == 1:
  696:                     output_subscript += s
  697:             normal_inds = ''.join(sorted(set(output_subscript) -
  698:                                          set(out_ellipse)))
  699: 
  700:             subscripts += "->" + out_ellipse + normal_inds
  701: 
  702:     # Build output string if does not exist
  703:     if "->" in subscripts:
  704:         input_subscripts, output_subscript = subscripts.split("->")
  705:     else:
  706:         input_subscripts = subscripts
  707:         # Build output subscripts
  708:         tmp_subscripts = subscripts.replace(",", "")
  709:         output_subscript = ""
  710:         for s in sorted(set(tmp_subscripts)):
  711:             if s not in einsum_symbols:
  712:                 raise ValueError(f"Character {s} is not a valid symbol.")
  713:             if tmp_subscripts.count(s) == 1:
  714:                 output_subscript += s
  715: 
  716:     # Make sure output subscripts are in the input
  717:     for char in output_subscript:
  718:         if output_subscript.count(char) != 1:
  719:             raise ValueError("Output character %s appeared more than once in "
  720:                              "the output." % char)
  721:         if char not in input_subscripts:
  722:             raise ValueError(f"Output character {char} did not appear in the input")
  723: 
  724:     # Make sure number operands is equivalent to the number of terms
  725:     if len(input_subscripts.split(',')) != len(operands):
  726:         raise ValueError("Number of einsum subscripts must be equal to the "
  727:                          "number of operands.")
  728: 
  729:     return (input_subscripts, output_subscript, operands)
  730: 
  731: 
  732: def _einsum_path_dispatcher(*operands, optimize=None, einsum_call=None):
  733:     # NOTE: technically, we should only dispatch on array-like arguments, not
  734:     # subscripts (given as strings). But separating operands into
  735:     # arrays/subscripts is a little tricky/slow (given einsum's two supported
  736:     # signatures), so as a practical shortcut we dispatch on everything.
  737:     # Strings will be ignored for dispatching since they don't define
  738:     # __array_function__.
  739:     return operands
  740: 
  741: 
  742: @array_function_dispatch(_einsum_path_dispatcher, module='numpy')
  743: def einsum_path(*operands, optimize='greedy', einsum_call=False):
  744:     """
  745:     einsum_path(subscripts, *operands, optimize='greedy')
  746: 
  747:     Evaluates the lowest cost contraction order for an einsum expression by
  748:     considering the creation of intermediate arrays.
  749: 
  750:     Parameters
  751:     ----------
  752:     subscripts : str
  753:         Specifies the subscripts for summation.
  754:     *operands : list of array_like
  755:         These are the arrays for the operation.
  756:     optimize : {bool, list, tuple, 'greedy', 'optimal'}
  757:         Choose the type of path. If a tuple is provided, the second argument is
  758:         assumed to be the maximum intermediate size created. If only a single
  759:         argument is provided the largest input or output array size is used
  760:         as a maximum intermediate size.
  761: 
  762:         * if a list is given that starts with ``einsum_path``, uses this as the
  763:           contraction path
  764:         * if False no optimization is taken
  765:         * if True defaults to the 'greedy' algorithm
  766:         * 'optimal' An algorithm that combinatorially explores all possible
  767:           ways of contracting the listed tensors and chooses the least costly
  768:           path. Scales exponentially with the number of terms in the
  769:           contraction.
  770:         * 'greedy' An algorithm that chooses the best pair contraction
  771:           at each step. Effectively, this algorithm searches the largest inner,
  772:           Hadamard, and then outer products at each step. Scales cubically with
  773:           the number of terms in the contraction. Equivalent to the 'optimal'
  774:           path for most contractions.
  775: 
  776:         Default is 'greedy'.
  777: 
  778:     Returns
  779:     -------
  780:     path : list of tuples
  781:         A list representation of the einsum path.
  782:     string_repr : str
  783:         A printable representation of the einsum path.
  784: 
  785:     Notes
  786:     -----
  787:     The resulting path indicates which terms of the input contraction should be
  788:     contracted first, the result of this contraction is then appended to the
  789:     end of the contraction list. This list can then be iterated over until all
  790:     intermediate contractions are complete.
  791: 
  792:     See Also
  793:     --------
  794:     einsum, linalg.multi_dot
  795: 
  796:     Examples
  797:     --------
  798: 
  799:     We can begin with a chain dot example. In this case, it is optimal to
  800:     contract the ``b`` and ``c`` tensors first as represented by the first
  801:     element of the path ``(1, 2)``. The resulting tensor is added to the end
  802:     of the contraction and the remaining contraction ``(0, 1)`` is then
  803:     completed.
  804: 
  805:     >>> np.random.seed(123)
  806:     >>> a = np.random.rand(2, 2)
  807:     >>> b = np.random.rand(2, 5)
  808:     >>> c = np.random.rand(5, 2)
  809:     >>> path_info = np.einsum_path('ij,jk,kl->il', a, b, c, optimize='greedy')
  810:     >>> print(path_info[0])
  811:     ['einsum_path', (1, 2), (0, 1)]
  812:     >>> print(path_info[1])
  813:       Complete contraction:  ij,jk,kl->il # may vary
  814:              Naive scaling:  4
  815:          Optimized scaling:  3
  816:           Naive FLOP count:  1.600e+02
  817:       Optimized FLOP count:  5.600e+01
  818:        Theoretical speedup:  2.857
  819:       Largest intermediate:  4.000e+00 elements
  820:     -------------------------------------------------------------------------
  821:     scaling                  current                                remaining
  822:     -------------------------------------------------------------------------
  823:        3                   kl,jk->jl                                ij,jl->il
  824:        3                   jl,ij->il                                   il->il
  825: 
  826: 
  827:     A more complex index transformation example.
  828: 
  829:     >>> I = np.random.rand(10, 10, 10, 10)
  830:     >>> C = np.random.rand(10, 10)
  831:     >>> path_info = np.einsum_path('ea,fb,abcd,gc,hd->efgh', C, C, I, C, C,
  832:     ...                            optimize='greedy')
  833: 
  834:     >>> print(path_info[0])
  835:     ['einsum_path', (0, 2), (0, 3), (0, 2), (0, 1)]
  836:     >>> print(path_info[1])
  837:       Complete contraction:  ea,fb,abcd,gc,hd->efgh # may vary
  838:              Naive scaling:  8
  839:          Optimized scaling:  5
  840:           Naive FLOP count:  8.000e+08
  841:       Optimized FLOP count:  8.000e+05
  842:        Theoretical speedup:  1000.000
  843:       Largest intermediate:  1.000e+04 elements
  844:     --------------------------------------------------------------------------
  845:     scaling                  current                                remaining
  846:     --------------------------------------------------------------------------
  847:        5               abcd,ea->bcde                      fb,gc,hd,bcde->efgh
  848:        5               bcde,fb->cdef                         gc,hd,cdef->efgh
  849:        5               cdef,gc->defg                            hd,defg->efgh
  850:        5               defg,hd->efgh                               efgh->efgh
  851:     """
  852: 
  853:     # Figure out what the path really is
  854:     path_type = optimize
  855:     if path_type is True:
  856:         path_type = 'greedy'
  857:     if path_type is None:
  858:         path_type = False
  859: 
  860:     explicit_einsum_path = False
  861:     memory_limit = None
  862: 
  863:     # No optimization or a named path algorithm
  864:     if (path_type is False) or isinstance(path_type, str):
  865:         pass
  866: 
  867:     # Given an explicit path
  868:     elif len(path_type) and (path_type[0] == 'einsum_path'):
  869:         explicit_einsum_path = True
  870: 
  871:     # Path tuple with memory limit
  872:     elif ((len(path_type) == 2) and isinstance(path_type[0], str) and
  873:             isinstance(path_type[1], (int, float))):
  874:         memory_limit = int(path_type[1])
  875:         path_type = path_type[0]
  876: 
  877:     else:
  878:         raise TypeError(f"Did not understand the path: {str(path_type)}")
  879: 
  880:     # Hidden option, only einsum should call this
  881:     einsum_call_arg = einsum_call
  882: 
  883:     # Python side parsing
  884:     input_subscripts, output_subscript, operands = (
  885:         _parse_einsum_input(operands)
  886:     )
  887: 
  888:     # Build a few useful list and sets
  889:     input_list = input_subscripts.split(',')
  890:     input_sets = [set(x) for x in input_list]
  891:     output_set = set(output_subscript)
  892:     indices = set(input_subscripts.replace(',', ''))
  893: 
  894:     # Get length of each unique dimension and ensure all dimensions are correct
  895:     dimension_dict = {}
  896:     broadcast_indices = [[] for x in range(len(input_list))]
  897:     for tnum, term in enumerate(input_list):
  898:         sh = operands[tnum].shape
  899:         if len(sh) != len(term):
  900:             raise ValueError("Einstein sum subscript %s does not contain the "
  901:                              "correct number of indices for operand %d."
  902:                              % (input_subscripts[tnum], tnum))
  903:         for cnum, char in enumerate(term):
  904:             dim = sh[cnum]
  905: 
  906:             # Build out broadcast indices
  907:             if dim == 1:
  908:                 broadcast_indices[tnum].append(char)
  909: 
  910:             if char in dimension_dict.keys():
  911:                 # For broadcasting cases we always want the largest dim size
  912:                 if dimension_dict[char] == 1:
  913:                     dimension_dict[char] = dim
  914:                 elif dim not in (1, dimension_dict[char]):
  915:                     raise ValueError("Size of label '%s' for operand %d (%d) "
  916:                                      "does not match previous terms (%d)."
  917:                                      % (char, tnum, dimension_dict[char], dim))
  918:             else:
  919:                 dimension_dict[char] = dim
  920: 
  921:     # Convert broadcast inds to sets
  922:     broadcast_indices = [set(x) for x in broadcast_indices]
  923: 
  924:     # Compute size of each input array plus the output array
  925:     size_list = [_compute_size_by_dict(term, dimension_dict)
  926:                  for term in input_list + [output_subscript]]
  927:     max_size = max(size_list)
  928: 
  929:     if memory_limit is None:
  930:         memory_arg = max_size
  931:     else:
  932:         memory_arg = memory_limit
  933: 
  934:     # Compute naive cost
  935:     # This isn't quite right, need to look into exactly how einsum does this
  936:     inner_product = (sum(len(x) for x in input_sets) - len(indices)) > 0
  937:     naive_cost = _flop_count(
  938:         indices, inner_product, len(input_list), dimension_dict
  939:     )
  940: 
  941:     # Compute the path
  942:     if explicit_einsum_path:
  943:         path = path_type[1:]
  944:     elif (
  945:         (path_type is False)
  946:         or (len(input_list) in [1, 2])
  947:         or (indices == output_set)
  948:     ):
  949:         # Nothing to be optimized, leave it to einsum
  950:         path = [tuple(range(len(input_list)))]
  951:     elif path_type == "greedy":
  952:         path = _greedy_path(
  953:             input_sets, output_set, dimension_dict, memory_arg
  954:         )
  955:     elif path_type == "optimal":
  956:         path = _optimal_path(
  957:             input_sets, output_set, dimension_dict, memory_arg
  958:         )
  959:     else:
  960:         raise KeyError("Path name %s not found", path_type)
  961: 
  962:     cost_list, scale_list, size_list, contraction_list = [], [], [], []
  963: 
  964:     # Build contraction tuple (positions, gemm, einsum_str, remaining)
  965:     for cnum, contract_inds in enumerate(path):
  966:         # Make sure we remove inds from right to left
  967:         contract_inds = tuple(sorted(contract_inds, reverse=True))
  968: 
  969:         contract = _find_contraction(contract_inds, input_sets, output_set)
  970:         out_inds, input_sets, idx_removed, idx_contract = contract
  971: 
  972:         cost = _flop_count(
  973:             idx_contract, idx_removed, len(contract_inds), dimension_dict
  974:         )
  975:         cost_list.append(cost)
  976:         scale_list.append(len(idx_contract))
  977:         size_list.append(_compute_size_by_dict(out_inds, dimension_dict))
  978: 
  979:         bcast = set()
  980:         tmp_inputs = []
  981:         for x in contract_inds:
  982:             tmp_inputs.append(input_list.pop(x))
  983:             bcast |= broadcast_indices.pop(x)
  984: 
  985:         new_bcast_inds = bcast - idx_removed
  986: 
  987:         # If we're broadcasting, nix blas
  988:         if not len(idx_removed & bcast):
  989:             do_blas = _can_dot(tmp_inputs, out_inds, idx_removed)
  990:         else:
  991:             do_blas = False
  992: 
  993:         # Last contraction
  994:         if (cnum - len(path)) == -1:
  995:             idx_result = output_subscript
  996:         else:
  997:             sort_result = [(dimension_dict[ind], ind) for ind in out_inds]
  998:             idx_result = "".join([x[1] for x in sorted(sort_result)])
  999: 
 1000:         input_list.append(idx_result)
 1001:         broadcast_indices.append(new_bcast_inds)
 1002:         einsum_str = ",".join(tmp_inputs) + "->" + idx_result
 1003: 
 1004:         contraction = (
 1005:             contract_inds, idx_removed, einsum_str, input_list[:], do_blas
 1006:         )
 1007:         contraction_list.append(contraction)
 1008: 
 1009:     opt_cost = sum(cost_list) + 1
 1010: 
 1011:     if len(input_list) != 1:
 1012:         # Explicit "einsum_path" is usually trusted, but we detect this kind of
 1013:         # mistake in order to prevent from returning an intermediate value.
 1014:         raise RuntimeError(
 1015:             f"Invalid einsum_path is specified: {len(input_list) - 1} more "
 1016:             "operands has to be contracted.")
 1017: 
 1018:     if einsum_call_arg:
 1019:         return (operands, contraction_list)
 1020: 
 1021:     # Return the path along with a nice string representation
 1022:     overall_contraction = input_subscripts + "->" + output_subscript
 1023:     header = ("scaling", "current", "remaining")
 1024: 
 1025:     speedup = naive_cost / opt_cost
 1026:     max_i = max(size_list)
 1027: 
 1028:     path_print = f"  Complete contraction:  {overall_contraction}\n"
 1029:     path_print += f"         Naive scaling:  {len(indices)}\n"
 1030:     path_print += "     Optimized scaling:  %d\n" % max(scale_list)
 1031:     path_print += f"      Naive FLOP count:  {naive_cost:.3e}\n"
 1032:     path_print += f"  Optimized FLOP count:  {opt_cost:.3e}\n"
 1033:     path_print += f"   Theoretical speedup:  {speedup:3.3f}\n"
 1034:     path_print += f"  Largest intermediate:  {max_i:.3e} elements\n"
 1035:     path_print += "-" * 74 + "\n"
 1036:     path_print += "%6s %24s %40s\n" % header
 1037:     path_print += "-" * 74
 1038: 
 1039:     for n, contraction in enumerate(contraction_list):
 1040:         inds, idx_rm, einsum_str, remaining, blas = contraction
 1041:         remaining_str = ",".join(remaining) + "->" + output_subscript
 1042:         path_run = (scale_list[n], einsum_str, remaining_str)
 1043:         path_print += "\n%4d    %24s %40s" % path_run
 1044: 
 1045:     path = ['einsum_path'] + path
 1046:     return (path, path_print)
 1047: 
 1048: 
 1049: def _einsum_dispatcher(*operands, out=None, optimize=None, **kwargs):
 1050:     # Arguably we dispatch on more arguments than we really should; see note in
 1051:     # _einsum_path_dispatcher for why.
 1052:     yield from operands
 1053:     yield out
 1054: 
 1055: 
 1056: # Rewrite einsum to handle different cases
 1057: @array_function_dispatch(_einsum_dispatcher, module='numpy')
 1058: def einsum(*operands, out=None, optimize=False, **kwargs):
 1059:     """
 1060:     einsum(subscripts, *operands, out=None, dtype=None, order='K',
 1061:            casting='safe', optimize=False)
 1062: 
 1063:     Evaluates the Einstein summation convention on the operands.
 1064: 
 1065:     Using the Einstein summation convention, many common multi-dimensional,
 1066:     linear algebraic array operations can be represented in a simple fashion.
 1067:     In *implicit* mode `einsum` computes these values.
 1068: 
 1069:     In *explicit* mode, `einsum` provides further flexibility to compute
 1070:     other array operations that might not be considered classical Einstein
 1071:     summation operations, by disabling, or forcing summation over specified
 1072:     subscript labels.
 1073: 
 1074:     See the notes and examples for clarification.
 1075: 
 1076:     Parameters
 1077:     ----------
 1078:     subscripts : str
 1079:         Specifies the subscripts for summation as comma separated list of
 1080:         subscript labels. An implicit (classical Einstein summation)
 1081:         calculation is performed unless the explicit indicator '->' is
 1082:         included as well as subscript labels of the precise output form.
 1083:     operands : list of array_like
 1084:         These are the arrays for the operation.
 1085:     out : ndarray, optional
 1086:         If provided, the calculation is done into this array.
 1087:     dtype : {data-type, None}, optional
 1088:         If provided, forces the calculation to use the data type specified.
 1089:         Note that you may have to also give a more liberal `casting`
 1090:         parameter to allow the conversions. Default is None.
 1091:     order : {'C', 'F', 'A', 'K'}, optional
 1092:         Controls the memory layout of the output. 'C' means it should
 1093:         be C contiguous. 'F' means it should be Fortran contiguous,
 1094:         'A' means it should be 'F' if the inputs are all 'F', 'C' otherwise.
 1095:         'K' means it should be as close to the layout as the inputs as
 1096:         is possible, including arbitrarily permuted axes.
 1097:         Default is 'K'.
 1098:     casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
 1099:         Controls what kind of data casting may occur.  Setting this to
 1100:         'unsafe' is not recommended, as it can adversely affect accumulations.
 1101: 
 1102:         * 'no' means the data types should not be cast at all.
 1103:         * 'equiv' means only byte-order changes are allowed.
 1104:         * 'safe' means only casts which can preserve values are allowed.
 1105:         * 'same_kind' means only safe casts or casts within a kind,
 1106:           like float64 to float32, are allowed.
 1107:         * 'unsafe' means any data conversions may be done.
 1108: 
 1109:         Default is 'safe'.
 1110:     optimize : {False, True, 'greedy', 'optimal'}, optional
 1111:         Controls if intermediate optimization should occur. No optimization
 1112:         will occur if False and True will default to the 'greedy' algorithm.
 1113:         Also accepts an explicit contraction list from the ``np.einsum_path``
 1114:         function. See ``np.einsum_path`` for more details. Defaults to False.
 1115: 
 1116:     Returns
 1117:     -------
 1118:     output : ndarray
 1119:         The calculation based on the Einstein summation convention.
 1120: 
 1121:     See Also
 1122:     --------
 1123:     einsum_path, dot, inner, outer, tensordot, linalg.multi_dot
 1124:     einsum:
 1125:         Similar verbose interface is provided by the
 1126:         `einops <https://github.com/arogozhnikov/einops>`_ package to cover
 1127:         additional operations: transpose, reshape/flatten, repeat/tile,
 1128:         squeeze/unsqueeze and reductions.
 1129:         The `opt_einsum <https://optimized-einsum.readthedocs.io/en/stable/>`_
 1130:         optimizes contraction order for einsum-like expressions
 1131:         in backend-agnostic manner.
 1132: 
 1133:     Notes
 1134:     -----
 1135:     The Einstein summation convention can be used to compute
 1136:     many multi-dimensional, linear algebraic array operations. `einsum`
 1137:     provides a succinct way of representing these.
 1138: 
 1139:     A non-exhaustive list of these operations,
 1140:     which can be computed by `einsum`, is shown below along with examples:
 1141: 
 1142:     * Trace of an array, :py:func:`numpy.trace`.
 1143:     * Return a diagonal, :py:func:`numpy.diag`.
 1144:     * Array axis summations, :py:func:`numpy.sum`.
 1145:     * Transpositions and permutations, :py:func:`numpy.transpose`.
 1146:     * Matrix multiplication and dot product, :py:func:`numpy.matmul`
 1147:         :py:func:`numpy.dot`.
 1148:     * Vector inner and outer products, :py:func:`numpy.inner`
 1149:         :py:func:`numpy.outer`.
 1150:     * Broadcasting, element-wise and scalar multiplication,
 1151:         :py:func:`numpy.multiply`.
 1152:     * Tensor contractions, :py:func:`numpy.tensordot`.
 1153:     * Chained array operations, in efficient calculation order,
 1154:         :py:func:`numpy.einsum_path`.
 1155: 
 1156:     The subscripts string is a comma-separated list of subscript labels,
 1157:     where each label refers to a dimension of the corresponding operand.
 1158:     Whenever a label is repeated it is summed, so ``np.einsum('i,i', a, b)``
 1159:     is equivalent to :py:func:`np.inner(a,b) <numpy.inner>`. If a label
 1160:     appears only once, it is not summed, so ``np.einsum('i', a)``
 1161:     produces a view of ``a`` with no changes. A further example
 1162:     ``np.einsum('ij,jk', a, b)`` describes traditional matrix multiplication
 1163:     and is equivalent to :py:func:`np.matmul(a,b) <numpy.matmul>`.
 1164:     Repeated subscript labels in one operand take the diagonal.
 1165:     For example, ``np.einsum('ii', a)`` is equivalent to
 1166:     :py:func:`np.trace(a) <numpy.trace>`.
 1167: 
 1168:     In *implicit mode*, the chosen subscripts are important
 1169:     since the axes of the output are reordered alphabetically.  This
 1170:     means that ``np.einsum('ij', a)`` doesn't affect a 2D array, while
 1171:     ``np.einsum('ji', a)`` takes its transpose. Additionally,
 1172:     ``np.einsum('ij,jk', a, b)`` returns a matrix multiplication, while,
 1173:     ``np.einsum('ij,jh', a, b)`` returns the transpose of the
 1174:     multiplication since subscript 'h' precedes subscript 'i'.
 1175: 
 1176:     In *explicit mode* the output can be directly controlled by
 1177:     specifying output subscript labels.  This requires the
 1178:     identifier '->' as well as the list of output subscript labels.
 1179:     This feature increases the flexibility of the function since
 1180:     summing can be disabled or forced when required. The call
 1181:     ``np.einsum('i->', a)`` is like :py:func:`np.sum(a) <numpy.sum>`
 1182:     if ``a`` is a 1-D array, and ``np.einsum('ii->i', a)``
 1183:     is like :py:func:`np.diag(a) <numpy.diag>` if ``a`` is a square 2-D array.
 1184:     The difference is that `einsum` does not allow broadcasting by default.
 1185:     Additionally ``np.einsum('ij,jh->ih', a, b)`` directly specifies the
 1186:     order of the output subscript labels and therefore returns matrix
 1187:     multiplication, unlike the example above in implicit mode.
 1188: 
 1189:     To enable and control broadcasting, use an ellipsis.  Default
 1190:     NumPy-style broadcasting is done by adding an ellipsis
 1191:     to the left of each term, like ``np.einsum('...ii->...i', a)``.
 1192:     ``np.einsum('...i->...', a)`` is like
 1193:     :py:func:`np.sum(a, axis=-1) <numpy.sum>` for array ``a`` of any shape.
 1194:     To take the trace along the first and last axes,
 1195:     you can do ``np.einsum('i...i', a)``, or to do a matrix-matrix
 1196:     product with the left-most indices instead of rightmost, one can do
 1197:     ``np.einsum('ij...,jk...->ik...', a, b)``.
 1198: 
 1199:     When there is only one operand, no axes are summed, and no output
 1200:     parameter is provided, a view into the operand is returned instead
 1201:     of a new array.  Thus, taking the diagonal as ``np.einsum('ii->i', a)``
 1202:     produces a view (changed in version 1.10.0).
 1203: 
 1204:     `einsum` also provides an alternative way to provide the subscripts and
 1205:     operands as ``einsum(op0, sublist0, op1, sublist1, ..., [sublistout])``.
 1206:     If the output shape is not provided in this format `einsum` will be
 1207:     calculated in implicit mode, otherwise it will be performed explicitly.
 1208:     The examples below have corresponding `einsum` calls with the two
 1209:     parameter methods.
 1210: 
 1211:     Views returned from einsum are now writeable whenever the input array
 1212:     is writeable. For example, ``np.einsum('ijk...->kji...', a)`` will now
 1213:     have the same effect as :py:func:`np.swapaxes(a, 0, 2) <numpy.swapaxes>`
 1214:     and ``np.einsum('ii->i', a)`` will return a writeable view of the diagonal
 1215:     of a 2D array.
 1216: 
 1217:     Added the ``optimize`` argument which will optimize the contraction order
 1218:     of an einsum expression. For a contraction with three or more operands
 1219:     this can greatly increase the computational efficiency at the cost of
 1220:     a larger memory footprint during computation.
 1221: 
 1222:     Typically a 'greedy' algorithm is applied which empirical tests have shown
 1223:     returns the optimal path in the majority of cases. In some cases 'optimal'
 1224:     will return the superlative path through a more expensive, exhaustive
 1225:     search. For iterative calculations it may be advisable to calculate
 1226:     the optimal path once and reuse that path by supplying it as an argument.
 1227:     An example is given below.
 1228: 
 1229:     See :py:func:`numpy.einsum_path` for more details.
 1230: 
 1231:     Examples
 1232:     --------
 1233:     >>> a = np.arange(25).reshape(5,5)
 1234:     >>> b = np.arange(5)
 1235:     >>> c = np.arange(6).reshape(2,3)
 1236: 
 1237:     Trace of a matrix:
 1238: 
 1239:     >>> np.einsum('ii', a)
 1240:     60
 1241:     >>> np.einsum(a, [0,0])
 1242:     60
 1243:     >>> np.trace(a)
 1244:     60
 1245: 
 1246:     Extract the diagonal (requires explicit form):
 1247: 
 1248:     >>> np.einsum('ii->i', a)
 1249:     array([ 0,  6, 12, 18, 24])
 1250:     >>> np.einsum(a, [0,0], [0])
 1251:     array([ 0,  6, 12, 18, 24])
 1252:     >>> np.diag(a)
 1253:     array([ 0,  6, 12, 18, 24])
 1254: 
 1255:     Sum over an axis (requires explicit form):
 1256: 
 1257:     >>> np.einsum('ij->i', a)
 1258:     array([ 10,  35,  60,  85, 110])
 1259:     >>> np.einsum(a, [0,1], [0])
 1260:     array([ 10,  35,  60,  85, 110])
 1261:     >>> np.sum(a, axis=1)
 1262:     array([ 10,  35,  60,  85, 110])
 1263: 
 1264:     For higher dimensional arrays summing a single axis can be done
 1265:     with ellipsis:
 1266: 
 1267:     >>> np.einsum('...j->...', a)
 1268:     array([ 10,  35,  60,  85, 110])
 1269:     >>> np.einsum(a, [Ellipsis,1], [Ellipsis])
 1270:     array([ 10,  35,  60,  85, 110])
 1271: 
 1272:     Compute a matrix transpose, or reorder any number of axes:
 1273: 
 1274:     >>> np.einsum('ji', c)
 1275:     array([[0, 3],
 1276:            [1, 4],
 1277:            [2, 5]])
 1278:     >>> np.einsum('ij->ji', c)
 1279:     array([[0, 3],
 1280:            [1, 4],
 1281:            [2, 5]])
 1282:     >>> np.einsum(c, [1,0])
 1283:     array([[0, 3],
 1284:            [1, 4],
 1285:            [2, 5]])
 1286:     >>> np.transpose(c)
 1287:     array([[0, 3],
 1288:            [1, 4],
 1289:            [2, 5]])
 1290: 
 1291:     Vector inner products:
 1292: 
 1293:     >>> np.einsum('i,i', b, b)
 1294:     30
 1295:     >>> np.einsum(b, [0], b, [0])
 1296:     30
 1297:     >>> np.inner(b,b)
 1298:     30
 1299: 
 1300:     Matrix vector multiplication:
 1301: 
 1302:     >>> np.einsum('ij,j', a, b)
 1303:     array([ 30,  80, 130, 180, 230])
 1304:     >>> np.einsum(a, [0,1], b, [1])
 1305:     array([ 30,  80, 130, 180, 230])
 1306:     >>> np.dot(a, b)
 1307:     array([ 30,  80, 130, 180, 230])
 1308:     >>> np.einsum('...j,j', a, b)
 1309:     array([ 30,  80, 130, 180, 230])
 1310: 
 1311:     Broadcasting and scalar multiplication:
 1312: 
 1313:     >>> np.einsum('..., ...', 3, c)
 1314:     array([[ 0,  3,  6],
 1315:            [ 9, 12, 15]])
 1316:     >>> np.einsum(',ij', 3, c)
 1317:     array([[ 0,  3,  6],
 1318:            [ 9, 12, 15]])
 1319:     >>> np.einsum(3, [Ellipsis], c, [Ellipsis])
 1320:     array([[ 0,  3,  6],
 1321:            [ 9, 12, 15]])
 1322:     >>> np.multiply(3, c)
 1323:     array([[ 0,  3,  6],
 1324:            [ 9, 12, 15]])
 1325: 
 1326:     Vector outer product:
 1327: 
 1328:     >>> np.einsum('i,j', np.arange(2)+1, b)
 1329:     array([[0, 1, 2, 3, 4],
 1330:            [0, 2, 4, 6, 8]])
 1331:     >>> np.einsum(np.arange(2)+1, [0], b, [1])
 1332:     array([[0, 1, 2, 3, 4],
 1333:            [0, 2, 4, 6, 8]])
 1334:     >>> np.outer(np.arange(2)+1, b)
 1335:     array([[0, 1, 2, 3, 4],
 1336:            [0, 2, 4, 6, 8]])
 1337: 
 1338:     Tensor contraction:
 1339: 
 1340:     >>> a = np.arange(60.).reshape(3,4,5)
 1341:     >>> b = np.arange(24.).reshape(4,3,2)
 1342:     >>> np.einsum('ijk,jil->kl', a, b)
 1343:     array([[4400., 4730.],
 1344:            [4532., 4874.],
 1345:            [4664., 5018.],
 1346:            [4796., 5162.],
 1347:            [4928., 5306.]])
 1348:     >>> np.einsum(a, [0,1,2], b, [1,0,3], [2,3])
 1349:     array([[4400., 4730.],
 1350:            [4532., 4874.],
 1351:            [4664., 5018.],
 1352:            [4796., 5162.],
 1353:            [4928., 5306.]])
 1354:     >>> np.tensordot(a,b, axes=([1,0],[0,1]))
 1355:     array([[4400., 4730.],
 1356:            [4532., 4874.],
 1357:            [4664., 5018.],
 1358:            [4796., 5162.],
 1359:            [4928., 5306.]])
 1360: 
 1361:     Writeable returned arrays (since version 1.10.0):
 1362: 
 1363:     >>> a = np.zeros((3, 3))
 1364:     >>> np.einsum('ii->i', a)[:] = 1
 1365:     >>> a
 1366:     array([[1., 0., 0.],
 1367:            [0., 1., 0.],
 1368:            [0., 0., 1.]])
 1369: 
 1370:     Example of ellipsis use:
 1371: 
 1372:     >>> a = np.arange(6).reshape((3,2))
 1373:     >>> b = np.arange(12).reshape((4,3))
 1374:     >>> np.einsum('ki,jk->ij', a, b)
 1375:     array([[10, 28, 46, 64],
 1376:            [13, 40, 67, 94]])
 1377:     >>> np.einsum('ki,...k->i...', a, b)
 1378:     array([[10, 28, 46, 64],
 1379:            [13, 40, 67, 94]])
 1380:     >>> np.einsum('k...,jk', a, b)
 1381:     array([[10, 28, 46, 64],
 1382:            [13, 40, 67, 94]])
 1383: 
 1384:     Chained array operations. For more complicated contractions, speed ups
 1385:     might be achieved by repeatedly computing a 'greedy' path or pre-computing
 1386:     the 'optimal' path and repeatedly applying it, using an `einsum_path`
 1387:     insertion (since version 1.12.0). Performance improvements can be
 1388:     particularly significant with larger arrays:
 1389: 
 1390:     >>> a = np.ones(64).reshape(2,4,8)
 1391: 
 1392:     Basic `einsum`: ~1520ms  (benchmarked on 3.1GHz Intel i5.)
 1393: 
 1394:     >>> for iteration in range(500):
 1395:     ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a)
 1396: 
 1397:     Sub-optimal `einsum` (due to repeated path calculation time): ~330ms
 1398: 
 1399:     >>> for iteration in range(500):
 1400:     ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a,
 1401:     ...         optimize='optimal')
 1402: 
 1403:     Greedy `einsum` (faster optimal path approximation): ~160ms
 1404: 
 1405:     >>> for iteration in range(500):
 1406:     ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize='greedy')
 1407: 
 1408:     Optimal `einsum` (best usage pattern in some use cases): ~110ms
 1409: 
 1410:     >>> path = np.einsum_path('ijk,ilm,njm,nlk,abc->',a,a,a,a,a,
 1411:     ...     optimize='optimal')[0]
 1412:     >>> for iteration in range(500):
 1413:     ...     _ = np.einsum('ijk,ilm,njm,nlk,abc->',a,a,a,a,a, optimize=path)
 1414: 
 1415:     """
 1416:     # Special handling if out is specified
 1417:     specified_out = out is not None
 1418: 
 1419:     # If no optimization, run pure einsum
 1420:     if optimize is False:
 1421:         if specified_out:
 1422:             kwargs['out'] = out
 1423:         return c_einsum(*operands, **kwargs)
 1424: 
 1425:     # Check the kwargs to avoid a more cryptic error later, without having to
 1426:     # repeat default values here
 1427:     valid_einsum_kwargs = ['dtype', 'order', 'casting']
 1428:     unknown_kwargs = [k for (k, v) in kwargs.items() if
 1429:                       k not in valid_einsum_kwargs]
 1430:     if len(unknown_kwargs):
 1431:         raise TypeError(f"Did not understand the following kwargs: {unknown_kwargs}")
 1432: 
 1433:     # Build the contraction list and operand
 1434:     operands, contraction_list = einsum_path(*operands, optimize=optimize,
 1435:                                              einsum_call=True)
 1436: 
 1437:     # Handle order kwarg for output array, c_einsum allows mixed case
 1438:     output_order = kwargs.pop('order', 'K')
 1439:     if output_order.upper() == 'A':
 1440:         if all(arr.flags.f_contiguous for arr in operands):
 1441:             output_order = 'F'
 1442:         else:
 1443:             output_order = 'C'
 1444: 
 1445:     # Start contraction loop
 1446:     for num, contraction in enumerate(contraction_list):
 1447:         inds, idx_rm, einsum_str, remaining, blas = contraction
 1448:         tmp_operands = [operands.pop(x) for x in inds]
 1449: 
 1450:         # Do we need to deal with the output?
 1451:         handle_out = specified_out and ((num + 1) == len(contraction_list))
 1452: 
 1453:         # Call tensordot if still possible
 1454:         if blas:
 1455:             # Checks have already been handled
 1456:             input_str, results_index = einsum_str.split('->')
 1457:             input_left, input_right = input_str.split(',')
 1458: 
 1459:             tensor_result = input_left + input_right
 1460:             for s in idx_rm:
 1461:                 tensor_result = tensor_result.replace(s, "")
 1462: 
 1463:             # Find indices to contract over
 1464:             left_pos, right_pos = [], []
 1465:             for s in sorted(idx_rm):
 1466:                 left_pos.append(input_left.find(s))
 1467:                 right_pos.append(input_right.find(s))
 1468: 
 1469:             # Contract!
 1470:             new_view = tensordot(
 1471:                 *tmp_operands, axes=(tuple(left_pos), tuple(right_pos))
 1472:             )
 1473: 
 1474:             # Build a new view if needed
 1475:             if (tensor_result != results_index) or handle_out:
 1476:                 if handle_out:
 1477:                     kwargs["out"] = out
 1478:                 new_view = c_einsum(
 1479:                     tensor_result + '->' + results_index, new_view, **kwargs
 1480:                 )
 1481: 
 1482:         # Call einsum
 1483:         else:
 1484:             # If out was specified
 1485:             if handle_out:
 1486:                 kwargs["out"] = out
 1487: 
 1488:             # Do the contraction
 1489:             new_view = c_einsum(einsum_str, *tmp_operands, **kwargs)
 1490: 
 1491:         # Append new items and dereference what we can
 1492:         operands.append(new_view)
 1493:         del tmp_operands, new_view
 1494: 
 1495:     if specified_out:
 1496:         return out
 1497:     else:
 1498:         return asanyarray(operands[0], order=output_order)
