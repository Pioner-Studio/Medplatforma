    1: # Copyright 2014-present MongoDB, Inc.
    2: #
    3: # Licensed under the Apache License, Version 2.0 (the "License"); you
    4: # may not use this file except in compliance with the License.  You
    5: # may obtain a copy of the License at
    6: #
    7: # http://www.apache.org/licenses/LICENSE-2.0
    8: #
    9: # Unless required by applicable law or agreed to in writing, software
   10: # distributed under the License is distributed on an "AS IS" BASIS,
   11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
   12: # implied.  See the License for the specific language governing
   13: # permissions and limitations under the License.
   14: 
   15: """Internal class to monitor a topology of one or more servers."""
   16: 
   17: from __future__ import annotations
   18: 
   19: import logging
   20: import os
   21: import queue
   22: import random
   23: import sys
   24: import time
   25: import warnings
   26: import weakref
   27: from pathlib import Path
   28: from typing import TYPE_CHECKING, Any, Callable, Mapping, Optional, cast
   29: 
   30: from pymongo import _csot, common, helpers, periodic_executor
   31: from pymongo.client_session import _ServerSession, _ServerSessionPool
   32: from pymongo.errors import (
   33:     ConnectionFailure,
   34:     InvalidOperation,
   35:     NetworkTimeout,
   36:     NotPrimaryError,
   37:     OperationFailure,
   38:     PyMongoError,
   39:     ServerSelectionTimeoutError,
   40:     WriteError,
   41: )
   42: from pymongo.hello import Hello
   43: from pymongo.lock import _create_lock
   44: from pymongo.logger import (
   45:     _SERVER_SELECTION_LOGGER,
   46:     _debug_log,
   47:     _ServerSelectionStatusMessage,
   48: )
   49: from pymongo.monitor import SrvMonitor
   50: from pymongo.pool import Pool, PoolOptions
   51: from pymongo.server import Server
   52: from pymongo.server_description import ServerDescription
   53: from pymongo.server_selectors import (
   54:     Selection,
   55:     any_server_selector,
   56:     arbiter_server_selector,
   57:     secondary_server_selector,
   58:     writable_server_selector,
   59: )
   60: from pymongo.topology_description import (
   61:     SRV_POLLING_TOPOLOGIES,
   62:     TOPOLOGY_TYPE,
   63:     TopologyDescription,
   64:     _updated_topology_description_srv_polling,
   65:     updated_topology_description,
   66: )
   67: 
   68: if TYPE_CHECKING:
   69:     from bson import ObjectId
   70:     from pymongo.settings import TopologySettings
   71:     from pymongo.typings import ClusterTime, _Address
   72: 
   73: 
   74: _pymongo_dir = str(Path(__file__).parent)
   75: 
   76: 
   77: def process_events_queue(queue_ref: weakref.ReferenceType[queue.Queue]) -> bool:
   78:     q = queue_ref()
   79:     if not q:
   80:         return False  # Cancel PeriodicExecutor.
   81: 
   82:     while True:
   83:         try:
   84:             event = q.get_nowait()
   85:         except queue.Empty:
   86:             break
   87:         else:
   88:             fn, args = event
   89:             fn(*args)
   90: 
   91:     return True  # Continue PeriodicExecutor.
   92: 
   93: 
   94: class Topology:
   95:     """Monitor a topology of one or more servers."""
   96: 
   97:     def __init__(self, topology_settings: TopologySettings):
   98:         self._topology_id = topology_settings._topology_id
   99:         self._listeners = topology_settings._pool_options._event_listeners
  100:         self._publish_server = self._listeners is not None and self._listeners.enabled_for_server
  101:         self._publish_tp = self._listeners is not None and self._listeners.enabled_for_topology
  102: 
  103:         # Create events queue if there are publishers.
  104:         self._events = None
  105:         self.__events_executor: Any = None
  106: 
  107:         if self._publish_server or self._publish_tp:
  108:             self._events = queue.Queue(maxsize=100)
  109: 
  110:         if self._publish_tp:
  111:             assert self._events is not None
  112:             self._events.put((self._listeners.publish_topology_opened, (self._topology_id,)))
  113:         self._settings = topology_settings
  114:         topology_description = TopologyDescription(
  115:             topology_settings.get_topology_type(),
  116:             topology_settings.get_server_descriptions(),
  117:             topology_settings.replica_set_name,
  118:             None,
  119:             None,
  120:             topology_settings,
  121:         )
  122: 
  123:         self._description = topology_description
  124:         if self._publish_tp:
  125:             assert self._events is not None
  126:             initial_td = TopologyDescription(
  127:                 TOPOLOGY_TYPE.Unknown, {}, None, None, None, self._settings
  128:             )
  129:             self._events.put(
  130:                 (
  131:                     self._listeners.publish_topology_description_changed,
  132:                     (initial_td, self._description, self._topology_id),
  133:                 )
  134:             )
  135: 
  136:         for seed in topology_settings.seeds:
  137:             if self._publish_server:
  138:                 assert self._events is not None
  139:                 self._events.put((self._listeners.publish_server_opened, (seed, self._topology_id)))
  140: 
  141:         # Store the seed list to help diagnose errors in _error_message().
  142:         self._seed_addresses = list(topology_description.server_descriptions())
  143:         self._opened = False
  144:         self._closed = False
  145:         self._lock = _create_lock()
  146:         self._condition = self._settings.condition_class(self._lock)
  147:         self._servers: dict[_Address, Server] = {}
  148:         self._pid: Optional[int] = None
  149:         self._max_cluster_time: Optional[ClusterTime] = None
  150:         self._session_pool = _ServerSessionPool()
  151: 
  152:         if self._publish_server or self._publish_tp:
  153:             assert self._events is not None
  154:             weak: weakref.ReferenceType[queue.Queue]
  155: 
  156:             def target() -> bool:
  157:                 return process_events_queue(weak)
  158: 
  159:             executor = periodic_executor.PeriodicExecutor(
  160:                 interval=common.EVENTS_QUEUE_FREQUENCY,
  161:                 min_interval=common.MIN_HEARTBEAT_INTERVAL,
  162:                 target=target,
  163:                 name="pymongo_events_thread",
  164:             )
  165: 
  166:             # We strongly reference the executor and it weakly references
  167:             # the queue via this closure. When the topology is freed, stop
  168:             # the executor soon.
  169:             weak = weakref.ref(self._events, executor.close)
  170:             self.__events_executor = executor
  171:             executor.open()
  172: 
  173:         self._srv_monitor = None
  174:         if self._settings.fqdn is not None and not self._settings.load_balanced:
  175:             self._srv_monitor = SrvMonitor(self, self._settings)
  176: 
  177:     def open(self) -> None:
  178:         """Start monitoring, or restart after a fork.
  179: 
  180:         No effect if called multiple times.
  181: 
  182:         .. warning:: Topology is shared among multiple threads and is protected
  183:           by mutual exclusion. Using Topology from a process other than the one
  184:           that initialized it will emit a warning and may result in deadlock. To
  185:           prevent this from happening, MongoClient must be created after any
  186:           forking.
  187: 
  188:         """
  189:         pid = os.getpid()
  190:         if self._pid is None:
  191:             self._pid = pid
  192:         elif pid != self._pid:
  193:             self._pid = pid
  194:             if sys.version_info[:2] >= (3, 12):
  195:                 kwargs = {"skip_file_prefixes": (_pymongo_dir,)}
  196:             else:
  197:                 kwargs = {"stacklevel": 6}
  198:             # Ignore B028 warning for missing stacklevel.
  199:             warnings.warn(  # type: ignore[call-overload] # noqa: B028
  200:                 "MongoClient opened before fork. May not be entirely fork-safe, "
  201:                 "proceed with caution. See PyMongo's documentation for details: "
  202:                 "https://pymongo.readthedocs.io/en/stable/faq.html#"
  203:                 "is-pymongo-fork-safe",
  204:                 **kwargs,
  205:             )
  206:             with self._lock:
  207:                 # Close servers and clear the pools.
  208:                 for server in self._servers.values():
  209:                     server.close()
  210:                 # Reset the session pool to avoid duplicate sessions in
  211:                 # the child process.
  212:                 self._session_pool.reset()
  213: 
  214:         with self._lock:
  215:             self._ensure_opened()
  216: 
  217:     def get_server_selection_timeout(self) -> float:
  218:         # CSOT: use remaining timeout when set.
  219:         timeout = _csot.remaining()
  220:         if timeout is None:
  221:             return self._settings.server_selection_timeout
  222:         return timeout
  223: 
  224:     def select_servers(
  225:         self,
  226:         selector: Callable[[Selection], Selection],
  227:         operation: str,
  228:         server_selection_timeout: Optional[float] = None,
  229:         address: Optional[_Address] = None,
  230:         operation_id: Optional[int] = None,
  231:     ) -> list[Server]:
  232:         """Return a list of Servers matching selector, or time out.
  233: 
  234:         :param selector: function that takes a list of Servers and returns
  235:             a subset of them.
  236:         :param operation: The name of the operation that the server is being selected for.
  237:         :param server_selection_timeout: maximum seconds to wait.
  238:             If not provided, the default value common.SERVER_SELECTION_TIMEOUT
  239:             is used.
  240:         :param address: optional server address to select.
  241: 
  242:         Calls self.open() if needed.
  243: 
  244:         Raises exc:`ServerSelectionTimeoutError` after
  245:         `server_selection_timeout` if no matching servers are found.
  246:         """
  247:         if server_selection_timeout is None:
  248:             server_timeout = self.get_server_selection_timeout()
  249:         else:
  250:             server_timeout = server_selection_timeout
  251: 
  252:         with self._lock:
  253:             server_descriptions = self._select_servers_loop(
  254:                 selector, server_timeout, operation, operation_id, address
  255:             )
  256: 
  257:             return [
  258:                 cast(Server, self.get_server_by_address(sd.address)) for sd in server_descriptions
  259:             ]
  260: 
  261:     def _select_servers_loop(
  262:         self,
  263:         selector: Callable[[Selection], Selection],
  264:         timeout: float,
  265:         operation: str,
  266:         operation_id: Optional[int],
  267:         address: Optional[_Address],
  268:     ) -> list[ServerDescription]:
  269:         """select_servers() guts. Hold the lock when calling this."""
  270:         now = time.monotonic()
  271:         end_time = now + timeout
  272:         logged_waiting = False
  273: 
  274:         if _SERVER_SELECTION_LOGGER.isEnabledFor(logging.DEBUG):
  275:             _debug_log(
  276:                 _SERVER_SELECTION_LOGGER,
  277:                 message=_ServerSelectionStatusMessage.STARTED,
  278:                 selector=selector,
  279:                 operation=operation,
  280:                 operationId=operation_id,
  281:                 topologyDescription=self.description,
  282:                 clientId=self.description._topology_settings._topology_id,
  283:             )
  284: 
  285:         server_descriptions = self._description.apply_selector(
  286:             selector, address, custom_selector=self._settings.server_selector
  287:         )
  288: 
  289:         while not server_descriptions:
  290:             # No suitable servers.
  291:             if timeout == 0 or now > end_time:
  292:                 if _SERVER_SELECTION_LOGGER.isEnabledFor(logging.DEBUG):
  293:                     _debug_log(
  294:                         _SERVER_SELECTION_LOGGER,
  295:                         message=_ServerSelectionStatusMessage.FAILED,
  296:                         selector=selector,
  297:                         operation=operation,
  298:                         operationId=operation_id,
  299:                         topologyDescription=self.description,
  300:                         clientId=self.description._topology_settings._topology_id,
  301:                         failure=self._error_message(selector),
  302:                     )
  303:                 raise ServerSelectionTimeoutError(
  304:                     f"{self._error_message(selector)}, Timeout: {timeout}s, Topology Description: {self.description!r}"
  305:                 )
  306: 
  307:             if not logged_waiting:
  308:                 _debug_log(
  309:                     _SERVER_SELECTION_LOGGER,
  310:                     message=_ServerSelectionStatusMessage.WAITING,
  311:                     selector=selector,
  312:                     operation=operation,
  313:                     operationId=operation_id,
  314:                     topologyDescription=self.description,
  315:                     clientId=self.description._topology_settings._topology_id,
  316:                     remainingTimeMS=int(end_time - time.monotonic()),
  317:                 )
  318:                 logged_waiting = True
  319: 
  320:             self._ensure_opened()
  321:             self._request_check_all()
  322: 
  323:             # Release the lock and wait for the topology description to
  324:             # change, or for a timeout. We won't miss any changes that
  325:             # came after our most recent apply_selector call, since we've
  326:             # held the lock until now.
  327:             self._condition.wait(common.MIN_HEARTBEAT_INTERVAL)
  328:             self._description.check_compatible()
  329:             now = time.monotonic()
  330:             server_descriptions = self._description.apply_selector(
  331:                 selector, address, custom_selector=self._settings.server_selector
  332:             )
  333: 
  334:         self._description.check_compatible()
  335:         return server_descriptions
  336: 
  337:     def _select_server(
  338:         self,
  339:         selector: Callable[[Selection], Selection],
  340:         operation: str,
  341:         server_selection_timeout: Optional[float] = None,
  342:         address: Optional[_Address] = None,
  343:         deprioritized_servers: Optional[list[Server]] = None,
  344:         operation_id: Optional[int] = None,
  345:     ) -> Server:
  346:         servers = self.select_servers(
  347:             selector, operation, server_selection_timeout, address, operation_id
  348:         )
  349:         servers = _filter_servers(servers, deprioritized_servers)
  350:         if len(servers) == 1:
  351:             return servers[0]
  352:         server1, server2 = random.sample(servers, 2)
  353:         if server1.pool.operation_count <= server2.pool.operation_count:
  354:             return server1
  355:         else:
  356:             return server2
  357: 
  358:     def select_server(
  359:         self,
  360:         selector: Callable[[Selection], Selection],
  361:         operation: str,
  362:         server_selection_timeout: Optional[float] = None,
  363:         address: Optional[_Address] = None,
  364:         deprioritized_servers: Optional[list[Server]] = None,
  365:         operation_id: Optional[int] = None,
  366:     ) -> Server:
  367:         """Like select_servers, but choose a random server if several match."""
  368:         server = self._select_server(
  369:             selector,
  370:             operation,
  371:             server_selection_timeout,
  372:             address,
  373:             deprioritized_servers,
  374:             operation_id=operation_id,
  375:         )
  376:         if _csot.get_timeout():
  377:             _csot.set_rtt(server.description.min_round_trip_time)
  378:         if _SERVER_SELECTION_LOGGER.isEnabledFor(logging.DEBUG):
  379:             _debug_log(
  380:                 _SERVER_SELECTION_LOGGER,
  381:                 message=_ServerSelectionStatusMessage.SUCCEEDED,
  382:                 selector=selector,
  383:                 operation=operation,
  384:                 operationId=operation_id,
  385:                 topologyDescription=self.description,
  386:                 clientId=self.description._topology_settings._topology_id,
  387:                 serverHost=server.description.address[0],
  388:                 serverPort=server.description.address[1],
  389:             )
  390:         return server
  391: 
  392:     def select_server_by_address(
  393:         self,
  394:         address: _Address,
  395:         operation: str,
  396:         server_selection_timeout: Optional[int] = None,
  397:         operation_id: Optional[int] = None,
  398:     ) -> Server:
  399:         """Return a Server for "address", reconnecting if necessary.
  400: 
  401:         If the server's type is not known, request an immediate check of all
  402:         servers. Time out after "server_selection_timeout" if the server
  403:         cannot be reached.
  404: 
  405:         :param address: A (host, port) pair.
  406:         :param operation: The name of the operation that the server is being selected for.
  407:         :param server_selection_timeout: maximum seconds to wait.
  408:             If not provided, the default value
  409:             common.SERVER_SELECTION_TIMEOUT is used.
  410:         :param operation_id: The unique id of the current operation being performed. Defaults to None if not provided.
  411: 
  412:         Calls self.open() if needed.
  413: 
  414:         Raises exc:`ServerSelectionTimeoutError` after
  415:         `server_selection_timeout` if no matching servers are found.
  416:         """
  417:         return self.select_server(
  418:             any_server_selector,
  419:             operation,
  420:             server_selection_timeout,
  421:             address,
  422:             operation_id=operation_id,
  423:         )
  424: 
  425:     def _process_change(
  426:         self,
  427:         server_description: ServerDescription,
  428:         reset_pool: bool = False,
  429:         interrupt_connections: bool = False,
  430:     ) -> None:
  431:         """Process a new ServerDescription on an opened topology.
  432: 
  433:         Hold the lock when calling this.
  434:         """
  435:         td_old = self._description
  436:         sd_old = td_old._server_descriptions[server_description.address]
  437:         if _is_stale_server_description(sd_old, server_description):
  438:             # This is a stale hello response. Ignore it.
  439:             return
  440: 
  441:         new_td = updated_topology_description(self._description, server_description)
  442:         # CMAP: Ensure the pool is "ready" when the server is selectable.
  443:         if server_description.is_readable or (
  444:             server_description.is_server_type_known and new_td.topology_type == TOPOLOGY_TYPE.Single
  445:         ):
  446:             server = self._servers.get(server_description.address)
  447:             if server:
  448:                 server.pool.ready()
  449: 
  450:         suppress_event = (self._publish_server or self._publish_tp) and sd_old == server_description
  451:         if self._publish_server and not suppress_event:
  452:             assert self._events is not None
  453:             self._events.put(
  454:                 (
  455:                     self._listeners.publish_server_description_changed,
  456:                     (sd_old, server_description, server_description.address, self._topology_id),
  457:                 )
  458:             )
  459: 
  460:         self._description = new_td
  461:         self._update_servers()
  462:         self._receive_cluster_time_no_lock(server_description.cluster_time)
  463: 
  464:         if self._publish_tp and not suppress_event:
  465:             assert self._events is not None
  466:             self._events.put(
  467:                 (
  468:                     self._listeners.publish_topology_description_changed,
  469:                     (td_old, self._description, self._topology_id),
  470:                 )
  471:             )
  472: 
  473:         # Shutdown SRV polling for unsupported cluster types.
  474:         # This is only applicable if the old topology was Unknown, and the
  475:         # new one is something other than Unknown or Sharded.
  476:         if self._srv_monitor and (
  477:             td_old.topology_type == TOPOLOGY_TYPE.Unknown
  478:             and self._description.topology_type not in SRV_POLLING_TOPOLOGIES
  479:         ):
  480:             self._srv_monitor.close()
  481: 
  482:         # Clear the pool from a failed heartbeat.
  483:         if reset_pool:
  484:             server = self._servers.get(server_description.address)
  485:             if server:
  486:                 server.pool.reset(interrupt_connections=interrupt_connections)
  487: 
  488:         # Wake waiters in select_servers().
  489:         self._condition.notify_all()
  490: 
  491:     def on_change(
  492:         self,
  493:         server_description: ServerDescription,
  494:         reset_pool: bool = False,
  495:         interrupt_connections: bool = False,
  496:     ) -> None:
  497:         """Process a new ServerDescription after an hello call completes."""
  498:         # We do no I/O holding the lock.
  499:         with self._lock:
  500:             # Monitors may continue working on hello calls for some time
  501:             # after a call to Topology.close, so this method may be called at
  502:             # any time. Ensure the topology is open before processing the
  503:             # change.
  504:             # Any monitored server was definitely in the topology description
  505:             # once. Check if it's still in the description or if some state-
  506:             # change removed it. E.g., we got a host list from the primary
  507:             # that didn't include this server.
  508:             if self._opened and self._description.has_server(server_description.address):
  509:                 self._process_change(server_description, reset_pool, interrupt_connections)
  510: 
  511:     def _process_srv_update(self, seedlist: list[tuple[str, Any]]) -> None:
  512:         """Process a new seedlist on an opened topology.
  513:         Hold the lock when calling this.
  514:         """
  515:         td_old = self._description
  516:         if td_old.topology_type not in SRV_POLLING_TOPOLOGIES:
  517:             return
  518:         self._description = _updated_topology_description_srv_polling(self._description, seedlist)
  519: 
  520:         self._update_servers()
  521: 
  522:         if self._publish_tp:
  523:             assert self._events is not None
  524:             self._events.put(
  525:                 (
  526:                     self._listeners.publish_topology_description_changed,
  527:                     (td_old, self._description, self._topology_id),
  528:                 )
  529:             )
  530: 
  531:     def on_srv_update(self, seedlist: list[tuple[str, Any]]) -> None:
  532:         """Process a new list of nodes obtained from scanning SRV records."""
  533:         # We do no I/O holding the lock.
  534:         with self._lock:
  535:             if self._opened:
  536:                 self._process_srv_update(seedlist)
  537: 
  538:     def get_server_by_address(self, address: _Address) -> Optional[Server]:
  539:         """Get a Server or None.
  540: 
  541:         Returns the current version of the server immediately, even if it's
  542:         Unknown or absent from the topology. Only use this in unittests.
  543:         In driver code, use select_server_by_address, since then you're
  544:         assured a recent view of the server's type and wire protocol version.
  545:         """
  546:         return self._servers.get(address)
  547: 
  548:     def has_server(self, address: _Address) -> bool:
  549:         return address in self._servers
  550: 
  551:     def get_primary(self) -> Optional[_Address]:
  552:         """Return primary's address or None."""
  553:         # Implemented here in Topology instead of MongoClient, so it can lock.
  554:         with self._lock:
  555:             topology_type = self._description.topology_type
  556:             if topology_type != TOPOLOGY_TYPE.ReplicaSetWithPrimary:
  557:                 return None
  558: 
  559:             return writable_server_selector(self._new_selection())[0].address
  560: 
  561:     def _get_replica_set_members(self, selector: Callable[[Selection], Selection]) -> set[_Address]:
  562:         """Return set of replica set member addresses."""
  563:         # Implemented here in Topology instead of MongoClient, so it can lock.
  564:         with self._lock:
  565:             topology_type = self._description.topology_type
  566:             if topology_type not in (
  567:                 TOPOLOGY_TYPE.ReplicaSetWithPrimary,
  568:                 TOPOLOGY_TYPE.ReplicaSetNoPrimary,
  569:             ):
  570:                 return set()
  571: 
  572:             return {sd.address for sd in iter(selector(self._new_selection()))}
  573: 
  574:     def get_secondaries(self) -> set[_Address]:
  575:         """Return set of secondary addresses."""
  576:         return self._get_replica_set_members(secondary_server_selector)
  577: 
  578:     def get_arbiters(self) -> set[_Address]:
  579:         """Return set of arbiter addresses."""
  580:         return self._get_replica_set_members(arbiter_server_selector)
  581: 
  582:     def max_cluster_time(self) -> Optional[ClusterTime]:
  583:         """Return a document, the highest seen $clusterTime."""
  584:         return self._max_cluster_time
  585: 
  586:     def _receive_cluster_time_no_lock(self, cluster_time: Optional[Mapping[str, Any]]) -> None:
  587:         # Driver Sessions Spec: "Whenever a driver receives a cluster time from
  588:         # a server it MUST compare it to the current highest seen cluster time
  589:         # for the deployment. If the new cluster time is higher than the
  590:         # highest seen cluster time it MUST become the new highest seen cluster
  591:         # time. Two cluster times are compared using only the BsonTimestamp
  592:         # value of the clusterTime embedded field."
  593:         if cluster_time:
  594:             # ">" uses bson.timestamp.Timestamp's comparison operator.
  595:             if (
  596:                 not self._max_cluster_time
  597:                 or cluster_time["clusterTime"] > self._max_cluster_time["clusterTime"]
  598:             ):
  599:                 self._max_cluster_time = cluster_time
  600: 
  601:     def receive_cluster_time(self, cluster_time: Optional[Mapping[str, Any]]) -> None:
  602:         with self._lock:
  603:             self._receive_cluster_time_no_lock(cluster_time)
  604: 
  605:     def request_check_all(self, wait_time: int = 5) -> None:
  606:         """Wake all monitors, wait for at least one to check its server."""
  607:         with self._lock:
  608:             self._request_check_all()
  609:             self._condition.wait(wait_time)
  610: 
  611:     def data_bearing_servers(self) -> list[ServerDescription]:
  612:         """Return a list of all data-bearing servers.
  613: 
  614:         This includes any server that might be selected for an operation.
  615:         """
  616:         if self._description.topology_type == TOPOLOGY_TYPE.Single:
  617:             return self._description.known_servers
  618:         return self._description.readable_servers
  619: 
  620:     def update_pool(self) -> None:
  621:         # Remove any stale sockets and add new sockets if pool is too small.
  622:         servers = []
  623:         with self._lock:
  624:             # Only update pools for data-bearing servers.
  625:             for sd in self.data_bearing_servers():
  626:                 server = self._servers[sd.address]
  627:                 servers.append((server, server.pool.gen.get_overall()))
  628: 
  629:         for server, generation in servers:
  630:             try:
  631:                 server.pool.remove_stale_sockets(generation)
  632:             except PyMongoError as exc:
  633:                 ctx = _ErrorContext(exc, 0, generation, False, None)
  634:                 self.handle_error(server.description.address, ctx)
  635:                 raise
  636: 
  637:     def close(self) -> None:
  638:         """Clear pools and terminate monitors. Topology does not reopen on
  639:         demand. Any further operations will raise
  640:         :exc:`~.errors.InvalidOperation`.
  641:         """
  642:         with self._lock:
  643:             for server in self._servers.values():
  644:                 server.close()
  645: 
  646:             # Mark all servers Unknown.
  647:             self._description = self._description.reset()
  648:             for address, sd in self._description.server_descriptions().items():
  649:                 if address in self._servers:
  650:                     self._servers[address].description = sd
  651: 
  652:             # Stop SRV polling thread.
  653:             if self._srv_monitor:
  654:                 self._srv_monitor.close()
  655: 
  656:             self._opened = False
  657:             self._closed = True
  658: 
  659:         # Publish only after releasing the lock.
  660:         if self._publish_tp:
  661:             assert self._events is not None
  662:             self._events.put((self._listeners.publish_topology_closed, (self._topology_id,)))
  663:         if self._publish_server or self._publish_tp:
  664:             self.__events_executor.close()
  665: 
  666:     @property
  667:     def description(self) -> TopologyDescription:
  668:         return self._description
  669: 
  670:     def pop_all_sessions(self) -> list[_ServerSession]:
  671:         """Pop all session ids from the pool."""
  672:         return self._session_pool.pop_all()
  673: 
  674:     def get_server_session(self, session_timeout_minutes: Optional[int]) -> _ServerSession:
  675:         """Start or resume a server session, or raise ConfigurationError."""
  676:         return self._session_pool.get_server_session(session_timeout_minutes)
  677: 
  678:     def return_server_session(self, server_session: _ServerSession) -> None:
  679:         self._session_pool.return_server_session(server_session)
  680: 
  681:     def _new_selection(self) -> Selection:
  682:         """A Selection object, initially including all known servers.
  683: 
  684:         Hold the lock when calling this.
  685:         """
  686:         return Selection.from_topology_description(self._description)
  687: 
  688:     def _ensure_opened(self) -> None:
  689:         """Start monitors, or restart after a fork.
  690: 
  691:         Hold the lock when calling this.
  692:         """
  693:         if self._closed:
  694:             raise InvalidOperation("Cannot use MongoClient after close")
  695: 
  696:         if not self._opened:
  697:             self._opened = True
  698:             self._update_servers()
  699: 
  700:             # Start or restart the events publishing thread.
  701:             if self._publish_tp or self._publish_server:
  702:                 self.__events_executor.open()
  703: 
  704:             # Start the SRV polling thread.
  705:             if self._srv_monitor and (self.description.topology_type in SRV_POLLING_TOPOLOGIES):
  706:                 self._srv_monitor.open()
  707: 
  708:             if self._settings.load_balanced:
  709:                 # Emit initial SDAM events for load balancer mode.
  710:                 self._process_change(
  711:                     ServerDescription(
  712:                         self._seed_addresses[0],
  713:                         Hello({"ok": 1, "serviceId": self._topology_id, "maxWireVersion": 13}),
  714:                     )
  715:                 )
  716: 
  717:         # Ensure that the monitors are open.
  718:         for server in self._servers.values():
  719:             server.open()
  720: 
  721:     def _is_stale_error(self, address: _Address, err_ctx: _ErrorContext) -> bool:
  722:         server = self._servers.get(address)
  723:         if server is None:
  724:             # Another thread removed this server from the topology.
  725:             return True
  726: 
  727:         if server._pool.stale_generation(err_ctx.sock_generation, err_ctx.service_id):
  728:             # This is an outdated error from a previous pool version.
  729:             return True
  730: 
  731:         # topologyVersion check, ignore error when cur_tv >= error_tv:
  732:         cur_tv = server.description.topology_version
  733:         error = err_ctx.error
  734:         error_tv = None
  735:         if error and hasattr(error, "details"):
  736:             if isinstance(error.details, dict):
  737:                 error_tv = error.details.get("topologyVersion")
  738: 
  739:         return _is_stale_error_topology_version(cur_tv, error_tv)
  740: 
  741:     def _handle_error(self, address: _Address, err_ctx: _ErrorContext) -> None:
  742:         if self._is_stale_error(address, err_ctx):
  743:             return
  744: 
  745:         server = self._servers[address]
  746:         error = err_ctx.error
  747:         service_id = err_ctx.service_id
  748: 
  749:         # Ignore a handshake error if the server is behind a load balancer but
  750:         # the service ID is unknown. This indicates that the error happened
  751:         # when dialing the connection or during the MongoDB  handshake, so we
  752:         # don't know the service ID to use for clearing the pool.
  753:         if self._settings.load_balanced and not service_id and not err_ctx.completed_handshake:
  754:             return
  755: 
  756:         if isinstance(error, NetworkTimeout) and err_ctx.completed_handshake:
  757:             # The socket has been closed. Don't reset the server.
  758:             # Server Discovery And Monitoring Spec: "When an application
  759:             # operation fails because of any network error besides a socket
  760:             # timeout...."
  761:             return
  762:         elif isinstance(error, WriteError):
  763:             # Ignore writeErrors.
  764:             return
  765:         elif isinstance(error, (NotPrimaryError, OperationFailure)):
  766:             # As per the SDAM spec if:
  767:             #   - the server sees a "not primary" error, and
  768:             #   - the server is not shutting down, and
  769:             #   - the server version is >= 4.2, then
  770:             # we keep the existing connection pool, but mark the server type
  771:             # as Unknown and request an immediate check of the server.
  772:             # Otherwise, we clear the connection pool, mark the server as
  773:             # Unknown and request an immediate check of the server.
  774:             if hasattr(error, "code"):
  775:                 err_code = error.code
  776:             else:
  777:                 # Default error code if one does not exist.
  778:                 default = 10107 if isinstance(error, NotPrimaryError) else None
  779:                 err_code = error.details.get("code", default)  # type: ignore[union-attr]
  780:             if err_code in helpers._NOT_PRIMARY_CODES:
  781:                 is_shutting_down = err_code in helpers._SHUTDOWN_CODES
  782:                 # Mark server Unknown, clear the pool, and request check.
  783:                 if not self._settings.load_balanced:
  784:                     self._process_change(ServerDescription(address, error=error))
  785:                 if is_shutting_down or (err_ctx.max_wire_version <= 7):
  786:                     # Clear the pool.
  787:                     server.reset(service_id)
  788:                 server.request_check()
  789:             elif not err_ctx.completed_handshake:
  790:                 # Unknown command error during the connection handshake.
  791:                 if not self._settings.load_balanced:
  792:                     self._process_change(ServerDescription(address, error=error))
  793:                 # Clear the pool.
  794:                 server.reset(service_id)
  795:         elif isinstance(error, ConnectionFailure):
  796:             # "Client MUST replace the server's description with type Unknown
  797:             # ... MUST NOT request an immediate check of the server."
  798:             if not self._settings.load_balanced:
  799:                 self._process_change(ServerDescription(address, error=error))
  800:             # Clear the pool.
  801:             server.reset(service_id)
  802:             # "When a client marks a server Unknown from `Network error when
  803:             # reading or writing`_, clients MUST cancel the hello check on
  804:             # that server and close the current monitoring connection."
  805:             server._monitor.cancel_check()
  806: 
  807:     def handle_error(self, address: _Address, err_ctx: _ErrorContext) -> None:
  808:         """Handle an application error.
  809: 
  810:         May reset the server to Unknown, clear the pool, and request an
  811:         immediate check depending on the error and the context.
  812:         """
  813:         with self._lock:
  814:             self._handle_error(address, err_ctx)
  815: 
  816:     def _request_check_all(self) -> None:
  817:         """Wake all monitors. Hold the lock when calling this."""
  818:         for server in self._servers.values():
  819:             server.request_check()
  820: 
  821:     def _update_servers(self) -> None:
  822:         """Sync our Servers from TopologyDescription.server_descriptions.
  823: 
  824:         Hold the lock while calling this.
  825:         """
  826:         for address, sd in self._description.server_descriptions().items():
  827:             if address not in self._servers:
  828:                 monitor = self._settings.monitor_class(
  829:                     server_description=sd,
  830:                     topology=self,
  831:                     pool=self._create_pool_for_monitor(address),
  832:                     topology_settings=self._settings,
  833:                 )
  834: 
  835:                 weak = None
  836:                 if self._publish_server and self._events is not None:
  837:                     weak = weakref.ref(self._events)
  838:                 server = Server(
  839:                     server_description=sd,
  840:                     pool=self._create_pool_for_server(address),
  841:                     monitor=monitor,
  842:                     topology_id=self._topology_id,
  843:                     listeners=self._listeners,
  844:                     events=weak,
  845:                 )
  846: 
  847:                 self._servers[address] = server
  848:                 server.open()
  849:             else:
  850:                 # Cache old is_writable value.
  851:                 was_writable = self._servers[address].description.is_writable
  852:                 # Update server description.
  853:                 self._servers[address].description = sd
  854:                 # Update is_writable value of the pool, if it changed.
  855:                 if was_writable != sd.is_writable:
  856:                     self._servers[address].pool.update_is_writable(sd.is_writable)
  857: 
  858:         for address, server in list(self._servers.items()):
  859:             if not self._description.has_server(address):
  860:                 server.close()
  861:                 self._servers.pop(address)
  862: 
  863:     def _create_pool_for_server(self, address: _Address) -> Pool:
  864:         return self._settings.pool_class(
  865:             address, self._settings.pool_options, client_id=self._topology_id
  866:         )
  867: 
  868:     def _create_pool_for_monitor(self, address: _Address) -> Pool:
  869:         options = self._settings.pool_options
  870: 
  871:         # According to the Server Discovery And Monitoring Spec, monitors use
  872:         # connect_timeout for both connect_timeout and socket_timeout. The
  873:         # pool only has one socket so maxPoolSize and so on aren't needed.
  874:         monitor_pool_options = PoolOptions(
  875:             connect_timeout=options.connect_timeout,
  876:             socket_timeout=options.connect_timeout,
  877:             ssl_context=options._ssl_context,
  878:             tls_allow_invalid_hostnames=options.tls_allow_invalid_hostnames,
  879:             event_listeners=options._event_listeners,
  880:             appname=options.appname,
  881:             driver=options.driver,
  882:             pause_enabled=False,
  883:             server_api=options.server_api,
  884:         )
  885: 
  886:         return self._settings.pool_class(
  887:             address, monitor_pool_options, handshake=False, client_id=self._topology_id
  888:         )
  889: 
  890:     def _error_message(self, selector: Callable[[Selection], Selection]) -> str:
  891:         """Format an error message if server selection fails.
  892: 
  893:         Hold the lock when calling this.
  894:         """
  895:         is_replica_set = self._description.topology_type in (
  896:             TOPOLOGY_TYPE.ReplicaSetWithPrimary,
  897:             TOPOLOGY_TYPE.ReplicaSetNoPrimary,
  898:         )
  899: 
  900:         if is_replica_set:
  901:             server_plural = "replica set members"
  902:         elif self._description.topology_type == TOPOLOGY_TYPE.Sharded:
  903:             server_plural = "mongoses"
  904:         else:
  905:             server_plural = "servers"
  906: 
  907:         if self._description.known_servers:
  908:             # We've connected, but no servers match the selector.
  909:             if selector is writable_server_selector:
  910:                 if is_replica_set:
  911:                     return "No primary available for writes"
  912:                 else:
  913:                     return "No %s available for writes" % server_plural
  914:             else:
  915:                 return f'No {server_plural} match selector "{selector}"'
  916:         else:
  917:             addresses = list(self._description.server_descriptions())
  918:             servers = list(self._description.server_descriptions().values())
  919:             if not servers:
  920:                 if is_replica_set:
  921:                     # We removed all servers because of the wrong setName?
  922:                     return 'No {} available for replica set name "{}"'.format(
  923:                         server_plural,
  924:                         self._settings.replica_set_name,
  925:                     )
  926:                 else:
  927:                     return "No %s available" % server_plural
  928: 
  929:             # 1 or more servers, all Unknown. Are they unknown for one reason?
  930:             error = servers[0].error
  931:             same = all(server.error == error for server in servers[1:])
  932:             if same:
  933:                 if error is None:
  934:                     # We're still discovering.
  935:                     return "No %s found yet" % server_plural
  936: 
  937:                 if is_replica_set and not set(addresses).intersection(self._seed_addresses):
  938:                     # We replaced our seeds with new hosts but can't reach any.
  939:                     return (
  940:                         "Could not reach any servers in %s. Replica set is"
  941:                         " configured with internal hostnames or IPs?" % addresses
  942:                     )
  943: 
  944:                 return str(error)
  945:             else:
  946:                 return ",".join(str(server.error) for server in servers if server.error)
  947: 
  948:     def __repr__(self) -> str:
  949:         msg = ""
  950:         if not self._opened:
  951:             msg = "CLOSED "
  952:         return f"<{self.__class__.__name__} {msg}{self._description!r}>"
  953: 
  954:     def eq_props(self) -> tuple[tuple[_Address, ...], Optional[str], Optional[str], str]:
  955:         """The properties to use for MongoClient/Topology equality checks."""
  956:         ts = self._settings
  957:         return (tuple(sorted(ts.seeds)), ts.replica_set_name, ts.fqdn, ts.srv_service_name)
  958: 
  959:     def __eq__(self, other: object) -> bool:
  960:         if isinstance(other, self.__class__):
  961:             return self.eq_props() == other.eq_props()
  962:         return NotImplemented
  963: 
  964:     def __hash__(self) -> int:
  965:         return hash(self.eq_props())
  966: 
  967: 
  968: class _ErrorContext:
  969:     """An error with context for SDAM error handling."""
  970: 
  971:     def __init__(
  972:         self,
  973:         error: BaseException,
  974:         max_wire_version: int,
  975:         sock_generation: int,
  976:         completed_handshake: bool,
  977:         service_id: Optional[ObjectId],
  978:     ):
  979:         self.error = error
  980:         self.max_wire_version = max_wire_version
  981:         self.sock_generation = sock_generation
  982:         self.completed_handshake = completed_handshake
  983:         self.service_id = service_id
  984: 
  985: 
  986: def _is_stale_error_topology_version(
  987:     current_tv: Optional[Mapping[str, Any]], error_tv: Optional[Mapping[str, Any]]
  988: ) -> bool:
  989:     """Return True if the error's topologyVersion is <= current."""
  990:     if current_tv is None or error_tv is None:
  991:         return False
  992:     if current_tv["processId"] != error_tv["processId"]:
  993:         return False
  994:     return current_tv["counter"] >= error_tv["counter"]
  995: 
  996: 
  997: def _is_stale_server_description(current_sd: ServerDescription, new_sd: ServerDescription) -> bool:
  998:     """Return True if the new topologyVersion is < current."""
  999:     current_tv, new_tv = current_sd.topology_version, new_sd.topology_version
 1000:     if current_tv is None or new_tv is None:
 1001:         return False
 1002:     if current_tv["processId"] != new_tv["processId"]:
 1003:         return False
 1004:     return current_tv["counter"] > new_tv["counter"]
 1005: 
 1006: 
 1007: def _filter_servers(
 1008:     candidates: list[Server], deprioritized_servers: Optional[list[Server]] = None
 1009: ) -> list[Server]:
 1010:     """Filter out deprioritized servers from a list of server candidates."""
 1011:     if not deprioritized_servers:
 1012:         return candidates
 1013: 
 1014:     filtered = [server for server in candidates if server not in deprioritized_servers]
 1015: 
 1016:     # If not possible to pick a prioritized server, return the original list
 1017:     return filtered or candidates
