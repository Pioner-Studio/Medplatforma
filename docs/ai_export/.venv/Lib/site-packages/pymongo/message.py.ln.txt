    1: # Copyright 2009-present MongoDB, Inc.
    2: #
    3: # Licensed under the Apache License, Version 2.0 (the "License");
    4: # you may not use this file except in compliance with the License.
    5: # You may obtain a copy of the License at
    6: #
    7: # http://www.apache.org/licenses/LICENSE-2.0
    8: #
    9: # Unless required by applicable law or agreed to in writing, software
   10: # distributed under the License is distributed on an "AS IS" BASIS,
   11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   12: # See the License for the specific language governing permissions and
   13: # limitations under the License.
   14: 
   15: """Tools for creating `messages
   16: <https://www.mongodb.com/docs/manual/reference/mongodb-wire-protocol/>`_ to be sent to
   17: MongoDB.
   18: 
   19: .. note:: This module is for internal use and is generally not needed by
   20:    application developers.
   21: """
   22: from __future__ import annotations
   23: 
   24: import datetime
   25: import logging
   26: import random
   27: import struct
   28: from io import BytesIO as _BytesIO
   29: from typing import (
   30:     TYPE_CHECKING,
   31:     Any,
   32:     Callable,
   33:     Iterable,
   34:     Mapping,
   35:     MutableMapping,
   36:     NoReturn,
   37:     Optional,
   38:     Union,
   39: )
   40: 
   41: import bson
   42: from bson import CodecOptions, _decode_selective, _dict_to_bson, _make_c_string, encode
   43: from bson.int64 import Int64
   44: from bson.raw_bson import (
   45:     _RAW_ARRAY_BSON_OPTIONS,
   46:     DEFAULT_RAW_BSON_OPTIONS,
   47:     RawBSONDocument,
   48:     _inflate_bson,
   49: )
   50: 
   51: try:
   52:     from pymongo import _cmessage  # type: ignore[attr-defined]
   53: 
   54:     _use_c = True
   55: except ImportError:
   56:     _use_c = False
   57: from pymongo.errors import (
   58:     ConfigurationError,
   59:     CursorNotFound,
   60:     DocumentTooLarge,
   61:     ExecutionTimeout,
   62:     InvalidOperation,
   63:     NotPrimaryError,
   64:     OperationFailure,
   65:     ProtocolError,
   66: )
   67: from pymongo.hello import HelloCompat
   68: from pymongo.helpers import _handle_reauth
   69: from pymongo.logger import _COMMAND_LOGGER, _CommandStatusMessage, _debug_log
   70: from pymongo.read_preferences import ReadPreference
   71: from pymongo.write_concern import WriteConcern
   72: 
   73: if TYPE_CHECKING:
   74:     from datetime import timedelta
   75: 
   76:     from pymongo.client_session import ClientSession
   77:     from pymongo.compression_support import SnappyContext, ZlibContext, ZstdContext
   78:     from pymongo.mongo_client import MongoClient
   79:     from pymongo.monitoring import _EventListeners
   80:     from pymongo.pool import Connection
   81:     from pymongo.read_concern import ReadConcern
   82:     from pymongo.read_preferences import _ServerMode
   83:     from pymongo.typings import _Address, _DocumentOut
   84: 
   85: MAX_INT32 = 2147483647
   86: MIN_INT32 = -2147483648
   87: 
   88: # Overhead allowed for encoded command documents.
   89: _COMMAND_OVERHEAD = 16382
   90: 
   91: _INSERT = 0
   92: _UPDATE = 1
   93: _DELETE = 2
   94: 
   95: _EMPTY = b""
   96: _BSONOBJ = b"\x03"
   97: _ZERO_8 = b"\x00"
   98: _ZERO_16 = b"\x00\x00"
   99: _ZERO_32 = b"\x00\x00\x00\x00"
  100: _ZERO_64 = b"\x00\x00\x00\x00\x00\x00\x00\x00"
  101: _SKIPLIM = b"\x00\x00\x00\x00\xff\xff\xff\xff"
  102: _OP_MAP = {
  103:     _INSERT: b"\x04documents\x00\x00\x00\x00\x00",
  104:     _UPDATE: b"\x04updates\x00\x00\x00\x00\x00",
  105:     _DELETE: b"\x04deletes\x00\x00\x00\x00\x00",
  106: }
  107: _FIELD_MAP = {"insert": "documents", "update": "updates", "delete": "deletes"}
  108: 
  109: _UNICODE_REPLACE_CODEC_OPTIONS: CodecOptions[Mapping[str, Any]] = CodecOptions(
  110:     unicode_decode_error_handler="replace"
  111: )
  112: 
  113: 
  114: def _randint() -> int:
  115:     """Generate a pseudo random 32 bit integer."""
  116:     return random.randint(MIN_INT32, MAX_INT32)  # noqa: S311
  117: 
  118: 
  119: def _maybe_add_read_preference(
  120:     spec: MutableMapping[str, Any], read_preference: _ServerMode
  121: ) -> MutableMapping[str, Any]:
  122:     """Add $readPreference to spec when appropriate."""
  123:     mode = read_preference.mode
  124:     document = read_preference.document
  125:     # Only add $readPreference if it's something other than primary to avoid
  126:     # problems with mongos versions that don't support read preferences. Also,
  127:     # for maximum backwards compatibility, don't add $readPreference for
  128:     # secondaryPreferred unless tags or maxStalenessSeconds are in use (setting
  129:     # the secondaryOkay bit has the same effect).
  130:     if mode and (mode != ReadPreference.SECONDARY_PREFERRED.mode or len(document) > 1):
  131:         if "$query" not in spec:
  132:             spec = {"$query": spec}
  133:         spec["$readPreference"] = document
  134:     return spec
  135: 
  136: 
  137: def _convert_exception(exception: Exception) -> dict[str, Any]:
  138:     """Convert an Exception into a failure document for publishing."""
  139:     return {"errmsg": str(exception), "errtype": exception.__class__.__name__}
  140: 
  141: 
  142: def _convert_write_result(
  143:     operation: str, command: Mapping[str, Any], result: Mapping[str, Any]
  144: ) -> dict[str, Any]:
  145:     """Convert a legacy write result to write command format."""
  146:     # Based on _merge_legacy from bulk.py
  147:     affected = result.get("n", 0)
  148:     res = {"ok": 1, "n": affected}
  149:     errmsg = result.get("errmsg", result.get("err", ""))
  150:     if errmsg:
  151:         # The write was successful on at least the primary so don't return.
  152:         if result.get("wtimeout"):
  153:             res["writeConcernError"] = {"errmsg": errmsg, "code": 64, "errInfo": {"wtimeout": True}}
  154:         else:
  155:             # The write failed.
  156:             error = {"index": 0, "code": result.get("code", 8), "errmsg": errmsg}
  157:             if "errInfo" in result:
  158:                 error["errInfo"] = result["errInfo"]
  159:             res["writeErrors"] = [error]
  160:             return res
  161:     if operation == "insert":
  162:         # GLE result for insert is always 0 in most MongoDB versions.
  163:         res["n"] = len(command["documents"])
  164:     elif operation == "update":
  165:         if "upserted" in result:
  166:             res["upserted"] = [{"index": 0, "_id": result["upserted"]}]
  167:         # Versions of MongoDB before 2.6 don't return the _id for an
  168:         # upsert if _id is not an ObjectId.
  169:         elif result.get("updatedExisting") is False and affected == 1:
  170:             # If _id is in both the update document *and* the query spec
  171:             # the update document _id takes precedence.
  172:             update = command["updates"][0]
  173:             _id = update["u"].get("_id", update["q"].get("_id"))
  174:             res["upserted"] = [{"index": 0, "_id": _id}]
  175:     return res
  176: 
  177: 
  178: _OPTIONS = {
  179:     "tailable": 2,
  180:     "oplogReplay": 8,
  181:     "noCursorTimeout": 16,
  182:     "awaitData": 32,
  183:     "allowPartialResults": 128,
  184: }
  185: 
  186: 
  187: _MODIFIERS = {
  188:     "$query": "filter",
  189:     "$orderby": "sort",
  190:     "$hint": "hint",
  191:     "$comment": "comment",
  192:     "$maxScan": "maxScan",
  193:     "$maxTimeMS": "maxTimeMS",
  194:     "$max": "max",
  195:     "$min": "min",
  196:     "$returnKey": "returnKey",
  197:     "$showRecordId": "showRecordId",
  198:     "$showDiskLoc": "showRecordId",  # <= MongoDb 3.0
  199:     "$snapshot": "snapshot",
  200: }
  201: 
  202: 
  203: def _gen_find_command(
  204:     coll: str,
  205:     spec: Mapping[str, Any],
  206:     projection: Optional[Union[Mapping[str, Any], Iterable[str]]],
  207:     skip: int,
  208:     limit: int,
  209:     batch_size: Optional[int],
  210:     options: Optional[int],
  211:     read_concern: ReadConcern,
  212:     collation: Optional[Mapping[str, Any]] = None,
  213:     session: Optional[ClientSession] = None,
  214:     allow_disk_use: Optional[bool] = None,
  215: ) -> dict[str, Any]:
  216:     """Generate a find command document."""
  217:     cmd: dict[str, Any] = {"find": coll}
  218:     if "$query" in spec:
  219:         cmd.update(
  220:             [
  221:                 (_MODIFIERS[key], val) if key in _MODIFIERS else (key, val)
  222:                 for key, val in spec.items()
  223:             ]
  224:         )
  225:         if "$explain" in cmd:
  226:             cmd.pop("$explain")
  227:         if "$readPreference" in cmd:
  228:             cmd.pop("$readPreference")
  229:     else:
  230:         cmd["filter"] = spec
  231: 
  232:     if projection:
  233:         cmd["projection"] = projection
  234:     if skip:
  235:         cmd["skip"] = skip
  236:     if limit:
  237:         cmd["limit"] = abs(limit)
  238:         if limit < 0:
  239:             cmd["singleBatch"] = True
  240:     if batch_size:
  241:         cmd["batchSize"] = batch_size
  242:     if read_concern.level and not (session and session.in_transaction):
  243:         cmd["readConcern"] = read_concern.document
  244:     if collation:
  245:         cmd["collation"] = collation
  246:     if allow_disk_use is not None:
  247:         cmd["allowDiskUse"] = allow_disk_use
  248:     if options:
  249:         cmd.update([(opt, True) for opt, val in _OPTIONS.items() if options & val])
  250: 
  251:     return cmd
  252: 
  253: 
  254: def _gen_get_more_command(
  255:     cursor_id: Optional[int],
  256:     coll: str,
  257:     batch_size: Optional[int],
  258:     max_await_time_ms: Optional[int],
  259:     comment: Optional[Any],
  260:     conn: Connection,
  261: ) -> dict[str, Any]:
  262:     """Generate a getMore command document."""
  263:     cmd: dict[str, Any] = {"getMore": cursor_id, "collection": coll}
  264:     if batch_size:
  265:         cmd["batchSize"] = batch_size
  266:     if max_await_time_ms is not None:
  267:         cmd["maxTimeMS"] = max_await_time_ms
  268:     if comment is not None and conn.max_wire_version >= 9:
  269:         cmd["comment"] = comment
  270:     return cmd
  271: 
  272: 
  273: class _Query:
  274:     """A query operation."""
  275: 
  276:     __slots__ = (
  277:         "flags",
  278:         "db",
  279:         "coll",
  280:         "ntoskip",
  281:         "spec",
  282:         "fields",
  283:         "codec_options",
  284:         "read_preference",
  285:         "limit",
  286:         "batch_size",
  287:         "name",
  288:         "read_concern",
  289:         "collation",
  290:         "session",
  291:         "client",
  292:         "allow_disk_use",
  293:         "_as_command",
  294:         "exhaust",
  295:     )
  296: 
  297:     # For compatibility with the _GetMore class.
  298:     conn_mgr = None
  299:     cursor_id = None
  300: 
  301:     def __init__(
  302:         self,
  303:         flags: int,
  304:         db: str,
  305:         coll: str,
  306:         ntoskip: int,
  307:         spec: Mapping[str, Any],
  308:         fields: Optional[Mapping[str, Any]],
  309:         codec_options: CodecOptions,
  310:         read_preference: _ServerMode,
  311:         limit: int,
  312:         batch_size: int,
  313:         read_concern: ReadConcern,
  314:         collation: Optional[Mapping[str, Any]],
  315:         session: Optional[ClientSession],
  316:         client: MongoClient,
  317:         allow_disk_use: Optional[bool],
  318:         exhaust: bool,
  319:     ):
  320:         self.flags = flags
  321:         self.db = db
  322:         self.coll = coll
  323:         self.ntoskip = ntoskip
  324:         self.spec = spec
  325:         self.fields = fields
  326:         self.codec_options = codec_options
  327:         self.read_preference = read_preference
  328:         self.read_concern = read_concern
  329:         self.limit = limit
  330:         self.batch_size = batch_size
  331:         self.collation = collation
  332:         self.session = session
  333:         self.client = client
  334:         self.allow_disk_use = allow_disk_use
  335:         self.name = "find"
  336:         self._as_command: Optional[tuple[dict[str, Any], str]] = None
  337:         self.exhaust = exhaust
  338: 
  339:     def reset(self) -> None:
  340:         self._as_command = None
  341: 
  342:     def namespace(self) -> str:
  343:         return f"{self.db}.{self.coll}"
  344: 
  345:     def use_command(self, conn: Connection) -> bool:
  346:         use_find_cmd = False
  347:         if not self.exhaust:
  348:             use_find_cmd = True
  349:         elif conn.max_wire_version >= 8:
  350:             # OP_MSG supports exhaust on MongoDB 4.2+
  351:             use_find_cmd = True
  352:         elif not self.read_concern.ok_for_legacy:
  353:             raise ConfigurationError(
  354:                 "read concern level of %s is not valid "
  355:                 "with a max wire version of %d." % (self.read_concern.level, conn.max_wire_version)
  356:             )
  357: 
  358:         conn.validate_session(self.client, self.session)
  359:         return use_find_cmd
  360: 
  361:     def as_command(
  362:         self, conn: Connection, apply_timeout: bool = False
  363:     ) -> tuple[dict[str, Any], str]:
  364:         """Return a find command document for this query."""
  365:         # We use the command twice: on the wire and for command monitoring.
  366:         # Generate it once, for speed and to avoid repeating side-effects.
  367:         if self._as_command is not None:
  368:             return self._as_command
  369: 
  370:         explain = "$explain" in self.spec
  371:         cmd: dict[str, Any] = _gen_find_command(
  372:             self.coll,
  373:             self.spec,
  374:             self.fields,
  375:             self.ntoskip,
  376:             self.limit,
  377:             self.batch_size,
  378:             self.flags,
  379:             self.read_concern,
  380:             self.collation,
  381:             self.session,
  382:             self.allow_disk_use,
  383:         )
  384:         if explain:
  385:             self.name = "explain"
  386:             cmd = {"explain": cmd}
  387:         session = self.session
  388:         conn.add_server_api(cmd)
  389:         if session:
  390:             session._apply_to(cmd, False, self.read_preference, conn)
  391:             # Explain does not support readConcern.
  392:             if not explain and not session.in_transaction:
  393:                 session._update_read_concern(cmd, conn)
  394:         conn.send_cluster_time(cmd, session, self.client)
  395:         # Support auto encryption
  396:         client = self.client
  397:         if client._encrypter and not client._encrypter._bypass_auto_encryption:
  398:             cmd = client._encrypter.encrypt(self.db, cmd, self.codec_options)
  399:         # Support CSOT
  400:         if apply_timeout:
  401:             conn.apply_timeout(client, cmd)
  402:         self._as_command = cmd, self.db
  403:         return self._as_command
  404: 
  405:     def get_message(
  406:         self, read_preference: _ServerMode, conn: Connection, use_cmd: bool = False
  407:     ) -> tuple[int, bytes, int]:
  408:         """Get a query message, possibly setting the secondaryOk bit."""
  409:         # Use the read_preference decided by _socket_from_server.
  410:         self.read_preference = read_preference
  411:         if read_preference.mode:
  412:             # Set the secondaryOk bit.
  413:             flags = self.flags | 4
  414:         else:
  415:             flags = self.flags
  416: 
  417:         ns = self.namespace()
  418:         spec = self.spec
  419: 
  420:         if use_cmd:
  421:             spec = self.as_command(conn, apply_timeout=True)[0]
  422:             request_id, msg, size, _ = _op_msg(
  423:                 0,
  424:                 spec,
  425:                 self.db,
  426:                 read_preference,
  427:                 self.codec_options,
  428:                 ctx=conn.compression_context,
  429:             )
  430:             return request_id, msg, size
  431: 
  432:         # OP_QUERY treats ntoreturn of -1 and 1 the same, return
  433:         # one document and close the cursor. We have to use 2 for
  434:         # batch size if 1 is specified.
  435:         ntoreturn = self.batch_size == 1 and 2 or self.batch_size
  436:         if self.limit:
  437:             if ntoreturn:
  438:                 ntoreturn = min(self.limit, ntoreturn)
  439:             else:
  440:                 ntoreturn = self.limit
  441: 
  442:         if conn.is_mongos:
  443:             assert isinstance(spec, MutableMapping)
  444:             spec = _maybe_add_read_preference(spec, read_preference)
  445: 
  446:         return _query(
  447:             flags,
  448:             ns,
  449:             self.ntoskip,
  450:             ntoreturn,
  451:             spec,
  452:             None if use_cmd else self.fields,
  453:             self.codec_options,
  454:             ctx=conn.compression_context,
  455:         )
  456: 
  457: 
  458: class _GetMore:
  459:     """A getmore operation."""
  460: 
  461:     __slots__ = (
  462:         "db",
  463:         "coll",
  464:         "ntoreturn",
  465:         "cursor_id",
  466:         "max_await_time_ms",
  467:         "codec_options",
  468:         "read_preference",
  469:         "session",
  470:         "client",
  471:         "conn_mgr",
  472:         "_as_command",
  473:         "exhaust",
  474:         "comment",
  475:     )
  476: 
  477:     name = "getMore"
  478: 
  479:     def __init__(
  480:         self,
  481:         db: str,
  482:         coll: str,
  483:         ntoreturn: int,
  484:         cursor_id: int,
  485:         codec_options: CodecOptions,
  486:         read_preference: _ServerMode,
  487:         session: Optional[ClientSession],
  488:         client: MongoClient,
  489:         max_await_time_ms: Optional[int],
  490:         conn_mgr: Any,
  491:         exhaust: bool,
  492:         comment: Any,
  493:     ):
  494:         self.db = db
  495:         self.coll = coll
  496:         self.ntoreturn = ntoreturn
  497:         self.cursor_id = cursor_id
  498:         self.codec_options = codec_options
  499:         self.read_preference = read_preference
  500:         self.session = session
  501:         self.client = client
  502:         self.max_await_time_ms = max_await_time_ms
  503:         self.conn_mgr = conn_mgr
  504:         self._as_command: Optional[tuple[dict[str, Any], str]] = None
  505:         self.exhaust = exhaust
  506:         self.comment = comment
  507: 
  508:     def reset(self) -> None:
  509:         self._as_command = None
  510: 
  511:     def namespace(self) -> str:
  512:         return f"{self.db}.{self.coll}"
  513: 
  514:     def use_command(self, conn: Connection) -> bool:
  515:         use_cmd = False
  516:         if not self.exhaust:
  517:             use_cmd = True
  518:         elif conn.max_wire_version >= 8:
  519:             # OP_MSG supports exhaust on MongoDB 4.2+
  520:             use_cmd = True
  521: 
  522:         conn.validate_session(self.client, self.session)
  523:         return use_cmd
  524: 
  525:     def as_command(
  526:         self, conn: Connection, apply_timeout: bool = False
  527:     ) -> tuple[dict[str, Any], str]:
  528:         """Return a getMore command document for this query."""
  529:         # See _Query.as_command for an explanation of this caching.
  530:         if self._as_command is not None:
  531:             return self._as_command
  532: 
  533:         cmd: dict[str, Any] = _gen_get_more_command(
  534:             self.cursor_id,
  535:             self.coll,
  536:             self.ntoreturn,
  537:             self.max_await_time_ms,
  538:             self.comment,
  539:             conn,
  540:         )
  541:         if self.session:
  542:             self.session._apply_to(cmd, False, self.read_preference, conn)
  543:         conn.add_server_api(cmd)
  544:         conn.send_cluster_time(cmd, self.session, self.client)
  545:         # Support auto encryption
  546:         client = self.client
  547:         if client._encrypter and not client._encrypter._bypass_auto_encryption:
  548:             cmd = client._encrypter.encrypt(self.db, cmd, self.codec_options)
  549:         # Support CSOT
  550:         if apply_timeout:
  551:             conn.apply_timeout(client, cmd=None)
  552:         self._as_command = cmd, self.db
  553:         return self._as_command
  554: 
  555:     def get_message(
  556:         self, dummy0: Any, conn: Connection, use_cmd: bool = False
  557:     ) -> Union[tuple[int, bytes, int], tuple[int, bytes]]:
  558:         """Get a getmore message."""
  559:         ns = self.namespace()
  560:         ctx = conn.compression_context
  561: 
  562:         if use_cmd:
  563:             spec = self.as_command(conn, apply_timeout=True)[0]
  564:             if self.conn_mgr and self.exhaust:
  565:                 flags = _OpMsg.EXHAUST_ALLOWED
  566:             else:
  567:                 flags = 0
  568:             request_id, msg, size, _ = _op_msg(
  569:                 flags, spec, self.db, None, self.codec_options, ctx=conn.compression_context
  570:             )
  571:             return request_id, msg, size
  572: 
  573:         return _get_more(ns, self.ntoreturn, self.cursor_id, ctx)
  574: 
  575: 
  576: class _RawBatchQuery(_Query):
  577:     def use_command(self, conn: Connection) -> bool:
  578:         # Compatibility checks.
  579:         super().use_command(conn)
  580:         if conn.max_wire_version >= 8:
  581:             # MongoDB 4.2+ supports exhaust over OP_MSG
  582:             return True
  583:         elif not self.exhaust:
  584:             return True
  585:         return False
  586: 
  587: 
  588: class _RawBatchGetMore(_GetMore):
  589:     def use_command(self, conn: Connection) -> bool:
  590:         # Compatibility checks.
  591:         super().use_command(conn)
  592:         if conn.max_wire_version >= 8:
  593:             # MongoDB 4.2+ supports exhaust over OP_MSG
  594:             return True
  595:         elif not self.exhaust:
  596:             return True
  597:         return False
  598: 
  599: 
  600: class _CursorAddress(tuple):
  601:     """The server address (host, port) of a cursor, with namespace property."""
  602: 
  603:     __namespace: Any
  604: 
  605:     def __new__(cls, address: _Address, namespace: str) -> _CursorAddress:
  606:         self = tuple.__new__(cls, address)
  607:         self.__namespace = namespace
  608:         return self
  609: 
  610:     @property
  611:     def namespace(self) -> str:
  612:         """The namespace this cursor."""
  613:         return self.__namespace
  614: 
  615:     def __hash__(self) -> int:
  616:         # Two _CursorAddress instances with different namespaces
  617:         # must not hash the same.
  618:         return ((*self, self.__namespace)).__hash__()
  619: 
  620:     def __eq__(self, other: object) -> bool:
  621:         if isinstance(other, _CursorAddress):
  622:             return tuple(self) == tuple(other) and self.namespace == other.namespace
  623:         return NotImplemented
  624: 
  625:     def __ne__(self, other: object) -> bool:
  626:         return not self == other
  627: 
  628: 
  629: _pack_compression_header = struct.Struct("<iiiiiiB").pack
  630: _COMPRESSION_HEADER_SIZE = 25
  631: 
  632: 
  633: def _compress(
  634:     operation: int, data: bytes, ctx: Union[SnappyContext, ZlibContext, ZstdContext]
  635: ) -> tuple[int, bytes]:
  636:     """Takes message data, compresses it, and adds an OP_COMPRESSED header."""
  637:     compressed = ctx.compress(data)
  638:     request_id = _randint()
  639: 
  640:     header = _pack_compression_header(
  641:         _COMPRESSION_HEADER_SIZE + len(compressed),  # Total message length
  642:         request_id,  # Request id
  643:         0,  # responseTo
  644:         2012,  # operation id
  645:         operation,  # original operation id
  646:         len(data),  # uncompressed message length
  647:         ctx.compressor_id,
  648:     )  # compressor id
  649:     return request_id, header + compressed
  650: 
  651: 
  652: _pack_header = struct.Struct("<iiii").pack
  653: 
  654: 
  655: def __pack_message(operation: int, data: bytes) -> tuple[int, bytes]:
  656:     """Takes message data and adds a message header based on the operation.
  657: 
  658:     Returns the resultant message string.
  659:     """
  660:     rid = _randint()
  661:     message = _pack_header(16 + len(data), rid, 0, operation)
  662:     return rid, message + data
  663: 
  664: 
  665: _pack_int = struct.Struct("<i").pack
  666: _pack_op_msg_flags_type = struct.Struct("<IB").pack
  667: _pack_byte = struct.Struct("<B").pack
  668: 
  669: 
  670: def _op_msg_no_header(
  671:     flags: int,
  672:     command: Mapping[str, Any],
  673:     identifier: str,
  674:     docs: Optional[list[Mapping[str, Any]]],
  675:     opts: CodecOptions,
  676: ) -> tuple[bytes, int, int]:
  677:     """Get a OP_MSG message.
  678: 
  679:     Note: this method handles multiple documents in a type one payload but
  680:     it does not perform batch splitting and the total message size is
  681:     only checked *after* generating the entire message.
  682:     """
  683:     # Encode the command document in payload 0 without checking keys.
  684:     encoded = _dict_to_bson(command, False, opts)
  685:     flags_type = _pack_op_msg_flags_type(flags, 0)
  686:     total_size = len(encoded)
  687:     max_doc_size = 0
  688:     if identifier and docs is not None:
  689:         type_one = _pack_byte(1)
  690:         cstring = _make_c_string(identifier)
  691:         encoded_docs = [_dict_to_bson(doc, False, opts) for doc in docs]
  692:         size = len(cstring) + sum(len(doc) for doc in encoded_docs) + 4
  693:         encoded_size = _pack_int(size)
  694:         total_size += size
  695:         max_doc_size = max(len(doc) for doc in encoded_docs)
  696:         data = [flags_type, encoded, type_one, encoded_size, cstring, *encoded_docs]
  697:     else:
  698:         data = [flags_type, encoded]
  699:     return b"".join(data), total_size, max_doc_size
  700: 
  701: 
  702: def _op_msg_compressed(
  703:     flags: int,
  704:     command: Mapping[str, Any],
  705:     identifier: str,
  706:     docs: Optional[list[Mapping[str, Any]]],
  707:     opts: CodecOptions,
  708:     ctx: Union[SnappyContext, ZlibContext, ZstdContext],
  709: ) -> tuple[int, bytes, int, int]:
  710:     """Internal OP_MSG message helper."""
  711:     msg, total_size, max_bson_size = _op_msg_no_header(flags, command, identifier, docs, opts)
  712:     rid, msg = _compress(2013, msg, ctx)
  713:     return rid, msg, total_size, max_bson_size
  714: 
  715: 
  716: def _op_msg_uncompressed(
  717:     flags: int,
  718:     command: Mapping[str, Any],
  719:     identifier: str,
  720:     docs: Optional[list[Mapping[str, Any]]],
  721:     opts: CodecOptions,
  722: ) -> tuple[int, bytes, int, int]:
  723:     """Internal compressed OP_MSG message helper."""
  724:     data, total_size, max_bson_size = _op_msg_no_header(flags, command, identifier, docs, opts)
  725:     request_id, op_message = __pack_message(2013, data)
  726:     return request_id, op_message, total_size, max_bson_size
  727: 
  728: 
  729: if _use_c:
  730:     _op_msg_uncompressed = _cmessage._op_msg
  731: 
  732: 
  733: def _op_msg(
  734:     flags: int,
  735:     command: MutableMapping[str, Any],
  736:     dbname: str,
  737:     read_preference: Optional[_ServerMode],
  738:     opts: CodecOptions,
  739:     ctx: Union[SnappyContext, ZlibContext, ZstdContext, None] = None,
  740: ) -> tuple[int, bytes, int, int]:
  741:     """Get a OP_MSG message."""
  742:     command["$db"] = dbname
  743:     # getMore commands do not send $readPreference.
  744:     if read_preference is not None and "$readPreference" not in command:
  745:         # Only send $readPreference if it's not primary (the default).
  746:         if read_preference.mode:
  747:             command["$readPreference"] = read_preference.document
  748:     name = next(iter(command))
  749:     try:
  750:         identifier = _FIELD_MAP[name]
  751:         docs = command.pop(identifier)
  752:     except KeyError:
  753:         identifier = ""
  754:         docs = None
  755:     try:
  756:         if ctx:
  757:             return _op_msg_compressed(flags, command, identifier, docs, opts, ctx)
  758:         return _op_msg_uncompressed(flags, command, identifier, docs, opts)
  759:     finally:
  760:         # Add the field back to the command.
  761:         if identifier:
  762:             command[identifier] = docs
  763: 
  764: 
  765: def _query_impl(
  766:     options: int,
  767:     collection_name: str,
  768:     num_to_skip: int,
  769:     num_to_return: int,
  770:     query: Mapping[str, Any],
  771:     field_selector: Optional[Mapping[str, Any]],
  772:     opts: CodecOptions,
  773: ) -> tuple[bytes, int]:
  774:     """Get an OP_QUERY message."""
  775:     encoded = _dict_to_bson(query, False, opts)
  776:     if field_selector:
  777:         efs = _dict_to_bson(field_selector, False, opts)
  778:     else:
  779:         efs = b""
  780:     max_bson_size = max(len(encoded), len(efs))
  781:     return (
  782:         b"".join(
  783:             [
  784:                 _pack_int(options),
  785:                 _make_c_string(collection_name),
  786:                 _pack_int(num_to_skip),
  787:                 _pack_int(num_to_return),
  788:                 encoded,
  789:                 efs,
  790:             ]
  791:         ),
  792:         max_bson_size,
  793:     )
  794: 
  795: 
  796: def _query_compressed(
  797:     options: int,
  798:     collection_name: str,
  799:     num_to_skip: int,
  800:     num_to_return: int,
  801:     query: Mapping[str, Any],
  802:     field_selector: Optional[Mapping[str, Any]],
  803:     opts: CodecOptions,
  804:     ctx: Union[SnappyContext, ZlibContext, ZstdContext],
  805: ) -> tuple[int, bytes, int]:
  806:     """Internal compressed query message helper."""
  807:     op_query, max_bson_size = _query_impl(
  808:         options, collection_name, num_to_skip, num_to_return, query, field_selector, opts
  809:     )
  810:     rid, msg = _compress(2004, op_query, ctx)
  811:     return rid, msg, max_bson_size
  812: 
  813: 
  814: def _query_uncompressed(
  815:     options: int,
  816:     collection_name: str,
  817:     num_to_skip: int,
  818:     num_to_return: int,
  819:     query: Mapping[str, Any],
  820:     field_selector: Optional[Mapping[str, Any]],
  821:     opts: CodecOptions,
  822: ) -> tuple[int, bytes, int]:
  823:     """Internal query message helper."""
  824:     op_query, max_bson_size = _query_impl(
  825:         options, collection_name, num_to_skip, num_to_return, query, field_selector, opts
  826:     )
  827:     rid, msg = __pack_message(2004, op_query)
  828:     return rid, msg, max_bson_size
  829: 
  830: 
  831: if _use_c:
  832:     _query_uncompressed = _cmessage._query_message
  833: 
  834: 
  835: def _query(
  836:     options: int,
  837:     collection_name: str,
  838:     num_to_skip: int,
  839:     num_to_return: int,
  840:     query: Mapping[str, Any],
  841:     field_selector: Optional[Mapping[str, Any]],
  842:     opts: CodecOptions,
  843:     ctx: Union[SnappyContext, ZlibContext, ZstdContext, None] = None,
  844: ) -> tuple[int, bytes, int]:
  845:     """Get a **query** message."""
  846:     if ctx:
  847:         return _query_compressed(
  848:             options, collection_name, num_to_skip, num_to_return, query, field_selector, opts, ctx
  849:         )
  850:     return _query_uncompressed(
  851:         options, collection_name, num_to_skip, num_to_return, query, field_selector, opts
  852:     )
  853: 
  854: 
  855: _pack_long_long = struct.Struct("<q").pack
  856: 
  857: 
  858: def _get_more_impl(collection_name: str, num_to_return: int, cursor_id: int) -> bytes:
  859:     """Get an OP_GET_MORE message."""
  860:     return b"".join(
  861:         [
  862:             _ZERO_32,
  863:             _make_c_string(collection_name),
  864:             _pack_int(num_to_return),
  865:             _pack_long_long(cursor_id),
  866:         ]
  867:     )
  868: 
  869: 
  870: def _get_more_compressed(
  871:     collection_name: str,
  872:     num_to_return: int,
  873:     cursor_id: int,
  874:     ctx: Union[SnappyContext, ZlibContext, ZstdContext],
  875: ) -> tuple[int, bytes]:
  876:     """Internal compressed getMore message helper."""
  877:     return _compress(2005, _get_more_impl(collection_name, num_to_return, cursor_id), ctx)
  878: 
  879: 
  880: def _get_more_uncompressed(
  881:     collection_name: str, num_to_return: int, cursor_id: int
  882: ) -> tuple[int, bytes]:
  883:     """Internal getMore message helper."""
  884:     return __pack_message(2005, _get_more_impl(collection_name, num_to_return, cursor_id))
  885: 
  886: 
  887: if _use_c:
  888:     _get_more_uncompressed = _cmessage._get_more_message
  889: 
  890: 
  891: def _get_more(
  892:     collection_name: str,
  893:     num_to_return: int,
  894:     cursor_id: int,
  895:     ctx: Union[SnappyContext, ZlibContext, ZstdContext, None] = None,
  896: ) -> tuple[int, bytes]:
  897:     """Get a **getMore** message."""
  898:     if ctx:
  899:         return _get_more_compressed(collection_name, num_to_return, cursor_id, ctx)
  900:     return _get_more_uncompressed(collection_name, num_to_return, cursor_id)
  901: 
  902: 
  903: class _BulkWriteContext:
  904:     """A wrapper around Connection for use with write splitting functions."""
  905: 
  906:     __slots__ = (
  907:         "db_name",
  908:         "conn",
  909:         "op_id",
  910:         "name",
  911:         "field",
  912:         "publish",
  913:         "start_time",
  914:         "listeners",
  915:         "session",
  916:         "compress",
  917:         "op_type",
  918:         "codec",
  919:     )
  920: 
  921:     def __init__(
  922:         self,
  923:         database_name: str,
  924:         cmd_name: str,
  925:         conn: Connection,
  926:         operation_id: int,
  927:         listeners: _EventListeners,
  928:         session: ClientSession,
  929:         op_type: int,
  930:         codec: CodecOptions,
  931:     ):
  932:         self.db_name = database_name
  933:         self.conn = conn
  934:         self.op_id = operation_id
  935:         self.listeners = listeners
  936:         self.publish = listeners.enabled_for_commands
  937:         self.name = cmd_name
  938:         self.field = _FIELD_MAP[self.name]
  939:         self.start_time = datetime.datetime.now()
  940:         self.session = session
  941:         self.compress = bool(conn.compression_context)
  942:         self.op_type = op_type
  943:         self.codec = codec
  944: 
  945:     def __batch_command(
  946:         self, cmd: MutableMapping[str, Any], docs: list[Mapping[str, Any]]
  947:     ) -> tuple[int, bytes, list[Mapping[str, Any]]]:
  948:         namespace = self.db_name + ".$cmd"
  949:         request_id, msg, to_send = _do_batched_op_msg(
  950:             namespace, self.op_type, cmd, docs, self.codec, self
  951:         )
  952:         if not to_send:
  953:             raise InvalidOperation("cannot do an empty bulk write")
  954:         return request_id, msg, to_send
  955: 
  956:     def execute(
  957:         self, cmd: MutableMapping[str, Any], docs: list[Mapping[str, Any]], client: MongoClient
  958:     ) -> tuple[Mapping[str, Any], list[Mapping[str, Any]]]:
  959:         request_id, msg, to_send = self.__batch_command(cmd, docs)
  960:         result = self.write_command(cmd, request_id, msg, to_send, client)
  961:         client._process_response(result, self.session)
  962:         return result, to_send
  963: 
  964:     def execute_unack(
  965:         self, cmd: MutableMapping[str, Any], docs: list[Mapping[str, Any]], client: MongoClient
  966:     ) -> list[Mapping[str, Any]]:
  967:         request_id, msg, to_send = self.__batch_command(cmd, docs)
  968:         # Though this isn't strictly a "legacy" write, the helper
  969:         # handles publishing commands and sending our message
  970:         # without receiving a result. Send 0 for max_doc_size
  971:         # to disable size checking. Size checking is handled while
  972:         # the documents are encoded to BSON.
  973:         self.unack_write(cmd, request_id, msg, 0, to_send, client)
  974:         return to_send
  975: 
  976:     @property
  977:     def max_bson_size(self) -> int:
  978:         """A proxy for SockInfo.max_bson_size."""
  979:         return self.conn.max_bson_size
  980: 
  981:     @property
  982:     def max_message_size(self) -> int:
  983:         """A proxy for SockInfo.max_message_size."""
  984:         if self.compress:
  985:             # Subtract 16 bytes for the message header.
  986:             return self.conn.max_message_size - 16
  987:         return self.conn.max_message_size
  988: 
  989:     @property
  990:     def max_write_batch_size(self) -> int:
  991:         """A proxy for SockInfo.max_write_batch_size."""
  992:         return self.conn.max_write_batch_size
  993: 
  994:     @property
  995:     def max_split_size(self) -> int:
  996:         """The maximum size of a BSON command before batch splitting."""
  997:         return self.max_bson_size
  998: 
  999:     def unack_write(
 1000:         self,
 1001:         cmd: MutableMapping[str, Any],
 1002:         request_id: int,
 1003:         msg: bytes,
 1004:         max_doc_size: int,
 1005:         docs: list[Mapping[str, Any]],
 1006:         client: MongoClient,
 1007:     ) -> Optional[Mapping[str, Any]]:
 1008:         """A proxy for Connection.unack_write that handles event publishing."""
 1009:         if _COMMAND_LOGGER.isEnabledFor(logging.DEBUG):
 1010:             _debug_log(
 1011:                 _COMMAND_LOGGER,
 1012:                 clientId=client._topology_settings._topology_id,
 1013:                 message=_CommandStatusMessage.STARTED,
 1014:                 command=cmd,
 1015:                 commandName=next(iter(cmd)),
 1016:                 databaseName=self.db_name,
 1017:                 requestId=request_id,
 1018:                 operationId=request_id,
 1019:                 driverConnectionId=self.conn.id,
 1020:                 serverConnectionId=self.conn.server_connection_id,
 1021:                 serverHost=self.conn.address[0],
 1022:                 serverPort=self.conn.address[1],
 1023:                 serviceId=self.conn.service_id,
 1024:             )
 1025:         if self.publish:
 1026:             cmd = self._start(cmd, request_id, docs)
 1027:         try:
 1028:             result = self.conn.unack_write(msg, max_doc_size)  # type: ignore[func-returns-value]
 1029:             duration = datetime.datetime.now() - self.start_time
 1030:             if result is not None:
 1031:                 reply = _convert_write_result(self.name, cmd, result)
 1032:             else:
 1033:                 # Comply with APM spec.
 1034:                 reply = {"ok": 1}
 1035:                 if _COMMAND_LOGGER.isEnabledFor(logging.DEBUG):
 1036:                     _debug_log(
 1037:                         _COMMAND_LOGGER,
 1038:                         clientId=client._topology_settings._topology_id,
 1039:                         message=_CommandStatusMessage.SUCCEEDED,
 1040:                         durationMS=duration,
 1041:                         reply=reply,
 1042:                         commandName=next(iter(cmd)),
 1043:                         databaseName=self.db_name,
 1044:                         requestId=request_id,
 1045:                         operationId=request_id,
 1046:                         driverConnectionId=self.conn.id,
 1047:                         serverConnectionId=self.conn.server_connection_id,
 1048:                         serverHost=self.conn.address[0],
 1049:                         serverPort=self.conn.address[1],
 1050:                         serviceId=self.conn.service_id,
 1051:                     )
 1052:             if self.publish:
 1053:                 self._succeed(request_id, reply, duration)
 1054:         except Exception as exc:
 1055:             duration = datetime.datetime.now() - self.start_time
 1056:             if isinstance(exc, OperationFailure):
 1057:                 failure: _DocumentOut = _convert_write_result(self.name, cmd, exc.details)  # type: ignore[arg-type]
 1058:             elif isinstance(exc, NotPrimaryError):
 1059:                 failure = exc.details  # type: ignore[assignment]
 1060:             else:
 1061:                 failure = _convert_exception(exc)
 1062:             if _COMMAND_LOGGER.isEnabledFor(logging.DEBUG):
 1063:                 _debug_log(
 1064:                     _COMMAND_LOGGER,
 1065:                     clientId=client._topology_settings._topology_id,
 1066:                     message=_CommandStatusMessage.FAILED,
 1067:                     durationMS=duration,
 1068:                     failure=failure,
 1069:                     commandName=next(iter(cmd)),
 1070:                     databaseName=self.db_name,
 1071:                     requestId=request_id,
 1072:                     operationId=request_id,
 1073:                     driverConnectionId=self.conn.id,
 1074:                     serverConnectionId=self.conn.server_connection_id,
 1075:                     serverHost=self.conn.address[0],
 1076:                     serverPort=self.conn.address[1],
 1077:                     serviceId=self.conn.service_id,
 1078:                     isServerSideError=isinstance(exc, OperationFailure),
 1079:                 )
 1080:             if self.publish:
 1081:                 assert self.start_time is not None
 1082:                 self._fail(request_id, failure, duration)
 1083:             raise
 1084:         finally:
 1085:             self.start_time = datetime.datetime.now()
 1086:         return result
 1087: 
 1088:     @_handle_reauth
 1089:     def write_command(
 1090:         self,
 1091:         cmd: MutableMapping[str, Any],
 1092:         request_id: int,
 1093:         msg: bytes,
 1094:         docs: list[Mapping[str, Any]],
 1095:         client: MongoClient,
 1096:     ) -> dict[str, Any]:
 1097:         """A proxy for SocketInfo.write_command that handles event publishing."""
 1098:         cmd[self.field] = docs
 1099:         if _COMMAND_LOGGER.isEnabledFor(logging.DEBUG):
 1100:             _debug_log(
 1101:                 _COMMAND_LOGGER,
 1102:                 clientId=client._topology_settings._topology_id,
 1103:                 message=_CommandStatusMessage.STARTED,
 1104:                 command=cmd,
 1105:                 commandName=next(iter(cmd)),
 1106:                 databaseName=self.db_name,
 1107:                 requestId=request_id,
 1108:                 operationId=request_id,
 1109:                 driverConnectionId=self.conn.id,
 1110:                 serverConnectionId=self.conn.server_connection_id,
 1111:                 serverHost=self.conn.address[0],
 1112:                 serverPort=self.conn.address[1],
 1113:                 serviceId=self.conn.service_id,
 1114:             )
 1115:         if self.publish:
 1116:             self._start(cmd, request_id, docs)
 1117:         try:
 1118:             reply = self.conn.write_command(request_id, msg, self.codec)
 1119:             duration = datetime.datetime.now() - self.start_time
 1120:             if _COMMAND_LOGGER.isEnabledFor(logging.DEBUG):
 1121:                 _debug_log(
 1122:                     _COMMAND_LOGGER,
 1123:                     clientId=client._topology_settings._topology_id,
 1124:                     message=_CommandStatusMessage.SUCCEEDED,
 1125:                     durationMS=duration,
 1126:                     reply=reply,
 1127:                     commandName=next(iter(cmd)),
 1128:                     databaseName=self.db_name,
 1129:                     requestId=request_id,
 1130:                     operationId=request_id,
 1131:                     driverConnectionId=self.conn.id,
 1132:                     serverConnectionId=self.conn.server_connection_id,
 1133:                     serverHost=self.conn.address[0],
 1134:                     serverPort=self.conn.address[1],
 1135:                     serviceId=self.conn.service_id,
 1136:                 )
 1137:             if self.publish:
 1138:                 self._succeed(request_id, reply, duration)
 1139:         except Exception as exc:
 1140:             duration = datetime.datetime.now() - self.start_time
 1141:             if isinstance(exc, (NotPrimaryError, OperationFailure)):
 1142:                 failure: _DocumentOut = exc.details  # type: ignore[assignment]
 1143:             else:
 1144:                 failure = _convert_exception(exc)
 1145:             if _COMMAND_LOGGER.isEnabledFor(logging.DEBUG):
 1146:                 _debug_log(
 1147:                     _COMMAND_LOGGER,
 1148:                     clientId=client._topology_settings._topology_id,
 1149:                     message=_CommandStatusMessage.FAILED,
 1150:                     durationMS=duration,
 1151:                     failure=failure,
 1152:                     commandName=next(iter(cmd)),
 1153:                     databaseName=self.db_name,
 1154:                     requestId=request_id,
 1155:                     operationId=request_id,
 1156:                     driverConnectionId=self.conn.id,
 1157:                     serverConnectionId=self.conn.server_connection_id,
 1158:                     serverHost=self.conn.address[0],
 1159:                     serverPort=self.conn.address[1],
 1160:                     serviceId=self.conn.service_id,
 1161:                     isServerSideError=isinstance(exc, OperationFailure),
 1162:                 )
 1163: 
 1164:             if self.publish:
 1165:                 self._fail(request_id, failure, duration)
 1166:             raise
 1167:         finally:
 1168:             self.start_time = datetime.datetime.now()
 1169:         return reply
 1170: 
 1171:     def _start(
 1172:         self, cmd: MutableMapping[str, Any], request_id: int, docs: list[Mapping[str, Any]]
 1173:     ) -> MutableMapping[str, Any]:
 1174:         """Publish a CommandStartedEvent."""
 1175:         cmd[self.field] = docs
 1176:         self.listeners.publish_command_start(
 1177:             cmd,
 1178:             self.db_name,
 1179:             request_id,
 1180:             self.conn.address,
 1181:             self.conn.server_connection_id,
 1182:             self.op_id,
 1183:             self.conn.service_id,
 1184:         )
 1185:         return cmd
 1186: 
 1187:     def _succeed(self, request_id: int, reply: _DocumentOut, duration: timedelta) -> None:
 1188:         """Publish a CommandSucceededEvent."""
 1189:         self.listeners.publish_command_success(
 1190:             duration,
 1191:             reply,
 1192:             self.name,
 1193:             request_id,
 1194:             self.conn.address,
 1195:             self.conn.server_connection_id,
 1196:             self.op_id,
 1197:             self.conn.service_id,
 1198:             database_name=self.db_name,
 1199:         )
 1200: 
 1201:     def _fail(self, request_id: int, failure: _DocumentOut, duration: timedelta) -> None:
 1202:         """Publish a CommandFailedEvent."""
 1203:         self.listeners.publish_command_failure(
 1204:             duration,
 1205:             failure,
 1206:             self.name,
 1207:             request_id,
 1208:             self.conn.address,
 1209:             self.conn.server_connection_id,
 1210:             self.op_id,
 1211:             self.conn.service_id,
 1212:             database_name=self.db_name,
 1213:         )
 1214: 
 1215: 
 1216: # From the Client Side Encryption spec:
 1217: # Because automatic encryption increases the size of commands, the driver
 1218: # MUST split bulk writes at a reduced size limit before undergoing automatic
 1219: # encryption. The write payload MUST be split at 2MiB (2097152).
 1220: _MAX_SPLIT_SIZE_ENC = 2097152
 1221: 
 1222: 
 1223: class _EncryptedBulkWriteContext(_BulkWriteContext):
 1224:     __slots__ = ()
 1225: 
 1226:     def __batch_command(
 1227:         self, cmd: MutableMapping[str, Any], docs: list[Mapping[str, Any]]
 1228:     ) -> tuple[dict[str, Any], list[Mapping[str, Any]]]:
 1229:         namespace = self.db_name + ".$cmd"
 1230:         msg, to_send = _encode_batched_write_command(
 1231:             namespace, self.op_type, cmd, docs, self.codec, self
 1232:         )
 1233:         if not to_send:
 1234:             raise InvalidOperation("cannot do an empty bulk write")
 1235: 
 1236:         # Chop off the OP_QUERY header to get a properly batched write command.
 1237:         cmd_start = msg.index(b"\x00", 4) + 9
 1238:         outgoing = _inflate_bson(memoryview(msg)[cmd_start:], DEFAULT_RAW_BSON_OPTIONS)
 1239:         return outgoing, to_send
 1240: 
 1241:     def execute(
 1242:         self, cmd: MutableMapping[str, Any], docs: list[Mapping[str, Any]], client: MongoClient
 1243:     ) -> tuple[Mapping[str, Any], list[Mapping[str, Any]]]:
 1244:         batched_cmd, to_send = self.__batch_command(cmd, docs)
 1245:         result: Mapping[str, Any] = self.conn.command(
 1246:             self.db_name, batched_cmd, codec_options=self.codec, session=self.session, client=client
 1247:         )
 1248:         return result, to_send
 1249: 
 1250:     def execute_unack(
 1251:         self, cmd: MutableMapping[str, Any], docs: list[Mapping[str, Any]], client: MongoClient
 1252:     ) -> list[Mapping[str, Any]]:
 1253:         batched_cmd, to_send = self.__batch_command(cmd, docs)
 1254:         self.conn.command(
 1255:             self.db_name,
 1256:             batched_cmd,
 1257:             write_concern=WriteConcern(w=0),
 1258:             session=self.session,
 1259:             client=client,
 1260:         )
 1261:         return to_send
 1262: 
 1263:     @property
 1264:     def max_split_size(self) -> int:
 1265:         """Reduce the batch splitting size."""
 1266:         return _MAX_SPLIT_SIZE_ENC
 1267: 
 1268: 
 1269: def _raise_document_too_large(operation: str, doc_size: int, max_size: int) -> NoReturn:
 1270:     """Internal helper for raising DocumentTooLarge."""
 1271:     if operation == "insert":
 1272:         raise DocumentTooLarge(
 1273:             "BSON document too large (%d bytes)"
 1274:             " - the connected server supports"
 1275:             " BSON document sizes up to %d"
 1276:             " bytes." % (doc_size, max_size)
 1277:         )
 1278:     else:
 1279:         # There's nothing intelligent we can say
 1280:         # about size for update and delete
 1281:         raise DocumentTooLarge(f"{operation!r} command document too large")
 1282: 
 1283: 
 1284: # OP_MSG -------------------------------------------------------------
 1285: 
 1286: 
 1287: _OP_MSG_MAP = {
 1288:     _INSERT: b"documents\x00",
 1289:     _UPDATE: b"updates\x00",
 1290:     _DELETE: b"deletes\x00",
 1291: }
 1292: 
 1293: 
 1294: def _batched_op_msg_impl(
 1295:     operation: int,
 1296:     command: Mapping[str, Any],
 1297:     docs: list[Mapping[str, Any]],
 1298:     ack: bool,
 1299:     opts: CodecOptions,
 1300:     ctx: _BulkWriteContext,
 1301:     buf: _BytesIO,
 1302: ) -> tuple[list[Mapping[str, Any]], int]:
 1303:     """Create a batched OP_MSG write."""
 1304:     max_bson_size = ctx.max_bson_size
 1305:     max_write_batch_size = ctx.max_write_batch_size
 1306:     max_message_size = ctx.max_message_size
 1307: 
 1308:     flags = b"\x00\x00\x00\x00" if ack else b"\x02\x00\x00\x00"
 1309:     # Flags
 1310:     buf.write(flags)
 1311: 
 1312:     # Type 0 Section
 1313:     buf.write(b"\x00")
 1314:     buf.write(_dict_to_bson(command, False, opts))
 1315: 
 1316:     # Type 1 Section
 1317:     buf.write(b"\x01")
 1318:     size_location = buf.tell()
 1319:     # Save space for size
 1320:     buf.write(b"\x00\x00\x00\x00")
 1321:     try:
 1322:         buf.write(_OP_MSG_MAP[operation])
 1323:     except KeyError:
 1324:         raise InvalidOperation("Unknown command") from None
 1325: 
 1326:     to_send = []
 1327:     idx = 0
 1328:     for doc in docs:
 1329:         # Encode the current operation
 1330:         value = _dict_to_bson(doc, False, opts)
 1331:         doc_length = len(value)
 1332:         new_message_size = buf.tell() + doc_length
 1333:         # Does first document exceed max_message_size?
 1334:         doc_too_large = idx == 0 and (new_message_size > max_message_size)
 1335:         # When OP_MSG is used unacknowledged we have to check
 1336:         # document size client side or applications won't be notified.
 1337:         # Otherwise we let the server deal with documents that are too large
 1338:         # since ordered=False causes those documents to be skipped instead of
 1339:         # halting the bulk write operation.
 1340:         unacked_doc_too_large = not ack and (doc_length > max_bson_size)
 1341:         if doc_too_large or unacked_doc_too_large:
 1342:             write_op = list(_FIELD_MAP.keys())[operation]
 1343:             _raise_document_too_large(write_op, len(value), max_bson_size)
 1344:         # We have enough data, return this batch.
 1345:         if new_message_size > max_message_size:
 1346:             break
 1347:         buf.write(value)
 1348:         to_send.append(doc)
 1349:         idx += 1
 1350:         # We have enough documents, return this batch.
 1351:         if idx == max_write_batch_size:
 1352:             break
 1353: 
 1354:     # Write type 1 section size
 1355:     length = buf.tell()
 1356:     buf.seek(size_location)
 1357:     buf.write(_pack_int(length - size_location))
 1358: 
 1359:     return to_send, length
 1360: 
 1361: 
 1362: def _encode_batched_op_msg(
 1363:     operation: int,
 1364:     command: Mapping[str, Any],
 1365:     docs: list[Mapping[str, Any]],
 1366:     ack: bool,
 1367:     opts: CodecOptions,
 1368:     ctx: _BulkWriteContext,
 1369: ) -> tuple[bytes, list[Mapping[str, Any]]]:
 1370:     """Encode the next batched insert, update, or delete operation
 1371:     as OP_MSG.
 1372:     """
 1373:     buf = _BytesIO()
 1374: 
 1375:     to_send, _ = _batched_op_msg_impl(operation, command, docs, ack, opts, ctx, buf)
 1376:     return buf.getvalue(), to_send
 1377: 
 1378: 
 1379: if _use_c:
 1380:     _encode_batched_op_msg = _cmessage._encode_batched_op_msg
 1381: 
 1382: 
 1383: def _batched_op_msg_compressed(
 1384:     operation: int,
 1385:     command: Mapping[str, Any],
 1386:     docs: list[Mapping[str, Any]],
 1387:     ack: bool,
 1388:     opts: CodecOptions,
 1389:     ctx: _BulkWriteContext,
 1390: ) -> tuple[int, bytes, list[Mapping[str, Any]]]:
 1391:     """Create the next batched insert, update, or delete operation
 1392:     with OP_MSG, compressed.
 1393:     """
 1394:     data, to_send = _encode_batched_op_msg(operation, command, docs, ack, opts, ctx)
 1395: 
 1396:     assert ctx.conn.compression_context is not None
 1397:     request_id, msg = _compress(2013, data, ctx.conn.compression_context)
 1398:     return request_id, msg, to_send
 1399: 
 1400: 
 1401: def _batched_op_msg(
 1402:     operation: int,
 1403:     command: Mapping[str, Any],
 1404:     docs: list[Mapping[str, Any]],
 1405:     ack: bool,
 1406:     opts: CodecOptions,
 1407:     ctx: _BulkWriteContext,
 1408: ) -> tuple[int, bytes, list[Mapping[str, Any]]]:
 1409:     """OP_MSG implementation entry point."""
 1410:     buf = _BytesIO()
 1411: 
 1412:     # Save space for message length and request id
 1413:     buf.write(_ZERO_64)
 1414:     # responseTo, opCode
 1415:     buf.write(b"\x00\x00\x00\x00\xdd\x07\x00\x00")
 1416: 
 1417:     to_send, length = _batched_op_msg_impl(operation, command, docs, ack, opts, ctx, buf)
 1418: 
 1419:     # Header - request id and message length
 1420:     buf.seek(4)
 1421:     request_id = _randint()
 1422:     buf.write(_pack_int(request_id))
 1423:     buf.seek(0)
 1424:     buf.write(_pack_int(length))
 1425: 
 1426:     return request_id, buf.getvalue(), to_send
 1427: 
 1428: 
 1429: if _use_c:
 1430:     _batched_op_msg = _cmessage._batched_op_msg
 1431: 
 1432: 
 1433: def _do_batched_op_msg(
 1434:     namespace: str,
 1435:     operation: int,
 1436:     command: MutableMapping[str, Any],
 1437:     docs: list[Mapping[str, Any]],
 1438:     opts: CodecOptions,
 1439:     ctx: _BulkWriteContext,
 1440: ) -> tuple[int, bytes, list[Mapping[str, Any]]]:
 1441:     """Create the next batched insert, update, or delete operation
 1442:     using OP_MSG.
 1443:     """
 1444:     command["$db"] = namespace.split(".", 1)[0]
 1445:     if "writeConcern" in command:
 1446:         ack = bool(command["writeConcern"].get("w", 1))
 1447:     else:
 1448:         ack = True
 1449:     if ctx.conn.compression_context:
 1450:         return _batched_op_msg_compressed(operation, command, docs, ack, opts, ctx)
 1451:     return _batched_op_msg(operation, command, docs, ack, opts, ctx)
 1452: 
 1453: 
 1454: # End OP_MSG -----------------------------------------------------
 1455: 
 1456: 
 1457: def _encode_batched_write_command(
 1458:     namespace: str,
 1459:     operation: int,
 1460:     command: MutableMapping[str, Any],
 1461:     docs: list[Mapping[str, Any]],
 1462:     opts: CodecOptions,
 1463:     ctx: _BulkWriteContext,
 1464: ) -> tuple[bytes, list[Mapping[str, Any]]]:
 1465:     """Encode the next batched insert, update, or delete command."""
 1466:     buf = _BytesIO()
 1467: 
 1468:     to_send, _ = _batched_write_command_impl(namespace, operation, command, docs, opts, ctx, buf)
 1469:     return buf.getvalue(), to_send
 1470: 
 1471: 
 1472: if _use_c:
 1473:     _encode_batched_write_command = _cmessage._encode_batched_write_command
 1474: 
 1475: 
 1476: def _batched_write_command_impl(
 1477:     namespace: str,
 1478:     operation: int,
 1479:     command: MutableMapping[str, Any],
 1480:     docs: list[Mapping[str, Any]],
 1481:     opts: CodecOptions,
 1482:     ctx: _BulkWriteContext,
 1483:     buf: _BytesIO,
 1484: ) -> tuple[list[Mapping[str, Any]], int]:
 1485:     """Create a batched OP_QUERY write command."""
 1486:     max_bson_size = ctx.max_bson_size
 1487:     max_write_batch_size = ctx.max_write_batch_size
 1488:     # Max BSON object size + 16k - 2 bytes for ending NUL bytes.
 1489:     # Server guarantees there is enough room: SERVER-10643.
 1490:     max_cmd_size = max_bson_size + _COMMAND_OVERHEAD
 1491:     max_split_size = ctx.max_split_size
 1492: 
 1493:     # No options
 1494:     buf.write(_ZERO_32)
 1495:     # Namespace as C string
 1496:     buf.write(namespace.encode("utf8"))
 1497:     buf.write(_ZERO_8)
 1498:     # Skip: 0, Limit: -1
 1499:     buf.write(_SKIPLIM)
 1500: 
 1501:     # Where to write command document length
 1502:     command_start = buf.tell()
 1503:     buf.write(encode(command))
 1504: 
 1505:     # Start of payload
 1506:     buf.seek(-1, 2)
 1507:     # Work around some Jython weirdness.
 1508:     buf.truncate()
 1509:     try:
 1510:         buf.write(_OP_MAP[operation])
 1511:     except KeyError:
 1512:         raise InvalidOperation("Unknown command") from None
 1513: 
 1514:     # Where to write list document length
 1515:     list_start = buf.tell() - 4
 1516:     to_send = []
 1517:     idx = 0
 1518:     for doc in docs:
 1519:         # Encode the current operation
 1520:         key = str(idx).encode("utf8")
 1521:         value = _dict_to_bson(doc, False, opts)
 1522:         # Is there enough room to add this document? max_cmd_size accounts for
 1523:         # the two trailing null bytes.
 1524:         doc_too_large = len(value) > max_cmd_size
 1525:         if doc_too_large:
 1526:             write_op = list(_FIELD_MAP.keys())[operation]
 1527:             _raise_document_too_large(write_op, len(value), max_bson_size)
 1528:         enough_data = idx >= 1 and (buf.tell() + len(key) + len(value)) >= max_split_size
 1529:         enough_documents = idx >= max_write_batch_size
 1530:         if enough_data or enough_documents:
 1531:             break
 1532:         buf.write(_BSONOBJ)
 1533:         buf.write(key)
 1534:         buf.write(_ZERO_8)
 1535:         buf.write(value)
 1536:         to_send.append(doc)
 1537:         idx += 1
 1538: 
 1539:     # Finalize the current OP_QUERY message.
 1540:     # Close list and command documents
 1541:     buf.write(_ZERO_16)
 1542: 
 1543:     # Write document lengths and request id
 1544:     length = buf.tell()
 1545:     buf.seek(list_start)
 1546:     buf.write(_pack_int(length - list_start - 1))
 1547:     buf.seek(command_start)
 1548:     buf.write(_pack_int(length - command_start))
 1549: 
 1550:     return to_send, length
 1551: 
 1552: 
 1553: class _OpReply:
 1554:     """A MongoDB OP_REPLY response message."""
 1555: 
 1556:     __slots__ = ("flags", "cursor_id", "number_returned", "documents")
 1557: 
 1558:     UNPACK_FROM = struct.Struct("<iqii").unpack_from
 1559:     OP_CODE = 1
 1560: 
 1561:     def __init__(self, flags: int, cursor_id: int, number_returned: int, documents: bytes):
 1562:         self.flags = flags
 1563:         self.cursor_id = Int64(cursor_id)
 1564:         self.number_returned = number_returned
 1565:         self.documents = documents
 1566: 
 1567:     def raw_response(
 1568:         self, cursor_id: Optional[int] = None, user_fields: Optional[Mapping[str, Any]] = None
 1569:     ) -> list[bytes]:
 1570:         """Check the response header from the database, without decoding BSON.
 1571: 
 1572:         Check the response for errors and unpack.
 1573: 
 1574:         Can raise CursorNotFound, NotPrimaryError, ExecutionTimeout, or
 1575:         OperationFailure.
 1576: 
 1577:         :param cursor_id: cursor_id we sent to get this response -
 1578:             used for raising an informative exception when we get cursor id not
 1579:             valid at server response.
 1580:         """
 1581:         if self.flags & 1:
 1582:             # Shouldn't get this response if we aren't doing a getMore
 1583:             if cursor_id is None:
 1584:                 raise ProtocolError("No cursor id for getMore operation")
 1585: 
 1586:             # Fake a getMore command response. OP_GET_MORE provides no
 1587:             # document.
 1588:             msg = "Cursor not found, cursor id: %d" % (cursor_id,)
 1589:             errobj = {"ok": 0, "errmsg": msg, "code": 43}
 1590:             raise CursorNotFound(msg, 43, errobj)
 1591:         elif self.flags & 2:
 1592:             error_object: dict = bson.BSON(self.documents).decode()
 1593:             # Fake the ok field if it doesn't exist.
 1594:             error_object.setdefault("ok", 0)
 1595:             if error_object["$err"].startswith(HelloCompat.LEGACY_ERROR):
 1596:                 raise NotPrimaryError(error_object["$err"], error_object)
 1597:             elif error_object.get("code") == 50:
 1598:                 default_msg = "operation exceeded time limit"
 1599:                 raise ExecutionTimeout(
 1600:                     error_object.get("$err", default_msg), error_object.get("code"), error_object
 1601:                 )
 1602:             raise OperationFailure(
 1603:                 "database error: %s" % error_object.get("$err"),
 1604:                 error_object.get("code"),
 1605:                 error_object,
 1606:             )
 1607:         if self.documents:
 1608:             return [self.documents]
 1609:         return []
 1610: 
 1611:     def unpack_response(
 1612:         self,
 1613:         cursor_id: Optional[int] = None,
 1614:         codec_options: CodecOptions = _UNICODE_REPLACE_CODEC_OPTIONS,
 1615:         user_fields: Optional[Mapping[str, Any]] = None,
 1616:         legacy_response: bool = False,
 1617:     ) -> list[dict[str, Any]]:
 1618:         """Unpack a response from the database and decode the BSON document(s).
 1619: 
 1620:         Check the response for errors and unpack, returning a dictionary
 1621:         containing the response data.
 1622: 
 1623:         Can raise CursorNotFound, NotPrimaryError, ExecutionTimeout, or
 1624:         OperationFailure.
 1625: 
 1626:         :param cursor_id: cursor_id we sent to get this response -
 1627:             used for raising an informative exception when we get cursor id not
 1628:             valid at server response
 1629:         :param codec_options: an instance of
 1630:             :class:`~bson.codec_options.CodecOptions`
 1631:         :param user_fields: Response fields that should be decoded
 1632:             using the TypeDecoders from codec_options, passed to
 1633:             bson._decode_all_selective.
 1634:         """
 1635:         self.raw_response(cursor_id)
 1636:         if legacy_response:
 1637:             return bson.decode_all(self.documents, codec_options)
 1638:         return bson._decode_all_selective(self.documents, codec_options, user_fields)
 1639: 
 1640:     def command_response(self, codec_options: CodecOptions) -> dict[str, Any]:
 1641:         """Unpack a command response."""
 1642:         docs = self.unpack_response(codec_options=codec_options)
 1643:         assert self.number_returned == 1
 1644:         return docs[0]
 1645: 
 1646:     def raw_command_response(self) -> NoReturn:
 1647:         """Return the bytes of the command response."""
 1648:         # This should never be called on _OpReply.
 1649:         raise NotImplementedError
 1650: 
 1651:     @property
 1652:     def more_to_come(self) -> bool:
 1653:         """Is the moreToCome bit set on this response?"""
 1654:         return False
 1655: 
 1656:     @classmethod
 1657:     def unpack(cls, msg: bytes) -> _OpReply:
 1658:         """Construct an _OpReply from raw bytes."""
 1659:         # PYTHON-945: ignore starting_from field.
 1660:         flags, cursor_id, _, number_returned = cls.UNPACK_FROM(msg)
 1661: 
 1662:         documents = msg[20:]
 1663:         return cls(flags, cursor_id, number_returned, documents)
 1664: 
 1665: 
 1666: class _OpMsg:
 1667:     """A MongoDB OP_MSG response message."""
 1668: 
 1669:     __slots__ = ("flags", "cursor_id", "number_returned", "payload_document")
 1670: 
 1671:     UNPACK_FROM = struct.Struct("<IBi").unpack_from
 1672:     OP_CODE = 2013
 1673: 
 1674:     # Flag bits.
 1675:     CHECKSUM_PRESENT = 1
 1676:     MORE_TO_COME = 1 << 1
 1677:     EXHAUST_ALLOWED = 1 << 16  # Only present on requests.
 1678: 
 1679:     def __init__(self, flags: int, payload_document: bytes):
 1680:         self.flags = flags
 1681:         self.payload_document = payload_document
 1682: 
 1683:     def raw_response(
 1684:         self,
 1685:         cursor_id: Optional[int] = None,
 1686:         user_fields: Optional[Mapping[str, Any]] = {},
 1687:     ) -> list[Mapping[str, Any]]:
 1688:         """
 1689:         cursor_id is ignored
 1690:         user_fields is used to determine which fields must not be decoded
 1691:         """
 1692:         inflated_response = _decode_selective(
 1693:             RawBSONDocument(self.payload_document), user_fields, _RAW_ARRAY_BSON_OPTIONS
 1694:         )
 1695:         return [inflated_response]
 1696: 
 1697:     def unpack_response(
 1698:         self,
 1699:         cursor_id: Optional[int] = None,
 1700:         codec_options: CodecOptions = _UNICODE_REPLACE_CODEC_OPTIONS,
 1701:         user_fields: Optional[Mapping[str, Any]] = None,
 1702:         legacy_response: bool = False,
 1703:     ) -> list[dict[str, Any]]:
 1704:         """Unpack a OP_MSG command response.
 1705: 
 1706:         :param cursor_id: Ignored, for compatibility with _OpReply.
 1707:         :param codec_options: an instance of
 1708:             :class:`~bson.codec_options.CodecOptions`
 1709:         :param user_fields: Response fields that should be decoded
 1710:             using the TypeDecoders from codec_options, passed to
 1711:             bson._decode_all_selective.
 1712:         """
 1713:         # If _OpMsg is in-use, this cannot be a legacy response.
 1714:         assert not legacy_response
 1715:         return bson._decode_all_selective(self.payload_document, codec_options, user_fields)
 1716: 
 1717:     def command_response(self, codec_options: CodecOptions) -> dict[str, Any]:
 1718:         """Unpack a command response."""
 1719:         return self.unpack_response(codec_options=codec_options)[0]
 1720: 
 1721:     def raw_command_response(self) -> bytes:
 1722:         """Return the bytes of the command response."""
 1723:         return self.payload_document
 1724: 
 1725:     @property
 1726:     def more_to_come(self) -> bool:
 1727:         """Is the moreToCome bit set on this response?"""
 1728:         return bool(self.flags & self.MORE_TO_COME)
 1729: 
 1730:     @classmethod
 1731:     def unpack(cls, msg: bytes) -> _OpMsg:
 1732:         """Construct an _OpMsg from raw bytes."""
 1733:         flags, first_payload_type, first_payload_size = cls.UNPACK_FROM(msg)
 1734:         if flags != 0:
 1735:             if flags & cls.CHECKSUM_PRESENT:
 1736:                 raise ProtocolError(f"Unsupported OP_MSG flag checksumPresent: 0x{flags:x}")
 1737: 
 1738:             if flags ^ cls.MORE_TO_COME:
 1739:                 raise ProtocolError(f"Unsupported OP_MSG flags: 0x{flags:x}")
 1740:         if first_payload_type != 0:
 1741:             raise ProtocolError(f"Unsupported OP_MSG payload type: 0x{first_payload_type:x}")
 1742: 
 1743:         if len(msg) != first_payload_size + 5:
 1744:             raise ProtocolError("Unsupported OP_MSG reply: >1 section")
 1745: 
 1746:         payload_document = msg[5:]
 1747:         return cls(flags, payload_document)
 1748: 
 1749: 
 1750: _UNPACK_REPLY: dict[int, Callable[[bytes], Union[_OpReply, _OpMsg]]] = {
 1751:     _OpReply.OP_CODE: _OpReply.unpack,
 1752:     _OpMsg.OP_CODE: _OpMsg.unpack,
 1753: }
