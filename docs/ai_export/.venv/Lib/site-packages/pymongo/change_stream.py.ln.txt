    1: # Copyright 2017 MongoDB, Inc.
    2: #
    3: # Licensed under the Apache License, Version 2.0 (the "License"); you
    4: # may not use this file except in compliance with the License.  You
    5: # may obtain a copy of the License at
    6: #
    7: # http://www.apache.org/licenses/LICENSE-2.0
    8: #
    9: # Unless required by applicable law or agreed to in writing, software
   10: # distributed under the License is distributed on an "AS IS" BASIS,
   11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
   12: # implied.  See the License for the specific language governing
   13: # permissions and limitations under the License.
   14: 
   15: """Watch changes on a collection, a database, or the entire cluster."""
   16: from __future__ import annotations
   17: 
   18: import copy
   19: from typing import TYPE_CHECKING, Any, Generic, Mapping, Optional, Type, Union
   20: 
   21: from bson import CodecOptions, _bson_to_dict
   22: from bson.raw_bson import RawBSONDocument
   23: from bson.timestamp import Timestamp
   24: from pymongo import _csot, common
   25: from pymongo.aggregation import (
   26:     _AggregationCommand,
   27:     _CollectionAggregationCommand,
   28:     _DatabaseAggregationCommand,
   29: )
   30: from pymongo.collation import validate_collation_or_none
   31: from pymongo.command_cursor import CommandCursor
   32: from pymongo.errors import (
   33:     ConnectionFailure,
   34:     CursorNotFound,
   35:     InvalidOperation,
   36:     OperationFailure,
   37:     PyMongoError,
   38: )
   39: from pymongo.operations import _Op
   40: from pymongo.typings import _CollationIn, _DocumentType, _Pipeline
   41: 
   42: # The change streams spec considers the following server errors from the
   43: # getMore command non-resumable. All other getMore errors are resumable.
   44: _RESUMABLE_GETMORE_ERRORS = frozenset(
   45:     [
   46:         6,  # HostUnreachable
   47:         7,  # HostNotFound
   48:         89,  # NetworkTimeout
   49:         91,  # ShutdownInProgress
   50:         189,  # PrimarySteppedDown
   51:         262,  # ExceededTimeLimit
   52:         9001,  # SocketException
   53:         10107,  # NotWritablePrimary
   54:         11600,  # InterruptedAtShutdown
   55:         11602,  # InterruptedDueToReplStateChange
   56:         13435,  # NotPrimaryNoSecondaryOk
   57:         13436,  # NotPrimaryOrSecondary
   58:         63,  # StaleShardVersion
   59:         150,  # StaleEpoch
   60:         13388,  # StaleConfig
   61:         234,  # RetryChangeStream
   62:         133,  # FailedToSatisfyReadPreference
   63:     ]
   64: )
   65: 
   66: 
   67: if TYPE_CHECKING:
   68:     from pymongo.client_session import ClientSession
   69:     from pymongo.collection import Collection
   70:     from pymongo.database import Database
   71:     from pymongo.mongo_client import MongoClient
   72:     from pymongo.pool import Connection
   73: 
   74: 
   75: def _resumable(exc: PyMongoError) -> bool:
   76:     """Return True if given a resumable change stream error."""
   77:     if isinstance(exc, (ConnectionFailure, CursorNotFound)):
   78:         return True
   79:     if isinstance(exc, OperationFailure):
   80:         if exc._max_wire_version is None:
   81:             return False
   82:         return (
   83:             exc._max_wire_version >= 9 and exc.has_error_label("ResumableChangeStreamError")
   84:         ) or (exc._max_wire_version < 9 and exc.code in _RESUMABLE_GETMORE_ERRORS)
   85:     return False
   86: 
   87: 
   88: class ChangeStream(Generic[_DocumentType]):
   89:     """The internal abstract base class for change stream cursors.
   90: 
   91:     Should not be called directly by application developers. Use
   92:     :meth:`pymongo.collection.Collection.watch`,
   93:     :meth:`pymongo.database.Database.watch`, or
   94:     :meth:`pymongo.mongo_client.MongoClient.watch` instead.
   95: 
   96:     .. versionadded:: 3.6
   97:     .. seealso:: The MongoDB documentation on `changeStreams <https://mongodb.com/docs/manual/changeStreams/>`_.
   98:     """
   99: 
  100:     def __init__(
  101:         self,
  102:         target: Union[
  103:             MongoClient[_DocumentType], Database[_DocumentType], Collection[_DocumentType]
  104:         ],
  105:         pipeline: Optional[_Pipeline],
  106:         full_document: Optional[str],
  107:         resume_after: Optional[Mapping[str, Any]],
  108:         max_await_time_ms: Optional[int],
  109:         batch_size: Optional[int],
  110:         collation: Optional[_CollationIn],
  111:         start_at_operation_time: Optional[Timestamp],
  112:         session: Optional[ClientSession],
  113:         start_after: Optional[Mapping[str, Any]],
  114:         comment: Optional[Any] = None,
  115:         full_document_before_change: Optional[str] = None,
  116:         show_expanded_events: Optional[bool] = None,
  117:     ) -> None:
  118:         if pipeline is None:
  119:             pipeline = []
  120:         pipeline = common.validate_list("pipeline", pipeline)
  121:         common.validate_string_or_none("full_document", full_document)
  122:         validate_collation_or_none(collation)
  123:         common.validate_non_negative_integer_or_none("batchSize", batch_size)
  124: 
  125:         self._decode_custom = False
  126:         self._orig_codec_options: CodecOptions[_DocumentType] = target.codec_options
  127:         if target.codec_options.type_registry._decoder_map:
  128:             self._decode_custom = True
  129:             # Keep the type registry so that we support encoding custom types
  130:             # in the pipeline.
  131:             self._target = target.with_options(  # type: ignore
  132:                 codec_options=target.codec_options.with_options(document_class=RawBSONDocument)
  133:             )
  134:         else:
  135:             self._target = target
  136: 
  137:         self._pipeline = copy.deepcopy(pipeline)
  138:         self._full_document = full_document
  139:         self._full_document_before_change = full_document_before_change
  140:         self._uses_start_after = start_after is not None
  141:         self._uses_resume_after = resume_after is not None
  142:         self._resume_token = copy.deepcopy(start_after or resume_after)
  143:         self._max_await_time_ms = max_await_time_ms
  144:         self._batch_size = batch_size
  145:         self._collation = collation
  146:         self._start_at_operation_time = start_at_operation_time
  147:         self._session = session
  148:         self._comment = comment
  149:         self._closed = False
  150:         self._timeout = self._target._timeout
  151:         self._show_expanded_events = show_expanded_events
  152:         # Initialize cursor.
  153:         self._cursor = self._create_cursor()
  154: 
  155:     @property
  156:     def _aggregation_command_class(self) -> Type[_AggregationCommand]:
  157:         """The aggregation command class to be used."""
  158:         raise NotImplementedError
  159: 
  160:     @property
  161:     def _client(self) -> MongoClient:
  162:         """The client against which the aggregation commands for
  163:         this ChangeStream will be run.
  164:         """
  165:         raise NotImplementedError
  166: 
  167:     def _change_stream_options(self) -> dict[str, Any]:
  168:         """Return the options dict for the $changeStream pipeline stage."""
  169:         options: dict[str, Any] = {}
  170:         if self._full_document is not None:
  171:             options["fullDocument"] = self._full_document
  172: 
  173:         if self._full_document_before_change is not None:
  174:             options["fullDocumentBeforeChange"] = self._full_document_before_change
  175: 
  176:         resume_token = self.resume_token
  177:         if resume_token is not None:
  178:             if self._uses_start_after:
  179:                 options["startAfter"] = resume_token
  180:             else:
  181:                 options["resumeAfter"] = resume_token
  182:         elif self._start_at_operation_time is not None:
  183:             options["startAtOperationTime"] = self._start_at_operation_time
  184: 
  185:         if self._show_expanded_events:
  186:             options["showExpandedEvents"] = self._show_expanded_events
  187: 
  188:         return options
  189: 
  190:     def _command_options(self) -> dict[str, Any]:
  191:         """Return the options dict for the aggregation command."""
  192:         options = {}
  193:         if self._max_await_time_ms is not None:
  194:             options["maxAwaitTimeMS"] = self._max_await_time_ms
  195:         if self._batch_size is not None:
  196:             options["batchSize"] = self._batch_size
  197:         return options
  198: 
  199:     def _aggregation_pipeline(self) -> list[dict[str, Any]]:
  200:         """Return the full aggregation pipeline for this ChangeStream."""
  201:         options = self._change_stream_options()
  202:         full_pipeline: list = [{"$changeStream": options}]
  203:         full_pipeline.extend(self._pipeline)
  204:         return full_pipeline
  205: 
  206:     def _process_result(self, result: Mapping[str, Any], conn: Connection) -> None:
  207:         """Callback that caches the postBatchResumeToken or
  208:         startAtOperationTime from a changeStream aggregate command response
  209:         containing an empty batch of change documents.
  210: 
  211:         This is implemented as a callback because we need access to the wire
  212:         version in order to determine whether to cache this value.
  213:         """
  214:         if not result["cursor"]["firstBatch"]:
  215:             if "postBatchResumeToken" in result["cursor"]:
  216:                 self._resume_token = result["cursor"]["postBatchResumeToken"]
  217:             elif (
  218:                 self._start_at_operation_time is None
  219:                 and self._uses_resume_after is False
  220:                 and self._uses_start_after is False
  221:                 and conn.max_wire_version >= 7
  222:             ):
  223:                 self._start_at_operation_time = result.get("operationTime")
  224:                 # PYTHON-2181: informative error on missing operationTime.
  225:                 if self._start_at_operation_time is None:
  226:                     raise OperationFailure(
  227:                         "Expected field 'operationTime' missing from command "
  228:                         f"response : {result!r}"
  229:                     )
  230: 
  231:     def _run_aggregation_cmd(
  232:         self, session: Optional[ClientSession], explicit_session: bool
  233:     ) -> CommandCursor:
  234:         """Run the full aggregation pipeline for this ChangeStream and return
  235:         the corresponding CommandCursor.
  236:         """
  237:         cmd = self._aggregation_command_class(
  238:             self._target,
  239:             CommandCursor,
  240:             self._aggregation_pipeline(),
  241:             self._command_options(),
  242:             explicit_session,
  243:             result_processor=self._process_result,
  244:             comment=self._comment,
  245:         )
  246:         return self._client._retryable_read(
  247:             cmd.get_cursor,
  248:             self._target._read_preference_for(session),
  249:             session,
  250:             operation=_Op.AGGREGATE,
  251:         )
  252: 
  253:     def _create_cursor(self) -> CommandCursor:
  254:         with self._client._tmp_session(self._session, close=False) as s:
  255:             return self._run_aggregation_cmd(session=s, explicit_session=self._session is not None)
  256: 
  257:     def _resume(self) -> None:
  258:         """Reestablish this change stream after a resumable error."""
  259:         try:
  260:             self._cursor.close()
  261:         except PyMongoError:
  262:             pass
  263:         self._cursor = self._create_cursor()
  264: 
  265:     def close(self) -> None:
  266:         """Close this ChangeStream."""
  267:         self._closed = True
  268:         self._cursor.close()
  269: 
  270:     def __iter__(self) -> ChangeStream[_DocumentType]:
  271:         return self
  272: 
  273:     @property
  274:     def resume_token(self) -> Optional[Mapping[str, Any]]:
  275:         """The cached resume token that will be used to resume after the most
  276:         recently returned change.
  277: 
  278:         .. versionadded:: 3.9
  279:         """
  280:         return copy.deepcopy(self._resume_token)
  281: 
  282:     @_csot.apply
  283:     def next(self) -> _DocumentType:
  284:         """Advance the cursor.
  285: 
  286:         This method blocks until the next change document is returned or an
  287:         unrecoverable error is raised. This method is used when iterating over
  288:         all changes in the cursor. For example::
  289: 
  290:             try:
  291:                 resume_token = None
  292:                 pipeline = [{'$match': {'operationType': 'insert'}}]
  293:                 with db.collection.watch(pipeline) as stream:
  294:                     for insert_change in stream:
  295:                         print(insert_change)
  296:                         resume_token = stream.resume_token
  297:             except pymongo.errors.PyMongoError:
  298:                 # The ChangeStream encountered an unrecoverable error or the
  299:                 # resume attempt failed to recreate the cursor.
  300:                 if resume_token is None:
  301:                     # There is no usable resume token because there was a
  302:                     # failure during ChangeStream initialization.
  303:                     logging.error('...')
  304:                 else:
  305:                     # Use the interrupted ChangeStream's resume token to create
  306:                     # a new ChangeStream. The new stream will continue from the
  307:                     # last seen insert change without missing any events.
  308:                     with db.collection.watch(
  309:                             pipeline, resume_after=resume_token) as stream:
  310:                         for insert_change in stream:
  311:                             print(insert_change)
  312: 
  313:         Raises :exc:`StopIteration` if this ChangeStream is closed.
  314:         """
  315:         while self.alive:
  316:             doc = self.try_next()
  317:             if doc is not None:
  318:                 return doc
  319: 
  320:         raise StopIteration
  321: 
  322:     __next__ = next
  323: 
  324:     @property
  325:     def alive(self) -> bool:
  326:         """Does this cursor have the potential to return more data?
  327: 
  328:         .. note:: Even if :attr:`alive` is ``True``, :meth:`next` can raise
  329:             :exc:`StopIteration` and :meth:`try_next` can return ``None``.
  330: 
  331:         .. versionadded:: 3.8
  332:         """
  333:         return not self._closed
  334: 
  335:     @_csot.apply
  336:     def try_next(self) -> Optional[_DocumentType]:
  337:         """Advance the cursor without blocking indefinitely.
  338: 
  339:         This method returns the next change document without waiting
  340:         indefinitely for the next change. For example::
  341: 
  342:             with db.collection.watch() as stream:
  343:                 while stream.alive:
  344:                     change = stream.try_next()
  345:                     # Note that the ChangeStream's resume token may be updated
  346:                     # even when no changes are returned.
  347:                     print("Current resume token: %r" % (stream.resume_token,))
  348:                     if change is not None:
  349:                         print("Change document: %r" % (change,))
  350:                         continue
  351:                     # We end up here when there are no recent changes.
  352:                     # Sleep for a while before trying again to avoid flooding
  353:                     # the server with getMore requests when no changes are
  354:                     # available.
  355:                     time.sleep(10)
  356: 
  357:         If no change document is cached locally then this method runs a single
  358:         getMore command. If the getMore yields any documents, the next
  359:         document is returned, otherwise, if the getMore returns no documents
  360:         (because there have been no changes) then ``None`` is returned.
  361: 
  362:         :return: The next change document or ``None`` when no document is available
  363:           after running a single getMore or when the cursor is closed.
  364: 
  365:         .. versionadded:: 3.8
  366:         """
  367:         if not self._closed and not self._cursor.alive:
  368:             self._resume()
  369: 
  370:         # Attempt to get the next change with at most one getMore and at most
  371:         # one resume attempt.
  372:         try:
  373:             try:
  374:                 change = self._cursor._try_next(True)
  375:             except PyMongoError as exc:
  376:                 if not _resumable(exc):
  377:                     raise
  378:                 self._resume()
  379:                 change = self._cursor._try_next(False)
  380:         except PyMongoError as exc:
  381:             # Close the stream after a fatal error.
  382:             if not _resumable(exc) and not exc.timeout:
  383:                 self.close()
  384:             raise
  385:         except Exception:
  386:             self.close()
  387:             raise
  388: 
  389:         # Check if the cursor was invalidated.
  390:         if not self._cursor.alive:
  391:             self._closed = True
  392: 
  393:         # If no changes are available.
  394:         if change is None:
  395:             # We have either iterated over all documents in the cursor,
  396:             # OR the most-recently returned batch is empty. In either case,
  397:             # update the cached resume token with the postBatchResumeToken if
  398:             # one was returned. We also clear the startAtOperationTime.
  399:             if self._cursor._post_batch_resume_token is not None:
  400:                 self._resume_token = self._cursor._post_batch_resume_token
  401:                 self._start_at_operation_time = None
  402:             return change
  403: 
  404:         # Else, changes are available.
  405:         try:
  406:             resume_token = change["_id"]
  407:         except KeyError:
  408:             self.close()
  409:             raise InvalidOperation(
  410:                 "Cannot provide resume functionality when the resume token is missing."
  411:             ) from None
  412: 
  413:         # If this is the last change document from the current batch, cache the
  414:         # postBatchResumeToken.
  415:         if not self._cursor._has_next() and self._cursor._post_batch_resume_token:
  416:             resume_token = self._cursor._post_batch_resume_token
  417: 
  418:         # Hereafter, don't use startAfter; instead use resumeAfter.
  419:         self._uses_start_after = False
  420:         self._uses_resume_after = True
  421: 
  422:         # Cache the resume token and clear startAtOperationTime.
  423:         self._resume_token = resume_token
  424:         self._start_at_operation_time = None
  425: 
  426:         if self._decode_custom:
  427:             return _bson_to_dict(change.raw, self._orig_codec_options)
  428:         return change
  429: 
  430:     def __enter__(self) -> ChangeStream[_DocumentType]:
  431:         return self
  432: 
  433:     def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
  434:         self.close()
  435: 
  436: 
  437: class CollectionChangeStream(ChangeStream[_DocumentType]):
  438:     """A change stream that watches changes on a single collection.
  439: 
  440:     Should not be called directly by application developers. Use
  441:     helper method :meth:`pymongo.collection.Collection.watch` instead.
  442: 
  443:     .. versionadded:: 3.7
  444:     """
  445: 
  446:     _target: Collection[_DocumentType]
  447: 
  448:     @property
  449:     def _aggregation_command_class(self) -> Type[_CollectionAggregationCommand]:
  450:         return _CollectionAggregationCommand
  451: 
  452:     @property
  453:     def _client(self) -> MongoClient[_DocumentType]:
  454:         return self._target.database.client
  455: 
  456: 
  457: class DatabaseChangeStream(ChangeStream[_DocumentType]):
  458:     """A change stream that watches changes on all collections in a database.
  459: 
  460:     Should not be called directly by application developers. Use
  461:     helper method :meth:`pymongo.database.Database.watch` instead.
  462: 
  463:     .. versionadded:: 3.7
  464:     """
  465: 
  466:     _target: Database[_DocumentType]
  467: 
  468:     @property
  469:     def _aggregation_command_class(self) -> Type[_DatabaseAggregationCommand]:
  470:         return _DatabaseAggregationCommand
  471: 
  472:     @property
  473:     def _client(self) -> MongoClient[_DocumentType]:
  474:         return self._target.client
  475: 
  476: 
  477: class ClusterChangeStream(DatabaseChangeStream[_DocumentType]):
  478:     """A change stream that watches changes on all collections in the cluster.
  479: 
  480:     Should not be called directly by application developers. Use
  481:     helper method :meth:`pymongo.mongo_client.MongoClient.watch` instead.
  482: 
  483:     .. versionadded:: 3.7
  484:     """
  485: 
  486:     def _change_stream_options(self) -> dict[str, Any]:
  487:         options = super()._change_stream_options()
  488:         options["allChangesForCluster"] = True
  489:         return options
