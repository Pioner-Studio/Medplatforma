    1: from __future__ import annotations
    2: 
    3: import functools
    4: import itertools
    5: import logging
    6: import os
    7: import posixpath
    8: import re
    9: import urllib.parse
   10: from collections.abc import Mapping
   11: from dataclasses import dataclass
   12: from typing import (
   13:     TYPE_CHECKING,
   14:     Any,
   15:     NamedTuple,
   16: )
   17: 
   18: from pip._internal.utils.deprecation import deprecated
   19: from pip._internal.utils.filetypes import WHEEL_EXTENSION
   20: from pip._internal.utils.hashes import Hashes
   21: from pip._internal.utils.misc import (
   22:     pairwise,
   23:     redact_auth_from_url,
   24:     split_auth_from_netloc,
   25:     splitext,
   26: )
   27: from pip._internal.utils.urls import path_to_url, url_to_path
   28: 
   29: if TYPE_CHECKING:
   30:     from pip._internal.index.collector import IndexContent
   31: 
   32: logger = logging.getLogger(__name__)
   33: 
   34: 
   35: # Order matters, earlier hashes have a precedence over later hashes for what
   36: # we will pick to use.
   37: _SUPPORTED_HASHES = ("sha512", "sha384", "sha256", "sha224", "sha1", "md5")
   38: 
   39: 
   40: @dataclass(frozen=True)
   41: class LinkHash:
   42:     """Links to content may have embedded hash values. This class parses those.
   43: 
   44:     `name` must be any member of `_SUPPORTED_HASHES`.
   45: 
   46:     This class can be converted to and from `ArchiveInfo`. While ArchiveInfo intends to
   47:     be JSON-serializable to conform to PEP 610, this class contains the logic for
   48:     parsing a hash name and value for correctness, and then checking whether that hash
   49:     conforms to a schema with `.is_hash_allowed()`."""
   50: 
   51:     name: str
   52:     value: str
   53: 
   54:     _hash_url_fragment_re = re.compile(
   55:         # NB: we do not validate that the second group (.*) is a valid hex
   56:         # digest. Instead, we simply keep that string in this class, and then check it
   57:         # against Hashes when hash-checking is needed. This is easier to debug than
   58:         # proactively discarding an invalid hex digest, as we handle incorrect hashes
   59:         # and malformed hashes in the same place.
   60:         r"[#&]({choices})=([^&]*)".format(
   61:             choices="|".join(re.escape(hash_name) for hash_name in _SUPPORTED_HASHES)
   62:         ),
   63:     )
   64: 
   65:     def __post_init__(self) -> None:
   66:         assert self.name in _SUPPORTED_HASHES
   67: 
   68:     @classmethod
   69:     @functools.cache
   70:     def find_hash_url_fragment(cls, url: str) -> LinkHash | None:
   71:         """Search a string for a checksum algorithm name and encoded output value."""
   72:         match = cls._hash_url_fragment_re.search(url)
   73:         if match is None:
   74:             return None
   75:         name, value = match.groups()
   76:         return cls(name=name, value=value)
   77: 
   78:     def as_dict(self) -> dict[str, str]:
   79:         return {self.name: self.value}
   80: 
   81:     def as_hashes(self) -> Hashes:
   82:         """Return a Hashes instance which checks only for the current hash."""
   83:         return Hashes({self.name: [self.value]})
   84: 
   85:     def is_hash_allowed(self, hashes: Hashes | None) -> bool:
   86:         """
   87:         Return True if the current hash is allowed by `hashes`.
   88:         """
   89:         if hashes is None:
   90:             return False
   91:         return hashes.is_hash_allowed(self.name, hex_digest=self.value)
   92: 
   93: 
   94: @dataclass(frozen=True)
   95: class MetadataFile:
   96:     """Information about a core metadata file associated with a distribution."""
   97: 
   98:     hashes: dict[str, str] | None
   99: 
  100:     def __post_init__(self) -> None:
  101:         if self.hashes is not None:
  102:             assert all(name in _SUPPORTED_HASHES for name in self.hashes)
  103: 
  104: 
  105: def supported_hashes(hashes: dict[str, str] | None) -> dict[str, str] | None:
  106:     # Remove any unsupported hash types from the mapping. If this leaves no
  107:     # supported hashes, return None
  108:     if hashes is None:
  109:         return None
  110:     hashes = {n: v for n, v in hashes.items() if n in _SUPPORTED_HASHES}
  111:     if not hashes:
  112:         return None
  113:     return hashes
  114: 
  115: 
  116: def _clean_url_path_part(part: str) -> str:
  117:     """
  118:     Clean a "part" of a URL path (i.e. after splitting on "@" characters).
  119:     """
  120:     # We unquote prior to quoting to make sure nothing is double quoted.
  121:     return urllib.parse.quote(urllib.parse.unquote(part))
  122: 
  123: 
  124: def _clean_file_url_path(part: str) -> str:
  125:     """
  126:     Clean the first part of a URL path that corresponds to a local
  127:     filesystem path (i.e. the first part after splitting on "@" characters).
  128:     """
  129:     # We unquote prior to quoting to make sure nothing is double quoted.
  130:     # Also, on Windows the path part might contain a drive letter which
  131:     # should not be quoted. On Linux where drive letters do not
  132:     # exist, the colon should be quoted. We rely on urllib.request
  133:     # to do the right thing here.
  134:     ret = urllib.request.pathname2url(urllib.request.url2pathname(part))
  135:     if ret.startswith("///"):
  136:         # Remove any URL authority section, leaving only the URL path.
  137:         ret = ret.removeprefix("//")
  138:     return ret
  139: 
  140: 
  141: # percent-encoded:                   /
  142: _reserved_chars_re = re.compile("(@|%2F)", re.IGNORECASE)
  143: 
  144: 
  145: def _clean_url_path(path: str, is_local_path: bool) -> str:
  146:     """
  147:     Clean the path portion of a URL.
  148:     """
  149:     if is_local_path:
  150:         clean_func = _clean_file_url_path
  151:     else:
  152:         clean_func = _clean_url_path_part
  153: 
  154:     # Split on the reserved characters prior to cleaning so that
  155:     # revision strings in VCS URLs are properly preserved.
  156:     parts = _reserved_chars_re.split(path)
  157: 
  158:     cleaned_parts = []
  159:     for to_clean, reserved in pairwise(itertools.chain(parts, [""])):
  160:         cleaned_parts.append(clean_func(to_clean))
  161:         # Normalize %xx escapes (e.g. %2f -> %2F)
  162:         cleaned_parts.append(reserved.upper())
  163: 
  164:     return "".join(cleaned_parts)
  165: 
  166: 
  167: def _ensure_quoted_url(url: str) -> str:
  168:     """
  169:     Make sure a link is fully quoted.
  170:     For example, if ' ' occurs in the URL, it will be replaced with "%20",
  171:     and without double-quoting other characters.
  172:     """
  173:     # Split the URL into parts according to the general structure
  174:     # `scheme://netloc/path?query#fragment`.
  175:     result = urllib.parse.urlsplit(url)
  176:     # If the netloc is empty, then the URL refers to a local filesystem path.
  177:     is_local_path = not result.netloc
  178:     path = _clean_url_path(result.path, is_local_path=is_local_path)
  179:     # Temporarily replace scheme with file to ensure the URL generated by
  180:     # urlunsplit() contains an empty netloc (file://) as per RFC 1738.
  181:     ret = urllib.parse.urlunsplit(result._replace(scheme="file", path=path))
  182:     ret = result.scheme + ret[4:]  # Restore original scheme.
  183:     return ret
  184: 
  185: 
  186: def _absolute_link_url(base_url: str, url: str) -> str:
  187:     """
  188:     A faster implementation of urllib.parse.urljoin with a shortcut
  189:     for absolute http/https URLs.
  190:     """
  191:     if url.startswith(("https://", "http://")):
  192:         return url
  193:     else:
  194:         return urllib.parse.urljoin(base_url, url)
  195: 
  196: 
  197: @functools.total_ordering
  198: class Link:
  199:     """Represents a parsed link from a Package Index's simple URL"""
  200: 
  201:     __slots__ = [
  202:         "_parsed_url",
  203:         "_url",
  204:         "_path",
  205:         "_hashes",
  206:         "comes_from",
  207:         "requires_python",
  208:         "yanked_reason",
  209:         "metadata_file_data",
  210:         "cache_link_parsing",
  211:         "egg_fragment",
  212:     ]
  213: 
  214:     def __init__(
  215:         self,
  216:         url: str,
  217:         comes_from: str | IndexContent | None = None,
  218:         requires_python: str | None = None,
  219:         yanked_reason: str | None = None,
  220:         metadata_file_data: MetadataFile | None = None,
  221:         cache_link_parsing: bool = True,
  222:         hashes: Mapping[str, str] | None = None,
  223:     ) -> None:
  224:         """
  225:         :param url: url of the resource pointed to (href of the link)
  226:         :param comes_from: instance of IndexContent where the link was found,
  227:             or string.
  228:         :param requires_python: String containing the `Requires-Python`
  229:             metadata field, specified in PEP 345. This may be specified by
  230:             a data-requires-python attribute in the HTML link tag, as
  231:             described in PEP 503.
  232:         :param yanked_reason: the reason the file has been yanked, if the
  233:             file has been yanked, or None if the file hasn't been yanked.
  234:             This is the value of the "data-yanked" attribute, if present, in
  235:             a simple repository HTML link. If the file has been yanked but
  236:             no reason was provided, this should be the empty string. See
  237:             PEP 592 for more information and the specification.
  238:         :param metadata_file_data: the metadata attached to the file, or None if
  239:             no such metadata is provided. This argument, if not None, indicates
  240:             that a separate metadata file exists, and also optionally supplies
  241:             hashes for that file.
  242:         :param cache_link_parsing: A flag that is used elsewhere to determine
  243:             whether resources retrieved from this link should be cached. PyPI
  244:             URLs should generally have this set to False, for example.
  245:         :param hashes: A mapping of hash names to digests to allow us to
  246:             determine the validity of a download.
  247:         """
  248: 
  249:         # The comes_from, requires_python, and metadata_file_data arguments are
  250:         # only used by classmethods of this class, and are not used in client
  251:         # code directly.
  252: 
  253:         # url can be a UNC windows share
  254:         if url.startswith("\\\\"):
  255:             url = path_to_url(url)
  256: 
  257:         self._parsed_url = urllib.parse.urlsplit(url)
  258:         # Store the url as a private attribute to prevent accidentally
  259:         # trying to set a new value.
  260:         self._url = url
  261:         # The .path property is hot, so calculate its value ahead of time.
  262:         self._path = urllib.parse.unquote(self._parsed_url.path)
  263: 
  264:         link_hash = LinkHash.find_hash_url_fragment(url)
  265:         hashes_from_link = {} if link_hash is None else link_hash.as_dict()
  266:         if hashes is None:
  267:             self._hashes = hashes_from_link
  268:         else:
  269:             self._hashes = {**hashes, **hashes_from_link}
  270: 
  271:         self.comes_from = comes_from
  272:         self.requires_python = requires_python if requires_python else None
  273:         self.yanked_reason = yanked_reason
  274:         self.metadata_file_data = metadata_file_data
  275: 
  276:         self.cache_link_parsing = cache_link_parsing
  277:         self.egg_fragment = self._egg_fragment()
  278: 
  279:     @classmethod
  280:     def from_json(
  281:         cls,
  282:         file_data: dict[str, Any],
  283:         page_url: str,
  284:     ) -> Link | None:
  285:         """
  286:         Convert an pypi json document from a simple repository page into a Link.
  287:         """
  288:         file_url = file_data.get("url")
  289:         if file_url is None:
  290:             return None
  291: 
  292:         url = _ensure_quoted_url(_absolute_link_url(page_url, file_url))
  293:         pyrequire = file_data.get("requires-python")
  294:         yanked_reason = file_data.get("yanked")
  295:         hashes = file_data.get("hashes", {})
  296: 
  297:         # PEP 714: Indexes must use the name core-metadata, but
  298:         # clients should support the old name as a fallback for compatibility.
  299:         metadata_info = file_data.get("core-metadata")
  300:         if metadata_info is None:
  301:             metadata_info = file_data.get("dist-info-metadata")
  302: 
  303:         # The metadata info value may be a boolean, or a dict of hashes.
  304:         if isinstance(metadata_info, dict):
  305:             # The file exists, and hashes have been supplied
  306:             metadata_file_data = MetadataFile(supported_hashes(metadata_info))
  307:         elif metadata_info:
  308:             # The file exists, but there are no hashes
  309:             metadata_file_data = MetadataFile(None)
  310:         else:
  311:             # False or not present: the file does not exist
  312:             metadata_file_data = None
  313: 
  314:         # The Link.yanked_reason expects an empty string instead of a boolean.
  315:         if yanked_reason and not isinstance(yanked_reason, str):
  316:             yanked_reason = ""
  317:         # The Link.yanked_reason expects None instead of False.
  318:         elif not yanked_reason:
  319:             yanked_reason = None
  320: 
  321:         return cls(
  322:             url,
  323:             comes_from=page_url,
  324:             requires_python=pyrequire,
  325:             yanked_reason=yanked_reason,
  326:             hashes=hashes,
  327:             metadata_file_data=metadata_file_data,
  328:         )
  329: 
  330:     @classmethod
  331:     def from_element(
  332:         cls,
  333:         anchor_attribs: dict[str, str | None],
  334:         page_url: str,
  335:         base_url: str,
  336:     ) -> Link | None:
  337:         """
  338:         Convert an anchor element's attributes in a simple repository page to a Link.
  339:         """
  340:         href = anchor_attribs.get("href")
  341:         if not href:
  342:             return None
  343: 
  344:         url = _ensure_quoted_url(_absolute_link_url(base_url, href))
  345:         pyrequire = anchor_attribs.get("data-requires-python")
  346:         yanked_reason = anchor_attribs.get("data-yanked")
  347: 
  348:         # PEP 714: Indexes must use the name data-core-metadata, but
  349:         # clients should support the old name as a fallback for compatibility.
  350:         metadata_info = anchor_attribs.get("data-core-metadata")
  351:         if metadata_info is None:
  352:             metadata_info = anchor_attribs.get("data-dist-info-metadata")
  353:         # The metadata info value may be the string "true", or a string of
  354:         # the form "hashname=hashval"
  355:         if metadata_info == "true":
  356:             # The file exists, but there are no hashes
  357:             metadata_file_data = MetadataFile(None)
  358:         elif metadata_info is None:
  359:             # The file does not exist
  360:             metadata_file_data = None
  361:         else:
  362:             # The file exists, and hashes have been supplied
  363:             hashname, sep, hashval = metadata_info.partition("=")
  364:             if sep == "=":
  365:                 metadata_file_data = MetadataFile(supported_hashes({hashname: hashval}))
  366:             else:
  367:                 # Error - data is wrong. Treat as no hashes supplied.
  368:                 logger.debug(
  369:                     "Index returned invalid data-dist-info-metadata value: %s",
  370:                     metadata_info,
  371:                 )
  372:                 metadata_file_data = MetadataFile(None)
  373: 
  374:         return cls(
  375:             url,
  376:             comes_from=page_url,
  377:             requires_python=pyrequire,
  378:             yanked_reason=yanked_reason,
  379:             metadata_file_data=metadata_file_data,
  380:         )
  381: 
  382:     def __str__(self) -> str:
  383:         if self.requires_python:
  384:             rp = f" (requires-python:{self.requires_python})"
  385:         else:
  386:             rp = ""
  387:         if self.comes_from:
  388:             return f"{self.redacted_url} (from {self.comes_from}){rp}"
  389:         else:
  390:             return self.redacted_url
  391: 
  392:     def __repr__(self) -> str:
  393:         return f"<Link {self}>"
  394: 
  395:     def __hash__(self) -> int:
  396:         return hash(self.url)
  397: 
  398:     def __eq__(self, other: Any) -> bool:
  399:         if not isinstance(other, Link):
  400:             return NotImplemented
  401:         return self.url == other.url
  402: 
  403:     def __lt__(self, other: Any) -> bool:
  404:         if not isinstance(other, Link):
  405:             return NotImplemented
  406:         return self.url < other.url
  407: 
  408:     @property
  409:     def url(self) -> str:
  410:         return self._url
  411: 
  412:     @property
  413:     def redacted_url(self) -> str:
  414:         return redact_auth_from_url(self.url)
  415: 
  416:     @property
  417:     def filename(self) -> str:
  418:         path = self.path.rstrip("/")
  419:         name = posixpath.basename(path)
  420:         if not name:
  421:             # Make sure we don't leak auth information if the netloc
  422:             # includes a username and password.
  423:             netloc, user_pass = split_auth_from_netloc(self.netloc)
  424:             return netloc
  425: 
  426:         name = urllib.parse.unquote(name)
  427:         assert name, f"URL {self._url!r} produced no filename"
  428:         return name
  429: 
  430:     @property
  431:     def file_path(self) -> str:
  432:         return url_to_path(self.url)
  433: 
  434:     @property
  435:     def scheme(self) -> str:
  436:         return self._parsed_url.scheme
  437: 
  438:     @property
  439:     def netloc(self) -> str:
  440:         """
  441:         This can contain auth information.
  442:         """
  443:         return self._parsed_url.netloc
  444: 
  445:     @property
  446:     def path(self) -> str:
  447:         return self._path
  448: 
  449:     def splitext(self) -> tuple[str, str]:
  450:         return splitext(posixpath.basename(self.path.rstrip("/")))
  451: 
  452:     @property
  453:     def ext(self) -> str:
  454:         return self.splitext()[1]
  455: 
  456:     @property
  457:     def url_without_fragment(self) -> str:
  458:         scheme, netloc, path, query, fragment = self._parsed_url
  459:         return urllib.parse.urlunsplit((scheme, netloc, path, query, ""))
  460: 
  461:     _egg_fragment_re = re.compile(r"[#&]egg=([^&]*)")
  462: 
  463:     # Per PEP 508.
  464:     _project_name_re = re.compile(
  465:         r"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$", re.IGNORECASE
  466:     )
  467: 
  468:     def _egg_fragment(self) -> str | None:
  469:         match = self._egg_fragment_re.search(self._url)
  470:         if not match:
  471:             return None
  472: 
  473:         # An egg fragment looks like a PEP 508 project name, along with
  474:         # an optional extras specifier. Anything else is invalid.
  475:         project_name = match.group(1)
  476:         if not self._project_name_re.match(project_name):
  477:             deprecated(
  478:                 reason=f"{self} contains an egg fragment with a non-PEP 508 name.",
  479:                 replacement="to use the req @ url syntax, and remove the egg fragment",
  480:                 gone_in="25.3",
  481:                 issue=13157,
  482:             )
  483: 
  484:         return project_name
  485: 
  486:     _subdirectory_fragment_re = re.compile(r"[#&]subdirectory=([^&]*)")
  487: 
  488:     @property
  489:     def subdirectory_fragment(self) -> str | None:
  490:         match = self._subdirectory_fragment_re.search(self._url)
  491:         if not match:
  492:             return None
  493:         return match.group(1)
  494: 
  495:     def metadata_link(self) -> Link | None:
  496:         """Return a link to the associated core metadata file (if any)."""
  497:         if self.metadata_file_data is None:
  498:             return None
  499:         metadata_url = f"{self.url_without_fragment}.metadata"
  500:         if self.metadata_file_data.hashes is None:
  501:             return Link(metadata_url)
  502:         return Link(metadata_url, hashes=self.metadata_file_data.hashes)
  503: 
  504:     def as_hashes(self) -> Hashes:
  505:         return Hashes({k: [v] for k, v in self._hashes.items()})
  506: 
  507:     @property
  508:     def hash(self) -> str | None:
  509:         return next(iter(self._hashes.values()), None)
  510: 
  511:     @property
  512:     def hash_name(self) -> str | None:
  513:         return next(iter(self._hashes), None)
  514: 
  515:     @property
  516:     def show_url(self) -> str:
  517:         return posixpath.basename(self._url.split("#", 1)[0].split("?", 1)[0])
  518: 
  519:     @property
  520:     def is_file(self) -> bool:
  521:         return self.scheme == "file"
  522: 
  523:     def is_existing_dir(self) -> bool:
  524:         return self.is_file and os.path.isdir(self.file_path)
  525: 
  526:     @property
  527:     def is_wheel(self) -> bool:
  528:         return self.ext == WHEEL_EXTENSION
  529: 
  530:     @property
  531:     def is_vcs(self) -> bool:
  532:         from pip._internal.vcs import vcs
  533: 
  534:         return self.scheme in vcs.all_schemes
  535: 
  536:     @property
  537:     def is_yanked(self) -> bool:
  538:         return self.yanked_reason is not None
  539: 
  540:     @property
  541:     def has_hash(self) -> bool:
  542:         return bool(self._hashes)
  543: 
  544:     def is_hash_allowed(self, hashes: Hashes | None) -> bool:
  545:         """
  546:         Return True if the link has a hash and it is allowed by `hashes`.
  547:         """
  548:         if hashes is None:
  549:             return False
  550:         return any(hashes.is_hash_allowed(k, v) for k, v in self._hashes.items())
  551: 
  552: 
  553: class _CleanResult(NamedTuple):
  554:     """Convert link for equivalency check.
  555: 
  556:     This is used in the resolver to check whether two URL-specified requirements
  557:     likely point to the same distribution and can be considered equivalent. This
  558:     equivalency logic avoids comparing URLs literally, which can be too strict
  559:     (e.g. "a=1&b=2" vs "b=2&a=1") and produce conflicts unexpecting to users.
  560: 
  561:     Currently this does three things:
  562: 
  563:     1. Drop the basic auth part. This is technically wrong since a server can
  564:        serve different content based on auth, but if it does that, it is even
  565:        impossible to guarantee two URLs without auth are equivalent, since
  566:        the user can input different auth information when prompted. So the
  567:        practical solution is to assume the auth doesn't affect the response.
  568:     2. Parse the query to avoid the ordering issue. Note that ordering under the
  569:        same key in the query are NOT cleaned; i.e. "a=1&a=2" and "a=2&a=1" are
  570:        still considered different.
  571:     3. Explicitly drop most of the fragment part, except ``subdirectory=`` and
  572:        hash values, since it should have no impact the downloaded content. Note
  573:        that this drops the "egg=" part historically used to denote the requested
  574:        project (and extras), which is wrong in the strictest sense, but too many
  575:        people are supplying it inconsistently to cause superfluous resolution
  576:        conflicts, so we choose to also ignore them.
  577:     """
  578: 
  579:     parsed: urllib.parse.SplitResult
  580:     query: dict[str, list[str]]
  581:     subdirectory: str
  582:     hashes: dict[str, str]
  583: 
  584: 
  585: def _clean_link(link: Link) -> _CleanResult:
  586:     parsed = link._parsed_url
  587:     netloc = parsed.netloc.rsplit("@", 1)[-1]
  588:     # According to RFC 8089, an empty host in file: means localhost.
  589:     if parsed.scheme == "file" and not netloc:
  590:         netloc = "localhost"
  591:     fragment = urllib.parse.parse_qs(parsed.fragment)
  592:     if "egg" in fragment:
  593:         logger.debug("Ignoring egg= fragment in %s", link)
  594:     try:
  595:         # If there are multiple subdirectory values, use the first one.
  596:         # This matches the behavior of Link.subdirectory_fragment.
  597:         subdirectory = fragment["subdirectory"][0]
  598:     except (IndexError, KeyError):
  599:         subdirectory = ""
  600:     # If there are multiple hash values under the same algorithm, use the
  601:     # first one. This matches the behavior of Link.hash_value.
  602:     hashes = {k: fragment[k][0] for k in _SUPPORTED_HASHES if k in fragment}
  603:     return _CleanResult(
  604:         parsed=parsed._replace(netloc=netloc, query="", fragment=""),
  605:         query=urllib.parse.parse_qs(parsed.query),
  606:         subdirectory=subdirectory,
  607:         hashes=hashes,
  608:     )
  609: 
  610: 
  611: @functools.cache
  612: def links_equivalent(link1: Link, link2: Link) -> bool:
  613:     return _clean_link(link1) == _clean_link(link2)
