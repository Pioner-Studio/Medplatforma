    1: """Prepares a distribution for installation"""
    2: 
    3: # The following comment should be removed at some point in the future.
    4: # mypy: strict-optional=False
    5: from __future__ import annotations
    6: 
    7: import mimetypes
    8: import os
    9: import shutil
   10: from collections.abc import Iterable
   11: from dataclasses import dataclass
   12: from pathlib import Path
   13: from typing import TYPE_CHECKING
   14: 
   15: from pip._vendor.packaging.utils import canonicalize_name
   16: 
   17: from pip._internal.build_env import BuildEnvironmentInstaller
   18: from pip._internal.distributions import make_distribution_for_install_requirement
   19: from pip._internal.distributions.installed import InstalledDistribution
   20: from pip._internal.exceptions import (
   21:     DirectoryUrlHashUnsupported,
   22:     HashMismatch,
   23:     HashUnpinned,
   24:     InstallationError,
   25:     MetadataInconsistent,
   26:     NetworkConnectionError,
   27:     VcsHashUnsupported,
   28: )
   29: from pip._internal.index.package_finder import PackageFinder
   30: from pip._internal.metadata import BaseDistribution, get_metadata_distribution
   31: from pip._internal.models.direct_url import ArchiveInfo
   32: from pip._internal.models.link import Link
   33: from pip._internal.models.wheel import Wheel
   34: from pip._internal.network.download import Downloader
   35: from pip._internal.network.lazy_wheel import (
   36:     HTTPRangeRequestUnsupported,
   37:     dist_from_wheel_url,
   38: )
   39: from pip._internal.network.session import PipSession
   40: from pip._internal.operations.build.build_tracker import BuildTracker
   41: from pip._internal.req.req_install import InstallRequirement
   42: from pip._internal.utils._log import getLogger
   43: from pip._internal.utils.direct_url_helpers import (
   44:     direct_url_for_editable,
   45:     direct_url_from_link,
   46: )
   47: from pip._internal.utils.hashes import Hashes, MissingHashes
   48: from pip._internal.utils.logging import indent_log
   49: from pip._internal.utils.misc import (
   50:     display_path,
   51:     hash_file,
   52:     hide_url,
   53:     redact_auth_from_requirement,
   54: )
   55: from pip._internal.utils.temp_dir import TempDirectory
   56: from pip._internal.utils.unpacking import unpack_file
   57: from pip._internal.vcs import vcs
   58: 
   59: if TYPE_CHECKING:
   60:     from pip._internal.cli.progress_bars import BarType
   61: 
   62: logger = getLogger(__name__)
   63: 
   64: 
   65: def _get_prepared_distribution(
   66:     req: InstallRequirement,
   67:     build_tracker: BuildTracker,
   68:     build_env_installer: BuildEnvironmentInstaller,
   69:     build_isolation: bool,
   70:     check_build_deps: bool,
   71: ) -> BaseDistribution:
   72:     """Prepare a distribution for installation."""
   73:     abstract_dist = make_distribution_for_install_requirement(req)
   74:     tracker_id = abstract_dist.build_tracker_id
   75:     if tracker_id is not None:
   76:         with build_tracker.track(req, tracker_id):
   77:             abstract_dist.prepare_distribution_metadata(
   78:                 build_env_installer, build_isolation, check_build_deps
   79:             )
   80:     return abstract_dist.get_metadata_distribution()
   81: 
   82: 
   83: def unpack_vcs_link(link: Link, location: str, verbosity: int) -> None:
   84:     vcs_backend = vcs.get_backend_for_scheme(link.scheme)
   85:     assert vcs_backend is not None
   86:     vcs_backend.unpack(location, url=hide_url(link.url), verbosity=verbosity)
   87: 
   88: 
   89: @dataclass
   90: class File:
   91:     path: str
   92:     content_type: str | None = None
   93: 
   94:     def __post_init__(self) -> None:
   95:         if self.content_type is None:
   96:             # Try to guess the file's MIME type. If the system MIME tables
   97:             # can't be loaded, give up.
   98:             try:
   99:                 self.content_type = mimetypes.guess_type(self.path)[0]
  100:             except OSError:
  101:                 pass
  102: 
  103: 
  104: def get_http_url(
  105:     link: Link,
  106:     download: Downloader,
  107:     download_dir: str | None = None,
  108:     hashes: Hashes | None = None,
  109: ) -> File:
  110:     temp_dir = TempDirectory(kind="unpack", globally_managed=True)
  111:     # If a download dir is specified, is the file already downloaded there?
  112:     already_downloaded_path = None
  113:     if download_dir:
  114:         already_downloaded_path = _check_download_dir(link, download_dir, hashes)
  115: 
  116:     if already_downloaded_path:
  117:         from_path = already_downloaded_path
  118:         content_type = None
  119:     else:
  120:         # let's download to a tmp dir
  121:         from_path, content_type = download(link, temp_dir.path)
  122:         if hashes:
  123:             hashes.check_against_path(from_path)
  124: 
  125:     return File(from_path, content_type)
  126: 
  127: 
  128: def get_file_url(
  129:     link: Link, download_dir: str | None = None, hashes: Hashes | None = None
  130: ) -> File:
  131:     """Get file and optionally check its hash."""
  132:     # If a download dir is specified, is the file already there and valid?
  133:     already_downloaded_path = None
  134:     if download_dir:
  135:         already_downloaded_path = _check_download_dir(link, download_dir, hashes)
  136: 
  137:     if already_downloaded_path:
  138:         from_path = already_downloaded_path
  139:     else:
  140:         from_path = link.file_path
  141: 
  142:     # If --require-hashes is off, `hashes` is either empty, the
  143:     # link's embedded hash, or MissingHashes; it is required to
  144:     # match. If --require-hashes is on, we are satisfied by any
  145:     # hash in `hashes` matching: a URL-based or an option-based
  146:     # one; no internet-sourced hash will be in `hashes`.
  147:     if hashes:
  148:         hashes.check_against_path(from_path)
  149:     return File(from_path, None)
  150: 
  151: 
  152: def unpack_url(
  153:     link: Link,
  154:     location: str,
  155:     download: Downloader,
  156:     verbosity: int,
  157:     download_dir: str | None = None,
  158:     hashes: Hashes | None = None,
  159: ) -> File | None:
  160:     """Unpack link into location, downloading if required.
  161: 
  162:     :param hashes: A Hashes object, one of whose embedded hashes must match,
  163:         or HashMismatch will be raised. If the Hashes is empty, no matches are
  164:         required, and unhashable types of requirements (like VCS ones, which
  165:         would ordinarily raise HashUnsupported) are allowed.
  166:     """
  167:     # non-editable vcs urls
  168:     if link.is_vcs:
  169:         unpack_vcs_link(link, location, verbosity=verbosity)
  170:         return None
  171: 
  172:     assert not link.is_existing_dir()
  173: 
  174:     # file urls
  175:     if link.is_file:
  176:         file = get_file_url(link, download_dir, hashes=hashes)
  177: 
  178:     # http urls
  179:     else:
  180:         file = get_http_url(
  181:             link,
  182:             download,
  183:             download_dir,
  184:             hashes=hashes,
  185:         )
  186: 
  187:     # unpack the archive to the build dir location. even when only downloading
  188:     # archives, they have to be unpacked to parse dependencies, except wheels
  189:     if not link.is_wheel:
  190:         unpack_file(file.path, location, file.content_type)
  191: 
  192:     return file
  193: 
  194: 
  195: def _check_download_dir(
  196:     link: Link,
  197:     download_dir: str,
  198:     hashes: Hashes | None,
  199:     warn_on_hash_mismatch: bool = True,
  200: ) -> str | None:
  201:     """Check download_dir for previously downloaded file with correct hash
  202:     If a correct file is found return its path else None
  203:     """
  204:     download_path = os.path.join(download_dir, link.filename)
  205: 
  206:     if not os.path.exists(download_path):
  207:         return None
  208: 
  209:     # If already downloaded, does its hash match?
  210:     logger.info("File was already downloaded %s", download_path)
  211:     if hashes:
  212:         try:
  213:             hashes.check_against_path(download_path)
  214:         except HashMismatch:
  215:             if warn_on_hash_mismatch:
  216:                 logger.warning(
  217:                     "Previously-downloaded file %s has bad hash. Re-downloading.",
  218:                     download_path,
  219:                 )
  220:             os.unlink(download_path)
  221:             return None
  222:     return download_path
  223: 
  224: 
  225: class RequirementPreparer:
  226:     """Prepares a Requirement"""
  227: 
  228:     def __init__(  # noqa: PLR0913 (too many parameters)
  229:         self,
  230:         *,
  231:         build_dir: str,
  232:         download_dir: str | None,
  233:         src_dir: str,
  234:         build_isolation: bool,
  235:         build_isolation_installer: BuildEnvironmentInstaller,
  236:         check_build_deps: bool,
  237:         build_tracker: BuildTracker,
  238:         session: PipSession,
  239:         progress_bar: BarType,
  240:         finder: PackageFinder,
  241:         require_hashes: bool,
  242:         use_user_site: bool,
  243:         lazy_wheel: bool,
  244:         verbosity: int,
  245:         legacy_resolver: bool,
  246:         resume_retries: int,
  247:     ) -> None:
  248:         super().__init__()
  249: 
  250:         self.src_dir = src_dir
  251:         self.build_dir = build_dir
  252:         self.build_tracker = build_tracker
  253:         self._session = session
  254:         self._download = Downloader(session, progress_bar, resume_retries)
  255:         self.finder = finder
  256: 
  257:         # Where still-packed archives should be written to. If None, they are
  258:         # not saved, and are deleted immediately after unpacking.
  259:         self.download_dir = download_dir
  260: 
  261:         # Is build isolation allowed?
  262:         self.build_isolation = build_isolation
  263:         self.build_env_installer = build_isolation_installer
  264: 
  265:         # Should check build dependencies?
  266:         self.check_build_deps = check_build_deps
  267: 
  268:         # Should hash-checking be required?
  269:         self.require_hashes = require_hashes
  270: 
  271:         # Should install in user site-packages?
  272:         self.use_user_site = use_user_site
  273: 
  274:         # Should wheels be downloaded lazily?
  275:         self.use_lazy_wheel = lazy_wheel
  276: 
  277:         # How verbose should underlying tooling be?
  278:         self.verbosity = verbosity
  279: 
  280:         # Are we using the legacy resolver?
  281:         self.legacy_resolver = legacy_resolver
  282: 
  283:         # Memoized downloaded files, as mapping of url: path.
  284:         self._downloaded: dict[str, str] = {}
  285: 
  286:         # Previous "header" printed for a link-based InstallRequirement
  287:         self._previous_requirement_header = ("", "")
  288: 
  289:     def _log_preparing_link(self, req: InstallRequirement) -> None:
  290:         """Provide context for the requirement being prepared."""
  291:         if req.link.is_file and not req.is_wheel_from_cache:
  292:             message = "Processing %s"
  293:             information = str(display_path(req.link.file_path))
  294:         else:
  295:             message = "Collecting %s"
  296:             information = redact_auth_from_requirement(req.req) if req.req else str(req)
  297: 
  298:         # If we used req.req, inject requirement source if available (this
  299:         # would already be included if we used req directly)
  300:         if req.req and req.comes_from:
  301:             if isinstance(req.comes_from, str):
  302:                 comes_from: str | None = req.comes_from
  303:             else:
  304:                 comes_from = req.comes_from.from_path()
  305:             if comes_from:
  306:                 information += f" (from {comes_from})"
  307: 
  308:         if (message, information) != self._previous_requirement_header:
  309:             self._previous_requirement_header = (message, information)
  310:             logger.info(message, information)
  311: 
  312:         if req.is_wheel_from_cache:
  313:             with indent_log():
  314:                 logger.info("Using cached %s", req.link.filename)
  315: 
  316:     def _ensure_link_req_src_dir(
  317:         self, req: InstallRequirement, parallel_builds: bool
  318:     ) -> None:
  319:         """Ensure source_dir of a linked InstallRequirement."""
  320:         # Since source_dir is only set for editable requirements.
  321:         if req.link.is_wheel:
  322:             # We don't need to unpack wheels, so no need for a source
  323:             # directory.
  324:             return
  325:         assert req.source_dir is None
  326:         if req.link.is_existing_dir():
  327:             # build local directories in-tree
  328:             req.source_dir = req.link.file_path
  329:             return
  330: 
  331:         # We always delete unpacked sdists after pip runs.
  332:         req.ensure_has_source_dir(
  333:             self.build_dir,
  334:             autodelete=True,
  335:             parallel_builds=parallel_builds,
  336:         )
  337:         req.ensure_pristine_source_checkout()
  338: 
  339:     def _get_linked_req_hashes(self, req: InstallRequirement) -> Hashes:
  340:         # By the time this is called, the requirement's link should have
  341:         # been checked so we can tell what kind of requirements req is
  342:         # and raise some more informative errors than otherwise.
  343:         # (For example, we can raise VcsHashUnsupported for a VCS URL
  344:         # rather than HashMissing.)
  345:         if not self.require_hashes:
  346:             return req.hashes(trust_internet=True)
  347: 
  348:         # We could check these first 2 conditions inside unpack_url
  349:         # and save repetition of conditions, but then we would
  350:         # report less-useful error messages for unhashable
  351:         # requirements, complaining that there's no hash provided.
  352:         if req.link.is_vcs:
  353:             raise VcsHashUnsupported()
  354:         if req.link.is_existing_dir():
  355:             raise DirectoryUrlHashUnsupported()
  356: 
  357:         # Unpinned packages are asking for trouble when a new version
  358:         # is uploaded.  This isn't a security check, but it saves users
  359:         # a surprising hash mismatch in the future.
  360:         # file:/// URLs aren't pinnable, so don't complain about them
  361:         # not being pinned.
  362:         if not req.is_direct and not req.is_pinned:
  363:             raise HashUnpinned()
  364: 
  365:         # If known-good hashes are missing for this requirement,
  366:         # shim it with a facade object that will provoke hash
  367:         # computation and then raise a HashMissing exception
  368:         # showing the user what the hash should be.
  369:         return req.hashes(trust_internet=False) or MissingHashes()
  370: 
  371:     def _fetch_metadata_only(
  372:         self,
  373:         req: InstallRequirement,
  374:     ) -> BaseDistribution | None:
  375:         if self.legacy_resolver:
  376:             logger.debug(
  377:                 "Metadata-only fetching is not used in the legacy resolver",
  378:             )
  379:             return None
  380:         if self.require_hashes:
  381:             logger.debug(
  382:                 "Metadata-only fetching is not used as hash checking is required",
  383:             )
  384:             return None
  385:         # Try PEP 658 metadata first, then fall back to lazy wheel if unavailable.
  386:         return self._fetch_metadata_using_link_data_attr(
  387:             req
  388:         ) or self._fetch_metadata_using_lazy_wheel(req.link)
  389: 
  390:     def _fetch_metadata_using_link_data_attr(
  391:         self,
  392:         req: InstallRequirement,
  393:     ) -> BaseDistribution | None:
  394:         """Fetch metadata from the data-dist-info-metadata attribute, if possible."""
  395:         # (1) Get the link to the metadata file, if provided by the backend.
  396:         metadata_link = req.link.metadata_link()
  397:         if metadata_link is None:
  398:             return None
  399:         assert req.req is not None
  400:         logger.verbose(
  401:             "Obtaining dependency information for %s from %s",
  402:             req.req,
  403:             metadata_link,
  404:         )
  405:         # (2) Download the contents of the METADATA file, separate from the dist itself.
  406:         metadata_file = get_http_url(
  407:             metadata_link,
  408:             self._download,
  409:             hashes=metadata_link.as_hashes(),
  410:         )
  411:         with open(metadata_file.path, "rb") as f:
  412:             metadata_contents = f.read()
  413:         # (3) Generate a dist just from those file contents.
  414:         metadata_dist = get_metadata_distribution(
  415:             metadata_contents,
  416:             req.link.filename,
  417:             req.req.name,
  418:         )
  419:         # (4) Ensure the Name: field from the METADATA file matches the name from the
  420:         #     install requirement.
  421:         #
  422:         #     NB: raw_name will fall back to the name from the install requirement if
  423:         #     the Name: field is not present, but it's noted in the raw_name docstring
  424:         #     that that should NEVER happen anyway.
  425:         if canonicalize_name(metadata_dist.raw_name) != canonicalize_name(req.req.name):
  426:             raise MetadataInconsistent(
  427:                 req, "Name", req.req.name, metadata_dist.raw_name
  428:             )
  429:         return metadata_dist
  430: 
  431:     def _fetch_metadata_using_lazy_wheel(
  432:         self,
  433:         link: Link,
  434:     ) -> BaseDistribution | None:
  435:         """Fetch metadata using lazy wheel, if possible."""
  436:         # --use-feature=fast-deps must be provided.
  437:         if not self.use_lazy_wheel:
  438:             return None
  439:         if link.is_file or not link.is_wheel:
  440:             logger.debug(
  441:                 "Lazy wheel is not used as %r does not point to a remote wheel",
  442:                 link,
  443:             )
  444:             return None
  445: 
  446:         wheel = Wheel(link.filename)
  447:         name = canonicalize_name(wheel.name)
  448:         logger.info(
  449:             "Obtaining dependency information from %s %s",
  450:             name,
  451:             wheel.version,
  452:         )
  453:         url = link.url.split("#", 1)[0]
  454:         try:
  455:             return dist_from_wheel_url(name, url, self._session)
  456:         except HTTPRangeRequestUnsupported:
  457:             logger.debug("%s does not support range requests", url)
  458:             return None
  459: 
  460:     def _complete_partial_requirements(
  461:         self,
  462:         partially_downloaded_reqs: Iterable[InstallRequirement],
  463:         parallel_builds: bool = False,
  464:     ) -> None:
  465:         """Download any requirements which were only fetched by metadata."""
  466:         # Download to a temporary directory. These will be copied over as
  467:         # needed for downstream 'download', 'wheel', and 'install' commands.
  468:         temp_dir = TempDirectory(kind="unpack", globally_managed=True).path
  469: 
  470:         # Map each link to the requirement that owns it. This allows us to set
  471:         # `req.local_file_path` on the appropriate requirement after passing
  472:         # all the links at once into BatchDownloader.
  473:         links_to_fully_download: dict[Link, InstallRequirement] = {}
  474:         for req in partially_downloaded_reqs:
  475:             assert req.link
  476:             links_to_fully_download[req.link] = req
  477: 
  478:         batch_download = self._download.batch(links_to_fully_download.keys(), temp_dir)
  479:         for link, (filepath, _) in batch_download:
  480:             logger.debug("Downloading link %s to %s", link, filepath)
  481:             req = links_to_fully_download[link]
  482:             # Record the downloaded file path so wheel reqs can extract a Distribution
  483:             # in .get_dist().
  484:             req.local_file_path = filepath
  485:             # Record that the file is downloaded so we don't do it again in
  486:             # _prepare_linked_requirement().
  487:             self._downloaded[req.link.url] = filepath
  488: 
  489:             # If this is an sdist, we need to unpack it after downloading, but the
  490:             # .source_dir won't be set up until we are in _prepare_linked_requirement().
  491:             # Add the downloaded archive to the install requirement to unpack after
  492:             # preparing the source dir.
  493:             if not req.is_wheel:
  494:                 req.needs_unpacked_archive(Path(filepath))
  495: 
  496:         # This step is necessary to ensure all lazy wheels are processed
  497:         # successfully by the 'download', 'wheel', and 'install' commands.
  498:         for req in partially_downloaded_reqs:
  499:             self._prepare_linked_requirement(req, parallel_builds)
  500: 
  501:     def prepare_linked_requirement(
  502:         self, req: InstallRequirement, parallel_builds: bool = False
  503:     ) -> BaseDistribution:
  504:         """Prepare a requirement to be obtained from req.link."""
  505:         assert req.link
  506:         self._log_preparing_link(req)
  507:         with indent_log():
  508:             # Check if the relevant file is already available
  509:             # in the download directory
  510:             file_path = None
  511:             if self.download_dir is not None and req.link.is_wheel:
  512:                 hashes = self._get_linked_req_hashes(req)
  513:                 file_path = _check_download_dir(
  514:                     req.link,
  515:                     self.download_dir,
  516:                     hashes,
  517:                     # When a locally built wheel has been found in cache, we don't warn
  518:                     # about re-downloading when the already downloaded wheel hash does
  519:                     # not match. This is because the hash must be checked against the
  520:                     # original link, not the cached link. It that case the already
  521:                     # downloaded file will be removed and re-fetched from cache (which
  522:                     # implies a hash check against the cache entry's origin.json).
  523:                     warn_on_hash_mismatch=not req.is_wheel_from_cache,
  524:                 )
  525: 
  526:             if file_path is not None:
  527:                 # The file is already available, so mark it as downloaded
  528:                 self._downloaded[req.link.url] = file_path
  529:             else:
  530:                 # The file is not available, attempt to fetch only metadata
  531:                 metadata_dist = self._fetch_metadata_only(req)
  532:                 if metadata_dist is not None:
  533:                     req.needs_more_preparation = True
  534:                     return metadata_dist
  535: 
  536:             # None of the optimizations worked, fully prepare the requirement
  537:             return self._prepare_linked_requirement(req, parallel_builds)
  538: 
  539:     def prepare_linked_requirements_more(
  540:         self, reqs: Iterable[InstallRequirement], parallel_builds: bool = False
  541:     ) -> None:
  542:         """Prepare linked requirements more, if needed."""
  543:         reqs = [req for req in reqs if req.needs_more_preparation]
  544:         for req in reqs:
  545:             # Determine if any of these requirements were already downloaded.
  546:             if self.download_dir is not None and req.link.is_wheel:
  547:                 hashes = self._get_linked_req_hashes(req)
  548:                 file_path = _check_download_dir(req.link, self.download_dir, hashes)
  549:                 if file_path is not None:
  550:                     self._downloaded[req.link.url] = file_path
  551:                     req.needs_more_preparation = False
  552: 
  553:         # Prepare requirements we found were already downloaded for some
  554:         # reason. The other downloads will be completed separately.
  555:         partially_downloaded_reqs: list[InstallRequirement] = []
  556:         for req in reqs:
  557:             if req.needs_more_preparation:
  558:                 partially_downloaded_reqs.append(req)
  559:             else:
  560:                 self._prepare_linked_requirement(req, parallel_builds)
  561: 
  562:         # TODO: separate this part out from RequirementPreparer when the v1
  563:         # resolver can be removed!
  564:         self._complete_partial_requirements(
  565:             partially_downloaded_reqs,
  566:             parallel_builds=parallel_builds,
  567:         )
  568: 
  569:     def _prepare_linked_requirement(
  570:         self, req: InstallRequirement, parallel_builds: bool
  571:     ) -> BaseDistribution:
  572:         assert req.link
  573:         link = req.link
  574: 
  575:         hashes = self._get_linked_req_hashes(req)
  576: 
  577:         if hashes and req.is_wheel_from_cache:
  578:             assert req.download_info is not None
  579:             assert link.is_wheel
  580:             assert link.is_file
  581:             # We need to verify hashes, and we have found the requirement in the cache
  582:             # of locally built wheels.
  583:             if (
  584:                 isinstance(req.download_info.info, ArchiveInfo)
  585:                 and req.download_info.info.hashes
  586:                 and hashes.has_one_of(req.download_info.info.hashes)
  587:             ):
  588:                 # At this point we know the requirement was built from a hashable source
  589:                 # artifact, and we verified that the cache entry's hash of the original
  590:                 # artifact matches one of the hashes we expect. We don't verify hashes
  591:                 # against the cached wheel, because the wheel is not the original.
  592:                 hashes = None
  593:             else:
  594:                 logger.warning(
  595:                     "The hashes of the source archive found in cache entry "
  596:                     "don't match, ignoring cached built wheel "
  597:                     "and re-downloading source."
  598:                 )
  599:                 req.link = req.cached_wheel_source_link
  600:                 link = req.link
  601: 
  602:         self._ensure_link_req_src_dir(req, parallel_builds)
  603: 
  604:         if link.is_existing_dir():
  605:             local_file = None
  606:         elif link.url not in self._downloaded:
  607:             try:
  608:                 local_file = unpack_url(
  609:                     link,
  610:                     req.source_dir,
  611:                     self._download,
  612:                     self.verbosity,
  613:                     self.download_dir,
  614:                     hashes,
  615:                 )
  616:             except NetworkConnectionError as exc:
  617:                 raise InstallationError(
  618:                     f"Could not install requirement {req} because of HTTP "
  619:                     f"error {exc} for URL {link}"
  620:                 )
  621:         else:
  622:             file_path = self._downloaded[link.url]
  623:             if hashes:
  624:                 hashes.check_against_path(file_path)
  625:             local_file = File(file_path, content_type=None)
  626: 
  627:         # If download_info is set, we got it from the wheel cache.
  628:         if req.download_info is None:
  629:             # Editables don't go through this function (see
  630:             # prepare_editable_requirement).
  631:             assert not req.editable
  632:             req.download_info = direct_url_from_link(link, req.source_dir)
  633:             # Make sure we have a hash in download_info. If we got it as part of the
  634:             # URL, it will have been verified and we can rely on it. Otherwise we
  635:             # compute it from the downloaded file.
  636:             # FIXME: https://github.com/pypa/pip/issues/11943
  637:             if (
  638:                 isinstance(req.download_info.info, ArchiveInfo)
  639:                 and not req.download_info.info.hashes
  640:                 and local_file
  641:             ):
  642:                 hash = hash_file(local_file.path)[0].hexdigest()
  643:                 # We populate info.hash for backward compatibility.
  644:                 # This will automatically populate info.hashes.
  645:                 req.download_info.info.hash = f"sha256={hash}"
  646: 
  647:         # For use in later processing,
  648:         # preserve the file path on the requirement.
  649:         if local_file:
  650:             req.local_file_path = local_file.path
  651: 
  652:         dist = _get_prepared_distribution(
  653:             req,
  654:             self.build_tracker,
  655:             self.build_env_installer,
  656:             self.build_isolation,
  657:             self.check_build_deps,
  658:         )
  659:         return dist
  660: 
  661:     def save_linked_requirement(self, req: InstallRequirement) -> None:
  662:         assert self.download_dir is not None
  663:         assert req.link is not None
  664:         link = req.link
  665:         if link.is_vcs or (link.is_existing_dir() and req.editable):
  666:             # Make a .zip of the source_dir we already created.
  667:             req.archive(self.download_dir)
  668:             return
  669: 
  670:         if link.is_existing_dir():
  671:             logger.debug(
  672:                 "Not copying link to destination directory "
  673:                 "since it is a directory: %s",
  674:                 link,
  675:             )
  676:             return
  677:         if req.local_file_path is None:
  678:             # No distribution was downloaded for this requirement.
  679:             return
  680: 
  681:         download_location = os.path.join(self.download_dir, link.filename)
  682:         if not os.path.exists(download_location):
  683:             shutil.copy(req.local_file_path, download_location)
  684:             download_path = display_path(download_location)
  685:             logger.info("Saved %s", download_path)
  686: 
  687:     def prepare_editable_requirement(
  688:         self,
  689:         req: InstallRequirement,
  690:     ) -> BaseDistribution:
  691:         """Prepare an editable requirement."""
  692:         assert req.editable, "cannot prepare a non-editable req as editable"
  693: 
  694:         logger.info("Obtaining %s", req)
  695: 
  696:         with indent_log():
  697:             if self.require_hashes:
  698:                 raise InstallationError(
  699:                     f"The editable requirement {req} cannot be installed when "
  700:                     "requiring hashes, because there is no single file to "
  701:                     "hash."
  702:                 )
  703:             req.ensure_has_source_dir(self.src_dir)
  704:             req.update_editable()
  705:             assert req.source_dir
  706:             req.download_info = direct_url_for_editable(req.unpacked_source_directory)
  707: 
  708:             dist = _get_prepared_distribution(
  709:                 req,
  710:                 self.build_tracker,
  711:                 self.build_env_installer,
  712:                 self.build_isolation,
  713:                 self.check_build_deps,
  714:             )
  715: 
  716:             req.check_if_exists(self.use_user_site)
  717: 
  718:         return dist
  719: 
  720:     def prepare_installed_requirement(
  721:         self,
  722:         req: InstallRequirement,
  723:         skip_reason: str,
  724:     ) -> BaseDistribution:
  725:         """Prepare an already-installed requirement."""
  726:         assert req.satisfied_by, "req should have been satisfied but isn't"
  727:         assert skip_reason is not None, (
  728:             "did not get skip reason skipped but req.satisfied_by "
  729:             f"is set to {req.satisfied_by}"
  730:         )
  731:         logger.info(
  732:             "Requirement %s: %s (%s)", skip_reason, req, req.satisfied_by.version
  733:         )
  734:         with indent_log():
  735:             if self.require_hashes:
  736:                 logger.debug(
  737:                     "Since it is already installed, we are trusting this "
  738:                     "package without checking its hash. To ensure a "
  739:                     "completely repeatable environment, install into an "
  740:                     "empty virtualenv."
  741:                 )
  742:             return InstalledDistribution(req).get_metadata_distribution()
