    1: """
    2: The main purpose of this module is to expose LinkCollector.collect_sources().
    3: """
    4: 
    5: from __future__ import annotations
    6: 
    7: import collections
    8: import email.message
    9: import functools
   10: import itertools
   11: import json
   12: import logging
   13: import os
   14: import urllib.parse
   15: import urllib.request
   16: from collections.abc import Iterable, MutableMapping, Sequence
   17: from dataclasses import dataclass
   18: from html.parser import HTMLParser
   19: from optparse import Values
   20: from typing import (
   21:     Callable,
   22:     NamedTuple,
   23:     Protocol,
   24: )
   25: 
   26: from pip._vendor import requests
   27: from pip._vendor.requests import Response
   28: from pip._vendor.requests.exceptions import RetryError, SSLError
   29: 
   30: from pip._internal.exceptions import NetworkConnectionError
   31: from pip._internal.models.link import Link
   32: from pip._internal.models.search_scope import SearchScope
   33: from pip._internal.network.session import PipSession
   34: from pip._internal.network.utils import raise_for_status
   35: from pip._internal.utils.filetypes import is_archive_file
   36: from pip._internal.utils.misc import redact_auth_from_url
   37: from pip._internal.vcs import vcs
   38: 
   39: from .sources import CandidatesFromPage, LinkSource, build_source
   40: 
   41: logger = logging.getLogger(__name__)
   42: 
   43: ResponseHeaders = MutableMapping[str, str]
   44: 
   45: 
   46: def _match_vcs_scheme(url: str) -> str | None:
   47:     """Look for VCS schemes in the URL.
   48: 
   49:     Returns the matched VCS scheme, or None if there's no match.
   50:     """
   51:     for scheme in vcs.schemes:
   52:         if url.lower().startswith(scheme) and url[len(scheme)] in "+:":
   53:             return scheme
   54:     return None
   55: 
   56: 
   57: class _NotAPIContent(Exception):
   58:     def __init__(self, content_type: str, request_desc: str) -> None:
   59:         super().__init__(content_type, request_desc)
   60:         self.content_type = content_type
   61:         self.request_desc = request_desc
   62: 
   63: 
   64: def _ensure_api_header(response: Response) -> None:
   65:     """
   66:     Check the Content-Type header to ensure the response contains a Simple
   67:     API Response.
   68: 
   69:     Raises `_NotAPIContent` if the content type is not a valid content-type.
   70:     """
   71:     content_type = response.headers.get("Content-Type", "Unknown")
   72: 
   73:     content_type_l = content_type.lower()
   74:     if content_type_l.startswith(
   75:         (
   76:             "text/html",
   77:             "application/vnd.pypi.simple.v1+html",
   78:             "application/vnd.pypi.simple.v1+json",
   79:         )
   80:     ):
   81:         return
   82: 
   83:     raise _NotAPIContent(content_type, response.request.method)
   84: 
   85: 
   86: class _NotHTTP(Exception):
   87:     pass
   88: 
   89: 
   90: def _ensure_api_response(url: str, session: PipSession) -> None:
   91:     """
   92:     Send a HEAD request to the URL, and ensure the response contains a simple
   93:     API Response.
   94: 
   95:     Raises `_NotHTTP` if the URL is not available for a HEAD request, or
   96:     `_NotAPIContent` if the content type is not a valid content type.
   97:     """
   98:     scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)
   99:     if scheme not in {"http", "https"}:
  100:         raise _NotHTTP()
  101: 
  102:     resp = session.head(url, allow_redirects=True)
  103:     raise_for_status(resp)
  104: 
  105:     _ensure_api_header(resp)
  106: 
  107: 
  108: def _get_simple_response(url: str, session: PipSession) -> Response:
  109:     """Access an Simple API response with GET, and return the response.
  110: 
  111:     This consists of three parts:
  112: 
  113:     1. If the URL looks suspiciously like an archive, send a HEAD first to
  114:        check the Content-Type is HTML or Simple API, to avoid downloading a
  115:        large file. Raise `_NotHTTP` if the content type cannot be determined, or
  116:        `_NotAPIContent` if it is not HTML or a Simple API.
  117:     2. Actually perform the request. Raise HTTP exceptions on network failures.
  118:     3. Check the Content-Type header to make sure we got a Simple API response,
  119:        and raise `_NotAPIContent` otherwise.
  120:     """
  121:     if is_archive_file(Link(url).filename):
  122:         _ensure_api_response(url, session=session)
  123: 
  124:     logger.debug("Getting page %s", redact_auth_from_url(url))
  125: 
  126:     resp = session.get(
  127:         url,
  128:         headers={
  129:             "Accept": ", ".join(
  130:                 [
  131:                     "application/vnd.pypi.simple.v1+json",
  132:                     "application/vnd.pypi.simple.v1+html; q=0.1",
  133:                     "text/html; q=0.01",
  134:                 ]
  135:             ),
  136:             # We don't want to blindly returned cached data for
  137:             # /simple/, because authors generally expecting that
  138:             # twine upload && pip install will function, but if
  139:             # they've done a pip install in the last ~10 minutes
  140:             # it won't. Thus by setting this to zero we will not
  141:             # blindly use any cached data, however the benefit of
  142:             # using max-age=0 instead of no-cache, is that we will
  143:             # still support conditional requests, so we will still
  144:             # minimize traffic sent in cases where the page hasn't
  145:             # changed at all, we will just always incur the round
  146:             # trip for the conditional GET now instead of only
  147:             # once per 10 minutes.
  148:             # For more information, please see pypa/pip#5670.
  149:             "Cache-Control": "max-age=0",
  150:         },
  151:     )
  152:     raise_for_status(resp)
  153: 
  154:     # The check for archives above only works if the url ends with
  155:     # something that looks like an archive. However that is not a
  156:     # requirement of an url. Unless we issue a HEAD request on every
  157:     # url we cannot know ahead of time for sure if something is a
  158:     # Simple API response or not. However we can check after we've
  159:     # downloaded it.
  160:     _ensure_api_header(resp)
  161: 
  162:     logger.debug(
  163:         "Fetched page %s as %s",
  164:         redact_auth_from_url(url),
  165:         resp.headers.get("Content-Type", "Unknown"),
  166:     )
  167: 
  168:     return resp
  169: 
  170: 
  171: def _get_encoding_from_headers(headers: ResponseHeaders) -> str | None:
  172:     """Determine if we have any encoding information in our headers."""
  173:     if headers and "Content-Type" in headers:
  174:         m = email.message.Message()
  175:         m["content-type"] = headers["Content-Type"]
  176:         charset = m.get_param("charset")
  177:         if charset:
  178:             return str(charset)
  179:     return None
  180: 
  181: 
  182: class CacheablePageContent:
  183:     def __init__(self, page: IndexContent) -> None:
  184:         assert page.cache_link_parsing
  185:         self.page = page
  186: 
  187:     def __eq__(self, other: object) -> bool:
  188:         return isinstance(other, type(self)) and self.page.url == other.page.url
  189: 
  190:     def __hash__(self) -> int:
  191:         return hash(self.page.url)
  192: 
  193: 
  194: class ParseLinks(Protocol):
  195:     def __call__(self, page: IndexContent) -> Iterable[Link]: ...
  196: 
  197: 
  198: def with_cached_index_content(fn: ParseLinks) -> ParseLinks:
  199:     """
  200:     Given a function that parses an Iterable[Link] from an IndexContent, cache the
  201:     function's result (keyed by CacheablePageContent), unless the IndexContent
  202:     `page` has `page.cache_link_parsing == False`.
  203:     """
  204: 
  205:     @functools.cache
  206:     def wrapper(cacheable_page: CacheablePageContent) -> list[Link]:
  207:         return list(fn(cacheable_page.page))
  208: 
  209:     @functools.wraps(fn)
  210:     def wrapper_wrapper(page: IndexContent) -> list[Link]:
  211:         if page.cache_link_parsing:
  212:             return wrapper(CacheablePageContent(page))
  213:         return list(fn(page))
  214: 
  215:     return wrapper_wrapper
  216: 
  217: 
  218: @with_cached_index_content
  219: def parse_links(page: IndexContent) -> Iterable[Link]:
  220:     """
  221:     Parse a Simple API's Index Content, and yield its anchor elements as Link objects.
  222:     """
  223: 
  224:     content_type_l = page.content_type.lower()
  225:     if content_type_l.startswith("application/vnd.pypi.simple.v1+json"):
  226:         data = json.loads(page.content)
  227:         for file in data.get("files", []):
  228:             link = Link.from_json(file, page.url)
  229:             if link is None:
  230:                 continue
  231:             yield link
  232:         return
  233: 
  234:     parser = HTMLLinkParser(page.url)
  235:     encoding = page.encoding or "utf-8"
  236:     parser.feed(page.content.decode(encoding))
  237: 
  238:     url = page.url
  239:     base_url = parser.base_url or url
  240:     for anchor in parser.anchors:
  241:         link = Link.from_element(anchor, page_url=url, base_url=base_url)
  242:         if link is None:
  243:             continue
  244:         yield link
  245: 
  246: 
  247: @dataclass(frozen=True)
  248: class IndexContent:
  249:     """Represents one response (or page), along with its URL.
  250: 
  251:     :param encoding: the encoding to decode the given content.
  252:     :param url: the URL from which the HTML was downloaded.
  253:     :param cache_link_parsing: whether links parsed from this page's url
  254:                                should be cached. PyPI index urls should
  255:                                have this set to False, for example.
  256:     """
  257: 
  258:     content: bytes
  259:     content_type: str
  260:     encoding: str | None
  261:     url: str
  262:     cache_link_parsing: bool = True
  263: 
  264:     def __str__(self) -> str:
  265:         return redact_auth_from_url(self.url)
  266: 
  267: 
  268: class HTMLLinkParser(HTMLParser):
  269:     """
  270:     HTMLParser that keeps the first base HREF and a list of all anchor
  271:     elements' attributes.
  272:     """
  273: 
  274:     def __init__(self, url: str) -> None:
  275:         super().__init__(convert_charrefs=True)
  276: 
  277:         self.url: str = url
  278:         self.base_url: str | None = None
  279:         self.anchors: list[dict[str, str | None]] = []
  280: 
  281:     def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:
  282:         if tag == "base" and self.base_url is None:
  283:             href = self.get_href(attrs)
  284:             if href is not None:
  285:                 self.base_url = href
  286:         elif tag == "a":
  287:             self.anchors.append(dict(attrs))
  288: 
  289:     def get_href(self, attrs: list[tuple[str, str | None]]) -> str | None:
  290:         for name, value in attrs:
  291:             if name == "href":
  292:                 return value
  293:         return None
  294: 
  295: 
  296: def _handle_get_simple_fail(
  297:     link: Link,
  298:     reason: str | Exception,
  299:     meth: Callable[..., None] | None = None,
  300: ) -> None:
  301:     if meth is None:
  302:         meth = logger.debug
  303:     meth("Could not fetch URL %s: %s - skipping", link, reason)
  304: 
  305: 
  306: def _make_index_content(
  307:     response: Response, cache_link_parsing: bool = True
  308: ) -> IndexContent:
  309:     encoding = _get_encoding_from_headers(response.headers)
  310:     return IndexContent(
  311:         response.content,
  312:         response.headers["Content-Type"],
  313:         encoding=encoding,
  314:         url=response.url,
  315:         cache_link_parsing=cache_link_parsing,
  316:     )
  317: 
  318: 
  319: def _get_index_content(link: Link, *, session: PipSession) -> IndexContent | None:
  320:     url = link.url.split("#", 1)[0]
  321: 
  322:     # Check for VCS schemes that do not support lookup as web pages.
  323:     vcs_scheme = _match_vcs_scheme(url)
  324:     if vcs_scheme:
  325:         logger.warning(
  326:             "Cannot look at %s URL %s because it does not support lookup as web pages.",
  327:             vcs_scheme,
  328:             link,
  329:         )
  330:         return None
  331: 
  332:     # Tack index.html onto file:// URLs that point to directories
  333:     scheme, _, path, _, _, _ = urllib.parse.urlparse(url)
  334:     if scheme == "file" and os.path.isdir(urllib.request.url2pathname(path)):
  335:         # add trailing slash if not present so urljoin doesn't trim
  336:         # final segment
  337:         if not url.endswith("/"):
  338:             url += "/"
  339:         # TODO: In the future, it would be nice if pip supported PEP 691
  340:         #       style responses in the file:// URLs, however there's no
  341:         #       standard file extension for application/vnd.pypi.simple.v1+json
  342:         #       so we'll need to come up with something on our own.
  343:         url = urllib.parse.urljoin(url, "index.html")
  344:         logger.debug(" file: URL is directory, getting %s", url)
  345: 
  346:     try:
  347:         resp = _get_simple_response(url, session=session)
  348:     except _NotHTTP:
  349:         logger.warning(
  350:             "Skipping page %s because it looks like an archive, and cannot "
  351:             "be checked by a HTTP HEAD request.",
  352:             link,
  353:         )
  354:     except _NotAPIContent as exc:
  355:         logger.warning(
  356:             "Skipping page %s because the %s request got Content-Type: %s. "
  357:             "The only supported Content-Types are application/vnd.pypi.simple.v1+json, "
  358:             "application/vnd.pypi.simple.v1+html, and text/html",
  359:             link,
  360:             exc.request_desc,
  361:             exc.content_type,
  362:         )
  363:     except NetworkConnectionError as exc:
  364:         _handle_get_simple_fail(link, exc)
  365:     except RetryError as exc:
  366:         _handle_get_simple_fail(link, exc)
  367:     except SSLError as exc:
  368:         reason = "There was a problem confirming the ssl certificate: "
  369:         reason += str(exc)
  370:         _handle_get_simple_fail(link, reason, meth=logger.info)
  371:     except requests.ConnectionError as exc:
  372:         _handle_get_simple_fail(link, f"connection error: {exc}")
  373:     except requests.Timeout:
  374:         _handle_get_simple_fail(link, "timed out")
  375:     else:
  376:         return _make_index_content(resp, cache_link_parsing=link.cache_link_parsing)
  377:     return None
  378: 
  379: 
  380: class CollectedSources(NamedTuple):
  381:     find_links: Sequence[LinkSource | None]
  382:     index_urls: Sequence[LinkSource | None]
  383: 
  384: 
  385: class LinkCollector:
  386:     """
  387:     Responsible for collecting Link objects from all configured locations,
  388:     making network requests as needed.
  389: 
  390:     The class's main method is its collect_sources() method.
  391:     """
  392: 
  393:     def __init__(
  394:         self,
  395:         session: PipSession,
  396:         search_scope: SearchScope,
  397:     ) -> None:
  398:         self.search_scope = search_scope
  399:         self.session = session
  400: 
  401:     @classmethod
  402:     def create(
  403:         cls,
  404:         session: PipSession,
  405:         options: Values,
  406:         suppress_no_index: bool = False,
  407:     ) -> LinkCollector:
  408:         """
  409:         :param session: The Session to use to make requests.
  410:         :param suppress_no_index: Whether to ignore the --no-index option
  411:             when constructing the SearchScope object.
  412:         """
  413:         index_urls = [options.index_url] + options.extra_index_urls
  414:         if options.no_index and not suppress_no_index:
  415:             logger.debug(
  416:                 "Ignoring indexes: %s",
  417:                 ",".join(redact_auth_from_url(url) for url in index_urls),
  418:             )
  419:             index_urls = []
  420: 
  421:         # Make sure find_links is a list before passing to create().
  422:         find_links = options.find_links or []
  423: 
  424:         search_scope = SearchScope.create(
  425:             find_links=find_links,
  426:             index_urls=index_urls,
  427:             no_index=options.no_index,
  428:         )
  429:         link_collector = LinkCollector(
  430:             session=session,
  431:             search_scope=search_scope,
  432:         )
  433:         return link_collector
  434: 
  435:     @property
  436:     def find_links(self) -> list[str]:
  437:         return self.search_scope.find_links
  438: 
  439:     def fetch_response(self, location: Link) -> IndexContent | None:
  440:         """
  441:         Fetch an HTML page containing package links.
  442:         """
  443:         return _get_index_content(location, session=self.session)
  444: 
  445:     def collect_sources(
  446:         self,
  447:         project_name: str,
  448:         candidates_from_page: CandidatesFromPage,
  449:     ) -> CollectedSources:
  450:         # The OrderedDict calls deduplicate sources by URL.
  451:         index_url_sources = collections.OrderedDict(
  452:             build_source(
  453:                 loc,
  454:                 candidates_from_page=candidates_from_page,
  455:                 page_validator=self.session.is_secure_origin,
  456:                 expand_dir=False,
  457:                 cache_link_parsing=False,
  458:                 project_name=project_name,
  459:             )
  460:             for loc in self.search_scope.get_index_urls_locations(project_name)
  461:         ).values()
  462:         find_links_sources = collections.OrderedDict(
  463:             build_source(
  464:                 loc,
  465:                 candidates_from_page=candidates_from_page,
  466:                 page_validator=self.session.is_secure_origin,
  467:                 expand_dir=True,
  468:                 cache_link_parsing=True,
  469:                 project_name=project_name,
  470:             )
  471:             for loc in self.find_links
  472:         ).values()
  473: 
  474:         if logger.isEnabledFor(logging.DEBUG):
  475:             lines = [
  476:                 f"* {s.link}"
  477:                 for s in itertools.chain(find_links_sources, index_url_sources)
  478:                 if s is not None and s.link is not None
  479:             ]
  480:             lines = [
  481:                 f"{len(lines)} location(s) to search "
  482:                 f"for versions of {project_name}:"
  483:             ] + lines
  484:             logger.debug("\n".join(lines))
  485: 
  486:         return CollectedSources(
  487:             find_links=list(find_links_sources),
  488:             index_urls=list(index_url_sources),
  489:         )
