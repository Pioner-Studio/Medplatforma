    1: # -*- coding: utf-8 -*-
    2: #
    3: # Copyright (C) 2012-2023 Vinay Sajip.
    4: # Licensed to the Python Software Foundation under a contributor agreement.
    5: # See LICENSE.txt and CONTRIBUTORS.txt.
    6: #
    7: 
    8: import gzip
    9: from io import BytesIO
   10: import json
   11: import logging
   12: import os
   13: import posixpath
   14: import re
   15: try:
   16:     import threading
   17: except ImportError:  # pragma: no cover
   18:     import dummy_threading as threading
   19: import zlib
   20: 
   21: from . import DistlibException
   22: from .compat import (urljoin, urlparse, urlunparse, url2pathname, pathname2url, queue, quote, unescape, build_opener,
   23:                      HTTPRedirectHandler as BaseRedirectHandler, text_type, Request, HTTPError, URLError)
   24: from .database import Distribution, DistributionPath, make_dist
   25: from .metadata import Metadata, MetadataInvalidError
   26: from .util import (cached_property, ensure_slash, split_filename, get_project_data, parse_requirement,
   27:                    parse_name_and_version, ServerProxy, normalize_name)
   28: from .version import get_scheme, UnsupportedVersionError
   29: from .wheel import Wheel, is_compatible
   30: 
   31: logger = logging.getLogger(__name__)
   32: 
   33: HASHER_HASH = re.compile(r'^(\w+)=([a-f0-9]+)')
   34: CHARSET = re.compile(r';\s*charset\s*=\s*(.*)\s*$', re.I)
   35: HTML_CONTENT_TYPE = re.compile('text/html|application/x(ht)?ml')
   36: DEFAULT_INDEX = 'https://pypi.org/pypi'
   37: 
   38: 
   39: def get_all_distribution_names(url=None):
   40:     """
   41:     Return all distribution names known by an index.
   42:     :param url: The URL of the index.
   43:     :return: A list of all known distribution names.
   44:     """
   45:     if url is None:
   46:         url = DEFAULT_INDEX
   47:     client = ServerProxy(url, timeout=3.0)
   48:     try:
   49:         return client.list_packages()
   50:     finally:
   51:         client('close')()
   52: 
   53: 
   54: class RedirectHandler(BaseRedirectHandler):
   55:     """
   56:     A class to work around a bug in some Python 3.2.x releases.
   57:     """
   58: 
   59:     # There's a bug in the base version for some 3.2.x
   60:     # (e.g. 3.2.2 on Ubuntu Oneiric). If a Location header
   61:     # returns e.g. /abc, it bails because it says the scheme ''
   62:     # is bogus, when actually it should use the request's
   63:     # URL for the scheme. See Python issue #13696.
   64:     def http_error_302(self, req, fp, code, msg, headers):
   65:         # Some servers (incorrectly) return multiple Location headers
   66:         # (so probably same goes for URI).  Use first header.
   67:         newurl = None
   68:         for key in ('location', 'uri'):
   69:             if key in headers:
   70:                 newurl = headers[key]
   71:                 break
   72:         if newurl is None:  # pragma: no cover
   73:             return
   74:         urlparts = urlparse(newurl)
   75:         if urlparts.scheme == '':
   76:             newurl = urljoin(req.get_full_url(), newurl)
   77:             if hasattr(headers, 'replace_header'):
   78:                 headers.replace_header(key, newurl)
   79:             else:
   80:                 headers[key] = newurl
   81:         return BaseRedirectHandler.http_error_302(self, req, fp, code, msg, headers)
   82: 
   83:     http_error_301 = http_error_303 = http_error_307 = http_error_302
   84: 
   85: 
   86: class Locator(object):
   87:     """
   88:     A base class for locators - things that locate distributions.
   89:     """
   90:     source_extensions = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz')
   91:     binary_extensions = ('.egg', '.exe', '.whl')
   92:     excluded_extensions = ('.pdf', )
   93: 
   94:     # A list of tags indicating which wheels you want to match. The default
   95:     # value of None matches against the tags compatible with the running
   96:     # Python. If you want to match other values, set wheel_tags on a locator
   97:     # instance to a list of tuples (pyver, abi, arch) which you want to match.
   98:     wheel_tags = None
   99: 
  100:     downloadable_extensions = source_extensions + ('.whl', )
  101: 
  102:     def __init__(self, scheme='default'):
  103:         """
  104:         Initialise an instance.
  105:         :param scheme: Because locators look for most recent versions, they
  106:                        need to know the version scheme to use. This specifies
  107:                        the current PEP-recommended scheme - use ``'legacy'``
  108:                        if you need to support existing distributions on PyPI.
  109:         """
  110:         self._cache = {}
  111:         self.scheme = scheme
  112:         # Because of bugs in some of the handlers on some of the platforms,
  113:         # we use our own opener rather than just using urlopen.
  114:         self.opener = build_opener(RedirectHandler())
  115:         # If get_project() is called from locate(), the matcher instance
  116:         # is set from the requirement passed to locate(). See issue #18 for
  117:         # why this can be useful to know.
  118:         self.matcher = None
  119:         self.errors = queue.Queue()
  120: 
  121:     def get_errors(self):
  122:         """
  123:         Return any errors which have occurred.
  124:         """
  125:         result = []
  126:         while not self.errors.empty():  # pragma: no cover
  127:             try:
  128:                 e = self.errors.get(False)
  129:                 result.append(e)
  130:             except self.errors.Empty:
  131:                 continue
  132:             self.errors.task_done()
  133:         return result
  134: 
  135:     def clear_errors(self):
  136:         """
  137:         Clear any errors which may have been logged.
  138:         """
  139:         # Just get the errors and throw them away
  140:         self.get_errors()
  141: 
  142:     def clear_cache(self):
  143:         self._cache.clear()
  144: 
  145:     def _get_scheme(self):
  146:         return self._scheme
  147: 
  148:     def _set_scheme(self, value):
  149:         self._scheme = value
  150: 
  151:     scheme = property(_get_scheme, _set_scheme)
  152: 
  153:     def _get_project(self, name):
  154:         """
  155:         For a given project, get a dictionary mapping available versions to Distribution
  156:         instances.
  157: 
  158:         This should be implemented in subclasses.
  159: 
  160:         If called from a locate() request, self.matcher will be set to a
  161:         matcher for the requirement to satisfy, otherwise it will be None.
  162:         """
  163:         raise NotImplementedError('Please implement in the subclass')
  164: 
  165:     def get_distribution_names(self):
  166:         """
  167:         Return all the distribution names known to this locator.
  168:         """
  169:         raise NotImplementedError('Please implement in the subclass')
  170: 
  171:     def get_project(self, name):
  172:         """
  173:         For a given project, get a dictionary mapping available versions to Distribution
  174:         instances.
  175: 
  176:         This calls _get_project to do all the work, and just implements a caching layer on top.
  177:         """
  178:         if self._cache is None:  # pragma: no cover
  179:             result = self._get_project(name)
  180:         elif name in self._cache:
  181:             result = self._cache[name]
  182:         else:
  183:             self.clear_errors()
  184:             result = self._get_project(name)
  185:             self._cache[name] = result
  186:         return result
  187: 
  188:     def score_url(self, url):
  189:         """
  190:         Give an url a score which can be used to choose preferred URLs
  191:         for a given project release.
  192:         """
  193:         t = urlparse(url)
  194:         basename = posixpath.basename(t.path)
  195:         compatible = True
  196:         is_wheel = basename.endswith('.whl')
  197:         is_downloadable = basename.endswith(self.downloadable_extensions)
  198:         if is_wheel:
  199:             compatible = is_compatible(Wheel(basename), self.wheel_tags)
  200:         return (t.scheme == 'https', 'pypi.org' in t.netloc, is_downloadable, is_wheel, compatible, basename)
  201: 
  202:     def prefer_url(self, url1, url2):
  203:         """
  204:         Choose one of two URLs where both are candidates for distribution
  205:         archives for the same version of a distribution (for example,
  206:         .tar.gz vs. zip).
  207: 
  208:         The current implementation favours https:// URLs over http://, archives
  209:         from PyPI over those from other locations, wheel compatibility (if a
  210:         wheel) and then the archive name.
  211:         """
  212:         result = url2
  213:         if url1:
  214:             s1 = self.score_url(url1)
  215:             s2 = self.score_url(url2)
  216:             if s1 > s2:
  217:                 result = url1
  218:             if result != url2:
  219:                 logger.debug('Not replacing %r with %r', url1, url2)
  220:             else:
  221:                 logger.debug('Replacing %r with %r', url1, url2)
  222:         return result
  223: 
  224:     def split_filename(self, filename, project_name):
  225:         """
  226:         Attempt to split a filename in project name, version and Python version.
  227:         """
  228:         return split_filename(filename, project_name)
  229: 
  230:     def convert_url_to_download_info(self, url, project_name):
  231:         """
  232:         See if a URL is a candidate for a download URL for a project (the URL
  233:         has typically been scraped from an HTML page).
  234: 
  235:         If it is, a dictionary is returned with keys "name", "version",
  236:         "filename" and "url"; otherwise, None is returned.
  237:         """
  238: 
  239:         def same_project(name1, name2):
  240:             return normalize_name(name1) == normalize_name(name2)
  241: 
  242:         result = None
  243:         scheme, netloc, path, params, query, frag = urlparse(url)
  244:         if frag.lower().startswith('egg='):  # pragma: no cover
  245:             logger.debug('%s: version hint in fragment: %r', project_name, frag)
  246:         m = HASHER_HASH.match(frag)
  247:         if m:
  248:             algo, digest = m.groups()
  249:         else:
  250:             algo, digest = None, None
  251:         origpath = path
  252:         if path and path[-1] == '/':  # pragma: no cover
  253:             path = path[:-1]
  254:         if path.endswith('.whl'):
  255:             try:
  256:                 wheel = Wheel(path)
  257:                 if not is_compatible(wheel, self.wheel_tags):
  258:                     logger.debug('Wheel not compatible: %s', path)
  259:                 else:
  260:                     if project_name is None:
  261:                         include = True
  262:                     else:
  263:                         include = same_project(wheel.name, project_name)
  264:                     if include:
  265:                         result = {
  266:                             'name': wheel.name,
  267:                             'version': wheel.version,
  268:                             'filename': wheel.filename,
  269:                             'url': urlunparse((scheme, netloc, origpath, params, query, '')),
  270:                             'python-version': ', '.join(['.'.join(list(v[2:])) for v in wheel.pyver]),
  271:                         }
  272:             except Exception:  # pragma: no cover
  273:                 logger.warning('invalid path for wheel: %s', path)
  274:         elif not path.endswith(self.downloadable_extensions):  # pragma: no cover
  275:             logger.debug('Not downloadable: %s', path)
  276:         else:  # downloadable extension
  277:             path = filename = posixpath.basename(path)
  278:             for ext in self.downloadable_extensions:
  279:                 if path.endswith(ext):
  280:                     path = path[:-len(ext)]
  281:                     t = self.split_filename(path, project_name)
  282:                     if not t:  # pragma: no cover
  283:                         logger.debug('No match for project/version: %s', path)
  284:                     else:
  285:                         name, version, pyver = t
  286:                         if not project_name or same_project(project_name, name):
  287:                             result = {
  288:                                 'name': name,
  289:                                 'version': version,
  290:                                 'filename': filename,
  291:                                 'url': urlunparse((scheme, netloc, origpath, params, query, '')),
  292:                             }
  293:                             if pyver:  # pragma: no cover
  294:                                 result['python-version'] = pyver
  295:                     break
  296:         if result and algo:
  297:             result['%s_digest' % algo] = digest
  298:         return result
  299: 
  300:     def _get_digest(self, info):
  301:         """
  302:         Get a digest from a dictionary by looking at a "digests" dictionary
  303:         or keys of the form 'algo_digest'.
  304: 
  305:         Returns a 2-tuple (algo, digest) if found, else None. Currently
  306:         looks only for SHA256, then MD5.
  307:         """
  308:         result = None
  309:         if 'digests' in info:
  310:             digests = info['digests']
  311:             for algo in ('sha256', 'md5'):
  312:                 if algo in digests:
  313:                     result = (algo, digests[algo])
  314:                     break
  315:         if not result:
  316:             for algo in ('sha256', 'md5'):
  317:                 key = '%s_digest' % algo
  318:                 if key in info:
  319:                     result = (algo, info[key])
  320:                     break
  321:         return result
  322: 
  323:     def _update_version_data(self, result, info):
  324:         """
  325:         Update a result dictionary (the final result from _get_project) with a
  326:         dictionary for a specific version, which typically holds information
  327:         gleaned from a filename or URL for an archive for the distribution.
  328:         """
  329:         name = info.pop('name')
  330:         version = info.pop('version')
  331:         if version in result:
  332:             dist = result[version]
  333:             md = dist.metadata
  334:         else:
  335:             dist = make_dist(name, version, scheme=self.scheme)
  336:             md = dist.metadata
  337:         dist.digest = digest = self._get_digest(info)
  338:         url = info['url']
  339:         result['digests'][url] = digest
  340:         if md.source_url != info['url']:
  341:             md.source_url = self.prefer_url(md.source_url, url)
  342:             result['urls'].setdefault(version, set()).add(url)
  343:         dist.locator = self
  344:         result[version] = dist
  345: 
  346:     def locate(self, requirement, prereleases=False):
  347:         """
  348:         Find the most recent distribution which matches the given
  349:         requirement.
  350: 
  351:         :param requirement: A requirement of the form 'foo (1.0)' or perhaps
  352:                             'foo (>= 1.0, < 2.0, != 1.3)'
  353:         :param prereleases: If ``True``, allow pre-release versions
  354:                             to be located. Otherwise, pre-release versions
  355:                             are not returned.
  356:         :return: A :class:`Distribution` instance, or ``None`` if no such
  357:                  distribution could be located.
  358:         """
  359:         result = None
  360:         r = parse_requirement(requirement)
  361:         if r is None:  # pragma: no cover
  362:             raise DistlibException('Not a valid requirement: %r' % requirement)
  363:         scheme = get_scheme(self.scheme)
  364:         self.matcher = matcher = scheme.matcher(r.requirement)
  365:         logger.debug('matcher: %s (%s)', matcher, type(matcher).__name__)
  366:         versions = self.get_project(r.name)
  367:         if len(versions) > 2:  # urls and digests keys are present
  368:             # sometimes, versions are invalid
  369:             slist = []
  370:             vcls = matcher.version_class
  371:             for k in versions:
  372:                 if k in ('urls', 'digests'):
  373:                     continue
  374:                 try:
  375:                     if not matcher.match(k):
  376:                         pass  # logger.debug('%s did not match %r', matcher, k)
  377:                     else:
  378:                         if prereleases or not vcls(k).is_prerelease:
  379:                             slist.append(k)
  380:                 except Exception:  # pragma: no cover
  381:                     logger.warning('error matching %s with %r', matcher, k)
  382:                     pass  # slist.append(k)
  383:             if len(slist) > 1:
  384:                 slist = sorted(slist, key=scheme.key)
  385:             if slist:
  386:                 logger.debug('sorted list: %s', slist)
  387:                 version = slist[-1]
  388:                 result = versions[version]
  389:         if result:
  390:             if r.extras:
  391:                 result.extras = r.extras
  392:             result.download_urls = versions.get('urls', {}).get(version, set())
  393:             d = {}
  394:             sd = versions.get('digests', {})
  395:             for url in result.download_urls:
  396:                 if url in sd:  # pragma: no cover
  397:                     d[url] = sd[url]
  398:             result.digests = d
  399:         self.matcher = None
  400:         return result
  401: 
  402: 
  403: class PyPIRPCLocator(Locator):
  404:     """
  405:     This locator uses XML-RPC to locate distributions. It therefore
  406:     cannot be used with simple mirrors (that only mirror file content).
  407:     """
  408: 
  409:     def __init__(self, url, **kwargs):
  410:         """
  411:         Initialise an instance.
  412: 
  413:         :param url: The URL to use for XML-RPC.
  414:         :param kwargs: Passed to the superclass constructor.
  415:         """
  416:         super(PyPIRPCLocator, self).__init__(**kwargs)
  417:         self.base_url = url
  418:         self.client = ServerProxy(url, timeout=3.0)
  419: 
  420:     def get_distribution_names(self):
  421:         """
  422:         Return all the distribution names known to this locator.
  423:         """
  424:         return set(self.client.list_packages())
  425: 
  426:     def _get_project(self, name):
  427:         result = {'urls': {}, 'digests': {}}
  428:         versions = self.client.package_releases(name, True)
  429:         for v in versions:
  430:             urls = self.client.release_urls(name, v)
  431:             data = self.client.release_data(name, v)
  432:             metadata = Metadata(scheme=self.scheme)
  433:             metadata.name = data['name']
  434:             metadata.version = data['version']
  435:             metadata.license = data.get('license')
  436:             metadata.keywords = data.get('keywords', [])
  437:             metadata.summary = data.get('summary')
  438:             dist = Distribution(metadata)
  439:             if urls:
  440:                 info = urls[0]
  441:                 metadata.source_url = info['url']
  442:                 dist.digest = self._get_digest(info)
  443:                 dist.locator = self
  444:                 result[v] = dist
  445:                 for info in urls:
  446:                     url = info['url']
  447:                     digest = self._get_digest(info)
  448:                     result['urls'].setdefault(v, set()).add(url)
  449:                     result['digests'][url] = digest
  450:         return result
  451: 
  452: 
  453: class PyPIJSONLocator(Locator):
  454:     """
  455:     This locator uses PyPI's JSON interface. It's very limited in functionality
  456:     and probably not worth using.
  457:     """
  458: 
  459:     def __init__(self, url, **kwargs):
  460:         super(PyPIJSONLocator, self).__init__(**kwargs)
  461:         self.base_url = ensure_slash(url)
  462: 
  463:     def get_distribution_names(self):
  464:         """
  465:         Return all the distribution names known to this locator.
  466:         """
  467:         raise NotImplementedError('Not available from this locator')
  468: 
  469:     def _get_project(self, name):
  470:         result = {'urls': {}, 'digests': {}}
  471:         url = urljoin(self.base_url, '%s/json' % quote(name))
  472:         try:
  473:             resp = self.opener.open(url)
  474:             data = resp.read().decode()  # for now
  475:             d = json.loads(data)
  476:             md = Metadata(scheme=self.scheme)
  477:             data = d['info']
  478:             md.name = data['name']
  479:             md.version = data['version']
  480:             md.license = data.get('license')
  481:             md.keywords = data.get('keywords', [])
  482:             md.summary = data.get('summary')
  483:             dist = Distribution(md)
  484:             dist.locator = self
  485:             # urls = d['urls']
  486:             result[md.version] = dist
  487:             for info in d['urls']:
  488:                 url = info['url']
  489:                 dist.download_urls.add(url)
  490:                 dist.digests[url] = self._get_digest(info)
  491:                 result['urls'].setdefault(md.version, set()).add(url)
  492:                 result['digests'][url] = self._get_digest(info)
  493:             # Now get other releases
  494:             for version, infos in d['releases'].items():
  495:                 if version == md.version:
  496:                     continue  # already done
  497:                 omd = Metadata(scheme=self.scheme)
  498:                 omd.name = md.name
  499:                 omd.version = version
  500:                 odist = Distribution(omd)
  501:                 odist.locator = self
  502:                 result[version] = odist
  503:                 for info in infos:
  504:                     url = info['url']
  505:                     odist.download_urls.add(url)
  506:                     odist.digests[url] = self._get_digest(info)
  507:                     result['urls'].setdefault(version, set()).add(url)
  508:                     result['digests'][url] = self._get_digest(info)
  509: 
  510: 
  511: #            for info in urls:
  512: #                md.source_url = info['url']
  513: #                dist.digest = self._get_digest(info)
  514: #                dist.locator = self
  515: #                for info in urls:
  516: #                    url = info['url']
  517: #                    result['urls'].setdefault(md.version, set()).add(url)
  518: #                    result['digests'][url] = self._get_digest(info)
  519:         except Exception as e:
  520:             self.errors.put(text_type(e))
  521:             logger.exception('JSON fetch failed: %s', e)
  522:         return result
  523: 
  524: 
  525: class Page(object):
  526:     """
  527:     This class represents a scraped HTML page.
  528:     """
  529:     # The following slightly hairy-looking regex just looks for the contents of
  530:     # an anchor link, which has an attribute "href" either immediately preceded
  531:     # or immediately followed by a "rel" attribute. The attribute values can be
  532:     # declared with double quotes, single quotes or no quotes - which leads to
  533:     # the length of the expression.
  534:     _href = re.compile(
  535:         """
  536: (rel\\s*=\\s*(?:"(?P<rel1>[^"]*)"|'(?P<rel2>[^']*)'|(?P<rel3>[^>\\s\n]*))\\s+)?
  537: href\\s*=\\s*(?:"(?P<url1>[^"]*)"|'(?P<url2>[^']*)'|(?P<url3>[^>\\s\n]*))
  538: (\\s+rel\\s*=\\s*(?:"(?P<rel4>[^"]*)"|'(?P<rel5>[^']*)'|(?P<rel6>[^>\\s\n]*)))?
  539: """, re.I | re.S | re.X)
  540:     _base = re.compile(r"""<base\s+href\s*=\s*['"]?([^'">]+)""", re.I | re.S)
  541: 
  542:     def __init__(self, data, url):
  543:         """
  544:         Initialise an instance with the Unicode page contents and the URL they
  545:         came from.
  546:         """
  547:         self.data = data
  548:         self.base_url = self.url = url
  549:         m = self._base.search(self.data)
  550:         if m:
  551:             self.base_url = m.group(1)
  552: 
  553:     _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\|-]', re.I)
  554: 
  555:     @cached_property
  556:     def links(self):
  557:         """
  558:         Return the URLs of all the links on a page together with information
  559:         about their "rel" attribute, for determining which ones to treat as
  560:         downloads and which ones to queue for further scraping.
  561:         """
  562: 
  563:         def clean(url):
  564:             "Tidy up an URL."
  565:             scheme, netloc, path, params, query, frag = urlparse(url)
  566:             return urlunparse((scheme, netloc, quote(path), params, query, frag))
  567: 
  568:         result = set()
  569:         for match in self._href.finditer(self.data):
  570:             d = match.groupdict('')
  571:             rel = (d['rel1'] or d['rel2'] or d['rel3'] or d['rel4'] or d['rel5'] or d['rel6'])
  572:             url = d['url1'] or d['url2'] or d['url3']
  573:             url = urljoin(self.base_url, url)
  574:             url = unescape(url)
  575:             url = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0)), url)
  576:             result.add((url, rel))
  577:         # We sort the result, hoping to bring the most recent versions
  578:         # to the front
  579:         result = sorted(result, key=lambda t: t[0], reverse=True)
  580:         return result
  581: 
  582: 
  583: class SimpleScrapingLocator(Locator):
  584:     """
  585:     A locator which scrapes HTML pages to locate downloads for a distribution.
  586:     This runs multiple threads to do the I/O; performance is at least as good
  587:     as pip's PackageFinder, which works in an analogous fashion.
  588:     """
  589: 
  590:     # These are used to deal with various Content-Encoding schemes.
  591:     decoders = {
  592:         'deflate': zlib.decompress,
  593:         'gzip': lambda b: gzip.GzipFile(fileobj=BytesIO(b)).read(),
  594:         'none': lambda b: b,
  595:     }
  596: 
  597:     def __init__(self, url, timeout=None, num_workers=10, **kwargs):
  598:         """
  599:         Initialise an instance.
  600:         :param url: The root URL to use for scraping.
  601:         :param timeout: The timeout, in seconds, to be applied to requests.
  602:                         This defaults to ``None`` (no timeout specified).
  603:         :param num_workers: The number of worker threads you want to do I/O,
  604:                             This defaults to 10.
  605:         :param kwargs: Passed to the superclass.
  606:         """
  607:         super(SimpleScrapingLocator, self).__init__(**kwargs)
  608:         self.base_url = ensure_slash(url)
  609:         self.timeout = timeout
  610:         self._page_cache = {}
  611:         self._seen = set()
  612:         self._to_fetch = queue.Queue()
  613:         self._bad_hosts = set()
  614:         self.skip_externals = False
  615:         self.num_workers = num_workers
  616:         self._lock = threading.RLock()
  617:         # See issue #45: we need to be resilient when the locator is used
  618:         # in a thread, e.g. with concurrent.futures. We can't use self._lock
  619:         # as it is for coordinating our internal threads - the ones created
  620:         # in _prepare_threads.
  621:         self._gplock = threading.RLock()
  622:         self.platform_check = False  # See issue #112
  623: 
  624:     def _prepare_threads(self):
  625:         """
  626:         Threads are created only when get_project is called, and terminate
  627:         before it returns. They are there primarily to parallelise I/O (i.e.
  628:         fetching web pages).
  629:         """
  630:         self._threads = []
  631:         for i in range(self.num_workers):
  632:             t = threading.Thread(target=self._fetch)
  633:             t.daemon = True
  634:             t.start()
  635:             self._threads.append(t)
  636: 
  637:     def _wait_threads(self):
  638:         """
  639:         Tell all the threads to terminate (by sending a sentinel value) and
  640:         wait for them to do so.
  641:         """
  642:         # Note that you need two loops, since you can't say which
  643:         # thread will get each sentinel
  644:         for t in self._threads:
  645:             self._to_fetch.put(None)  # sentinel
  646:         for t in self._threads:
  647:             t.join()
  648:         self._threads = []
  649: 
  650:     def _get_project(self, name):
  651:         result = {'urls': {}, 'digests': {}}
  652:         with self._gplock:
  653:             self.result = result
  654:             self.project_name = name
  655:             url = urljoin(self.base_url, '%s/' % quote(name))
  656:             self._seen.clear()
  657:             self._page_cache.clear()
  658:             self._prepare_threads()
  659:             try:
  660:                 logger.debug('Queueing %s', url)
  661:                 self._to_fetch.put(url)
  662:                 self._to_fetch.join()
  663:             finally:
  664:                 self._wait_threads()
  665:             del self.result
  666:         return result
  667: 
  668:     platform_dependent = re.compile(r'\b(linux_(i\d86|x86_64|arm\w+)|'
  669:                                     r'win(32|_amd64)|macosx_?\d+)\b', re.I)
  670: 
  671:     def _is_platform_dependent(self, url):
  672:         """
  673:         Does an URL refer to a platform-specific download?
  674:         """
  675:         return self.platform_dependent.search(url)
  676: 
  677:     def _process_download(self, url):
  678:         """
  679:         See if an URL is a suitable download for a project.
  680: 
  681:         If it is, register information in the result dictionary (for
  682:         _get_project) about the specific version it's for.
  683: 
  684:         Note that the return value isn't actually used other than as a boolean
  685:         value.
  686:         """
  687:         if self.platform_check and self._is_platform_dependent(url):
  688:             info = None
  689:         else:
  690:             info = self.convert_url_to_download_info(url, self.project_name)
  691:         logger.debug('process_download: %s -> %s', url, info)
  692:         if info:
  693:             with self._lock:  # needed because self.result is shared
  694:                 self._update_version_data(self.result, info)
  695:         return info
  696: 
  697:     def _should_queue(self, link, referrer, rel):
  698:         """
  699:         Determine whether a link URL from a referring page and with a
  700:         particular "rel" attribute should be queued for scraping.
  701:         """
  702:         scheme, netloc, path, _, _, _ = urlparse(link)
  703:         if path.endswith(self.source_extensions + self.binary_extensions + self.excluded_extensions):
  704:             result = False
  705:         elif self.skip_externals and not link.startswith(self.base_url):
  706:             result = False
  707:         elif not referrer.startswith(self.base_url):
  708:             result = False
  709:         elif rel not in ('homepage', 'download'):
  710:             result = False
  711:         elif scheme not in ('http', 'https', 'ftp'):
  712:             result = False
  713:         elif self._is_platform_dependent(link):
  714:             result = False
  715:         else:
  716:             host = netloc.split(':', 1)[0]
  717:             if host.lower() == 'localhost':
  718:                 result = False
  719:             else:
  720:                 result = True
  721:         logger.debug('should_queue: %s (%s) from %s -> %s', link, rel, referrer, result)
  722:         return result
  723: 
  724:     def _fetch(self):
  725:         """
  726:         Get a URL to fetch from the work queue, get the HTML page, examine its
  727:         links for download candidates and candidates for further scraping.
  728: 
  729:         This is a handy method to run in a thread.
  730:         """
  731:         while True:
  732:             url = self._to_fetch.get()
  733:             try:
  734:                 if url:
  735:                     page = self.get_page(url)
  736:                     if page is None:  # e.g. after an error
  737:                         continue
  738:                     for link, rel in page.links:
  739:                         if link not in self._seen:
  740:                             try:
  741:                                 self._seen.add(link)
  742:                                 if (not self._process_download(link) and self._should_queue(link, url, rel)):
  743:                                     logger.debug('Queueing %s from %s', link, url)
  744:                                     self._to_fetch.put(link)
  745:                             except MetadataInvalidError:  # e.g. invalid versions
  746:                                 pass
  747:             except Exception as e:  # pragma: no cover
  748:                 self.errors.put(text_type(e))
  749:             finally:
  750:                 # always do this, to avoid hangs :-)
  751:                 self._to_fetch.task_done()
  752:             if not url:
  753:                 # logger.debug('Sentinel seen, quitting.')
  754:                 break
  755: 
  756:     def get_page(self, url):
  757:         """
  758:         Get the HTML for an URL, possibly from an in-memory cache.
  759: 
  760:         XXX TODO Note: this cache is never actually cleared. It's assumed that
  761:         the data won't get stale over the lifetime of a locator instance (not
  762:         necessarily true for the default_locator).
  763:         """
  764:         # http://peak.telecommunity.com/DevCenter/EasyInstall#package-index-api
  765:         scheme, netloc, path, _, _, _ = urlparse(url)
  766:         if scheme == 'file' and os.path.isdir(url2pathname(path)):
  767:             url = urljoin(ensure_slash(url), 'index.html')
  768: 
  769:         if url in self._page_cache:
  770:             result = self._page_cache[url]
  771:             logger.debug('Returning %s from cache: %s', url, result)
  772:         else:
  773:             host = netloc.split(':', 1)[0]
  774:             result = None
  775:             if host in self._bad_hosts:
  776:                 logger.debug('Skipping %s due to bad host %s', url, host)
  777:             else:
  778:                 req = Request(url, headers={'Accept-encoding': 'identity'})
  779:                 try:
  780:                     logger.debug('Fetching %s', url)
  781:                     resp = self.opener.open(req, timeout=self.timeout)
  782:                     logger.debug('Fetched %s', url)
  783:                     headers = resp.info()
  784:                     content_type = headers.get('Content-Type', '')
  785:                     if HTML_CONTENT_TYPE.match(content_type):
  786:                         final_url = resp.geturl()
  787:                         data = resp.read()
  788:                         encoding = headers.get('Content-Encoding')
  789:                         if encoding:
  790:                             decoder = self.decoders[encoding]  # fail if not found
  791:                             data = decoder(data)
  792:                         encoding = 'utf-8'
  793:                         m = CHARSET.search(content_type)
  794:                         if m:
  795:                             encoding = m.group(1)
  796:                         try:
  797:                             data = data.decode(encoding)
  798:                         except UnicodeError:  # pragma: no cover
  799:                             data = data.decode('latin-1')  # fallback
  800:                         result = Page(data, final_url)
  801:                         self._page_cache[final_url] = result
  802:                 except HTTPError as e:
  803:                     if e.code != 404:
  804:                         logger.exception('Fetch failed: %s: %s', url, e)
  805:                 except URLError as e:  # pragma: no cover
  806:                     logger.exception('Fetch failed: %s: %s', url, e)
  807:                     with self._lock:
  808:                         self._bad_hosts.add(host)
  809:                 except Exception as e:  # pragma: no cover
  810:                     logger.exception('Fetch failed: %s: %s', url, e)
  811:                 finally:
  812:                     self._page_cache[url] = result  # even if None (failure)
  813:         return result
  814: 
  815:     _distname_re = re.compile('<a href=[^>]*>([^<]+)<')
  816: 
  817:     def get_distribution_names(self):
  818:         """
  819:         Return all the distribution names known to this locator.
  820:         """
  821:         result = set()
  822:         page = self.get_page(self.base_url)
  823:         if not page:
  824:             raise DistlibException('Unable to get %s' % self.base_url)
  825:         for match in self._distname_re.finditer(page.data):
  826:             result.add(match.group(1))
  827:         return result
  828: 
  829: 
  830: class DirectoryLocator(Locator):
  831:     """
  832:     This class locates distributions in a directory tree.
  833:     """
  834: 
  835:     def __init__(self, path, **kwargs):
  836:         """
  837:         Initialise an instance.
  838:         :param path: The root of the directory tree to search.
  839:         :param kwargs: Passed to the superclass constructor,
  840:                        except for:
  841:                        * recursive - if True (the default), subdirectories are
  842:                          recursed into. If False, only the top-level directory
  843:                          is searched,
  844:         """
  845:         self.recursive = kwargs.pop('recursive', True)
  846:         super(DirectoryLocator, self).__init__(**kwargs)
  847:         path = os.path.abspath(path)
  848:         if not os.path.isdir(path):  # pragma: no cover
  849:             raise DistlibException('Not a directory: %r' % path)
  850:         self.base_dir = path
  851: 
  852:     def should_include(self, filename, parent):
  853:         """
  854:         Should a filename be considered as a candidate for a distribution
  855:         archive? As well as the filename, the directory which contains it
  856:         is provided, though not used by the current implementation.
  857:         """
  858:         return filename.endswith(self.downloadable_extensions)
  859: 
  860:     def _get_project(self, name):
  861:         result = {'urls': {}, 'digests': {}}
  862:         for root, dirs, files in os.walk(self.base_dir):
  863:             for fn in files:
  864:                 if self.should_include(fn, root):
  865:                     fn = os.path.join(root, fn)
  866:                     url = urlunparse(('file', '', pathname2url(os.path.abspath(fn)), '', '', ''))
  867:                     info = self.convert_url_to_download_info(url, name)
  868:                     if info:
  869:                         self._update_version_data(result, info)
  870:             if not self.recursive:
  871:                 break
  872:         return result
  873: 
  874:     def get_distribution_names(self):
  875:         """
  876:         Return all the distribution names known to this locator.
  877:         """
  878:         result = set()
  879:         for root, dirs, files in os.walk(self.base_dir):
  880:             for fn in files:
  881:                 if self.should_include(fn, root):
  882:                     fn = os.path.join(root, fn)
  883:                     url = urlunparse(('file', '', pathname2url(os.path.abspath(fn)), '', '', ''))
  884:                     info = self.convert_url_to_download_info(url, None)
  885:                     if info:
  886:                         result.add(info['name'])
  887:             if not self.recursive:
  888:                 break
  889:         return result
  890: 
  891: 
  892: class JSONLocator(Locator):
  893:     """
  894:     This locator uses special extended metadata (not available on PyPI) and is
  895:     the basis of performant dependency resolution in distlib. Other locators
  896:     require archive downloads before dependencies can be determined! As you
  897:     might imagine, that can be slow.
  898:     """
  899: 
  900:     def get_distribution_names(self):
  901:         """
  902:         Return all the distribution names known to this locator.
  903:         """
  904:         raise NotImplementedError('Not available from this locator')
  905: 
  906:     def _get_project(self, name):
  907:         result = {'urls': {}, 'digests': {}}
  908:         data = get_project_data(name)
  909:         if data:
  910:             for info in data.get('files', []):
  911:                 if info['ptype'] != 'sdist' or info['pyversion'] != 'source':
  912:                     continue
  913:                 # We don't store summary in project metadata as it makes
  914:                 # the data bigger for no benefit during dependency
  915:                 # resolution
  916:                 dist = make_dist(data['name'],
  917:                                  info['version'],
  918:                                  summary=data.get('summary', 'Placeholder for summary'),
  919:                                  scheme=self.scheme)
  920:                 md = dist.metadata
  921:                 md.source_url = info['url']
  922:                 # TODO SHA256 digest
  923:                 if 'digest' in info and info['digest']:
  924:                     dist.digest = ('md5', info['digest'])
  925:                 md.dependencies = info.get('requirements', {})
  926:                 dist.exports = info.get('exports', {})
  927:                 result[dist.version] = dist
  928:                 result['urls'].setdefault(dist.version, set()).add(info['url'])
  929:         return result
  930: 
  931: 
  932: class DistPathLocator(Locator):
  933:     """
  934:     This locator finds installed distributions in a path. It can be useful for
  935:     adding to an :class:`AggregatingLocator`.
  936:     """
  937: 
  938:     def __init__(self, distpath, **kwargs):
  939:         """
  940:         Initialise an instance.
  941: 
  942:         :param distpath: A :class:`DistributionPath` instance to search.
  943:         """
  944:         super(DistPathLocator, self).__init__(**kwargs)
  945:         assert isinstance(distpath, DistributionPath)
  946:         self.distpath = distpath
  947: 
  948:     def _get_project(self, name):
  949:         dist = self.distpath.get_distribution(name)
  950:         if dist is None:
  951:             result = {'urls': {}, 'digests': {}}
  952:         else:
  953:             result = {
  954:                 dist.version: dist,
  955:                 'urls': {
  956:                     dist.version: set([dist.source_url])
  957:                 },
  958:                 'digests': {
  959:                     dist.version: set([None])
  960:                 }
  961:             }
  962:         return result
  963: 
  964: 
  965: class AggregatingLocator(Locator):
  966:     """
  967:     This class allows you to chain and/or merge a list of locators.
  968:     """
  969: 
  970:     def __init__(self, *locators, **kwargs):
  971:         """
  972:         Initialise an instance.
  973: 
  974:         :param locators: The list of locators to search.
  975:         :param kwargs: Passed to the superclass constructor,
  976:                        except for:
  977:                        * merge - if False (the default), the first successful
  978:                          search from any of the locators is returned. If True,
  979:                          the results from all locators are merged (this can be
  980:                          slow).
  981:         """
  982:         self.merge = kwargs.pop('merge', False)
  983:         self.locators = locators
  984:         super(AggregatingLocator, self).__init__(**kwargs)
  985: 
  986:     def clear_cache(self):
  987:         super(AggregatingLocator, self).clear_cache()
  988:         for locator in self.locators:
  989:             locator.clear_cache()
  990: 
  991:     def _set_scheme(self, value):
  992:         self._scheme = value
  993:         for locator in self.locators:
  994:             locator.scheme = value
  995: 
  996:     scheme = property(Locator.scheme.fget, _set_scheme)
  997: 
  998:     def _get_project(self, name):
  999:         result = {}
 1000:         for locator in self.locators:
 1001:             d = locator.get_project(name)
 1002:             if d:
 1003:                 if self.merge:
 1004:                     files = result.get('urls', {})
 1005:                     digests = result.get('digests', {})
 1006:                     # next line could overwrite result['urls'], result['digests']
 1007:                     result.update(d)
 1008:                     df = result.get('urls')
 1009:                     if files and df:
 1010:                         for k, v in files.items():
 1011:                             if k in df:
 1012:                                 df[k] |= v
 1013:                             else:
 1014:                                 df[k] = v
 1015:                     dd = result.get('digests')
 1016:                     if digests and dd:
 1017:                         dd.update(digests)
 1018:                 else:
 1019:                     # See issue #18. If any dists are found and we're looking
 1020:                     # for specific constraints, we only return something if
 1021:                     # a match is found. For example, if a DirectoryLocator
 1022:                     # returns just foo (1.0) while we're looking for
 1023:                     # foo (>= 2.0), we'll pretend there was nothing there so
 1024:                     # that subsequent locators can be queried. Otherwise we
 1025:                     # would just return foo (1.0) which would then lead to a
 1026:                     # failure to find foo (>= 2.0), because other locators
 1027:                     # weren't searched. Note that this only matters when
 1028:                     # merge=False.
 1029:                     if self.matcher is None:
 1030:                         found = True
 1031:                     else:
 1032:                         found = False
 1033:                         for k in d:
 1034:                             if self.matcher.match(k):
 1035:                                 found = True
 1036:                                 break
 1037:                     if found:
 1038:                         result = d
 1039:                         break
 1040:         return result
 1041: 
 1042:     def get_distribution_names(self):
 1043:         """
 1044:         Return all the distribution names known to this locator.
 1045:         """
 1046:         result = set()
 1047:         for locator in self.locators:
 1048:             try:
 1049:                 result |= locator.get_distribution_names()
 1050:             except NotImplementedError:
 1051:                 pass
 1052:         return result
 1053: 
 1054: 
 1055: # We use a legacy scheme simply because most of the dists on PyPI use legacy
 1056: # versions which don't conform to PEP 440.
 1057: default_locator = AggregatingLocator(
 1058:     # JSONLocator(), # don't use as PEP 426 is withdrawn
 1059:     SimpleScrapingLocator('https://pypi.org/simple/', timeout=3.0),
 1060:     scheme='legacy')
 1061: 
 1062: locate = default_locator.locate
 1063: 
 1064: 
 1065: class DependencyFinder(object):
 1066:     """
 1067:     Locate dependencies for distributions.
 1068:     """
 1069: 
 1070:     def __init__(self, locator=None):
 1071:         """
 1072:         Initialise an instance, using the specified locator
 1073:         to locate distributions.
 1074:         """
 1075:         self.locator = locator or default_locator
 1076:         self.scheme = get_scheme(self.locator.scheme)
 1077: 
 1078:     def add_distribution(self, dist):
 1079:         """
 1080:         Add a distribution to the finder. This will update internal information
 1081:         about who provides what.
 1082:         :param dist: The distribution to add.
 1083:         """
 1084:         logger.debug('adding distribution %s', dist)
 1085:         name = dist.key
 1086:         self.dists_by_name[name] = dist
 1087:         self.dists[(name, dist.version)] = dist
 1088:         for p in dist.provides:
 1089:             name, version = parse_name_and_version(p)
 1090:             logger.debug('Add to provided: %s, %s, %s', name, version, dist)
 1091:             self.provided.setdefault(name, set()).add((version, dist))
 1092: 
 1093:     def remove_distribution(self, dist):
 1094:         """
 1095:         Remove a distribution from the finder. This will update internal
 1096:         information about who provides what.
 1097:         :param dist: The distribution to remove.
 1098:         """
 1099:         logger.debug('removing distribution %s', dist)
 1100:         name = dist.key
 1101:         del self.dists_by_name[name]
 1102:         del self.dists[(name, dist.version)]
 1103:         for p in dist.provides:
 1104:             name, version = parse_name_and_version(p)
 1105:             logger.debug('Remove from provided: %s, %s, %s', name, version, dist)
 1106:             s = self.provided[name]
 1107:             s.remove((version, dist))
 1108:             if not s:
 1109:                 del self.provided[name]
 1110: 
 1111:     def get_matcher(self, reqt):
 1112:         """
 1113:         Get a version matcher for a requirement.
 1114:         :param reqt: The requirement
 1115:         :type reqt: str
 1116:         :return: A version matcher (an instance of
 1117:                  :class:`distlib.version.Matcher`).
 1118:         """
 1119:         try:
 1120:             matcher = self.scheme.matcher(reqt)
 1121:         except UnsupportedVersionError:  # pragma: no cover
 1122:             # XXX compat-mode if cannot read the version
 1123:             name = reqt.split()[0]
 1124:             matcher = self.scheme.matcher(name)
 1125:         return matcher
 1126: 
 1127:     def find_providers(self, reqt):
 1128:         """
 1129:         Find the distributions which can fulfill a requirement.
 1130: 
 1131:         :param reqt: The requirement.
 1132:          :type reqt: str
 1133:         :return: A set of distribution which can fulfill the requirement.
 1134:         """
 1135:         matcher = self.get_matcher(reqt)
 1136:         name = matcher.key  # case-insensitive
 1137:         result = set()
 1138:         provided = self.provided
 1139:         if name in provided:
 1140:             for version, provider in provided[name]:
 1141:                 try:
 1142:                     match = matcher.match(version)
 1143:                 except UnsupportedVersionError:
 1144:                     match = False
 1145: 
 1146:                 if match:
 1147:                     result.add(provider)
 1148:                     break
 1149:         return result
 1150: 
 1151:     def try_to_replace(self, provider, other, problems):
 1152:         """
 1153:         Attempt to replace one provider with another. This is typically used
 1154:         when resolving dependencies from multiple sources, e.g. A requires
 1155:         (B >= 1.0) while C requires (B >= 1.1).
 1156: 
 1157:         For successful replacement, ``provider`` must meet all the requirements
 1158:         which ``other`` fulfills.
 1159: 
 1160:         :param provider: The provider we are trying to replace with.
 1161:         :param other: The provider we're trying to replace.
 1162:         :param problems: If False is returned, this will contain what
 1163:                          problems prevented replacement. This is currently
 1164:                          a tuple of the literal string 'cantreplace',
 1165:                          ``provider``, ``other``  and the set of requirements
 1166:                          that ``provider`` couldn't fulfill.
 1167:         :return: True if we can replace ``other`` with ``provider``, else
 1168:                  False.
 1169:         """
 1170:         rlist = self.reqts[other]
 1171:         unmatched = set()
 1172:         for s in rlist:
 1173:             matcher = self.get_matcher(s)
 1174:             if not matcher.match(provider.version):
 1175:                 unmatched.add(s)
 1176:         if unmatched:
 1177:             # can't replace other with provider
 1178:             problems.add(('cantreplace', provider, other, frozenset(unmatched)))
 1179:             result = False
 1180:         else:
 1181:             # can replace other with provider
 1182:             self.remove_distribution(other)
 1183:             del self.reqts[other]
 1184:             for s in rlist:
 1185:                 self.reqts.setdefault(provider, set()).add(s)
 1186:             self.add_distribution(provider)
 1187:             result = True
 1188:         return result
 1189: 
 1190:     def find(self, requirement, meta_extras=None, prereleases=False):
 1191:         """
 1192:         Find a distribution and all distributions it depends on.
 1193: 
 1194:         :param requirement: The requirement specifying the distribution to
 1195:                             find, or a Distribution instance.
 1196:         :param meta_extras: A list of meta extras such as :test:, :build: and
 1197:                             so on.
 1198:         :param prereleases: If ``True``, allow pre-release versions to be
 1199:                             returned - otherwise, don't return prereleases
 1200:                             unless they're all that's available.
 1201: 
 1202:         Return a set of :class:`Distribution` instances and a set of
 1203:         problems.
 1204: 
 1205:         The distributions returned should be such that they have the
 1206:         :attr:`required` attribute set to ``True`` if they were
 1207:         from the ``requirement`` passed to ``find()``, and they have the
 1208:         :attr:`build_time_dependency` attribute set to ``True`` unless they
 1209:         are post-installation dependencies of the ``requirement``.
 1210: 
 1211:         The problems should be a tuple consisting of the string
 1212:         ``'unsatisfied'`` and the requirement which couldn't be satisfied
 1213:         by any distribution known to the locator.
 1214:         """
 1215: 
 1216:         self.provided = {}
 1217:         self.dists = {}
 1218:         self.dists_by_name = {}
 1219:         self.reqts = {}
 1220: 
 1221:         meta_extras = set(meta_extras or [])
 1222:         if ':*:' in meta_extras:
 1223:             meta_extras.remove(':*:')
 1224:             # :meta: and :run: are implicitly included
 1225:             meta_extras |= set([':test:', ':build:', ':dev:'])
 1226: 
 1227:         if isinstance(requirement, Distribution):
 1228:             dist = odist = requirement
 1229:             logger.debug('passed %s as requirement', odist)
 1230:         else:
 1231:             dist = odist = self.locator.locate(requirement, prereleases=prereleases)
 1232:             if dist is None:
 1233:                 raise DistlibException('Unable to locate %r' % requirement)
 1234:             logger.debug('located %s', odist)
 1235:         dist.requested = True
 1236:         problems = set()
 1237:         todo = set([dist])
 1238:         install_dists = set([odist])
 1239:         while todo:
 1240:             dist = todo.pop()
 1241:             name = dist.key  # case-insensitive
 1242:             if name not in self.dists_by_name:
 1243:                 self.add_distribution(dist)
 1244:             else:
 1245:                 # import pdb; pdb.set_trace()
 1246:                 other = self.dists_by_name[name]
 1247:                 if other != dist:
 1248:                     self.try_to_replace(dist, other, problems)
 1249: 
 1250:             ireqts = dist.run_requires | dist.meta_requires
 1251:             sreqts = dist.build_requires
 1252:             ereqts = set()
 1253:             if meta_extras and dist in install_dists:
 1254:                 for key in ('test', 'build', 'dev'):
 1255:                     e = ':%s:' % key
 1256:                     if e in meta_extras:
 1257:                         ereqts |= getattr(dist, '%s_requires' % key)
 1258:             all_reqts = ireqts | sreqts | ereqts
 1259:             for r in all_reqts:
 1260:                 providers = self.find_providers(r)
 1261:                 if not providers:
 1262:                     logger.debug('No providers found for %r', r)
 1263:                     provider = self.locator.locate(r, prereleases=prereleases)
 1264:                     # If no provider is found and we didn't consider
 1265:                     # prereleases, consider them now.
 1266:                     if provider is None and not prereleases:
 1267:                         provider = self.locator.locate(r, prereleases=True)
 1268:                     if provider is None:
 1269:                         logger.debug('Cannot satisfy %r', r)
 1270:                         problems.add(('unsatisfied', r))
 1271:                     else:
 1272:                         n, v = provider.key, provider.version
 1273:                         if (n, v) not in self.dists:
 1274:                             todo.add(provider)
 1275:                         providers.add(provider)
 1276:                         if r in ireqts and dist in install_dists:
 1277:                             install_dists.add(provider)
 1278:                             logger.debug('Adding %s to install_dists', provider.name_and_version)
 1279:                 for p in providers:
 1280:                     name = p.key
 1281:                     if name not in self.dists_by_name:
 1282:                         self.reqts.setdefault(p, set()).add(r)
 1283:                     else:
 1284:                         other = self.dists_by_name[name]
 1285:                         if other != p:
 1286:                             # see if other can be replaced by p
 1287:                             self.try_to_replace(p, other, problems)
 1288: 
 1289:         dists = set(self.dists.values())
 1290:         for dist in dists:
 1291:             dist.build_time_dependency = dist not in install_dists
 1292:             if dist.build_time_dependency:
 1293:                 logger.debug('%s is a build-time dependency only.', dist.name_and_version)
 1294:         logger.debug('find done for %s', odist)
 1295:         return dists, problems
