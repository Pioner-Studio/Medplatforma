    1: from __future__ import annotations
    2: 
    3: from typing import (
    4:     TYPE_CHECKING,
    5:     Any,
    6: )
    7: 
    8: import numpy as np
    9: 
   10: from pandas._libs.lib import infer_dtype
   11: from pandas._libs.tslibs import iNaT
   12: from pandas.errors import NoBufferPresent
   13: from pandas.util._decorators import cache_readonly
   14: 
   15: from pandas.core.dtypes.dtypes import BaseMaskedDtype
   16: 
   17: import pandas as pd
   18: from pandas import (
   19:     ArrowDtype,
   20:     DatetimeTZDtype,
   21: )
   22: from pandas.api.types import is_string_dtype
   23: from pandas.core.interchange.buffer import (
   24:     PandasBuffer,
   25:     PandasBufferPyarrow,
   26: )
   27: from pandas.core.interchange.dataframe_protocol import (
   28:     Column,
   29:     ColumnBuffers,
   30:     ColumnNullType,
   31:     DtypeKind,
   32: )
   33: from pandas.core.interchange.utils import (
   34:     ArrowCTypes,
   35:     Endianness,
   36:     dtype_to_arrow_c_fmt,
   37: )
   38: 
   39: if TYPE_CHECKING:
   40:     from pandas.core.interchange.dataframe_protocol import Buffer
   41: 
   42: _NP_KINDS = {
   43:     "i": DtypeKind.INT,
   44:     "u": DtypeKind.UINT,
   45:     "f": DtypeKind.FLOAT,
   46:     "b": DtypeKind.BOOL,
   47:     "U": DtypeKind.STRING,
   48:     "M": DtypeKind.DATETIME,
   49:     "m": DtypeKind.DATETIME,
   50: }
   51: 
   52: _NULL_DESCRIPTION = {
   53:     DtypeKind.FLOAT: (ColumnNullType.USE_NAN, None),
   54:     DtypeKind.DATETIME: (ColumnNullType.USE_SENTINEL, iNaT),
   55:     DtypeKind.INT: (ColumnNullType.NON_NULLABLE, None),
   56:     DtypeKind.UINT: (ColumnNullType.NON_NULLABLE, None),
   57:     DtypeKind.BOOL: (ColumnNullType.NON_NULLABLE, None),
   58:     # Null values for categoricals are stored as `-1` sentinel values
   59:     # in the category date (e.g., `col.values.codes` is int8 np.ndarray)
   60:     DtypeKind.CATEGORICAL: (ColumnNullType.USE_SENTINEL, -1),
   61:     # follow Arrow in using 1 as valid value and 0 for missing/null value
   62:     DtypeKind.STRING: (ColumnNullType.USE_BYTEMASK, 0),
   63: }
   64: 
   65: _NO_VALIDITY_BUFFER = {
   66:     ColumnNullType.NON_NULLABLE: "This column is non-nullable",
   67:     ColumnNullType.USE_NAN: "This column uses NaN as null",
   68:     ColumnNullType.USE_SENTINEL: "This column uses a sentinel value",
   69: }
   70: 
   71: 
   72: class PandasColumn(Column):
   73:     """
   74:     A column object, with only the methods and properties required by the
   75:     interchange protocol defined.
   76:     A column can contain one or more chunks. Each chunk can contain up to three
   77:     buffers - a data buffer, a mask buffer (depending on null representation),
   78:     and an offsets buffer (if variable-size binary; e.g., variable-length
   79:     strings).
   80:     Note: this Column object can only be produced by ``__dataframe__``, so
   81:           doesn't need its own version or ``__column__`` protocol.
   82:     """
   83: 
   84:     def __init__(self, column: pd.Series, allow_copy: bool = True) -> None:
   85:         """
   86:         Note: doesn't deal with extension arrays yet, just assume a regular
   87:         Series/ndarray for now.
   88:         """
   89:         if isinstance(column, pd.DataFrame):
   90:             raise TypeError(
   91:                 "Expected a Series, got a DataFrame. This likely happened "
   92:                 "because you called __dataframe__ on a DataFrame which, "
   93:                 "after converting column names to string, resulted in duplicated "
   94:                 f"names: {column.columns}. Please rename these columns before "
   95:                 "using the interchange protocol."
   96:             )
   97:         if not isinstance(column, pd.Series):
   98:             raise NotImplementedError(f"Columns of type {type(column)} not handled yet")
   99: 
  100:         # Store the column as a private attribute
  101:         self._col = column
  102:         self._allow_copy = allow_copy
  103: 
  104:     def size(self) -> int:
  105:         """
  106:         Size of the column, in elements.
  107:         """
  108:         return self._col.size
  109: 
  110:     @property
  111:     def offset(self) -> int:
  112:         """
  113:         Offset of first element. Always zero.
  114:         """
  115:         # TODO: chunks are implemented now, probably this should return something
  116:         return 0
  117: 
  118:     @cache_readonly
  119:     def dtype(self) -> tuple[DtypeKind, int, str, str]:
  120:         dtype = self._col.dtype
  121: 
  122:         if isinstance(dtype, pd.CategoricalDtype):
  123:             codes = self._col.values.codes
  124:             (
  125:                 _,
  126:                 bitwidth,
  127:                 c_arrow_dtype_f_str,
  128:                 _,
  129:             ) = self._dtype_from_pandasdtype(codes.dtype)
  130:             return (
  131:                 DtypeKind.CATEGORICAL,
  132:                 bitwidth,
  133:                 c_arrow_dtype_f_str,
  134:                 Endianness.NATIVE,
  135:             )
  136:         elif is_string_dtype(dtype):
  137:             if infer_dtype(self._col) in ("string", "empty"):
  138:                 return (
  139:                     DtypeKind.STRING,
  140:                     8,
  141:                     dtype_to_arrow_c_fmt(dtype),
  142:                     Endianness.NATIVE,
  143:                 )
  144:             raise NotImplementedError("Non-string object dtypes are not supported yet")
  145:         else:
  146:             return self._dtype_from_pandasdtype(dtype)
  147: 
  148:     def _dtype_from_pandasdtype(self, dtype) -> tuple[DtypeKind, int, str, str]:
  149:         """
  150:         See `self.dtype` for details.
  151:         """
  152:         # Note: 'c' (complex) not handled yet (not in array spec v1).
  153:         #       'b', 'B' (bytes), 'S', 'a', (old-style string) 'V' (void) not handled
  154:         #       datetime and timedelta both map to datetime (is timedelta handled?)
  155: 
  156:         kind = _NP_KINDS.get(dtype.kind, None)
  157:         if kind is None:
  158:             # Not a NumPy dtype. Check if it's a categorical maybe
  159:             raise ValueError(f"Data type {dtype} not supported by interchange protocol")
  160:         if isinstance(dtype, ArrowDtype):
  161:             byteorder = dtype.numpy_dtype.byteorder
  162:         elif isinstance(dtype, DatetimeTZDtype):
  163:             byteorder = dtype.base.byteorder  # type: ignore[union-attr]
  164:         elif isinstance(dtype, BaseMaskedDtype):
  165:             byteorder = dtype.numpy_dtype.byteorder
  166:         else:
  167:             byteorder = dtype.byteorder
  168: 
  169:         if dtype == "bool[pyarrow]":
  170:             # return early to avoid the `* 8` below, as this is a bitmask
  171:             # rather than a bytemask
  172:             return (
  173:                 kind,
  174:                 dtype.itemsize,  # pyright: ignore[reportGeneralTypeIssues]
  175:                 ArrowCTypes.BOOL,
  176:                 byteorder,
  177:             )
  178: 
  179:         return kind, dtype.itemsize * 8, dtype_to_arrow_c_fmt(dtype), byteorder
  180: 
  181:     @property
  182:     def describe_categorical(self):
  183:         """
  184:         If the dtype is categorical, there are two options:
  185:         - There are only values in the data buffer.
  186:         - There is a separate non-categorical Column encoding for categorical values.
  187: 
  188:         Raises TypeError if the dtype is not categorical
  189: 
  190:         Content of returned dict:
  191:             - "is_ordered" : bool, whether the ordering of dictionary indices is
  192:                              semantically meaningful.
  193:             - "is_dictionary" : bool, whether a dictionary-style mapping of
  194:                                 categorical values to other objects exists
  195:             - "categories" : Column representing the (implicit) mapping of indices to
  196:                              category values (e.g. an array of cat1, cat2, ...).
  197:                              None if not a dictionary-style categorical.
  198:         """
  199:         if not self.dtype[0] == DtypeKind.CATEGORICAL:
  200:             raise TypeError(
  201:                 "describe_categorical only works on a column with categorical dtype!"
  202:             )
  203: 
  204:         return {
  205:             "is_ordered": self._col.cat.ordered,
  206:             "is_dictionary": True,
  207:             "categories": PandasColumn(pd.Series(self._col.cat.categories)),
  208:         }
  209: 
  210:     @property
  211:     def describe_null(self):
  212:         if isinstance(self._col.dtype, BaseMaskedDtype):
  213:             column_null_dtype = ColumnNullType.USE_BYTEMASK
  214:             null_value = 1
  215:             return column_null_dtype, null_value
  216:         if isinstance(self._col.dtype, ArrowDtype):
  217:             # We already rechunk (if necessary / allowed) upon initialization, so this
  218:             # is already single-chunk by the time we get here.
  219:             if self._col.array._pa_array.chunks[0].buffers()[0] is None:  # type: ignore[attr-defined]
  220:                 return ColumnNullType.NON_NULLABLE, None
  221:             return ColumnNullType.USE_BITMASK, 0
  222:         kind = self.dtype[0]
  223:         try:
  224:             null, value = _NULL_DESCRIPTION[kind]
  225:         except KeyError:
  226:             raise NotImplementedError(f"Data type {kind} not yet supported")
  227: 
  228:         return null, value
  229: 
  230:     @cache_readonly
  231:     def null_count(self) -> int:
  232:         """
  233:         Number of null elements. Should always be known.
  234:         """
  235:         return self._col.isna().sum().item()
  236: 
  237:     @property
  238:     def metadata(self) -> dict[str, pd.Index]:
  239:         """
  240:         Store specific metadata of the column.
  241:         """
  242:         return {"pandas.index": self._col.index}
  243: 
  244:     def num_chunks(self) -> int:
  245:         """
  246:         Return the number of chunks the column consists of.
  247:         """
  248:         return 1
  249: 
  250:     def get_chunks(self, n_chunks: int | None = None):
  251:         """
  252:         Return an iterator yielding the chunks.
  253:         See `DataFrame.get_chunks` for details on ``n_chunks``.
  254:         """
  255:         if n_chunks and n_chunks > 1:
  256:             size = len(self._col)
  257:             step = size // n_chunks
  258:             if size % n_chunks != 0:
  259:                 step += 1
  260:             for start in range(0, step * n_chunks, step):
  261:                 yield PandasColumn(
  262:                     self._col.iloc[start : start + step], self._allow_copy
  263:                 )
  264:         else:
  265:             yield self
  266: 
  267:     def get_buffers(self) -> ColumnBuffers:
  268:         """
  269:         Return a dictionary containing the underlying buffers.
  270:         The returned dictionary has the following contents:
  271:             - "data": a two-element tuple whose first element is a buffer
  272:                       containing the data and whose second element is the data
  273:                       buffer's associated dtype.
  274:             - "validity": a two-element tuple whose first element is a buffer
  275:                           containing mask values indicating missing data and
  276:                           whose second element is the mask value buffer's
  277:                           associated dtype. None if the null representation is
  278:                           not a bit or byte mask.
  279:             - "offsets": a two-element tuple whose first element is a buffer
  280:                          containing the offset values for variable-size binary
  281:                          data (e.g., variable-length strings) and whose second
  282:                          element is the offsets buffer's associated dtype. None
  283:                          if the data buffer does not have an associated offsets
  284:                          buffer.
  285:         """
  286:         buffers: ColumnBuffers = {
  287:             "data": self._get_data_buffer(),
  288:             "validity": None,
  289:             "offsets": None,
  290:         }
  291: 
  292:         try:
  293:             buffers["validity"] = self._get_validity_buffer()
  294:         except NoBufferPresent:
  295:             pass
  296: 
  297:         try:
  298:             buffers["offsets"] = self._get_offsets_buffer()
  299:         except NoBufferPresent:
  300:             pass
  301: 
  302:         return buffers
  303: 
  304:     def _get_data_buffer(
  305:         self,
  306:     ) -> tuple[Buffer, tuple[DtypeKind, int, str, str]]:
  307:         """
  308:         Return the buffer containing the data and the buffer's associated dtype.
  309:         """
  310:         buffer: Buffer
  311:         if self.dtype[0] in (
  312:             DtypeKind.INT,
  313:             DtypeKind.UINT,
  314:             DtypeKind.FLOAT,
  315:             DtypeKind.BOOL,
  316:             DtypeKind.DATETIME,
  317:         ):
  318:             # self.dtype[2] is an ArrowCTypes.TIMESTAMP where the tz will make
  319:             # it longer than 4 characters
  320:             dtype = self.dtype
  321:             if self.dtype[0] == DtypeKind.DATETIME and len(self.dtype[2]) > 4:
  322:                 np_arr = self._col.dt.tz_convert(None).to_numpy()
  323:             else:
  324:                 arr = self._col.array
  325:                 if isinstance(self._col.dtype, BaseMaskedDtype):
  326:                     np_arr = arr._data  # type: ignore[attr-defined]
  327:                 elif isinstance(self._col.dtype, ArrowDtype):
  328:                     # We already rechunk (if necessary / allowed) upon initialization,
  329:                     # so this is already single-chunk by the time we get here.
  330:                     arr = arr._pa_array.chunks[0]  # type: ignore[attr-defined]
  331:                     buffer = PandasBufferPyarrow(
  332:                         arr.buffers()[1],  # type: ignore[attr-defined]
  333:                         length=len(arr),
  334:                     )
  335:                     return buffer, dtype
  336:                 else:
  337:                     np_arr = arr._ndarray  # type: ignore[attr-defined]
  338:             buffer = PandasBuffer(np_arr, allow_copy=self._allow_copy)
  339:         elif self.dtype[0] == DtypeKind.CATEGORICAL:
  340:             codes = self._col.values._codes
  341:             buffer = PandasBuffer(codes, allow_copy=self._allow_copy)
  342:             dtype = self._dtype_from_pandasdtype(codes.dtype)
  343:         elif self.dtype[0] == DtypeKind.STRING:
  344:             # Marshal the strings from a NumPy object array into a byte array
  345:             buf = self._col.to_numpy()
  346:             b = bytearray()
  347: 
  348:             # TODO: this for-loop is slow; can be implemented in Cython/C/C++ later
  349:             for obj in buf:
  350:                 if isinstance(obj, str):
  351:                     b.extend(obj.encode(encoding="utf-8"))
  352: 
  353:             # Convert the byte array to a Pandas "buffer" using
  354:             # a NumPy array as the backing store
  355:             buffer = PandasBuffer(np.frombuffer(b, dtype="uint8"))
  356: 
  357:             # Define the dtype for the returned buffer
  358:             # TODO: this will need correcting
  359:             # https://github.com/pandas-dev/pandas/issues/54781
  360:             dtype = self.dtype
  361:         else:
  362:             raise NotImplementedError(f"Data type {self._col.dtype} not handled yet")
  363: 
  364:         return buffer, dtype
  365: 
  366:     def _get_validity_buffer(self) -> tuple[Buffer, Any] | None:
  367:         """
  368:         Return the buffer containing the mask values indicating missing data and
  369:         the buffer's associated dtype.
  370:         Raises NoBufferPresent if null representation is not a bit or byte mask.
  371:         """
  372:         null, invalid = self.describe_null
  373:         buffer: Buffer
  374:         if isinstance(self._col.dtype, ArrowDtype):
  375:             # We already rechunk (if necessary / allowed) upon initialization, so this
  376:             # is already single-chunk by the time we get here.
  377:             arr = self._col.array._pa_array.chunks[0]  # type: ignore[attr-defined]
  378:             dtype = (DtypeKind.BOOL, 1, ArrowCTypes.BOOL, Endianness.NATIVE)
  379:             if arr.buffers()[0] is None:
  380:                 return None
  381:             buffer = PandasBufferPyarrow(
  382:                 arr.buffers()[0],
  383:                 length=len(arr),
  384:             )
  385:             return buffer, dtype
  386: 
  387:         if isinstance(self._col.dtype, BaseMaskedDtype):
  388:             mask = self._col.array._mask  # type: ignore[attr-defined]
  389:             buffer = PandasBuffer(mask)
  390:             dtype = (DtypeKind.BOOL, 8, ArrowCTypes.BOOL, Endianness.NATIVE)
  391:             return buffer, dtype
  392: 
  393:         if self.dtype[0] == DtypeKind.STRING:
  394:             # For now, use byte array as the mask.
  395:             # TODO: maybe store as bit array to save space?..
  396:             buf = self._col.to_numpy()
  397: 
  398:             # Determine the encoding for valid values
  399:             valid = invalid == 0
  400:             invalid = not valid
  401: 
  402:             mask = np.zeros(shape=(len(buf),), dtype=np.bool_)
  403:             for i, obj in enumerate(buf):
  404:                 mask[i] = valid if isinstance(obj, str) else invalid
  405: 
  406:             # Convert the mask array to a Pandas "buffer" using
  407:             # a NumPy array as the backing store
  408:             buffer = PandasBuffer(mask)
  409: 
  410:             # Define the dtype of the returned buffer
  411:             dtype = (DtypeKind.BOOL, 8, ArrowCTypes.BOOL, Endianness.NATIVE)
  412: 
  413:             return buffer, dtype
  414: 
  415:         try:
  416:             msg = f"{_NO_VALIDITY_BUFFER[null]} so does not have a separate mask"
  417:         except KeyError:
  418:             # TODO: implement for other bit/byte masks?
  419:             raise NotImplementedError("See self.describe_null")
  420: 
  421:         raise NoBufferPresent(msg)
  422: 
  423:     def _get_offsets_buffer(self) -> tuple[PandasBuffer, Any]:
  424:         """
  425:         Return the buffer containing the offset values for variable-size binary
  426:         data (e.g., variable-length strings) and the buffer's associated dtype.
  427:         Raises NoBufferPresent if the data buffer does not have an associated
  428:         offsets buffer.
  429:         """
  430:         if self.dtype[0] == DtypeKind.STRING:
  431:             # For each string, we need to manually determine the next offset
  432:             values = self._col.to_numpy()
  433:             ptr = 0
  434:             offsets = np.zeros(shape=(len(values) + 1,), dtype=np.int64)
  435:             for i, v in enumerate(values):
  436:                 # For missing values (in this case, `np.nan` values)
  437:                 # we don't increment the pointer
  438:                 if isinstance(v, str):
  439:                     b = v.encode(encoding="utf-8")
  440:                     ptr += len(b)
  441: 
  442:                 offsets[i + 1] = ptr
  443: 
  444:             # Convert the offsets to a Pandas "buffer" using
  445:             # the NumPy array as the backing store
  446:             buffer = PandasBuffer(offsets)
  447: 
  448:             # Assemble the buffer dtype info
  449:             dtype = (
  450:                 DtypeKind.INT,
  451:                 64,
  452:                 ArrowCTypes.INT64,
  453:                 Endianness.NATIVE,
  454:             )  # note: currently only support native endianness
  455:         else:
  456:             raise NoBufferPresent(
  457:                 "This column has a fixed-length dtype so "
  458:                 "it does not have an offsets buffer"
  459:             )
  460: 
  461:         return buffer, dtype
