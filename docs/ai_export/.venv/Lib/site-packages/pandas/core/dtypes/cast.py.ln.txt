    1: """
    2: Routines for casting.
    3: """
    4: 
    5: from __future__ import annotations
    6: 
    7: import datetime as dt
    8: import functools
    9: from typing import (
   10:     TYPE_CHECKING,
   11:     Any,
   12:     Literal,
   13:     TypeVar,
   14:     cast,
   15:     overload,
   16: )
   17: import warnings
   18: 
   19: import numpy as np
   20: 
   21: from pandas._config import using_pyarrow_string_dtype
   22: 
   23: from pandas._libs import (
   24:     Interval,
   25:     Period,
   26:     lib,
   27: )
   28: from pandas._libs.missing import (
   29:     NA,
   30:     NAType,
   31:     checknull,
   32: )
   33: from pandas._libs.tslibs import (
   34:     NaT,
   35:     OutOfBoundsDatetime,
   36:     OutOfBoundsTimedelta,
   37:     Timedelta,
   38:     Timestamp,
   39:     is_supported_dtype,
   40: )
   41: from pandas._libs.tslibs.timedeltas import array_to_timedelta64
   42: from pandas.compat.numpy import np_version_gt2
   43: from pandas.errors import (
   44:     IntCastingNaNError,
   45:     LossySetitemError,
   46: )
   47: 
   48: from pandas.core.dtypes.common import (
   49:     ensure_int8,
   50:     ensure_int16,
   51:     ensure_int32,
   52:     ensure_int64,
   53:     ensure_object,
   54:     ensure_str,
   55:     is_bool,
   56:     is_complex,
   57:     is_float,
   58:     is_integer,
   59:     is_object_dtype,
   60:     is_scalar,
   61:     is_string_dtype,
   62:     pandas_dtype as pandas_dtype_func,
   63: )
   64: from pandas.core.dtypes.dtypes import (
   65:     ArrowDtype,
   66:     BaseMaskedDtype,
   67:     CategoricalDtype,
   68:     DatetimeTZDtype,
   69:     ExtensionDtype,
   70:     IntervalDtype,
   71:     PandasExtensionDtype,
   72:     PeriodDtype,
   73: )
   74: from pandas.core.dtypes.generic import (
   75:     ABCExtensionArray,
   76:     ABCIndex,
   77:     ABCSeries,
   78: )
   79: from pandas.core.dtypes.inference import is_list_like
   80: from pandas.core.dtypes.missing import (
   81:     is_valid_na_for_dtype,
   82:     isna,
   83:     na_value_for_dtype,
   84:     notna,
   85: )
   86: 
   87: from pandas.io._util import _arrow_dtype_mapping
   88: 
   89: if TYPE_CHECKING:
   90:     from collections.abc import (
   91:         Sequence,
   92:         Sized,
   93:     )
   94: 
   95:     from pandas._typing import (
   96:         ArrayLike,
   97:         Dtype,
   98:         DtypeObj,
   99:         NumpyIndexT,
  100:         Scalar,
  101:         npt,
  102:     )
  103: 
  104:     from pandas import Index
  105:     from pandas.core.arrays import (
  106:         Categorical,
  107:         DatetimeArray,
  108:         ExtensionArray,
  109:         IntervalArray,
  110:         PeriodArray,
  111:         TimedeltaArray,
  112:     )
  113: 
  114: 
  115: _int8_max = np.iinfo(np.int8).max
  116: _int16_max = np.iinfo(np.int16).max
  117: _int32_max = np.iinfo(np.int32).max
  118: 
  119: _dtype_obj = np.dtype(object)
  120: 
  121: NumpyArrayT = TypeVar("NumpyArrayT", bound=np.ndarray)
  122: 
  123: 
  124: def maybe_convert_platform(
  125:     values: list | tuple | range | np.ndarray | ExtensionArray,
  126: ) -> ArrayLike:
  127:     """try to do platform conversion, allow ndarray or list here"""
  128:     arr: ArrayLike
  129: 
  130:     if isinstance(values, (list, tuple, range)):
  131:         arr = construct_1d_object_array_from_listlike(values)
  132:     else:
  133:         # The caller is responsible for ensuring that we have np.ndarray
  134:         #  or ExtensionArray here.
  135:         arr = values
  136: 
  137:     if arr.dtype == _dtype_obj:
  138:         arr = cast(np.ndarray, arr)
  139:         arr = lib.maybe_convert_objects(arr)
  140: 
  141:     return arr
  142: 
  143: 
  144: def is_nested_object(obj) -> bool:
  145:     """
  146:     return a boolean if we have a nested object, e.g. a Series with 1 or
  147:     more Series elements
  148: 
  149:     This may not be necessarily be performant.
  150: 
  151:     """
  152:     return bool(
  153:         isinstance(obj, ABCSeries)
  154:         and is_object_dtype(obj.dtype)
  155:         and any(isinstance(v, ABCSeries) for v in obj._values)
  156:     )
  157: 
  158: 
  159: def maybe_box_datetimelike(value: Scalar, dtype: Dtype | None = None) -> Scalar:
  160:     """
  161:     Cast scalar to Timestamp or Timedelta if scalar is datetime-like
  162:     and dtype is not object.
  163: 
  164:     Parameters
  165:     ----------
  166:     value : scalar
  167:     dtype : Dtype, optional
  168: 
  169:     Returns
  170:     -------
  171:     scalar
  172:     """
  173:     if dtype == _dtype_obj:
  174:         pass
  175:     elif isinstance(value, (np.datetime64, dt.datetime)):
  176:         value = Timestamp(value)
  177:     elif isinstance(value, (np.timedelta64, dt.timedelta)):
  178:         value = Timedelta(value)
  179: 
  180:     return value
  181: 
  182: 
  183: def maybe_box_native(value: Scalar | None | NAType) -> Scalar | None | NAType:
  184:     """
  185:     If passed a scalar cast the scalar to a python native type.
  186: 
  187:     Parameters
  188:     ----------
  189:     value : scalar or Series
  190: 
  191:     Returns
  192:     -------
  193:     scalar or Series
  194:     """
  195:     if is_float(value):
  196:         value = float(value)
  197:     elif is_integer(value):
  198:         value = int(value)
  199:     elif is_bool(value):
  200:         value = bool(value)
  201:     elif isinstance(value, (np.datetime64, np.timedelta64)):
  202:         value = maybe_box_datetimelike(value)
  203:     elif value is NA:
  204:         value = None
  205:     return value
  206: 
  207: 
  208: def _maybe_unbox_datetimelike(value: Scalar, dtype: DtypeObj) -> Scalar:
  209:     """
  210:     Convert a Timedelta or Timestamp to timedelta64 or datetime64 for setting
  211:     into a numpy array.  Failing to unbox would risk dropping nanoseconds.
  212: 
  213:     Notes
  214:     -----
  215:     Caller is responsible for checking dtype.kind in "mM"
  216:     """
  217:     if is_valid_na_for_dtype(value, dtype):
  218:         # GH#36541: can't fill array directly with pd.NaT
  219:         # > np.empty(10, dtype="datetime64[ns]").fill(pd.NaT)
  220:         # ValueError: cannot convert float NaN to integer
  221:         value = dtype.type("NaT", "ns")
  222:     elif isinstance(value, Timestamp):
  223:         if value.tz is None:
  224:             value = value.to_datetime64()
  225:         elif not isinstance(dtype, DatetimeTZDtype):
  226:             raise TypeError("Cannot unbox tzaware Timestamp to tznaive dtype")
  227:     elif isinstance(value, Timedelta):
  228:         value = value.to_timedelta64()
  229: 
  230:     _disallow_mismatched_datetimelike(value, dtype)
  231:     return value
  232: 
  233: 
  234: def _disallow_mismatched_datetimelike(value, dtype: DtypeObj):
  235:     """
  236:     numpy allows np.array(dt64values, dtype="timedelta64[ns]") and
  237:     vice-versa, but we do not want to allow this, so we need to
  238:     check explicitly
  239:     """
  240:     vdtype = getattr(value, "dtype", None)
  241:     if vdtype is None:
  242:         return
  243:     elif (vdtype.kind == "m" and dtype.kind == "M") or (
  244:         vdtype.kind == "M" and dtype.kind == "m"
  245:     ):
  246:         raise TypeError(f"Cannot cast {repr(value)} to {dtype}")
  247: 
  248: 
  249: @overload
  250: def maybe_downcast_to_dtype(result: np.ndarray, dtype: str | np.dtype) -> np.ndarray:
  251:     ...
  252: 
  253: 
  254: @overload
  255: def maybe_downcast_to_dtype(result: ExtensionArray, dtype: str | np.dtype) -> ArrayLike:
  256:     ...
  257: 
  258: 
  259: def maybe_downcast_to_dtype(result: ArrayLike, dtype: str | np.dtype) -> ArrayLike:
  260:     """
  261:     try to cast to the specified dtype (e.g. convert back to bool/int
  262:     or could be an astype of float64->float32
  263:     """
  264:     if isinstance(result, ABCSeries):
  265:         result = result._values
  266:     do_round = False
  267: 
  268:     if isinstance(dtype, str):
  269:         if dtype == "infer":
  270:             inferred_type = lib.infer_dtype(result, skipna=False)
  271:             if inferred_type == "boolean":
  272:                 dtype = "bool"
  273:             elif inferred_type == "integer":
  274:                 dtype = "int64"
  275:             elif inferred_type == "datetime64":
  276:                 dtype = "datetime64[ns]"
  277:             elif inferred_type in ["timedelta", "timedelta64"]:
  278:                 dtype = "timedelta64[ns]"
  279: 
  280:             # try to upcast here
  281:             elif inferred_type == "floating":
  282:                 dtype = "int64"
  283:                 if issubclass(result.dtype.type, np.number):
  284:                     do_round = True
  285: 
  286:             else:
  287:                 # TODO: complex?  what if result is already non-object?
  288:                 dtype = "object"
  289: 
  290:         dtype = np.dtype(dtype)
  291: 
  292:     if not isinstance(dtype, np.dtype):
  293:         # enforce our signature annotation
  294:         raise TypeError(dtype)  # pragma: no cover
  295: 
  296:     converted = maybe_downcast_numeric(result, dtype, do_round)
  297:     if converted is not result:
  298:         return converted
  299: 
  300:     # a datetimelike
  301:     # GH12821, iNaT is cast to float
  302:     if dtype.kind in "mM" and result.dtype.kind in "if":
  303:         result = result.astype(dtype)
  304: 
  305:     elif dtype.kind == "m" and result.dtype == _dtype_obj:
  306:         # test_where_downcast_to_td64
  307:         result = cast(np.ndarray, result)
  308:         result = array_to_timedelta64(result)
  309: 
  310:     elif dtype == np.dtype("M8[ns]") and result.dtype == _dtype_obj:
  311:         result = cast(np.ndarray, result)
  312:         return np.asarray(maybe_cast_to_datetime(result, dtype=dtype))
  313: 
  314:     return result
  315: 
  316: 
  317: @overload
  318: def maybe_downcast_numeric(
  319:     result: np.ndarray, dtype: np.dtype, do_round: bool = False
  320: ) -> np.ndarray:
  321:     ...
  322: 
  323: 
  324: @overload
  325: def maybe_downcast_numeric(
  326:     result: ExtensionArray, dtype: DtypeObj, do_round: bool = False
  327: ) -> ArrayLike:
  328:     ...
  329: 
  330: 
  331: def maybe_downcast_numeric(
  332:     result: ArrayLike, dtype: DtypeObj, do_round: bool = False
  333: ) -> ArrayLike:
  334:     """
  335:     Subset of maybe_downcast_to_dtype restricted to numeric dtypes.
  336: 
  337:     Parameters
  338:     ----------
  339:     result : ndarray or ExtensionArray
  340:     dtype : np.dtype or ExtensionDtype
  341:     do_round : bool
  342: 
  343:     Returns
  344:     -------
  345:     ndarray or ExtensionArray
  346:     """
  347:     if not isinstance(dtype, np.dtype) or not isinstance(result.dtype, np.dtype):
  348:         # e.g. SparseDtype has no itemsize attr
  349:         return result
  350: 
  351:     def trans(x):
  352:         if do_round:
  353:             return x.round()
  354:         return x
  355: 
  356:     if dtype.kind == result.dtype.kind:
  357:         # don't allow upcasts here (except if empty)
  358:         if result.dtype.itemsize <= dtype.itemsize and result.size:
  359:             return result
  360: 
  361:     if dtype.kind in "biu":
  362:         if not result.size:
  363:             # if we don't have any elements, just astype it
  364:             return trans(result).astype(dtype)
  365: 
  366:         if isinstance(result, np.ndarray):
  367:             element = result.item(0)
  368:         else:
  369:             element = result.iloc[0]
  370:         if not isinstance(element, (np.integer, np.floating, int, float, bool)):
  371:             # a comparable, e.g. a Decimal may slip in here
  372:             return result
  373: 
  374:         if (
  375:             issubclass(result.dtype.type, (np.object_, np.number))
  376:             and notna(result).all()
  377:         ):
  378:             new_result = trans(result).astype(dtype)
  379:             if new_result.dtype.kind == "O" or result.dtype.kind == "O":
  380:                 # np.allclose may raise TypeError on object-dtype
  381:                 if (new_result == result).all():
  382:                     return new_result
  383:             else:
  384:                 if np.allclose(new_result, result, rtol=0):
  385:                     return new_result
  386: 
  387:     elif (
  388:         issubclass(dtype.type, np.floating)
  389:         and result.dtype.kind != "b"
  390:         and not is_string_dtype(result.dtype)
  391:     ):
  392:         with warnings.catch_warnings():
  393:             warnings.filterwarnings(
  394:                 "ignore", "overflow encountered in cast", RuntimeWarning
  395:             )
  396:             new_result = result.astype(dtype)
  397: 
  398:         # Adjust tolerances based on floating point size
  399:         size_tols = {4: 5e-4, 8: 5e-8, 16: 5e-16}
  400: 
  401:         atol = size_tols.get(new_result.dtype.itemsize, 0.0)
  402: 
  403:         # Check downcast float values are still equal within 7 digits when
  404:         # converting from float64 to float32
  405:         if np.allclose(new_result, result, equal_nan=True, rtol=0.0, atol=atol):
  406:             return new_result
  407: 
  408:     elif dtype.kind == result.dtype.kind == "c":
  409:         new_result = result.astype(dtype)
  410: 
  411:         if np.array_equal(new_result, result, equal_nan=True):
  412:             # TODO: use tolerance like we do for float?
  413:             return new_result
  414: 
  415:     return result
  416: 
  417: 
  418: def maybe_upcast_numeric_to_64bit(arr: NumpyIndexT) -> NumpyIndexT:
  419:     """
  420:     If array is a int/uint/float bit size lower than 64 bit, upcast it to 64 bit.
  421: 
  422:     Parameters
  423:     ----------
  424:     arr : ndarray or ExtensionArray
  425: 
  426:     Returns
  427:     -------
  428:     ndarray or ExtensionArray
  429:     """
  430:     dtype = arr.dtype
  431:     if dtype.kind == "i" and dtype != np.int64:
  432:         return arr.astype(np.int64)
  433:     elif dtype.kind == "u" and dtype != np.uint64:
  434:         return arr.astype(np.uint64)
  435:     elif dtype.kind == "f" and dtype != np.float64:
  436:         return arr.astype(np.float64)
  437:     else:
  438:         return arr
  439: 
  440: 
  441: def maybe_cast_pointwise_result(
  442:     result: ArrayLike,
  443:     dtype: DtypeObj,
  444:     numeric_only: bool = False,
  445:     same_dtype: bool = True,
  446: ) -> ArrayLike:
  447:     """
  448:     Try casting result of a pointwise operation back to the original dtype if
  449:     appropriate.
  450: 
  451:     Parameters
  452:     ----------
  453:     result : array-like
  454:         Result to cast.
  455:     dtype : np.dtype or ExtensionDtype
  456:         Input Series from which result was calculated.
  457:     numeric_only : bool, default False
  458:         Whether to cast only numerics or datetimes as well.
  459:     same_dtype : bool, default True
  460:         Specify dtype when calling _from_sequence
  461: 
  462:     Returns
  463:     -------
  464:     result : array-like
  465:         result maybe casted to the dtype.
  466:     """
  467: 
  468:     if isinstance(dtype, ExtensionDtype):
  469:         cls = dtype.construct_array_type()
  470:         if same_dtype:
  471:             result = _maybe_cast_to_extension_array(cls, result, dtype=dtype)
  472:         else:
  473:             result = _maybe_cast_to_extension_array(cls, result)
  474: 
  475:     elif (numeric_only and dtype.kind in "iufcb") or not numeric_only:
  476:         result = maybe_downcast_to_dtype(result, dtype)
  477: 
  478:     return result
  479: 
  480: 
  481: def _maybe_cast_to_extension_array(
  482:     cls: type[ExtensionArray], obj: ArrayLike, dtype: ExtensionDtype | None = None
  483: ) -> ArrayLike:
  484:     """
  485:     Call to `_from_sequence` that returns the object unchanged on Exception.
  486: 
  487:     Parameters
  488:     ----------
  489:     cls : class, subclass of ExtensionArray
  490:     obj : arraylike
  491:         Values to pass to cls._from_sequence
  492:     dtype : ExtensionDtype, optional
  493: 
  494:     Returns
  495:     -------
  496:     ExtensionArray or obj
  497:     """
  498:     result: ArrayLike
  499: 
  500:     if dtype is not None:
  501:         try:
  502:             result = cls._from_scalars(obj, dtype=dtype)
  503:         except (TypeError, ValueError):
  504:             return obj
  505:         return result
  506: 
  507:     try:
  508:         result = cls._from_sequence(obj, dtype=dtype)
  509:     except Exception:
  510:         # We can't predict what downstream EA constructors may raise
  511:         result = obj
  512:     return result
  513: 
  514: 
  515: @overload
  516: def ensure_dtype_can_hold_na(dtype: np.dtype) -> np.dtype:
  517:     ...
  518: 
  519: 
  520: @overload
  521: def ensure_dtype_can_hold_na(dtype: ExtensionDtype) -> ExtensionDtype:
  522:     ...
  523: 
  524: 
  525: def ensure_dtype_can_hold_na(dtype: DtypeObj) -> DtypeObj:
  526:     """
  527:     If we have a dtype that cannot hold NA values, find the best match that can.
  528:     """
  529:     if isinstance(dtype, ExtensionDtype):
  530:         if dtype._can_hold_na:
  531:             return dtype
  532:         elif isinstance(dtype, IntervalDtype):
  533:             # TODO(GH#45349): don't special-case IntervalDtype, allow
  534:             #  overriding instead of returning object below.
  535:             return IntervalDtype(np.float64, closed=dtype.closed)
  536:         return _dtype_obj
  537:     elif dtype.kind == "b":
  538:         return _dtype_obj
  539:     elif dtype.kind in "iu":
  540:         return np.dtype(np.float64)
  541:     return dtype
  542: 
  543: 
  544: _canonical_nans = {
  545:     np.datetime64: np.datetime64("NaT", "ns"),
  546:     np.timedelta64: np.timedelta64("NaT", "ns"),
  547:     type(np.nan): np.nan,
  548: }
  549: 
  550: 
  551: def maybe_promote(dtype: np.dtype, fill_value=np.nan):
  552:     """
  553:     Find the minimal dtype that can hold both the given dtype and fill_value.
  554: 
  555:     Parameters
  556:     ----------
  557:     dtype : np.dtype
  558:     fill_value : scalar, default np.nan
  559: 
  560:     Returns
  561:     -------
  562:     dtype
  563:         Upcasted from dtype argument if necessary.
  564:     fill_value
  565:         Upcasted from fill_value argument if necessary.
  566: 
  567:     Raises
  568:     ------
  569:     ValueError
  570:         If fill_value is a non-scalar and dtype is not object.
  571:     """
  572:     orig = fill_value
  573:     orig_is_nat = False
  574:     if checknull(fill_value):
  575:         # https://github.com/pandas-dev/pandas/pull/39692#issuecomment-1441051740
  576:         #  avoid cache misses with NaN/NaT values that are not singletons
  577:         if fill_value is not NA:
  578:             try:
  579:                 orig_is_nat = np.isnat(fill_value)
  580:             except TypeError:
  581:                 pass
  582: 
  583:         fill_value = _canonical_nans.get(type(fill_value), fill_value)
  584: 
  585:     # for performance, we are using a cached version of the actual implementation
  586:     # of the function in _maybe_promote. However, this doesn't always work (in case
  587:     # of non-hashable arguments), so we fallback to the actual implementation if needed
  588:     try:
  589:         # error: Argument 3 to "__call__" of "_lru_cache_wrapper" has incompatible type
  590:         # "Type[Any]"; expected "Hashable"  [arg-type]
  591:         dtype, fill_value = _maybe_promote_cached(
  592:             dtype, fill_value, type(fill_value)  # type: ignore[arg-type]
  593:         )
  594:     except TypeError:
  595:         # if fill_value is not hashable (required for caching)
  596:         dtype, fill_value = _maybe_promote(dtype, fill_value)
  597: 
  598:     if (dtype == _dtype_obj and orig is not None) or (
  599:         orig_is_nat and np.datetime_data(orig)[0] != "ns"
  600:     ):
  601:         # GH#51592,53497 restore our potentially non-canonical fill_value
  602:         fill_value = orig
  603:     return dtype, fill_value
  604: 
  605: 
  606: @functools.lru_cache
  607: def _maybe_promote_cached(dtype, fill_value, fill_value_type):
  608:     # The cached version of _maybe_promote below
  609:     # This also use fill_value_type as (unused) argument to use this in the
  610:     # cache lookup -> to differentiate 1 and True
  611:     return _maybe_promote(dtype, fill_value)
  612: 
  613: 
  614: def _maybe_promote(dtype: np.dtype, fill_value=np.nan):
  615:     # The actual implementation of the function, use `maybe_promote` above for
  616:     # a cached version.
  617:     if not is_scalar(fill_value):
  618:         # with object dtype there is nothing to promote, and the user can
  619:         #  pass pretty much any weird fill_value they like
  620:         if dtype != object:
  621:             # with object dtype there is nothing to promote, and the user can
  622:             #  pass pretty much any weird fill_value they like
  623:             raise ValueError("fill_value must be a scalar")
  624:         dtype = _dtype_obj
  625:         return dtype, fill_value
  626: 
  627:     if is_valid_na_for_dtype(fill_value, dtype) and dtype.kind in "iufcmM":
  628:         dtype = ensure_dtype_can_hold_na(dtype)
  629:         fv = na_value_for_dtype(dtype)
  630:         return dtype, fv
  631: 
  632:     elif isinstance(dtype, CategoricalDtype):
  633:         if fill_value in dtype.categories or isna(fill_value):
  634:             return dtype, fill_value
  635:         else:
  636:             return object, ensure_object(fill_value)
  637: 
  638:     elif isna(fill_value):
  639:         dtype = _dtype_obj
  640:         if fill_value is None:
  641:             # but we retain e.g. pd.NA
  642:             fill_value = np.nan
  643:         return dtype, fill_value
  644: 
  645:     # returns tuple of (dtype, fill_value)
  646:     if issubclass(dtype.type, np.datetime64):
  647:         inferred, fv = infer_dtype_from_scalar(fill_value)
  648:         if inferred == dtype:
  649:             return dtype, fv
  650: 
  651:         from pandas.core.arrays import DatetimeArray
  652: 
  653:         dta = DatetimeArray._from_sequence([], dtype="M8[ns]")
  654:         try:
  655:             fv = dta._validate_setitem_value(fill_value)
  656:             return dta.dtype, fv
  657:         except (ValueError, TypeError):
  658:             return _dtype_obj, fill_value
  659: 
  660:     elif issubclass(dtype.type, np.timedelta64):
  661:         inferred, fv = infer_dtype_from_scalar(fill_value)
  662:         if inferred == dtype:
  663:             return dtype, fv
  664: 
  665:         elif inferred.kind == "m":
  666:             # different unit, e.g. passed np.timedelta64(24, "h") with dtype=m8[ns]
  667:             # see if we can losslessly cast it to our dtype
  668:             unit = np.datetime_data(dtype)[0]
  669:             try:
  670:                 td = Timedelta(fill_value).as_unit(unit, round_ok=False)
  671:             except OutOfBoundsTimedelta:
  672:                 return _dtype_obj, fill_value
  673:             else:
  674:                 return dtype, td.asm8
  675: 
  676:         return _dtype_obj, fill_value
  677: 
  678:     elif is_float(fill_value):
  679:         if issubclass(dtype.type, np.bool_):
  680:             dtype = np.dtype(np.object_)
  681: 
  682:         elif issubclass(dtype.type, np.integer):
  683:             dtype = np.dtype(np.float64)
  684: 
  685:         elif dtype.kind == "f":
  686:             mst = np.min_scalar_type(fill_value)
  687:             if mst > dtype:
  688:                 # e.g. mst is np.float64 and dtype is np.float32
  689:                 dtype = mst
  690: 
  691:         elif dtype.kind == "c":
  692:             mst = np.min_scalar_type(fill_value)
  693:             dtype = np.promote_types(dtype, mst)
  694: 
  695:     elif is_bool(fill_value):
  696:         if not issubclass(dtype.type, np.bool_):
  697:             dtype = np.dtype(np.object_)
  698: 
  699:     elif is_integer(fill_value):
  700:         if issubclass(dtype.type, np.bool_):
  701:             dtype = np.dtype(np.object_)
  702: 
  703:         elif issubclass(dtype.type, np.integer):
  704:             if not np_can_cast_scalar(fill_value, dtype):  # type: ignore[arg-type]
  705:                 # upcast to prevent overflow
  706:                 mst = np.min_scalar_type(fill_value)
  707:                 dtype = np.promote_types(dtype, mst)
  708:                 if dtype.kind == "f":
  709:                     # Case where we disagree with numpy
  710:                     dtype = np.dtype(np.object_)
  711: 
  712:     elif is_complex(fill_value):
  713:         if issubclass(dtype.type, np.bool_):
  714:             dtype = np.dtype(np.object_)
  715: 
  716:         elif issubclass(dtype.type, (np.integer, np.floating)):
  717:             mst = np.min_scalar_type(fill_value)
  718:             dtype = np.promote_types(dtype, mst)
  719: 
  720:         elif dtype.kind == "c":
  721:             mst = np.min_scalar_type(fill_value)
  722:             if mst > dtype:
  723:                 # e.g. mst is np.complex128 and dtype is np.complex64
  724:                 dtype = mst
  725: 
  726:     else:
  727:         dtype = np.dtype(np.object_)
  728: 
  729:     # in case we have a string that looked like a number
  730:     if issubclass(dtype.type, (bytes, str)):
  731:         dtype = np.dtype(np.object_)
  732: 
  733:     fill_value = _ensure_dtype_type(fill_value, dtype)
  734:     return dtype, fill_value
  735: 
  736: 
  737: def _ensure_dtype_type(value, dtype: np.dtype):
  738:     """
  739:     Ensure that the given value is an instance of the given dtype.
  740: 
  741:     e.g. if out dtype is np.complex64_, we should have an instance of that
  742:     as opposed to a python complex object.
  743: 
  744:     Parameters
  745:     ----------
  746:     value : object
  747:     dtype : np.dtype
  748: 
  749:     Returns
  750:     -------
  751:     object
  752:     """
  753:     # Start with exceptions in which we do _not_ cast to numpy types
  754: 
  755:     if dtype == _dtype_obj:
  756:         return value
  757: 
  758:     # Note: before we get here we have already excluded isna(value)
  759:     return dtype.type(value)
  760: 
  761: 
  762: def infer_dtype_from(val) -> tuple[DtypeObj, Any]:
  763:     """
  764:     Interpret the dtype from a scalar or array.
  765: 
  766:     Parameters
  767:     ----------
  768:     val : object
  769:     """
  770:     if not is_list_like(val):
  771:         return infer_dtype_from_scalar(val)
  772:     return infer_dtype_from_array(val)
  773: 
  774: 
  775: def infer_dtype_from_scalar(val) -> tuple[DtypeObj, Any]:
  776:     """
  777:     Interpret the dtype from a scalar.
  778: 
  779:     Parameters
  780:     ----------
  781:     val : object
  782:     """
  783:     dtype: DtypeObj = _dtype_obj
  784: 
  785:     # a 1-element ndarray
  786:     if isinstance(val, np.ndarray):
  787:         if val.ndim != 0:
  788:             msg = "invalid ndarray passed to infer_dtype_from_scalar"
  789:             raise ValueError(msg)
  790: 
  791:         dtype = val.dtype
  792:         val = lib.item_from_zerodim(val)
  793: 
  794:     elif isinstance(val, str):
  795:         # If we create an empty array using a string to infer
  796:         # the dtype, NumPy will only allocate one character per entry
  797:         # so this is kind of bad. Alternately we could use np.repeat
  798:         # instead of np.empty (but then you still don't want things
  799:         # coming out as np.str_!
  800: 
  801:         dtype = _dtype_obj
  802:         if using_pyarrow_string_dtype():
  803:             from pandas.core.arrays.string_ import StringDtype
  804: 
  805:             dtype = StringDtype(storage="pyarrow_numpy")
  806: 
  807:     elif isinstance(val, (np.datetime64, dt.datetime)):
  808:         try:
  809:             val = Timestamp(val)
  810:         except OutOfBoundsDatetime:
  811:             return _dtype_obj, val
  812: 
  813:         if val is NaT or val.tz is None:
  814:             val = val.to_datetime64()
  815:             dtype = val.dtype
  816:             # TODO: test with datetime(2920, 10, 1) based on test_replace_dtypes
  817:         else:
  818:             dtype = DatetimeTZDtype(unit=val.unit, tz=val.tz)
  819: 
  820:     elif isinstance(val, (np.timedelta64, dt.timedelta)):
  821:         try:
  822:             val = Timedelta(val)
  823:         except (OutOfBoundsTimedelta, OverflowError):
  824:             dtype = _dtype_obj
  825:         else:
  826:             if val is NaT:
  827:                 val = np.timedelta64("NaT", "ns")
  828:             else:
  829:                 val = val.asm8
  830:             dtype = val.dtype
  831: 
  832:     elif is_bool(val):
  833:         dtype = np.dtype(np.bool_)
  834: 
  835:     elif is_integer(val):
  836:         if isinstance(val, np.integer):
  837:             dtype = np.dtype(type(val))
  838:         else:
  839:             dtype = np.dtype(np.int64)
  840: 
  841:         try:
  842:             np.array(val, dtype=dtype)
  843:         except OverflowError:
  844:             dtype = np.array(val).dtype
  845: 
  846:     elif is_float(val):
  847:         if isinstance(val, np.floating):
  848:             dtype = np.dtype(type(val))
  849:         else:
  850:             dtype = np.dtype(np.float64)
  851: 
  852:     elif is_complex(val):
  853:         dtype = np.dtype(np.complex128)
  854: 
  855:     if isinstance(val, Period):
  856:         dtype = PeriodDtype(freq=val.freq)
  857:     elif isinstance(val, Interval):
  858:         subtype = infer_dtype_from_scalar(val.left)[0]
  859:         dtype = IntervalDtype(subtype=subtype, closed=val.closed)
  860: 
  861:     return dtype, val
  862: 
  863: 
  864: def dict_compat(d: dict[Scalar, Scalar]) -> dict[Scalar, Scalar]:
  865:     """
  866:     Convert datetimelike-keyed dicts to a Timestamp-keyed dict.
  867: 
  868:     Parameters
  869:     ----------
  870:     d: dict-like object
  871: 
  872:     Returns
  873:     -------
  874:     dict
  875:     """
  876:     return {maybe_box_datetimelike(key): value for key, value in d.items()}
  877: 
  878: 
  879: def infer_dtype_from_array(arr) -> tuple[DtypeObj, ArrayLike]:
  880:     """
  881:     Infer the dtype from an array.
  882: 
  883:     Parameters
  884:     ----------
  885:     arr : array
  886: 
  887:     Returns
  888:     -------
  889:     tuple (pandas-compat dtype, array)
  890: 
  891: 
  892:     Examples
  893:     --------
  894:     >>> np.asarray([1, '1'])
  895:     array(['1', '1'], dtype='<U21')
  896: 
  897:     >>> infer_dtype_from_array([1, '1'])
  898:     (dtype('O'), [1, '1'])
  899:     """
  900:     if isinstance(arr, np.ndarray):
  901:         return arr.dtype, arr
  902: 
  903:     if not is_list_like(arr):
  904:         raise TypeError("'arr' must be list-like")
  905: 
  906:     arr_dtype = getattr(arr, "dtype", None)
  907:     if isinstance(arr_dtype, ExtensionDtype):
  908:         return arr.dtype, arr
  909: 
  910:     elif isinstance(arr, ABCSeries):
  911:         return arr.dtype, np.asarray(arr)
  912: 
  913:     # don't force numpy coerce with nan's
  914:     inferred = lib.infer_dtype(arr, skipna=False)
  915:     if inferred in ["string", "bytes", "mixed", "mixed-integer"]:
  916:         return (np.dtype(np.object_), arr)
  917: 
  918:     arr = np.asarray(arr)
  919:     return arr.dtype, arr
  920: 
  921: 
  922: def _maybe_infer_dtype_type(element):
  923:     """
  924:     Try to infer an object's dtype, for use in arithmetic ops.
  925: 
  926:     Uses `element.dtype` if that's available.
  927:     Objects implementing the iterator protocol are cast to a NumPy array,
  928:     and from there the array's type is used.
  929: 
  930:     Parameters
  931:     ----------
  932:     element : object
  933:         Possibly has a `.dtype` attribute, and possibly the iterator
  934:         protocol.
  935: 
  936:     Returns
  937:     -------
  938:     tipo : type
  939: 
  940:     Examples
  941:     --------
  942:     >>> from collections import namedtuple
  943:     >>> Foo = namedtuple("Foo", "dtype")
  944:     >>> _maybe_infer_dtype_type(Foo(np.dtype("i8")))
  945:     dtype('int64')
  946:     """
  947:     tipo = None
  948:     if hasattr(element, "dtype"):
  949:         tipo = element.dtype
  950:     elif is_list_like(element):
  951:         element = np.asarray(element)
  952:         tipo = element.dtype
  953:     return tipo
  954: 
  955: 
  956: def invalidate_string_dtypes(dtype_set: set[DtypeObj]) -> None:
  957:     """
  958:     Change string like dtypes to object for
  959:     ``DataFrame.select_dtypes()``.
  960:     """
  961:     # error: Argument 1 to <set> has incompatible type "Type[generic]"; expected
  962:     # "Union[dtype[Any], ExtensionDtype, None]"
  963:     # error: Argument 2 to <set> has incompatible type "Type[generic]"; expected
  964:     # "Union[dtype[Any], ExtensionDtype, None]"
  965:     non_string_dtypes = dtype_set - {
  966:         np.dtype("S").type,  # type: ignore[arg-type]
  967:         np.dtype("<U").type,  # type: ignore[arg-type]
  968:     }
  969:     if non_string_dtypes != dtype_set:
  970:         raise TypeError("string dtypes are not allowed, use 'object' instead")
  971: 
  972: 
  973: def coerce_indexer_dtype(indexer, categories) -> np.ndarray:
  974:     """coerce the indexer input array to the smallest dtype possible"""
  975:     length = len(categories)
  976:     if length < _int8_max:
  977:         return ensure_int8(indexer)
  978:     elif length < _int16_max:
  979:         return ensure_int16(indexer)
  980:     elif length < _int32_max:
  981:         return ensure_int32(indexer)
  982:     return ensure_int64(indexer)
  983: 
  984: 
  985: def convert_dtypes(
  986:     input_array: ArrayLike,
  987:     convert_string: bool = True,
  988:     convert_integer: bool = True,
  989:     convert_boolean: bool = True,
  990:     convert_floating: bool = True,
  991:     infer_objects: bool = False,
  992:     dtype_backend: Literal["numpy_nullable", "pyarrow"] = "numpy_nullable",
  993: ) -> DtypeObj:
  994:     """
  995:     Convert objects to best possible type, and optionally,
  996:     to types supporting ``pd.NA``.
  997: 
  998:     Parameters
  999:     ----------
 1000:     input_array : ExtensionArray or np.ndarray
 1001:     convert_string : bool, default True
 1002:         Whether object dtypes should be converted to ``StringDtype()``.
 1003:     convert_integer : bool, default True
 1004:         Whether, if possible, conversion can be done to integer extension types.
 1005:     convert_boolean : bool, defaults True
 1006:         Whether object dtypes should be converted to ``BooleanDtypes()``.
 1007:     convert_floating : bool, defaults True
 1008:         Whether, if possible, conversion can be done to floating extension types.
 1009:         If `convert_integer` is also True, preference will be give to integer
 1010:         dtypes if the floats can be faithfully casted to integers.
 1011:     infer_objects : bool, defaults False
 1012:         Whether to also infer objects to float/int if possible. Is only hit if the
 1013:         object array contains pd.NA.
 1014:     dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'
 1015:         Back-end data type applied to the resultant :class:`DataFrame`
 1016:         (still experimental). Behaviour is as follows:
 1017: 
 1018:         * ``"numpy_nullable"``: returns nullable-dtype-backed :class:`DataFrame`
 1019:           (default).
 1020:         * ``"pyarrow"``: returns pyarrow-backed nullable :class:`ArrowDtype`
 1021:           DataFrame.
 1022: 
 1023:         .. versionadded:: 2.0
 1024: 
 1025:     Returns
 1026:     -------
 1027:     np.dtype, or ExtensionDtype
 1028:     """
 1029:     inferred_dtype: str | DtypeObj
 1030: 
 1031:     if (
 1032:         convert_string or convert_integer or convert_boolean or convert_floating
 1033:     ) and isinstance(input_array, np.ndarray):
 1034:         if input_array.dtype == object:
 1035:             inferred_dtype = lib.infer_dtype(input_array)
 1036:         else:
 1037:             inferred_dtype = input_array.dtype
 1038: 
 1039:         if is_string_dtype(inferred_dtype):
 1040:             if not convert_string or inferred_dtype == "bytes":
 1041:                 inferred_dtype = input_array.dtype
 1042:             else:
 1043:                 inferred_dtype = pandas_dtype_func("string")
 1044: 
 1045:         if convert_integer:
 1046:             target_int_dtype = pandas_dtype_func("Int64")
 1047: 
 1048:             if input_array.dtype.kind in "iu":
 1049:                 from pandas.core.arrays.integer import NUMPY_INT_TO_DTYPE
 1050: 
 1051:                 inferred_dtype = NUMPY_INT_TO_DTYPE.get(
 1052:                     input_array.dtype, target_int_dtype
 1053:                 )
 1054:             elif input_array.dtype.kind in "fcb":
 1055:                 # TODO: de-dup with maybe_cast_to_integer_array?
 1056:                 arr = input_array[notna(input_array)]
 1057:                 if (arr.astype(int) == arr).all():
 1058:                     inferred_dtype = target_int_dtype
 1059:                 else:
 1060:                     inferred_dtype = input_array.dtype
 1061:             elif (
 1062:                 infer_objects
 1063:                 and input_array.dtype == object
 1064:                 and (isinstance(inferred_dtype, str) and inferred_dtype == "integer")
 1065:             ):
 1066:                 inferred_dtype = target_int_dtype
 1067: 
 1068:         if convert_floating:
 1069:             if input_array.dtype.kind in "fcb":
 1070:                 # i.e. numeric but not integer
 1071:                 from pandas.core.arrays.floating import NUMPY_FLOAT_TO_DTYPE
 1072: 
 1073:                 inferred_float_dtype: DtypeObj = NUMPY_FLOAT_TO_DTYPE.get(
 1074:                     input_array.dtype, pandas_dtype_func("Float64")
 1075:                 )
 1076:                 # if we could also convert to integer, check if all floats
 1077:                 # are actually integers
 1078:                 if convert_integer:
 1079:                     # TODO: de-dup with maybe_cast_to_integer_array?
 1080:                     arr = input_array[notna(input_array)]
 1081:                     if (arr.astype(int) == arr).all():
 1082:                         inferred_dtype = pandas_dtype_func("Int64")
 1083:                     else:
 1084:                         inferred_dtype = inferred_float_dtype
 1085:                 else:
 1086:                     inferred_dtype = inferred_float_dtype
 1087:             elif (
 1088:                 infer_objects
 1089:                 and input_array.dtype == object
 1090:                 and (
 1091:                     isinstance(inferred_dtype, str)
 1092:                     and inferred_dtype == "mixed-integer-float"
 1093:                 )
 1094:             ):
 1095:                 inferred_dtype = pandas_dtype_func("Float64")
 1096: 
 1097:         if convert_boolean:
 1098:             if input_array.dtype.kind == "b":
 1099:                 inferred_dtype = pandas_dtype_func("boolean")
 1100:             elif isinstance(inferred_dtype, str) and inferred_dtype == "boolean":
 1101:                 inferred_dtype = pandas_dtype_func("boolean")
 1102: 
 1103:         if isinstance(inferred_dtype, str):
 1104:             # If we couldn't do anything else, then we retain the dtype
 1105:             inferred_dtype = input_array.dtype
 1106: 
 1107:     else:
 1108:         inferred_dtype = input_array.dtype
 1109: 
 1110:     if dtype_backend == "pyarrow":
 1111:         from pandas.core.arrays.arrow.array import to_pyarrow_type
 1112:         from pandas.core.arrays.string_ import StringDtype
 1113: 
 1114:         assert not isinstance(inferred_dtype, str)
 1115: 
 1116:         if (
 1117:             (convert_integer and inferred_dtype.kind in "iu")
 1118:             or (convert_floating and inferred_dtype.kind in "fc")
 1119:             or (convert_boolean and inferred_dtype.kind == "b")
 1120:             or (convert_string and isinstance(inferred_dtype, StringDtype))
 1121:             or (
 1122:                 inferred_dtype.kind not in "iufcb"
 1123:                 and not isinstance(inferred_dtype, StringDtype)
 1124:             )
 1125:         ):
 1126:             if isinstance(inferred_dtype, PandasExtensionDtype) and not isinstance(
 1127:                 inferred_dtype, DatetimeTZDtype
 1128:             ):
 1129:                 base_dtype = inferred_dtype.base
 1130:             elif isinstance(inferred_dtype, (BaseMaskedDtype, ArrowDtype)):
 1131:                 base_dtype = inferred_dtype.numpy_dtype
 1132:             elif isinstance(inferred_dtype, StringDtype):
 1133:                 base_dtype = np.dtype(str)
 1134:             else:
 1135:                 base_dtype = inferred_dtype
 1136:             if (
 1137:                 base_dtype.kind == "O"  # type: ignore[union-attr]
 1138:                 and input_array.size > 0
 1139:                 and isna(input_array).all()
 1140:             ):
 1141:                 import pyarrow as pa
 1142: 
 1143:                 pa_type = pa.null()
 1144:             else:
 1145:                 pa_type = to_pyarrow_type(base_dtype)
 1146:             if pa_type is not None:
 1147:                 inferred_dtype = ArrowDtype(pa_type)
 1148:     elif dtype_backend == "numpy_nullable" and isinstance(inferred_dtype, ArrowDtype):
 1149:         # GH 53648
 1150:         inferred_dtype = _arrow_dtype_mapping()[inferred_dtype.pyarrow_dtype]
 1151: 
 1152:     # error: Incompatible return value type (got "Union[str, Union[dtype[Any],
 1153:     # ExtensionDtype]]", expected "Union[dtype[Any], ExtensionDtype]")
 1154:     return inferred_dtype  # type: ignore[return-value]
 1155: 
 1156: 
 1157: def maybe_infer_to_datetimelike(
 1158:     value: npt.NDArray[np.object_],
 1159: ) -> np.ndarray | DatetimeArray | TimedeltaArray | PeriodArray | IntervalArray:
 1160:     """
 1161:     we might have a array (or single object) that is datetime like,
 1162:     and no dtype is passed don't change the value unless we find a
 1163:     datetime/timedelta set
 1164: 
 1165:     this is pretty strict in that a datetime/timedelta is REQUIRED
 1166:     in addition to possible nulls/string likes
 1167: 
 1168:     Parameters
 1169:     ----------
 1170:     value : np.ndarray[object]
 1171: 
 1172:     Returns
 1173:     -------
 1174:     np.ndarray, DatetimeArray, TimedeltaArray, PeriodArray, or IntervalArray
 1175: 
 1176:     """
 1177:     if not isinstance(value, np.ndarray) or value.dtype != object:
 1178:         # Caller is responsible for passing only ndarray[object]
 1179:         raise TypeError(type(value))  # pragma: no cover
 1180:     if value.ndim != 1:
 1181:         # Caller is responsible
 1182:         raise ValueError(value.ndim)  # pragma: no cover
 1183: 
 1184:     if not len(value):
 1185:         return value
 1186: 
 1187:     # error: Incompatible return value type (got "Union[ExtensionArray,
 1188:     # ndarray[Any, Any]]", expected "Union[ndarray[Any, Any], DatetimeArray,
 1189:     # TimedeltaArray, PeriodArray, IntervalArray]")
 1190:     return lib.maybe_convert_objects(  # type: ignore[return-value]
 1191:         value,
 1192:         # Here we do not convert numeric dtypes, as if we wanted that,
 1193:         #  numpy would have done it for us.
 1194:         convert_numeric=False,
 1195:         convert_non_numeric=True,
 1196:         dtype_if_all_nat=np.dtype("M8[ns]"),
 1197:     )
 1198: 
 1199: 
 1200: def maybe_cast_to_datetime(
 1201:     value: np.ndarray | list, dtype: np.dtype
 1202: ) -> ExtensionArray | np.ndarray:
 1203:     """
 1204:     try to cast the array/value to a datetimelike dtype, converting float
 1205:     nan to iNaT
 1206: 
 1207:     Caller is responsible for handling ExtensionDtype cases and non dt64/td64
 1208:     cases.
 1209:     """
 1210:     from pandas.core.arrays.datetimes import DatetimeArray
 1211:     from pandas.core.arrays.timedeltas import TimedeltaArray
 1212: 
 1213:     assert dtype.kind in "mM"
 1214:     if not is_list_like(value):
 1215:         raise TypeError("value must be listlike")
 1216: 
 1217:     # TODO: _from_sequence would raise ValueError in cases where
 1218:     #  _ensure_nanosecond_dtype raises TypeError
 1219:     _ensure_nanosecond_dtype(dtype)
 1220: 
 1221:     if lib.is_np_dtype(dtype, "m"):
 1222:         res = TimedeltaArray._from_sequence(value, dtype=dtype)
 1223:         return res
 1224:     else:
 1225:         try:
 1226:             dta = DatetimeArray._from_sequence(value, dtype=dtype)
 1227:         except ValueError as err:
 1228:             # We can give a Series-specific exception message.
 1229:             if "cannot supply both a tz and a timezone-naive dtype" in str(err):
 1230:                 raise ValueError(
 1231:                     "Cannot convert timezone-aware data to "
 1232:                     "timezone-naive dtype. Use "
 1233:                     "pd.Series(values).dt.tz_localize(None) instead."
 1234:                 ) from err
 1235:             raise
 1236: 
 1237:         return dta
 1238: 
 1239: 
 1240: def _ensure_nanosecond_dtype(dtype: DtypeObj) -> None:
 1241:     """
 1242:     Convert dtypes with granularity less than nanosecond to nanosecond
 1243: 
 1244:     >>> _ensure_nanosecond_dtype(np.dtype("M8[us]"))
 1245: 
 1246:     >>> _ensure_nanosecond_dtype(np.dtype("M8[D]"))
 1247:     Traceback (most recent call last):
 1248:         ...
 1249:     TypeError: dtype=datetime64[D] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'
 1250: 
 1251:     >>> _ensure_nanosecond_dtype(np.dtype("m8[ps]"))
 1252:     Traceback (most recent call last):
 1253:         ...
 1254:     TypeError: dtype=timedelta64[ps] is not supported. Supported resolutions are 's', 'ms', 'us', and 'ns'
 1255:     """  # noqa: E501
 1256:     msg = (
 1257:         f"The '{dtype.name}' dtype has no unit. "
 1258:         f"Please pass in '{dtype.name}[ns]' instead."
 1259:     )
 1260: 
 1261:     # unpack e.g. SparseDtype
 1262:     dtype = getattr(dtype, "subtype", dtype)
 1263: 
 1264:     if not isinstance(dtype, np.dtype):
 1265:         # i.e. datetime64tz
 1266:         pass
 1267: 
 1268:     elif dtype.kind in "mM":
 1269:         if not is_supported_dtype(dtype):
 1270:             # pre-2.0 we would silently swap in nanos for lower-resolutions,
 1271:             #  raise for above-nano resolutions
 1272:             if dtype.name in ["datetime64", "timedelta64"]:
 1273:                 raise ValueError(msg)
 1274:             # TODO: ValueError or TypeError? existing test
 1275:             #  test_constructor_generic_timestamp_bad_frequency expects TypeError
 1276:             raise TypeError(
 1277:                 f"dtype={dtype} is not supported. Supported resolutions are 's', "
 1278:                 "'ms', 'us', and 'ns'"
 1279:             )
 1280: 
 1281: 
 1282: # TODO: other value-dependent functions to standardize here include
 1283: #  Index._find_common_type_compat
 1284: def find_result_type(left_dtype: DtypeObj, right: Any) -> DtypeObj:
 1285:     """
 1286:     Find the type/dtype for the result of an operation between objects.
 1287: 
 1288:     This is similar to find_common_type, but looks at the right object instead
 1289:     of just its dtype. This can be useful in particular when the right
 1290:     object does not have a `dtype`.
 1291: 
 1292:     Parameters
 1293:     ----------
 1294:     left_dtype : np.dtype or ExtensionDtype
 1295:     right : Any
 1296: 
 1297:     Returns
 1298:     -------
 1299:     np.dtype or ExtensionDtype
 1300: 
 1301:     See also
 1302:     --------
 1303:     find_common_type
 1304:     numpy.result_type
 1305:     """
 1306:     new_dtype: DtypeObj
 1307: 
 1308:     if (
 1309:         isinstance(left_dtype, np.dtype)
 1310:         and left_dtype.kind in "iuc"
 1311:         and (lib.is_integer(right) or lib.is_float(right))
 1312:     ):
 1313:         # e.g. with int8 dtype and right=512, we want to end up with
 1314:         # np.int16, whereas infer_dtype_from(512) gives np.int64,
 1315:         #  which will make us upcast too far.
 1316:         if lib.is_float(right) and right.is_integer() and left_dtype.kind != "f":
 1317:             right = int(right)
 1318:         # After NEP 50, numpy won't inspect Python scalars
 1319:         # TODO: do we need to recreate numpy's inspection logic for floats too
 1320:         # (this breaks some tests)
 1321:         if isinstance(right, int) and not isinstance(right, np.integer):
 1322:             # This gives an unsigned type by default
 1323:             # (if our number is positive)
 1324: 
 1325:             # If our left dtype is signed, we might not want this since
 1326:             # this might give us 1 dtype too big
 1327:             # We should check if the corresponding int dtype (e.g. int64 for uint64)
 1328:             # can hold the number
 1329:             right_dtype = np.min_scalar_type(right)
 1330:             if right == 0:
 1331:                 # Special case 0
 1332:                 right = left_dtype
 1333:             elif (
 1334:                 not np.issubdtype(left_dtype, np.unsignedinteger)
 1335:                 and 0 < right <= np.iinfo(right_dtype).max
 1336:             ):
 1337:                 # If left dtype isn't unsigned, check if it fits in the signed dtype
 1338:                 right = np.dtype(f"i{right_dtype.itemsize}")
 1339:             else:
 1340:                 right = right_dtype
 1341: 
 1342:         new_dtype = np.result_type(left_dtype, right)
 1343: 
 1344:     elif is_valid_na_for_dtype(right, left_dtype):
 1345:         # e.g. IntervalDtype[int] and None/np.nan
 1346:         new_dtype = ensure_dtype_can_hold_na(left_dtype)
 1347: 
 1348:     else:
 1349:         dtype, _ = infer_dtype_from(right)
 1350:         new_dtype = find_common_type([left_dtype, dtype])
 1351: 
 1352:     return new_dtype
 1353: 
 1354: 
 1355: def common_dtype_categorical_compat(
 1356:     objs: Sequence[Index | ArrayLike], dtype: DtypeObj
 1357: ) -> DtypeObj:
 1358:     """
 1359:     Update the result of find_common_type to account for NAs in a Categorical.
 1360: 
 1361:     Parameters
 1362:     ----------
 1363:     objs : list[np.ndarray | ExtensionArray | Index]
 1364:     dtype : np.dtype or ExtensionDtype
 1365: 
 1366:     Returns
 1367:     -------
 1368:     np.dtype or ExtensionDtype
 1369:     """
 1370:     # GH#38240
 1371: 
 1372:     # TODO: more generally, could do `not can_hold_na(dtype)`
 1373:     if lib.is_np_dtype(dtype, "iu"):
 1374:         for obj in objs:
 1375:             # We don't want to accientally allow e.g. "categorical" str here
 1376:             obj_dtype = getattr(obj, "dtype", None)
 1377:             if isinstance(obj_dtype, CategoricalDtype):
 1378:                 if isinstance(obj, ABCIndex):
 1379:                     # This check may already be cached
 1380:                     hasnas = obj.hasnans
 1381:                 else:
 1382:                     # Categorical
 1383:                     hasnas = cast("Categorical", obj)._hasna
 1384: 
 1385:                 if hasnas:
 1386:                     # see test_union_int_categorical_with_nan
 1387:                     dtype = np.dtype(np.float64)
 1388:                     break
 1389:     return dtype
 1390: 
 1391: 
 1392: def np_find_common_type(*dtypes: np.dtype) -> np.dtype:
 1393:     """
 1394:     np.find_common_type implementation pre-1.25 deprecation using np.result_type
 1395:     https://github.com/pandas-dev/pandas/pull/49569#issuecomment-1308300065
 1396: 
 1397:     Parameters
 1398:     ----------
 1399:     dtypes : np.dtypes
 1400: 
 1401:     Returns
 1402:     -------
 1403:     np.dtype
 1404:     """
 1405:     try:
 1406:         common_dtype = np.result_type(*dtypes)
 1407:         if common_dtype.kind in "mMSU":
 1408:             # NumPy promotion currently (1.25) misbehaves for for times and strings,
 1409:             # so fall back to object (find_common_dtype did unless there
 1410:             # was only one dtype)
 1411:             common_dtype = np.dtype("O")
 1412: 
 1413:     except TypeError:
 1414:         common_dtype = np.dtype("O")
 1415:     return common_dtype
 1416: 
 1417: 
 1418: @overload
 1419: def find_common_type(types: list[np.dtype]) -> np.dtype:
 1420:     ...
 1421: 
 1422: 
 1423: @overload
 1424: def find_common_type(types: list[ExtensionDtype]) -> DtypeObj:
 1425:     ...
 1426: 
 1427: 
 1428: @overload
 1429: def find_common_type(types: list[DtypeObj]) -> DtypeObj:
 1430:     ...
 1431: 
 1432: 
 1433: def find_common_type(types):
 1434:     """
 1435:     Find a common data type among the given dtypes.
 1436: 
 1437:     Parameters
 1438:     ----------
 1439:     types : list of dtypes
 1440: 
 1441:     Returns
 1442:     -------
 1443:     pandas extension or numpy dtype
 1444: 
 1445:     See Also
 1446:     --------
 1447:     numpy.find_common_type
 1448: 
 1449:     """
 1450:     if not types:
 1451:         raise ValueError("no types given")
 1452: 
 1453:     first = types[0]
 1454: 
 1455:     # workaround for find_common_type([np.dtype('datetime64[ns]')] * 2)
 1456:     # => object
 1457:     if lib.dtypes_all_equal(list(types)):
 1458:         return first
 1459: 
 1460:     # get unique types (dict.fromkeys is used as order-preserving set())
 1461:     types = list(dict.fromkeys(types).keys())
 1462: 
 1463:     if any(isinstance(t, ExtensionDtype) for t in types):
 1464:         for t in types:
 1465:             if isinstance(t, ExtensionDtype):
 1466:                 res = t._get_common_dtype(types)
 1467:                 if res is not None:
 1468:                     return res
 1469:         return np.dtype("object")
 1470: 
 1471:     # take lowest unit
 1472:     if all(lib.is_np_dtype(t, "M") for t in types):
 1473:         return np.dtype(max(types))
 1474:     if all(lib.is_np_dtype(t, "m") for t in types):
 1475:         return np.dtype(max(types))
 1476: 
 1477:     # don't mix bool / int or float or complex
 1478:     # this is different from numpy, which casts bool with float/int as int
 1479:     has_bools = any(t.kind == "b" for t in types)
 1480:     if has_bools:
 1481:         for t in types:
 1482:             if t.kind in "iufc":
 1483:                 return np.dtype("object")
 1484: 
 1485:     return np_find_common_type(*types)
 1486: 
 1487: 
 1488: def construct_2d_arraylike_from_scalar(
 1489:     value: Scalar, length: int, width: int, dtype: np.dtype, copy: bool
 1490: ) -> np.ndarray:
 1491:     shape = (length, width)
 1492: 
 1493:     if dtype.kind in "mM":
 1494:         value = _maybe_box_and_unbox_datetimelike(value, dtype)
 1495:     elif dtype == _dtype_obj:
 1496:         if isinstance(value, (np.timedelta64, np.datetime64)):
 1497:             # calling np.array below would cast to pytimedelta/pydatetime
 1498:             out = np.empty(shape, dtype=object)
 1499:             out.fill(value)
 1500:             return out
 1501: 
 1502:     # Attempt to coerce to a numpy array
 1503:     try:
 1504:         if not copy:
 1505:             arr = np.asarray(value, dtype=dtype)
 1506:         else:
 1507:             arr = np.array(value, dtype=dtype, copy=copy)
 1508:     except (ValueError, TypeError) as err:
 1509:         raise TypeError(
 1510:             f"DataFrame constructor called with incompatible data and dtype: {err}"
 1511:         ) from err
 1512: 
 1513:     if arr.ndim != 0:
 1514:         raise ValueError("DataFrame constructor not properly called!")
 1515: 
 1516:     return np.full(shape, arr)
 1517: 
 1518: 
 1519: def construct_1d_arraylike_from_scalar(
 1520:     value: Scalar, length: int, dtype: DtypeObj | None
 1521: ) -> ArrayLike:
 1522:     """
 1523:     create a np.ndarray / pandas type of specified shape and dtype
 1524:     filled with values
 1525: 
 1526:     Parameters
 1527:     ----------
 1528:     value : scalar value
 1529:     length : int
 1530:     dtype : pandas_dtype or np.dtype
 1531: 
 1532:     Returns
 1533:     -------
 1534:     np.ndarray / pandas type of length, filled with value
 1535: 
 1536:     """
 1537: 
 1538:     if dtype is None:
 1539:         try:
 1540:             dtype, value = infer_dtype_from_scalar(value)
 1541:         except OutOfBoundsDatetime:
 1542:             dtype = _dtype_obj
 1543: 
 1544:     if isinstance(dtype, ExtensionDtype):
 1545:         cls = dtype.construct_array_type()
 1546:         seq = [] if length == 0 else [value]
 1547:         subarr = cls._from_sequence(seq, dtype=dtype).repeat(length)
 1548: 
 1549:     else:
 1550:         if length and dtype.kind in "iu" and isna(value):
 1551:             # coerce if we have nan for an integer dtype
 1552:             dtype = np.dtype("float64")
 1553:         elif lib.is_np_dtype(dtype, "US"):
 1554:             # we need to coerce to object dtype to avoid
 1555:             # to allow numpy to take our string as a scalar value
 1556:             dtype = np.dtype("object")
 1557:             if not isna(value):
 1558:                 value = ensure_str(value)
 1559:         elif dtype.kind in "mM":
 1560:             value = _maybe_box_and_unbox_datetimelike(value, dtype)
 1561: 
 1562:         subarr = np.empty(length, dtype=dtype)
 1563:         if length:
 1564:             # GH 47391: numpy > 1.24 will raise filling np.nan into int dtypes
 1565:             subarr.fill(value)
 1566: 
 1567:     return subarr
 1568: 
 1569: 
 1570: def _maybe_box_and_unbox_datetimelike(value: Scalar, dtype: DtypeObj):
 1571:     # Caller is responsible for checking dtype.kind in "mM"
 1572: 
 1573:     if isinstance(value, dt.datetime):
 1574:         # we dont want to box dt64, in particular datetime64("NaT")
 1575:         value = maybe_box_datetimelike(value, dtype)
 1576: 
 1577:     return _maybe_unbox_datetimelike(value, dtype)
 1578: 
 1579: 
 1580: def construct_1d_object_array_from_listlike(values: Sized) -> np.ndarray:
 1581:     """
 1582:     Transform any list-like object in a 1-dimensional numpy array of object
 1583:     dtype.
 1584: 
 1585:     Parameters
 1586:     ----------
 1587:     values : any iterable which has a len()
 1588: 
 1589:     Raises
 1590:     ------
 1591:     TypeError
 1592:         * If `values` does not have a len()
 1593: 
 1594:     Returns
 1595:     -------
 1596:     1-dimensional numpy array of dtype object
 1597:     """
 1598:     # numpy will try to interpret nested lists as further dimensions, hence
 1599:     # making a 1D array that contains list-likes is a bit tricky:
 1600:     result = np.empty(len(values), dtype="object")
 1601:     result[:] = values
 1602:     return result
 1603: 
 1604: 
 1605: def maybe_cast_to_integer_array(arr: list | np.ndarray, dtype: np.dtype) -> np.ndarray:
 1606:     """
 1607:     Takes any dtype and returns the casted version, raising for when data is
 1608:     incompatible with integer/unsigned integer dtypes.
 1609: 
 1610:     Parameters
 1611:     ----------
 1612:     arr : np.ndarray or list
 1613:         The array to cast.
 1614:     dtype : np.dtype
 1615:         The integer dtype to cast the array to.
 1616: 
 1617:     Returns
 1618:     -------
 1619:     ndarray
 1620:         Array of integer or unsigned integer dtype.
 1621: 
 1622:     Raises
 1623:     ------
 1624:     OverflowError : the dtype is incompatible with the data
 1625:     ValueError : loss of precision has occurred during casting
 1626: 
 1627:     Examples
 1628:     --------
 1629:     If you try to coerce negative values to unsigned integers, it raises:
 1630: 
 1631:     >>> pd.Series([-1], dtype="uint64")
 1632:     Traceback (most recent call last):
 1633:         ...
 1634:     OverflowError: Trying to coerce negative values to unsigned integers
 1635: 
 1636:     Also, if you try to coerce float values to integers, it raises:
 1637: 
 1638:     >>> maybe_cast_to_integer_array([1, 2, 3.5], dtype=np.dtype("int64"))
 1639:     Traceback (most recent call last):
 1640:         ...
 1641:     ValueError: Trying to coerce float values to integers
 1642:     """
 1643:     assert dtype.kind in "iu"
 1644: 
 1645:     try:
 1646:         if not isinstance(arr, np.ndarray):
 1647:             with warnings.catch_warnings():
 1648:                 # We already disallow dtype=uint w/ negative numbers
 1649:                 # (test_constructor_coercion_signed_to_unsigned) so safe to ignore.
 1650:                 if not np_version_gt2:
 1651:                     warnings.filterwarnings(
 1652:                         "ignore",
 1653:                         "NumPy will stop allowing conversion of "
 1654:                         "out-of-bound Python int",
 1655:                         DeprecationWarning,
 1656:                     )
 1657:                 casted = np.asarray(arr, dtype=dtype)
 1658:         else:
 1659:             with warnings.catch_warnings():
 1660:                 warnings.filterwarnings("ignore", category=RuntimeWarning)
 1661:                 casted = arr.astype(dtype, copy=False)
 1662:     except OverflowError as err:
 1663:         raise OverflowError(
 1664:             "The elements provided in the data cannot all be "
 1665:             f"casted to the dtype {dtype}"
 1666:         ) from err
 1667: 
 1668:     if isinstance(arr, np.ndarray) and arr.dtype == dtype:
 1669:         # avoid expensive array_equal check
 1670:         return casted
 1671: 
 1672:     with warnings.catch_warnings():
 1673:         warnings.filterwarnings("ignore", category=RuntimeWarning)
 1674:         warnings.filterwarnings(
 1675:             "ignore", "elementwise comparison failed", FutureWarning
 1676:         )
 1677:         if np.array_equal(arr, casted):
 1678:             return casted
 1679: 
 1680:     # We do this casting to allow for proper
 1681:     # data and dtype checking.
 1682:     #
 1683:     # We didn't do this earlier because NumPy
 1684:     # doesn't handle `uint64` correctly.
 1685:     arr = np.asarray(arr)
 1686: 
 1687:     if np.issubdtype(arr.dtype, str):
 1688:         # TODO(numpy-2.0 min): This case will raise an OverflowError above
 1689:         if (casted.astype(str) == arr).all():
 1690:             return casted
 1691:         raise ValueError(f"string values cannot be losslessly cast to {dtype}")
 1692: 
 1693:     if dtype.kind == "u" and (arr < 0).any():
 1694:         # TODO: can this be hit anymore after numpy 2.0?
 1695:         raise OverflowError("Trying to coerce negative values to unsigned integers")
 1696: 
 1697:     if arr.dtype.kind == "f":
 1698:         if not np.isfinite(arr).all():
 1699:             raise IntCastingNaNError(
 1700:                 "Cannot convert non-finite values (NA or inf) to integer"
 1701:             )
 1702:         raise ValueError("Trying to coerce float values to integers")
 1703:     if arr.dtype == object:
 1704:         raise ValueError("Trying to coerce float values to integers")
 1705: 
 1706:     if casted.dtype < arr.dtype:
 1707:         # TODO: Can this path be hit anymore with numpy > 2
 1708:         # GH#41734 e.g. [1, 200, 923442] and dtype="int8" -> overflows
 1709:         raise ValueError(
 1710:             f"Values are too large to be losslessly converted to {dtype}. "
 1711:             f"To cast anyway, use pd.Series(values).astype({dtype})"
 1712:         )
 1713: 
 1714:     if arr.dtype.kind in "mM":
 1715:         # test_constructor_maskedarray_nonfloat
 1716:         raise TypeError(
 1717:             f"Constructing a Series or DataFrame from {arr.dtype} values and "
 1718:             f"dtype={dtype} is not supported. Use values.view({dtype}) instead."
 1719:         )
 1720: 
 1721:     # No known cases that get here, but raising explicitly to cover our bases.
 1722:     raise ValueError(f"values cannot be losslessly cast to {dtype}")
 1723: 
 1724: 
 1725: def can_hold_element(arr: ArrayLike, element: Any) -> bool:
 1726:     """
 1727:     Can we do an inplace setitem with this element in an array with this dtype?
 1728: 
 1729:     Parameters
 1730:     ----------
 1731:     arr : np.ndarray or ExtensionArray
 1732:     element : Any
 1733: 
 1734:     Returns
 1735:     -------
 1736:     bool
 1737:     """
 1738:     dtype = arr.dtype
 1739:     if not isinstance(dtype, np.dtype) or dtype.kind in "mM":
 1740:         if isinstance(dtype, (PeriodDtype, IntervalDtype, DatetimeTZDtype, np.dtype)):
 1741:             # np.dtype here catches datetime64ns and timedelta64ns; we assume
 1742:             #  in this case that we have DatetimeArray/TimedeltaArray
 1743:             arr = cast(
 1744:                 "PeriodArray | DatetimeArray | TimedeltaArray | IntervalArray", arr
 1745:             )
 1746:             try:
 1747:                 arr._validate_setitem_value(element)
 1748:                 return True
 1749:             except (ValueError, TypeError):
 1750:                 return False
 1751: 
 1752:         # This is technically incorrect, but maintains the behavior of
 1753:         # ExtensionBlock._can_hold_element
 1754:         return True
 1755: 
 1756:     try:
 1757:         np_can_hold_element(dtype, element)
 1758:         return True
 1759:     except (TypeError, LossySetitemError):
 1760:         return False
 1761: 
 1762: 
 1763: def np_can_hold_element(dtype: np.dtype, element: Any) -> Any:
 1764:     """
 1765:     Raise if we cannot losslessly set this element into an ndarray with this dtype.
 1766: 
 1767:     Specifically about places where we disagree with numpy.  i.e. there are
 1768:     cases where numpy will raise in doing the setitem that we do not check
 1769:     for here, e.g. setting str "X" into a numeric ndarray.
 1770: 
 1771:     Returns
 1772:     -------
 1773:     Any
 1774:         The element, potentially cast to the dtype.
 1775: 
 1776:     Raises
 1777:     ------
 1778:     ValueError : If we cannot losslessly store this element with this dtype.
 1779:     """
 1780:     if dtype == _dtype_obj:
 1781:         return element
 1782: 
 1783:     tipo = _maybe_infer_dtype_type(element)
 1784: 
 1785:     if dtype.kind in "iu":
 1786:         if isinstance(element, range):
 1787:             if _dtype_can_hold_range(element, dtype):
 1788:                 return element
 1789:             raise LossySetitemError
 1790: 
 1791:         if is_integer(element) or (is_float(element) and element.is_integer()):
 1792:             # e.g. test_setitem_series_int8 if we have a python int 1
 1793:             #  tipo may be np.int32, despite the fact that it will fit
 1794:             #  in smaller int dtypes.
 1795:             info = np.iinfo(dtype)
 1796:             if info.min <= element <= info.max:
 1797:                 return dtype.type(element)
 1798:             raise LossySetitemError
 1799: 
 1800:         if tipo is not None:
 1801:             if tipo.kind not in "iu":
 1802:                 if isinstance(element, np.ndarray) and element.dtype.kind == "f":
 1803:                     # If all can be losslessly cast to integers, then we can hold them
 1804:                     with np.errstate(invalid="ignore"):
 1805:                         # We check afterwards if cast was losslessly, so no need to show
 1806:                         # the warning
 1807:                         casted = element.astype(dtype)
 1808:                     comp = casted == element
 1809:                     if comp.all():
 1810:                         # Return the casted values bc they can be passed to
 1811:                         #  np.putmask, whereas the raw values cannot.
 1812:                         #  see TestSetitemFloatNDarrayIntoIntegerSeries
 1813:                         return casted
 1814:                     raise LossySetitemError
 1815: 
 1816:                 elif isinstance(element, ABCExtensionArray) and isinstance(
 1817:                     element.dtype, CategoricalDtype
 1818:                 ):
 1819:                     # GH#52927 setting Categorical value into non-EA frame
 1820:                     # TODO: general-case for EAs?
 1821:                     try:
 1822:                         casted = element.astype(dtype)
 1823:                     except (ValueError, TypeError):
 1824:                         raise LossySetitemError
 1825:                     # Check for cases of either
 1826:                     #  a) lossy overflow/rounding or
 1827:                     #  b) semantic changes like dt64->int64
 1828:                     comp = casted == element
 1829:                     if not comp.all():
 1830:                         raise LossySetitemError
 1831:                     return casted
 1832: 
 1833:                 # Anything other than integer we cannot hold
 1834:                 raise LossySetitemError
 1835:             if (
 1836:                 dtype.kind == "u"
 1837:                 and isinstance(element, np.ndarray)
 1838:                 and element.dtype.kind == "i"
 1839:             ):
 1840:                 # see test_where_uint64
 1841:                 casted = element.astype(dtype)
 1842:                 if (casted == element).all():
 1843:                     # TODO: faster to check (element >=0).all()?  potential
 1844:                     #  itemsize issues there?
 1845:                     return casted
 1846:                 raise LossySetitemError
 1847:             if dtype.itemsize < tipo.itemsize:
 1848:                 raise LossySetitemError
 1849:             if not isinstance(tipo, np.dtype):
 1850:                 # i.e. nullable IntegerDtype; we can put this into an ndarray
 1851:                 #  losslessly iff it has no NAs
 1852:                 arr = element._values if isinstance(element, ABCSeries) else element
 1853:                 if arr._hasna:
 1854:                     raise LossySetitemError
 1855:                 return element
 1856: 
 1857:             return element
 1858: 
 1859:         raise LossySetitemError
 1860: 
 1861:     if dtype.kind == "f":
 1862:         if lib.is_integer(element) or lib.is_float(element):
 1863:             casted = dtype.type(element)
 1864:             if np.isnan(casted) or casted == element:
 1865:                 return casted
 1866:             # otherwise e.g. overflow see TestCoercionFloat32
 1867:             raise LossySetitemError
 1868: 
 1869:         if tipo is not None:
 1870:             # TODO: itemsize check?
 1871:             if tipo.kind not in "iuf":
 1872:                 # Anything other than float/integer we cannot hold
 1873:                 raise LossySetitemError
 1874:             if not isinstance(tipo, np.dtype):
 1875:                 # i.e. nullable IntegerDtype or FloatingDtype;
 1876:                 #  we can put this into an ndarray losslessly iff it has no NAs
 1877:                 if element._hasna:
 1878:                     raise LossySetitemError
 1879:                 return element
 1880:             elif tipo.itemsize > dtype.itemsize or tipo.kind != dtype.kind:
 1881:                 if isinstance(element, np.ndarray):
 1882:                     # e.g. TestDataFrameIndexingWhere::test_where_alignment
 1883:                     casted = element.astype(dtype)
 1884:                     if np.array_equal(casted, element, equal_nan=True):
 1885:                         return casted
 1886:                     raise LossySetitemError
 1887: 
 1888:             return element
 1889: 
 1890:         raise LossySetitemError
 1891: 
 1892:     if dtype.kind == "c":
 1893:         if lib.is_integer(element) or lib.is_complex(element) or lib.is_float(element):
 1894:             if np.isnan(element):
 1895:                 # see test_where_complex GH#6345
 1896:                 return dtype.type(element)
 1897: 
 1898:             with warnings.catch_warnings():
 1899:                 warnings.filterwarnings("ignore")
 1900:                 casted = dtype.type(element)
 1901:             if casted == element:
 1902:                 return casted
 1903:             # otherwise e.g. overflow see test_32878_complex_itemsize
 1904:             raise LossySetitemError
 1905: 
 1906:         if tipo is not None:
 1907:             if tipo.kind in "iufc":
 1908:                 return element
 1909:             raise LossySetitemError
 1910:         raise LossySetitemError
 1911: 
 1912:     if dtype.kind == "b":
 1913:         if tipo is not None:
 1914:             if tipo.kind == "b":
 1915:                 if not isinstance(tipo, np.dtype):
 1916:                     # i.e. we have a BooleanArray
 1917:                     if element._hasna:
 1918:                         # i.e. there are pd.NA elements
 1919:                         raise LossySetitemError
 1920:                 return element
 1921:             raise LossySetitemError
 1922:         if lib.is_bool(element):
 1923:             return element
 1924:         raise LossySetitemError
 1925: 
 1926:     if dtype.kind == "S":
 1927:         # TODO: test tests.frame.methods.test_replace tests get here,
 1928:         #  need more targeted tests.  xref phofl has a PR about this
 1929:         if tipo is not None:
 1930:             if tipo.kind == "S" and tipo.itemsize <= dtype.itemsize:
 1931:                 return element
 1932:             raise LossySetitemError
 1933:         if isinstance(element, bytes) and len(element) <= dtype.itemsize:
 1934:             return element
 1935:         raise LossySetitemError
 1936: 
 1937:     if dtype.kind == "V":
 1938:         # i.e. np.void, which cannot hold _anything_
 1939:         raise LossySetitemError
 1940: 
 1941:     raise NotImplementedError(dtype)
 1942: 
 1943: 
 1944: def _dtype_can_hold_range(rng: range, dtype: np.dtype) -> bool:
 1945:     """
 1946:     _maybe_infer_dtype_type infers to int64 (and float64 for very large endpoints),
 1947:     but in many cases a range can be held by a smaller integer dtype.
 1948:     Check if this is one of those cases.
 1949:     """
 1950:     if not len(rng):
 1951:         return True
 1952:     return np_can_cast_scalar(rng.start, dtype) and np_can_cast_scalar(rng.stop, dtype)
 1953: 
 1954: 
 1955: def np_can_cast_scalar(element: Scalar, dtype: np.dtype) -> bool:
 1956:     """
 1957:     np.can_cast pandas-equivalent for pre 2-0 behavior that allowed scalar
 1958:     inference
 1959: 
 1960:     Parameters
 1961:     ----------
 1962:     element : Scalar
 1963:     dtype : np.dtype
 1964: 
 1965:     Returns
 1966:     -------
 1967:     bool
 1968:     """
 1969:     try:
 1970:         np_can_hold_element(dtype, element)
 1971:         return True
 1972:     except (LossySetitemError, NotImplementedError):
 1973:         return False
