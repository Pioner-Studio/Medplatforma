    1: """
    2: Define extension dtypes.
    3: """
    4: from __future__ import annotations
    5: 
    6: from datetime import (
    7:     date,
    8:     datetime,
    9:     time,
   10:     timedelta,
   11: )
   12: from decimal import Decimal
   13: import re
   14: from typing import (
   15:     TYPE_CHECKING,
   16:     Any,
   17:     cast,
   18: )
   19: import warnings
   20: 
   21: import numpy as np
   22: import pytz
   23: 
   24: from pandas._libs import (
   25:     lib,
   26:     missing as libmissing,
   27: )
   28: from pandas._libs.interval import Interval
   29: from pandas._libs.properties import cache_readonly
   30: from pandas._libs.tslibs import (
   31:     BaseOffset,
   32:     NaT,
   33:     NaTType,
   34:     Period,
   35:     Timedelta,
   36:     Timestamp,
   37:     timezones,
   38:     to_offset,
   39:     tz_compare,
   40: )
   41: from pandas._libs.tslibs.dtypes import (
   42:     PeriodDtypeBase,
   43:     abbrev_to_npy_unit,
   44: )
   45: from pandas._libs.tslibs.offsets import BDay
   46: from pandas.compat import pa_version_under10p1
   47: from pandas.errors import PerformanceWarning
   48: from pandas.util._exceptions import find_stack_level
   49: 
   50: from pandas.core.dtypes.base import (
   51:     ExtensionDtype,
   52:     StorageExtensionDtype,
   53:     register_extension_dtype,
   54: )
   55: from pandas.core.dtypes.generic import (
   56:     ABCCategoricalIndex,
   57:     ABCIndex,
   58:     ABCRangeIndex,
   59: )
   60: from pandas.core.dtypes.inference import (
   61:     is_bool,
   62:     is_list_like,
   63: )
   64: 
   65: from pandas.util import capitalize_first_letter
   66: 
   67: if not pa_version_under10p1:
   68:     import pyarrow as pa
   69: 
   70: if TYPE_CHECKING:
   71:     from collections.abc import MutableMapping
   72:     from datetime import tzinfo
   73: 
   74:     import pyarrow as pa  # noqa: TCH004
   75: 
   76:     from pandas._typing import (
   77:         Dtype,
   78:         DtypeObj,
   79:         IntervalClosedType,
   80:         Ordered,
   81:         Self,
   82:         npt,
   83:         type_t,
   84:     )
   85: 
   86:     from pandas import (
   87:         Categorical,
   88:         CategoricalIndex,
   89:         DatetimeIndex,
   90:         Index,
   91:         IntervalIndex,
   92:         PeriodIndex,
   93:     )
   94:     from pandas.core.arrays import (
   95:         BaseMaskedArray,
   96:         DatetimeArray,
   97:         IntervalArray,
   98:         NumpyExtensionArray,
   99:         PeriodArray,
  100:         SparseArray,
  101:     )
  102:     from pandas.core.arrays.arrow import ArrowExtensionArray
  103: 
  104: str_type = str
  105: 
  106: 
  107: class PandasExtensionDtype(ExtensionDtype):
  108:     """
  109:     A np.dtype duck-typed class, suitable for holding a custom dtype.
  110: 
  111:     THIS IS NOT A REAL NUMPY DTYPE
  112:     """
  113: 
  114:     type: Any
  115:     kind: Any
  116:     # The Any type annotations above are here only because mypy seems to have a
  117:     # problem dealing with multiple inheritance from PandasExtensionDtype
  118:     # and ExtensionDtype's @properties in the subclasses below. The kind and
  119:     # type variables in those subclasses are explicitly typed below.
  120:     subdtype = None
  121:     str: str_type
  122:     num = 100
  123:     shape: tuple[int, ...] = ()
  124:     itemsize = 8
  125:     base: DtypeObj | None = None
  126:     isbuiltin = 0
  127:     isnative = 0
  128:     _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}
  129: 
  130:     def __repr__(self) -> str_type:
  131:         """
  132:         Return a string representation for a particular object.
  133:         """
  134:         return str(self)
  135: 
  136:     def __hash__(self) -> int:
  137:         raise NotImplementedError("sub-classes should implement an __hash__ method")
  138: 
  139:     def __getstate__(self) -> dict[str_type, Any]:
  140:         # pickle support; we don't want to pickle the cache
  141:         return {k: getattr(self, k, None) for k in self._metadata}
  142: 
  143:     @classmethod
  144:     def reset_cache(cls) -> None:
  145:         """clear the cache"""
  146:         cls._cache_dtypes = {}
  147: 
  148: 
  149: class CategoricalDtypeType(type):
  150:     """
  151:     the type of CategoricalDtype, this metaclass determines subclass ability
  152:     """
  153: 
  154: 
  155: @register_extension_dtype
  156: class CategoricalDtype(PandasExtensionDtype, ExtensionDtype):
  157:     """
  158:     Type for categorical data with the categories and orderedness.
  159: 
  160:     Parameters
  161:     ----------
  162:     categories : sequence, optional
  163:         Must be unique, and must not contain any nulls.
  164:         The categories are stored in an Index,
  165:         and if an index is provided the dtype of that index will be used.
  166:     ordered : bool or None, default False
  167:         Whether or not this categorical is treated as a ordered categorical.
  168:         None can be used to maintain the ordered value of existing categoricals when
  169:         used in operations that combine categoricals, e.g. astype, and will resolve to
  170:         False if there is no existing ordered to maintain.
  171: 
  172:     Attributes
  173:     ----------
  174:     categories
  175:     ordered
  176: 
  177:     Methods
  178:     -------
  179:     None
  180: 
  181:     See Also
  182:     --------
  183:     Categorical : Represent a categorical variable in classic R / S-plus fashion.
  184: 
  185:     Notes
  186:     -----
  187:     This class is useful for specifying the type of a ``Categorical``
  188:     independent of the values. See :ref:`categorical.categoricaldtype`
  189:     for more.
  190: 
  191:     Examples
  192:     --------
  193:     >>> t = pd.CategoricalDtype(categories=['b', 'a'], ordered=True)
  194:     >>> pd.Series(['a', 'b', 'a', 'c'], dtype=t)
  195:     0      a
  196:     1      b
  197:     2      a
  198:     3    NaN
  199:     dtype: category
  200:     Categories (2, object): ['b' < 'a']
  201: 
  202:     An empty CategoricalDtype with a specific dtype can be created
  203:     by providing an empty index. As follows,
  204: 
  205:     >>> pd.CategoricalDtype(pd.DatetimeIndex([])).categories.dtype
  206:     dtype('<M8[ns]')
  207:     """
  208: 
  209:     # TODO: Document public vs. private API
  210:     name = "category"
  211:     type: type[CategoricalDtypeType] = CategoricalDtypeType
  212:     kind: str_type = "O"
  213:     str = "|O08"
  214:     base = np.dtype("O")
  215:     _metadata = ("categories", "ordered")
  216:     _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}
  217:     _supports_2d = False
  218:     _can_fast_transpose = False
  219: 
  220:     def __init__(self, categories=None, ordered: Ordered = False) -> None:
  221:         self._finalize(categories, ordered, fastpath=False)
  222: 
  223:     @classmethod
  224:     def _from_fastpath(
  225:         cls, categories=None, ordered: bool | None = None
  226:     ) -> CategoricalDtype:
  227:         self = cls.__new__(cls)
  228:         self._finalize(categories, ordered, fastpath=True)
  229:         return self
  230: 
  231:     @classmethod
  232:     def _from_categorical_dtype(
  233:         cls, dtype: CategoricalDtype, categories=None, ordered: Ordered | None = None
  234:     ) -> CategoricalDtype:
  235:         if categories is ordered is None:
  236:             return dtype
  237:         if categories is None:
  238:             categories = dtype.categories
  239:         if ordered is None:
  240:             ordered = dtype.ordered
  241:         return cls(categories, ordered)
  242: 
  243:     @classmethod
  244:     def _from_values_or_dtype(
  245:         cls,
  246:         values=None,
  247:         categories=None,
  248:         ordered: bool | None = None,
  249:         dtype: Dtype | None = None,
  250:     ) -> CategoricalDtype:
  251:         """
  252:         Construct dtype from the input parameters used in :class:`Categorical`.
  253: 
  254:         This constructor method specifically does not do the factorization
  255:         step, if that is needed to find the categories. This constructor may
  256:         therefore return ``CategoricalDtype(categories=None, ordered=None)``,
  257:         which may not be useful. Additional steps may therefore have to be
  258:         taken to create the final dtype.
  259: 
  260:         The return dtype is specified from the inputs in this prioritized
  261:         order:
  262:         1. if dtype is a CategoricalDtype, return dtype
  263:         2. if dtype is the string 'category', create a CategoricalDtype from
  264:            the supplied categories and ordered parameters, and return that.
  265:         3. if values is a categorical, use value.dtype, but override it with
  266:            categories and ordered if either/both of those are not None.
  267:         4. if dtype is None and values is not a categorical, construct the
  268:            dtype from categories and ordered, even if either of those is None.
  269: 
  270:         Parameters
  271:         ----------
  272:         values : list-like, optional
  273:             The list-like must be 1-dimensional.
  274:         categories : list-like, optional
  275:             Categories for the CategoricalDtype.
  276:         ordered : bool, optional
  277:             Designating if the categories are ordered.
  278:         dtype : CategoricalDtype or the string "category", optional
  279:             If ``CategoricalDtype``, cannot be used together with
  280:             `categories` or `ordered`.
  281: 
  282:         Returns
  283:         -------
  284:         CategoricalDtype
  285: 
  286:         Examples
  287:         --------
  288:         >>> pd.CategoricalDtype._from_values_or_dtype()
  289:         CategoricalDtype(categories=None, ordered=None, categories_dtype=None)
  290:         >>> pd.CategoricalDtype._from_values_or_dtype(
  291:         ...     categories=['a', 'b'], ordered=True
  292:         ... )
  293:         CategoricalDtype(categories=['a', 'b'], ordered=True, categories_dtype=object)
  294:         >>> dtype1 = pd.CategoricalDtype(['a', 'b'], ordered=True)
  295:         >>> dtype2 = pd.CategoricalDtype(['x', 'y'], ordered=False)
  296:         >>> c = pd.Categorical([0, 1], dtype=dtype1)
  297:         >>> pd.CategoricalDtype._from_values_or_dtype(
  298:         ...     c, ['x', 'y'], ordered=True, dtype=dtype2
  299:         ... )
  300:         Traceback (most recent call last):
  301:             ...
  302:         ValueError: Cannot specify `categories` or `ordered` together with
  303:         `dtype`.
  304: 
  305:         The supplied dtype takes precedence over values' dtype:
  306: 
  307:         >>> pd.CategoricalDtype._from_values_or_dtype(c, dtype=dtype2)
  308:         CategoricalDtype(categories=['x', 'y'], ordered=False, categories_dtype=object)
  309:         """
  310: 
  311:         if dtype is not None:
  312:             # The dtype argument takes precedence over values.dtype (if any)
  313:             if isinstance(dtype, str):
  314:                 if dtype == "category":
  315:                     if ordered is None and cls.is_dtype(values):
  316:                         # GH#49309 preserve orderedness
  317:                         ordered = values.dtype.ordered
  318: 
  319:                     dtype = CategoricalDtype(categories, ordered)
  320:                 else:
  321:                     raise ValueError(f"Unknown dtype {repr(dtype)}")
  322:             elif categories is not None or ordered is not None:
  323:                 raise ValueError(
  324:                     "Cannot specify `categories` or `ordered` together with `dtype`."
  325:                 )
  326:             elif not isinstance(dtype, CategoricalDtype):
  327:                 raise ValueError(f"Cannot not construct CategoricalDtype from {dtype}")
  328:         elif cls.is_dtype(values):
  329:             # If no "dtype" was passed, use the one from "values", but honor
  330:             # the "ordered" and "categories" arguments
  331:             dtype = values.dtype._from_categorical_dtype(
  332:                 values.dtype, categories, ordered
  333:             )
  334:         else:
  335:             # If dtype=None and values is not categorical, create a new dtype.
  336:             # Note: This could potentially have categories=None and
  337:             # ordered=None.
  338:             dtype = CategoricalDtype(categories, ordered)
  339: 
  340:         return cast(CategoricalDtype, dtype)
  341: 
  342:     @classmethod
  343:     def construct_from_string(cls, string: str_type) -> CategoricalDtype:
  344:         """
  345:         Construct a CategoricalDtype from a string.
  346: 
  347:         Parameters
  348:         ----------
  349:         string : str
  350:             Must be the string "category" in order to be successfully constructed.
  351: 
  352:         Returns
  353:         -------
  354:         CategoricalDtype
  355:             Instance of the dtype.
  356: 
  357:         Raises
  358:         ------
  359:         TypeError
  360:             If a CategoricalDtype cannot be constructed from the input.
  361:         """
  362:         if not isinstance(string, str):
  363:             raise TypeError(
  364:                 f"'construct_from_string' expects a string, got {type(string)}"
  365:             )
  366:         if string != cls.name:
  367:             raise TypeError(f"Cannot construct a 'CategoricalDtype' from '{string}'")
  368: 
  369:         # need ordered=None to ensure that operations specifying dtype="category" don't
  370:         # override the ordered value for existing categoricals
  371:         return cls(ordered=None)
  372: 
  373:     def _finalize(self, categories, ordered: Ordered, fastpath: bool = False) -> None:
  374:         if ordered is not None:
  375:             self.validate_ordered(ordered)
  376: 
  377:         if categories is not None:
  378:             categories = self.validate_categories(categories, fastpath=fastpath)
  379: 
  380:         self._categories = categories
  381:         self._ordered = ordered
  382: 
  383:     def __setstate__(self, state: MutableMapping[str_type, Any]) -> None:
  384:         # for pickle compat. __get_state__ is defined in the
  385:         # PandasExtensionDtype superclass and uses the public properties to
  386:         # pickle -> need to set the settable private ones here (see GH26067)
  387:         self._categories = state.pop("categories", None)
  388:         self._ordered = state.pop("ordered", False)
  389: 
  390:     def __hash__(self) -> int:
  391:         # _hash_categories returns a uint64, so use the negative
  392:         # space for when we have unknown categories to avoid a conflict
  393:         if self.categories is None:
  394:             if self.ordered:
  395:                 return -1
  396:             else:
  397:                 return -2
  398:         # We *do* want to include the real self.ordered here
  399:         return int(self._hash_categories)
  400: 
  401:     def __eq__(self, other: object) -> bool:
  402:         """
  403:         Rules for CDT equality:
  404:         1) Any CDT is equal to the string 'category'
  405:         2) Any CDT is equal to itself
  406:         3) Any CDT is equal to a CDT with categories=None regardless of ordered
  407:         4) A CDT with ordered=True is only equal to another CDT with
  408:            ordered=True and identical categories in the same order
  409:         5) A CDT with ordered={False, None} is only equal to another CDT with
  410:            ordered={False, None} and identical categories, but same order is
  411:            not required. There is no distinction between False/None.
  412:         6) Any other comparison returns False
  413:         """
  414:         if isinstance(other, str):
  415:             return other == self.name
  416:         elif other is self:
  417:             return True
  418:         elif not (hasattr(other, "ordered") and hasattr(other, "categories")):
  419:             return False
  420:         elif self.categories is None or other.categories is None:
  421:             # For non-fully-initialized dtypes, these are only equal to
  422:             #  - the string "category" (handled above)
  423:             #  - other CategoricalDtype with categories=None
  424:             return self.categories is other.categories
  425:         elif self.ordered or other.ordered:
  426:             # At least one has ordered=True; equal if both have ordered=True
  427:             # and the same values for categories in the same order.
  428:             return (self.ordered == other.ordered) and self.categories.equals(
  429:                 other.categories
  430:             )
  431:         else:
  432:             # Neither has ordered=True; equal if both have the same categories,
  433:             # but same order is not necessary.  There is no distinction between
  434:             # ordered=False and ordered=None: CDT(., False) and CDT(., None)
  435:             # will be equal if they have the same categories.
  436:             left = self.categories
  437:             right = other.categories
  438: 
  439:             # GH#36280 the ordering of checks here is for performance
  440:             if not left.dtype == right.dtype:
  441:                 return False
  442: 
  443:             if len(left) != len(right):
  444:                 return False
  445: 
  446:             if self.categories.equals(other.categories):
  447:                 # Check and see if they happen to be identical categories
  448:                 return True
  449: 
  450:             if left.dtype != object:
  451:                 # Faster than calculating hash
  452:                 indexer = left.get_indexer(right)
  453:                 # Because left and right have the same length and are unique,
  454:                 #  `indexer` not having any -1s implies that there is a
  455:                 #  bijection between `left` and `right`.
  456:                 return (indexer != -1).all()
  457: 
  458:             # With object-dtype we need a comparison that identifies
  459:             #  e.g. int(2) as distinct from float(2)
  460:             return set(left) == set(right)
  461: 
  462:     def __repr__(self) -> str_type:
  463:         if self.categories is None:
  464:             data = "None"
  465:             dtype = "None"
  466:         else:
  467:             data = self.categories._format_data(name=type(self).__name__)
  468:             if isinstance(self.categories, ABCRangeIndex):
  469:                 data = str(self.categories._range)
  470:             data = data.rstrip(", ")
  471:             dtype = self.categories.dtype
  472: 
  473:         return (
  474:             f"CategoricalDtype(categories={data}, ordered={self.ordered}, "
  475:             f"categories_dtype={dtype})"
  476:         )
  477: 
  478:     @cache_readonly
  479:     def _hash_categories(self) -> int:
  480:         from pandas.core.util.hashing import (
  481:             combine_hash_arrays,
  482:             hash_array,
  483:             hash_tuples,
  484:         )
  485: 
  486:         categories = self.categories
  487:         ordered = self.ordered
  488: 
  489:         if len(categories) and isinstance(categories[0], tuple):
  490:             # assumes if any individual category is a tuple, then all our. ATM
  491:             # I don't really want to support just some of the categories being
  492:             # tuples.
  493:             cat_list = list(categories)  # breaks if a np.array of categories
  494:             cat_array = hash_tuples(cat_list)
  495:         else:
  496:             if categories.dtype == "O" and len({type(x) for x in categories}) != 1:
  497:                 # TODO: hash_array doesn't handle mixed types. It casts
  498:                 # everything to a str first, which means we treat
  499:                 # {'1', '2'} the same as {'1', 2}
  500:                 # find a better solution
  501:                 hashed = hash((tuple(categories), ordered))
  502:                 return hashed
  503: 
  504:             if DatetimeTZDtype.is_dtype(categories.dtype):
  505:                 # Avoid future warning.
  506:                 categories = categories.view("datetime64[ns]")
  507: 
  508:             cat_array = hash_array(np.asarray(categories), categorize=False)
  509:         if ordered:
  510:             cat_array = np.vstack(
  511:                 [cat_array, np.arange(len(cat_array), dtype=cat_array.dtype)]
  512:             )
  513:         else:
  514:             cat_array = np.array([cat_array])
  515:         combined_hashed = combine_hash_arrays(iter(cat_array), num_items=len(cat_array))
  516:         return np.bitwise_xor.reduce(combined_hashed)
  517: 
  518:     @classmethod
  519:     def construct_array_type(cls) -> type_t[Categorical]:
  520:         """
  521:         Return the array type associated with this dtype.
  522: 
  523:         Returns
  524:         -------
  525:         type
  526:         """
  527:         from pandas import Categorical
  528: 
  529:         return Categorical
  530: 
  531:     @staticmethod
  532:     def validate_ordered(ordered: Ordered) -> None:
  533:         """
  534:         Validates that we have a valid ordered parameter. If
  535:         it is not a boolean, a TypeError will be raised.
  536: 
  537:         Parameters
  538:         ----------
  539:         ordered : object
  540:             The parameter to be verified.
  541: 
  542:         Raises
  543:         ------
  544:         TypeError
  545:             If 'ordered' is not a boolean.
  546:         """
  547:         if not is_bool(ordered):
  548:             raise TypeError("'ordered' must either be 'True' or 'False'")
  549: 
  550:     @staticmethod
  551:     def validate_categories(categories, fastpath: bool = False) -> Index:
  552:         """
  553:         Validates that we have good categories
  554: 
  555:         Parameters
  556:         ----------
  557:         categories : array-like
  558:         fastpath : bool
  559:             Whether to skip nan and uniqueness checks
  560: 
  561:         Returns
  562:         -------
  563:         categories : Index
  564:         """
  565:         from pandas.core.indexes.base import Index
  566: 
  567:         if not fastpath and not is_list_like(categories):
  568:             raise TypeError(
  569:                 f"Parameter 'categories' must be list-like, was {repr(categories)}"
  570:             )
  571:         if not isinstance(categories, ABCIndex):
  572:             categories = Index._with_infer(categories, tupleize_cols=False)
  573: 
  574:         if not fastpath:
  575:             if categories.hasnans:
  576:                 raise ValueError("Categorical categories cannot be null")
  577: 
  578:             if not categories.is_unique:
  579:                 raise ValueError("Categorical categories must be unique")
  580: 
  581:         if isinstance(categories, ABCCategoricalIndex):
  582:             categories = categories.categories
  583: 
  584:         return categories
  585: 
  586:     def update_dtype(self, dtype: str_type | CategoricalDtype) -> CategoricalDtype:
  587:         """
  588:         Returns a CategoricalDtype with categories and ordered taken from dtype
  589:         if specified, otherwise falling back to self if unspecified
  590: 
  591:         Parameters
  592:         ----------
  593:         dtype : CategoricalDtype
  594: 
  595:         Returns
  596:         -------
  597:         new_dtype : CategoricalDtype
  598:         """
  599:         if isinstance(dtype, str) and dtype == "category":
  600:             # dtype='category' should not change anything
  601:             return self
  602:         elif not self.is_dtype(dtype):
  603:             raise ValueError(
  604:                 f"a CategoricalDtype must be passed to perform an update, "
  605:                 f"got {repr(dtype)}"
  606:             )
  607:         else:
  608:             # from here on, dtype is a CategoricalDtype
  609:             dtype = cast(CategoricalDtype, dtype)
  610: 
  611:         # update categories/ordered unless they've been explicitly passed as None
  612:         new_categories = (
  613:             dtype.categories if dtype.categories is not None else self.categories
  614:         )
  615:         new_ordered = dtype.ordered if dtype.ordered is not None else self.ordered
  616: 
  617:         return CategoricalDtype(new_categories, new_ordered)
  618: 
  619:     @property
  620:     def categories(self) -> Index:
  621:         """
  622:         An ``Index`` containing the unique categories allowed.
  623: 
  624:         Examples
  625:         --------
  626:         >>> cat_type = pd.CategoricalDtype(categories=['a', 'b'], ordered=True)
  627:         >>> cat_type.categories
  628:         Index(['a', 'b'], dtype='object')
  629:         """
  630:         return self._categories
  631: 
  632:     @property
  633:     def ordered(self) -> Ordered:
  634:         """
  635:         Whether the categories have an ordered relationship.
  636: 
  637:         Examples
  638:         --------
  639:         >>> cat_type = pd.CategoricalDtype(categories=['a', 'b'], ordered=True)
  640:         >>> cat_type.ordered
  641:         True
  642: 
  643:         >>> cat_type = pd.CategoricalDtype(categories=['a', 'b'], ordered=False)
  644:         >>> cat_type.ordered
  645:         False
  646:         """
  647:         return self._ordered
  648: 
  649:     @property
  650:     def _is_boolean(self) -> bool:
  651:         from pandas.core.dtypes.common import is_bool_dtype
  652: 
  653:         return is_bool_dtype(self.categories)
  654: 
  655:     def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:
  656:         # check if we have all categorical dtype with identical categories
  657:         if all(isinstance(x, CategoricalDtype) for x in dtypes):
  658:             first = dtypes[0]
  659:             if all(first == other for other in dtypes[1:]):
  660:                 return first
  661: 
  662:         # special case non-initialized categorical
  663:         # TODO we should figure out the expected return value in general
  664:         non_init_cats = [
  665:             isinstance(x, CategoricalDtype) and x.categories is None for x in dtypes
  666:         ]
  667:         if all(non_init_cats):
  668:             return self
  669:         elif any(non_init_cats):
  670:             return None
  671: 
  672:         # categorical is aware of Sparse -> extract sparse subdtypes
  673:         dtypes = [x.subtype if isinstance(x, SparseDtype) else x for x in dtypes]
  674:         # extract the categories' dtype
  675:         non_cat_dtypes = [
  676:             x.categories.dtype if isinstance(x, CategoricalDtype) else x for x in dtypes
  677:         ]
  678:         # TODO should categorical always give an answer?
  679:         from pandas.core.dtypes.cast import find_common_type
  680: 
  681:         return find_common_type(non_cat_dtypes)
  682: 
  683:     @cache_readonly
  684:     def index_class(self) -> type_t[CategoricalIndex]:
  685:         from pandas import CategoricalIndex
  686: 
  687:         return CategoricalIndex
  688: 
  689: 
  690: @register_extension_dtype
  691: class DatetimeTZDtype(PandasExtensionDtype):
  692:     """
  693:     An ExtensionDtype for timezone-aware datetime data.
  694: 
  695:     **This is not an actual numpy dtype**, but a duck type.
  696: 
  697:     Parameters
  698:     ----------
  699:     unit : str, default "ns"
  700:         The precision of the datetime data. Currently limited
  701:         to ``"ns"``.
  702:     tz : str, int, or datetime.tzinfo
  703:         The timezone.
  704: 
  705:     Attributes
  706:     ----------
  707:     unit
  708:     tz
  709: 
  710:     Methods
  711:     -------
  712:     None
  713: 
  714:     Raises
  715:     ------
  716:     ZoneInfoNotFoundError
  717:         When the requested timezone cannot be found.
  718: 
  719:     Examples
  720:     --------
  721:     >>> from zoneinfo import ZoneInfo
  722:     >>> pd.DatetimeTZDtype(tz=ZoneInfo('UTC'))
  723:     datetime64[ns, UTC]
  724: 
  725:     >>> pd.DatetimeTZDtype(tz=ZoneInfo('Europe/Paris'))
  726:     datetime64[ns, Europe/Paris]
  727:     """
  728: 
  729:     type: type[Timestamp] = Timestamp
  730:     kind: str_type = "M"
  731:     num = 101
  732:     _metadata = ("unit", "tz")
  733:     _match = re.compile(r"(datetime64|M8)\[(?P<unit>.+), (?P<tz>.+)\]")
  734:     _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}
  735:     _supports_2d = True
  736:     _can_fast_transpose = True
  737: 
  738:     @property
  739:     def na_value(self) -> NaTType:
  740:         return NaT
  741: 
  742:     @cache_readonly
  743:     def base(self) -> DtypeObj:  # type: ignore[override]
  744:         return np.dtype(f"M8[{self.unit}]")
  745: 
  746:     # error: Signature of "str" incompatible with supertype "PandasExtensionDtype"
  747:     @cache_readonly
  748:     def str(self) -> str:  # type: ignore[override]
  749:         return f"|M8[{self.unit}]"
  750: 
  751:     def __init__(self, unit: str_type | DatetimeTZDtype = "ns", tz=None) -> None:
  752:         if isinstance(unit, DatetimeTZDtype):
  753:             # error: "str" has no attribute "tz"
  754:             unit, tz = unit.unit, unit.tz  # type: ignore[attr-defined]
  755: 
  756:         if unit != "ns":
  757:             if isinstance(unit, str) and tz is None:
  758:                 # maybe a string like datetime64[ns, tz], which we support for
  759:                 # now.
  760:                 result = type(self).construct_from_string(unit)
  761:                 unit = result.unit
  762:                 tz = result.tz
  763:                 msg = (
  764:                     f"Passing a dtype alias like 'datetime64[ns, {tz}]' "
  765:                     "to DatetimeTZDtype is no longer supported. Use "
  766:                     "'DatetimeTZDtype.construct_from_string()' instead."
  767:                 )
  768:                 raise ValueError(msg)
  769:             if unit not in ["s", "ms", "us", "ns"]:
  770:                 raise ValueError("DatetimeTZDtype only supports s, ms, us, ns units")
  771: 
  772:         if tz:
  773:             tz = timezones.maybe_get_tz(tz)
  774:             tz = timezones.tz_standardize(tz)
  775:         elif tz is not None:
  776:             raise pytz.UnknownTimeZoneError(tz)
  777:         if tz is None:
  778:             raise TypeError("A 'tz' is required.")
  779: 
  780:         self._unit = unit
  781:         self._tz = tz
  782: 
  783:     @cache_readonly
  784:     def _creso(self) -> int:
  785:         """
  786:         The NPY_DATETIMEUNIT corresponding to this dtype's resolution.
  787:         """
  788:         return abbrev_to_npy_unit(self.unit)
  789: 
  790:     @property
  791:     def unit(self) -> str_type:
  792:         """
  793:         The precision of the datetime data.
  794: 
  795:         Examples
  796:         --------
  797:         >>> from zoneinfo import ZoneInfo
  798:         >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))
  799:         >>> dtype.unit
  800:         'ns'
  801:         """
  802:         return self._unit
  803: 
  804:     @property
  805:     def tz(self) -> tzinfo:
  806:         """
  807:         The timezone.
  808: 
  809:         Examples
  810:         --------
  811:         >>> from zoneinfo import ZoneInfo
  812:         >>> dtype = pd.DatetimeTZDtype(tz=ZoneInfo('America/Los_Angeles'))
  813:         >>> dtype.tz
  814:         zoneinfo.ZoneInfo(key='America/Los_Angeles')
  815:         """
  816:         return self._tz
  817: 
  818:     @classmethod
  819:     def construct_array_type(cls) -> type_t[DatetimeArray]:
  820:         """
  821:         Return the array type associated with this dtype.
  822: 
  823:         Returns
  824:         -------
  825:         type
  826:         """
  827:         from pandas.core.arrays import DatetimeArray
  828: 
  829:         return DatetimeArray
  830: 
  831:     @classmethod
  832:     def construct_from_string(cls, string: str_type) -> DatetimeTZDtype:
  833:         """
  834:         Construct a DatetimeTZDtype from a string.
  835: 
  836:         Parameters
  837:         ----------
  838:         string : str
  839:             The string alias for this DatetimeTZDtype.
  840:             Should be formatted like ``datetime64[ns, <tz>]``,
  841:             where ``<tz>`` is the timezone name.
  842: 
  843:         Examples
  844:         --------
  845:         >>> DatetimeTZDtype.construct_from_string('datetime64[ns, UTC]')
  846:         datetime64[ns, UTC]
  847:         """
  848:         if not isinstance(string, str):
  849:             raise TypeError(
  850:                 f"'construct_from_string' expects a string, got {type(string)}"
  851:             )
  852: 
  853:         msg = f"Cannot construct a 'DatetimeTZDtype' from '{string}'"
  854:         match = cls._match.match(string)
  855:         if match:
  856:             d = match.groupdict()
  857:             try:
  858:                 return cls(unit=d["unit"], tz=d["tz"])
  859:             except (KeyError, TypeError, ValueError) as err:
  860:                 # KeyError if maybe_get_tz tries and fails to get a
  861:                 #  pytz timezone (actually pytz.UnknownTimeZoneError).
  862:                 # TypeError if we pass a nonsense tz;
  863:                 # ValueError if we pass a unit other than "ns"
  864:                 raise TypeError(msg) from err
  865:         raise TypeError(msg)
  866: 
  867:     def __str__(self) -> str_type:
  868:         return f"datetime64[{self.unit}, {self.tz}]"
  869: 
  870:     @property
  871:     def name(self) -> str_type:
  872:         """A string representation of the dtype."""
  873:         return str(self)
  874: 
  875:     def __hash__(self) -> int:
  876:         # make myself hashable
  877:         # TODO: update this.
  878:         return hash(str(self))
  879: 
  880:     def __eq__(self, other: object) -> bool:
  881:         if isinstance(other, str):
  882:             if other.startswith("M8["):
  883:                 other = f"datetime64[{other[3:]}"
  884:             return other == self.name
  885: 
  886:         return (
  887:             isinstance(other, DatetimeTZDtype)
  888:             and self.unit == other.unit
  889:             and tz_compare(self.tz, other.tz)
  890:         )
  891: 
  892:     def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> DatetimeArray:
  893:         """
  894:         Construct DatetimeArray from pyarrow Array/ChunkedArray.
  895: 
  896:         Note: If the units in the pyarrow Array are the same as this
  897:         DatetimeDtype, then values corresponding to the integer representation
  898:         of ``NaT`` (e.g. one nanosecond before :attr:`pandas.Timestamp.min`)
  899:         are converted to ``NaT``, regardless of the null indicator in the
  900:         pyarrow array.
  901: 
  902:         Parameters
  903:         ----------
  904:         array : pyarrow.Array or pyarrow.ChunkedArray
  905:             The Arrow array to convert to DatetimeArray.
  906: 
  907:         Returns
  908:         -------
  909:         extension array : DatetimeArray
  910:         """
  911:         import pyarrow
  912: 
  913:         from pandas.core.arrays import DatetimeArray
  914: 
  915:         array = array.cast(pyarrow.timestamp(unit=self._unit), safe=True)
  916: 
  917:         if isinstance(array, pyarrow.Array):
  918:             np_arr = array.to_numpy(zero_copy_only=False)
  919:         else:
  920:             np_arr = array.to_numpy()
  921: 
  922:         return DatetimeArray._simple_new(np_arr, dtype=self)
  923: 
  924:     def __setstate__(self, state) -> None:
  925:         # for pickle compat. __get_state__ is defined in the
  926:         # PandasExtensionDtype superclass and uses the public properties to
  927:         # pickle -> need to set the settable private ones here (see GH26067)
  928:         self._tz = state["tz"]
  929:         self._unit = state["unit"]
  930: 
  931:     def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:
  932:         if all(isinstance(t, DatetimeTZDtype) and t.tz == self.tz for t in dtypes):
  933:             np_dtype = np.max([cast(DatetimeTZDtype, t).base for t in [self, *dtypes]])
  934:             unit = np.datetime_data(np_dtype)[0]
  935:             return type(self)(unit=unit, tz=self.tz)
  936:         return super()._get_common_dtype(dtypes)
  937: 
  938:     @cache_readonly
  939:     def index_class(self) -> type_t[DatetimeIndex]:
  940:         from pandas import DatetimeIndex
  941: 
  942:         return DatetimeIndex
  943: 
  944: 
  945: @register_extension_dtype
  946: class PeriodDtype(PeriodDtypeBase, PandasExtensionDtype):
  947:     """
  948:     An ExtensionDtype for Period data.
  949: 
  950:     **This is not an actual numpy dtype**, but a duck type.
  951: 
  952:     Parameters
  953:     ----------
  954:     freq : str or DateOffset
  955:         The frequency of this PeriodDtype.
  956: 
  957:     Attributes
  958:     ----------
  959:     freq
  960: 
  961:     Methods
  962:     -------
  963:     None
  964: 
  965:     Examples
  966:     --------
  967:     >>> pd.PeriodDtype(freq='D')
  968:     period[D]
  969: 
  970:     >>> pd.PeriodDtype(freq=pd.offsets.MonthEnd())
  971:     period[M]
  972:     """
  973: 
  974:     type: type[Period] = Period
  975:     kind: str_type = "O"
  976:     str = "|O08"
  977:     base = np.dtype("O")
  978:     num = 102
  979:     _metadata = ("freq",)
  980:     _match = re.compile(r"(P|p)eriod\[(?P<freq>.+)\]")
  981:     # error: Incompatible types in assignment (expression has type
  982:     # "Dict[int, PandasExtensionDtype]", base class "PandasExtensionDtype"
  983:     # defined the type as "Dict[str, PandasExtensionDtype]")  [assignment]
  984:     _cache_dtypes: dict[BaseOffset, int] = {}  # type: ignore[assignment]
  985:     __hash__ = PeriodDtypeBase.__hash__
  986:     _freq: BaseOffset
  987:     _supports_2d = True
  988:     _can_fast_transpose = True
  989: 
  990:     def __new__(cls, freq) -> PeriodDtype:  # noqa: PYI034
  991:         """
  992:         Parameters
  993:         ----------
  994:         freq : PeriodDtype, BaseOffset, or string
  995:         """
  996:         if isinstance(freq, PeriodDtype):
  997:             return freq
  998: 
  999:         if not isinstance(freq, BaseOffset):
 1000:             freq = cls._parse_dtype_strict(freq)
 1001: 
 1002:         if isinstance(freq, BDay):
 1003:             # GH#53446
 1004:             # TODO(3.0): enforcing this will close GH#10575
 1005:             warnings.warn(
 1006:                 "PeriodDtype[B] is deprecated and will be removed in a future "
 1007:                 "version. Use a DatetimeIndex with freq='B' instead",
 1008:                 FutureWarning,
 1009:                 stacklevel=find_stack_level(),
 1010:             )
 1011: 
 1012:         try:
 1013:             dtype_code = cls._cache_dtypes[freq]
 1014:         except KeyError:
 1015:             dtype_code = freq._period_dtype_code
 1016:             cls._cache_dtypes[freq] = dtype_code
 1017:         u = PeriodDtypeBase.__new__(cls, dtype_code, freq.n)
 1018:         u._freq = freq
 1019:         return u
 1020: 
 1021:     def __reduce__(self) -> tuple[type_t[Self], tuple[str_type]]:
 1022:         return type(self), (self.name,)
 1023: 
 1024:     @property
 1025:     def freq(self) -> BaseOffset:
 1026:         """
 1027:         The frequency object of this PeriodDtype.
 1028: 
 1029:         Examples
 1030:         --------
 1031:         >>> dtype = pd.PeriodDtype(freq='D')
 1032:         >>> dtype.freq
 1033:         <Day>
 1034:         """
 1035:         return self._freq
 1036: 
 1037:     @classmethod
 1038:     def _parse_dtype_strict(cls, freq: str_type) -> BaseOffset:
 1039:         if isinstance(freq, str):  # note: freq is already of type str!
 1040:             if freq.startswith(("Period[", "period[")):
 1041:                 m = cls._match.search(freq)
 1042:                 if m is not None:
 1043:                     freq = m.group("freq")
 1044: 
 1045:             freq_offset = to_offset(freq, is_period=True)
 1046:             if freq_offset is not None:
 1047:                 return freq_offset
 1048: 
 1049:         raise TypeError(
 1050:             "PeriodDtype argument should be string or BaseOffset, "
 1051:             f"got {type(freq).__name__}"
 1052:         )
 1053: 
 1054:     @classmethod
 1055:     def construct_from_string(cls, string: str_type) -> PeriodDtype:
 1056:         """
 1057:         Strict construction from a string, raise a TypeError if not
 1058:         possible
 1059:         """
 1060:         if (
 1061:             isinstance(string, str)
 1062:             and (string.startswith(("period[", "Period[")))
 1063:             or isinstance(string, BaseOffset)
 1064:         ):
 1065:             # do not parse string like U as period[U]
 1066:             # avoid tuple to be regarded as freq
 1067:             try:
 1068:                 return cls(freq=string)
 1069:             except ValueError:
 1070:                 pass
 1071:         if isinstance(string, str):
 1072:             msg = f"Cannot construct a 'PeriodDtype' from '{string}'"
 1073:         else:
 1074:             msg = f"'construct_from_string' expects a string, got {type(string)}"
 1075:         raise TypeError(msg)
 1076: 
 1077:     def __str__(self) -> str_type:
 1078:         return self.name
 1079: 
 1080:     @property
 1081:     def name(self) -> str_type:
 1082:         return f"period[{self._freqstr}]"
 1083: 
 1084:     @property
 1085:     def na_value(self) -> NaTType:
 1086:         return NaT
 1087: 
 1088:     def __eq__(self, other: object) -> bool:
 1089:         if isinstance(other, str):
 1090:             return other in [self.name, capitalize_first_letter(self.name)]
 1091: 
 1092:         return super().__eq__(other)
 1093: 
 1094:     def __ne__(self, other: object) -> bool:
 1095:         return not self.__eq__(other)
 1096: 
 1097:     @classmethod
 1098:     def is_dtype(cls, dtype: object) -> bool:
 1099:         """
 1100:         Return a boolean if we if the passed type is an actual dtype that we
 1101:         can match (via string or type)
 1102:         """
 1103:         if isinstance(dtype, str):
 1104:             # PeriodDtype can be instantiated from freq string like "U",
 1105:             # but doesn't regard freq str like "U" as dtype.
 1106:             if dtype.startswith(("period[", "Period[")):
 1107:                 try:
 1108:                     return cls._parse_dtype_strict(dtype) is not None
 1109:                 except ValueError:
 1110:                     return False
 1111:             else:
 1112:                 return False
 1113:         return super().is_dtype(dtype)
 1114: 
 1115:     @classmethod
 1116:     def construct_array_type(cls) -> type_t[PeriodArray]:
 1117:         """
 1118:         Return the array type associated with this dtype.
 1119: 
 1120:         Returns
 1121:         -------
 1122:         type
 1123:         """
 1124:         from pandas.core.arrays import PeriodArray
 1125: 
 1126:         return PeriodArray
 1127: 
 1128:     def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> PeriodArray:
 1129:         """
 1130:         Construct PeriodArray from pyarrow Array/ChunkedArray.
 1131:         """
 1132:         import pyarrow
 1133: 
 1134:         from pandas.core.arrays import PeriodArray
 1135:         from pandas.core.arrays.arrow._arrow_utils import (
 1136:             pyarrow_array_to_numpy_and_mask,
 1137:         )
 1138: 
 1139:         if isinstance(array, pyarrow.Array):
 1140:             chunks = [array]
 1141:         else:
 1142:             chunks = array.chunks
 1143: 
 1144:         results = []
 1145:         for arr in chunks:
 1146:             data, mask = pyarrow_array_to_numpy_and_mask(arr, dtype=np.dtype(np.int64))
 1147:             parr = PeriodArray(data.copy(), dtype=self, copy=False)
 1148:             # error: Invalid index type "ndarray[Any, dtype[bool_]]" for "PeriodArray";
 1149:             # expected type "Union[int, Sequence[int], Sequence[bool], slice]"
 1150:             parr[~mask] = NaT  # type: ignore[index]
 1151:             results.append(parr)
 1152: 
 1153:         if not results:
 1154:             return PeriodArray(np.array([], dtype="int64"), dtype=self, copy=False)
 1155:         return PeriodArray._concat_same_type(results)
 1156: 
 1157:     @cache_readonly
 1158:     def index_class(self) -> type_t[PeriodIndex]:
 1159:         from pandas import PeriodIndex
 1160: 
 1161:         return PeriodIndex
 1162: 
 1163: 
 1164: @register_extension_dtype
 1165: class IntervalDtype(PandasExtensionDtype):
 1166:     """
 1167:     An ExtensionDtype for Interval data.
 1168: 
 1169:     **This is not an actual numpy dtype**, but a duck type.
 1170: 
 1171:     Parameters
 1172:     ----------
 1173:     subtype : str, np.dtype
 1174:         The dtype of the Interval bounds.
 1175: 
 1176:     Attributes
 1177:     ----------
 1178:     subtype
 1179: 
 1180:     Methods
 1181:     -------
 1182:     None
 1183: 
 1184:     Examples
 1185:     --------
 1186:     >>> pd.IntervalDtype(subtype='int64', closed='both')
 1187:     interval[int64, both]
 1188:     """
 1189: 
 1190:     name = "interval"
 1191:     kind: str_type = "O"
 1192:     str = "|O08"
 1193:     base = np.dtype("O")
 1194:     num = 103
 1195:     _metadata = (
 1196:         "subtype",
 1197:         "closed",
 1198:     )
 1199: 
 1200:     _match = re.compile(
 1201:         r"(I|i)nterval\[(?P<subtype>[^,]+(\[.+\])?)"
 1202:         r"(, (?P<closed>(right|left|both|neither)))?\]"
 1203:     )
 1204: 
 1205:     _cache_dtypes: dict[str_type, PandasExtensionDtype] = {}
 1206:     _subtype: None | np.dtype
 1207:     _closed: IntervalClosedType | None
 1208: 
 1209:     def __init__(self, subtype=None, closed: IntervalClosedType | None = None) -> None:
 1210:         from pandas.core.dtypes.common import (
 1211:             is_string_dtype,
 1212:             pandas_dtype,
 1213:         )
 1214: 
 1215:         if closed is not None and closed not in {"right", "left", "both", "neither"}:
 1216:             raise ValueError("closed must be one of 'right', 'left', 'both', 'neither'")
 1217: 
 1218:         if isinstance(subtype, IntervalDtype):
 1219:             if closed is not None and closed != subtype.closed:
 1220:                 raise ValueError(
 1221:                     "dtype.closed and 'closed' do not match. "
 1222:                     "Try IntervalDtype(dtype.subtype, closed) instead."
 1223:                 )
 1224:             self._subtype = subtype._subtype
 1225:             self._closed = subtype._closed
 1226:         elif subtype is None:
 1227:             # we are called as an empty constructor
 1228:             # generally for pickle compat
 1229:             self._subtype = None
 1230:             self._closed = closed
 1231:         elif isinstance(subtype, str) and subtype.lower() == "interval":
 1232:             self._subtype = None
 1233:             self._closed = closed
 1234:         else:
 1235:             if isinstance(subtype, str):
 1236:                 m = IntervalDtype._match.search(subtype)
 1237:                 if m is not None:
 1238:                     gd = m.groupdict()
 1239:                     subtype = gd["subtype"]
 1240:                     if gd.get("closed", None) is not None:
 1241:                         if closed is not None:
 1242:                             if closed != gd["closed"]:
 1243:                                 raise ValueError(
 1244:                                     "'closed' keyword does not match value "
 1245:                                     "specified in dtype string"
 1246:                                 )
 1247:                         closed = gd["closed"]  # type: ignore[assignment]
 1248: 
 1249:             try:
 1250:                 subtype = pandas_dtype(subtype)
 1251:             except TypeError as err:
 1252:                 raise TypeError("could not construct IntervalDtype") from err
 1253:             if CategoricalDtype.is_dtype(subtype) or is_string_dtype(subtype):
 1254:                 # GH 19016
 1255:                 msg = (
 1256:                     "category, object, and string subtypes are not supported "
 1257:                     "for IntervalDtype"
 1258:                 )
 1259:                 raise TypeError(msg)
 1260:             self._subtype = subtype
 1261:             self._closed = closed
 1262: 
 1263:     @cache_readonly
 1264:     def _can_hold_na(self) -> bool:
 1265:         subtype = self._subtype
 1266:         if subtype is None:
 1267:             # partially-initialized
 1268:             raise NotImplementedError(
 1269:                 "_can_hold_na is not defined for partially-initialized IntervalDtype"
 1270:             )
 1271:         if subtype.kind in "iu":
 1272:             return False
 1273:         return True
 1274: 
 1275:     @property
 1276:     def closed(self) -> IntervalClosedType:
 1277:         return self._closed  # type: ignore[return-value]
 1278: 
 1279:     @property
 1280:     def subtype(self):
 1281:         """
 1282:         The dtype of the Interval bounds.
 1283: 
 1284:         Examples
 1285:         --------
 1286:         >>> dtype = pd.IntervalDtype(subtype='int64', closed='both')
 1287:         >>> dtype.subtype
 1288:         dtype('int64')
 1289:         """
 1290:         return self._subtype
 1291: 
 1292:     @classmethod
 1293:     def construct_array_type(cls) -> type[IntervalArray]:
 1294:         """
 1295:         Return the array type associated with this dtype.
 1296: 
 1297:         Returns
 1298:         -------
 1299:         type
 1300:         """
 1301:         from pandas.core.arrays import IntervalArray
 1302: 
 1303:         return IntervalArray
 1304: 
 1305:     @classmethod
 1306:     def construct_from_string(cls, string: str_type) -> IntervalDtype:
 1307:         """
 1308:         attempt to construct this type from a string, raise a TypeError
 1309:         if its not possible
 1310:         """
 1311:         if not isinstance(string, str):
 1312:             raise TypeError(
 1313:                 f"'construct_from_string' expects a string, got {type(string)}"
 1314:             )
 1315: 
 1316:         if string.lower() == "interval" or cls._match.search(string) is not None:
 1317:             return cls(string)
 1318: 
 1319:         msg = (
 1320:             f"Cannot construct a 'IntervalDtype' from '{string}'.\n\n"
 1321:             "Incorrectly formatted string passed to constructor. "
 1322:             "Valid formats include Interval or Interval[dtype] "
 1323:             "where dtype is numeric, datetime, or timedelta"
 1324:         )
 1325:         raise TypeError(msg)
 1326: 
 1327:     @property
 1328:     def type(self) -> type[Interval]:
 1329:         return Interval
 1330: 
 1331:     def __str__(self) -> str_type:
 1332:         if self.subtype is None:
 1333:             return "interval"
 1334:         if self.closed is None:
 1335:             # Only partially initialized GH#38394
 1336:             return f"interval[{self.subtype}]"
 1337:         return f"interval[{self.subtype}, {self.closed}]"
 1338: 
 1339:     def __hash__(self) -> int:
 1340:         # make myself hashable
 1341:         return hash(str(self))
 1342: 
 1343:     def __eq__(self, other: object) -> bool:
 1344:         if isinstance(other, str):
 1345:             return other.lower() in (self.name.lower(), str(self).lower())
 1346:         elif not isinstance(other, IntervalDtype):
 1347:             return False
 1348:         elif self.subtype is None or other.subtype is None:
 1349:             # None should match any subtype
 1350:             return True
 1351:         elif self.closed != other.closed:
 1352:             return False
 1353:         else:
 1354:             return self.subtype == other.subtype
 1355: 
 1356:     def __setstate__(self, state) -> None:
 1357:         # for pickle compat. __get_state__ is defined in the
 1358:         # PandasExtensionDtype superclass and uses the public properties to
 1359:         # pickle -> need to set the settable private ones here (see GH26067)
 1360:         self._subtype = state["subtype"]
 1361: 
 1362:         # backward-compat older pickles won't have "closed" key
 1363:         self._closed = state.pop("closed", None)
 1364: 
 1365:     @classmethod
 1366:     def is_dtype(cls, dtype: object) -> bool:
 1367:         """
 1368:         Return a boolean if we if the passed type is an actual dtype that we
 1369:         can match (via string or type)
 1370:         """
 1371:         if isinstance(dtype, str):
 1372:             if dtype.lower().startswith("interval"):
 1373:                 try:
 1374:                     return cls.construct_from_string(dtype) is not None
 1375:                 except (ValueError, TypeError):
 1376:                     return False
 1377:             else:
 1378:                 return False
 1379:         return super().is_dtype(dtype)
 1380: 
 1381:     def __from_arrow__(self, array: pa.Array | pa.ChunkedArray) -> IntervalArray:
 1382:         """
 1383:         Construct IntervalArray from pyarrow Array/ChunkedArray.
 1384:         """
 1385:         import pyarrow
 1386: 
 1387:         from pandas.core.arrays import IntervalArray
 1388: 
 1389:         if isinstance(array, pyarrow.Array):
 1390:             chunks = [array]
 1391:         else:
 1392:             chunks = array.chunks
 1393: 
 1394:         results = []
 1395:         for arr in chunks:
 1396:             if isinstance(arr, pyarrow.ExtensionArray):
 1397:                 arr = arr.storage
 1398:             left = np.asarray(arr.field("left"), dtype=self.subtype)
 1399:             right = np.asarray(arr.field("right"), dtype=self.subtype)
 1400:             iarr = IntervalArray.from_arrays(left, right, closed=self.closed)
 1401:             results.append(iarr)
 1402: 
 1403:         if not results:
 1404:             return IntervalArray.from_arrays(
 1405:                 np.array([], dtype=self.subtype),
 1406:                 np.array([], dtype=self.subtype),
 1407:                 closed=self.closed,
 1408:             )
 1409:         return IntervalArray._concat_same_type(results)
 1410: 
 1411:     def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:
 1412:         if not all(isinstance(x, IntervalDtype) for x in dtypes):
 1413:             return None
 1414: 
 1415:         closed = cast("IntervalDtype", dtypes[0]).closed
 1416:         if not all(cast("IntervalDtype", x).closed == closed for x in dtypes):
 1417:             return np.dtype(object)
 1418: 
 1419:         from pandas.core.dtypes.cast import find_common_type
 1420: 
 1421:         common = find_common_type([cast("IntervalDtype", x).subtype for x in dtypes])
 1422:         if common == object:
 1423:             return np.dtype(object)
 1424:         return IntervalDtype(common, closed=closed)
 1425: 
 1426:     @cache_readonly
 1427:     def index_class(self) -> type_t[IntervalIndex]:
 1428:         from pandas import IntervalIndex
 1429: 
 1430:         return IntervalIndex
 1431: 
 1432: 
 1433: class NumpyEADtype(ExtensionDtype):
 1434:     """
 1435:     A Pandas ExtensionDtype for NumPy dtypes.
 1436: 
 1437:     This is mostly for internal compatibility, and is not especially
 1438:     useful on its own.
 1439: 
 1440:     Parameters
 1441:     ----------
 1442:     dtype : object
 1443:         Object to be converted to a NumPy data type object.
 1444: 
 1445:     See Also
 1446:     --------
 1447:     numpy.dtype
 1448:     """
 1449: 
 1450:     _metadata = ("_dtype",)
 1451:     _supports_2d = False
 1452:     _can_fast_transpose = False
 1453: 
 1454:     def __init__(self, dtype: npt.DTypeLike | NumpyEADtype | None) -> None:
 1455:         if isinstance(dtype, NumpyEADtype):
 1456:             # make constructor idempotent
 1457:             dtype = dtype.numpy_dtype
 1458:         self._dtype = np.dtype(dtype)
 1459: 
 1460:     def __repr__(self) -> str:
 1461:         return f"NumpyEADtype({repr(self.name)})"
 1462: 
 1463:     @property
 1464:     def numpy_dtype(self) -> np.dtype:
 1465:         """
 1466:         The NumPy dtype this NumpyEADtype wraps.
 1467:         """
 1468:         return self._dtype
 1469: 
 1470:     @property
 1471:     def name(self) -> str:
 1472:         """
 1473:         A bit-width name for this data-type.
 1474:         """
 1475:         return self._dtype.name
 1476: 
 1477:     @property
 1478:     def type(self) -> type[np.generic]:
 1479:         """
 1480:         The type object used to instantiate a scalar of this NumPy data-type.
 1481:         """
 1482:         return self._dtype.type
 1483: 
 1484:     @property
 1485:     def _is_numeric(self) -> bool:
 1486:         # exclude object, str, unicode, void.
 1487:         return self.kind in set("biufc")
 1488: 
 1489:     @property
 1490:     def _is_boolean(self) -> bool:
 1491:         return self.kind == "b"
 1492: 
 1493:     @classmethod
 1494:     def construct_from_string(cls, string: str) -> NumpyEADtype:
 1495:         try:
 1496:             dtype = np.dtype(string)
 1497:         except TypeError as err:
 1498:             if not isinstance(string, str):
 1499:                 msg = f"'construct_from_string' expects a string, got {type(string)}"
 1500:             else:
 1501:                 msg = f"Cannot construct a 'NumpyEADtype' from '{string}'"
 1502:             raise TypeError(msg) from err
 1503:         return cls(dtype)
 1504: 
 1505:     @classmethod
 1506:     def construct_array_type(cls) -> type_t[NumpyExtensionArray]:
 1507:         """
 1508:         Return the array type associated with this dtype.
 1509: 
 1510:         Returns
 1511:         -------
 1512:         type
 1513:         """
 1514:         from pandas.core.arrays import NumpyExtensionArray
 1515: 
 1516:         return NumpyExtensionArray
 1517: 
 1518:     @property
 1519:     def kind(self) -> str:
 1520:         """
 1521:         A character code (one of 'biufcmMOSUV') identifying the general kind of data.
 1522:         """
 1523:         return self._dtype.kind
 1524: 
 1525:     @property
 1526:     def itemsize(self) -> int:
 1527:         """
 1528:         The element size of this data-type object.
 1529:         """
 1530:         return self._dtype.itemsize
 1531: 
 1532: 
 1533: class BaseMaskedDtype(ExtensionDtype):
 1534:     """
 1535:     Base class for dtypes for BaseMaskedArray subclasses.
 1536:     """
 1537: 
 1538:     base = None
 1539:     type: type
 1540: 
 1541:     @property
 1542:     def na_value(self) -> libmissing.NAType:
 1543:         return libmissing.NA
 1544: 
 1545:     @cache_readonly
 1546:     def numpy_dtype(self) -> np.dtype:
 1547:         """Return an instance of our numpy dtype"""
 1548:         return np.dtype(self.type)
 1549: 
 1550:     @cache_readonly
 1551:     def kind(self) -> str:
 1552:         return self.numpy_dtype.kind
 1553: 
 1554:     @cache_readonly
 1555:     def itemsize(self) -> int:
 1556:         """Return the number of bytes in this dtype"""
 1557:         return self.numpy_dtype.itemsize
 1558: 
 1559:     @classmethod
 1560:     def construct_array_type(cls) -> type_t[BaseMaskedArray]:
 1561:         """
 1562:         Return the array type associated with this dtype.
 1563: 
 1564:         Returns
 1565:         -------
 1566:         type
 1567:         """
 1568:         raise NotImplementedError
 1569: 
 1570:     @classmethod
 1571:     def from_numpy_dtype(cls, dtype: np.dtype) -> BaseMaskedDtype:
 1572:         """
 1573:         Construct the MaskedDtype corresponding to the given numpy dtype.
 1574:         """
 1575:         if dtype.kind == "b":
 1576:             from pandas.core.arrays.boolean import BooleanDtype
 1577: 
 1578:             return BooleanDtype()
 1579:         elif dtype.kind in "iu":
 1580:             from pandas.core.arrays.integer import NUMPY_INT_TO_DTYPE
 1581: 
 1582:             return NUMPY_INT_TO_DTYPE[dtype]
 1583:         elif dtype.kind == "f":
 1584:             from pandas.core.arrays.floating import NUMPY_FLOAT_TO_DTYPE
 1585: 
 1586:             return NUMPY_FLOAT_TO_DTYPE[dtype]
 1587:         else:
 1588:             raise NotImplementedError(dtype)
 1589: 
 1590:     def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:
 1591:         # We unwrap any masked dtypes, find the common dtype we would use
 1592:         #  for that, then re-mask the result.
 1593:         from pandas.core.dtypes.cast import find_common_type
 1594: 
 1595:         new_dtype = find_common_type(
 1596:             [
 1597:                 dtype.numpy_dtype if isinstance(dtype, BaseMaskedDtype) else dtype
 1598:                 for dtype in dtypes
 1599:             ]
 1600:         )
 1601:         if not isinstance(new_dtype, np.dtype):
 1602:             # If we ever support e.g. Masked[DatetimeArray] then this will change
 1603:             return None
 1604:         try:
 1605:             return type(self).from_numpy_dtype(new_dtype)
 1606:         except (KeyError, NotImplementedError):
 1607:             return None
 1608: 
 1609: 
 1610: @register_extension_dtype
 1611: class SparseDtype(ExtensionDtype):
 1612:     """
 1613:     Dtype for data stored in :class:`SparseArray`.
 1614: 
 1615:     This dtype implements the pandas ExtensionDtype interface.
 1616: 
 1617:     Parameters
 1618:     ----------
 1619:     dtype : str, ExtensionDtype, numpy.dtype, type, default numpy.float64
 1620:         The dtype of the underlying array storing the non-fill value values.
 1621:     fill_value : scalar, optional
 1622:         The scalar value not stored in the SparseArray. By default, this
 1623:         depends on `dtype`.
 1624: 
 1625:         =========== ==========
 1626:         dtype       na_value
 1627:         =========== ==========
 1628:         float       ``np.nan``
 1629:         int         ``0``
 1630:         bool        ``False``
 1631:         datetime64  ``pd.NaT``
 1632:         timedelta64 ``pd.NaT``
 1633:         =========== ==========
 1634: 
 1635:         The default value may be overridden by specifying a `fill_value`.
 1636: 
 1637:     Attributes
 1638:     ----------
 1639:     None
 1640: 
 1641:     Methods
 1642:     -------
 1643:     None
 1644: 
 1645:     Examples
 1646:     --------
 1647:     >>> ser = pd.Series([1, 0, 0], dtype=pd.SparseDtype(dtype=int, fill_value=0))
 1648:     >>> ser
 1649:     0    1
 1650:     1    0
 1651:     2    0
 1652:     dtype: Sparse[int64, 0]
 1653:     >>> ser.sparse.density
 1654:     0.3333333333333333
 1655:     """
 1656: 
 1657:     _is_immutable = True
 1658: 
 1659:     # We include `_is_na_fill_value` in the metadata to avoid hash collisions
 1660:     # between SparseDtype(float, 0.0) and SparseDtype(float, nan).
 1661:     # Without is_na_fill_value in the comparison, those would be equal since
 1662:     # hash(nan) is (sometimes?) 0.
 1663:     _metadata = ("_dtype", "_fill_value", "_is_na_fill_value")
 1664: 
 1665:     def __init__(self, dtype: Dtype = np.float64, fill_value: Any = None) -> None:
 1666:         if isinstance(dtype, type(self)):
 1667:             if fill_value is None:
 1668:                 fill_value = dtype.fill_value
 1669:             dtype = dtype.subtype
 1670: 
 1671:         from pandas.core.dtypes.common import (
 1672:             is_string_dtype,
 1673:             pandas_dtype,
 1674:         )
 1675:         from pandas.core.dtypes.missing import na_value_for_dtype
 1676: 
 1677:         dtype = pandas_dtype(dtype)
 1678:         if is_string_dtype(dtype):
 1679:             dtype = np.dtype("object")
 1680:         if not isinstance(dtype, np.dtype):
 1681:             # GH#53160
 1682:             raise TypeError("SparseDtype subtype must be a numpy dtype")
 1683: 
 1684:         if fill_value is None:
 1685:             fill_value = na_value_for_dtype(dtype)
 1686: 
 1687:         self._dtype = dtype
 1688:         self._fill_value = fill_value
 1689:         self._check_fill_value()
 1690: 
 1691:     def __hash__(self) -> int:
 1692:         # Python3 doesn't inherit __hash__ when a base class overrides
 1693:         # __eq__, so we explicitly do it here.
 1694:         return super().__hash__()
 1695: 
 1696:     def __eq__(self, other: object) -> bool:
 1697:         # We have to override __eq__ to handle NA values in _metadata.
 1698:         # The base class does simple == checks, which fail for NA.
 1699:         if isinstance(other, str):
 1700:             try:
 1701:                 other = self.construct_from_string(other)
 1702:             except TypeError:
 1703:                 return False
 1704: 
 1705:         if isinstance(other, type(self)):
 1706:             subtype = self.subtype == other.subtype
 1707:             if self._is_na_fill_value:
 1708:                 # this case is complicated by two things:
 1709:                 # SparseDtype(float, float(nan)) == SparseDtype(float, np.nan)
 1710:                 # SparseDtype(float, np.nan)     != SparseDtype(float, pd.NaT)
 1711:                 # i.e. we want to treat any floating-point NaN as equal, but
 1712:                 # not a floating-point NaN and a datetime NaT.
 1713:                 fill_value = (
 1714:                     other._is_na_fill_value
 1715:                     and isinstance(self.fill_value, type(other.fill_value))
 1716:                     or isinstance(other.fill_value, type(self.fill_value))
 1717:                 )
 1718:             else:
 1719:                 with warnings.catch_warnings():
 1720:                     # Ignore spurious numpy warning
 1721:                     warnings.filterwarnings(
 1722:                         "ignore",
 1723:                         "elementwise comparison failed",
 1724:                         category=DeprecationWarning,
 1725:                     )
 1726: 
 1727:                     fill_value = self.fill_value == other.fill_value
 1728: 
 1729:             return subtype and fill_value
 1730:         return False
 1731: 
 1732:     @property
 1733:     def fill_value(self):
 1734:         """
 1735:         The fill value of the array.
 1736: 
 1737:         Converting the SparseArray to a dense ndarray will fill the
 1738:         array with this value.
 1739: 
 1740:         .. warning::
 1741: 
 1742:            It's possible to end up with a SparseArray that has ``fill_value``
 1743:            values in ``sp_values``. This can occur, for example, when setting
 1744:            ``SparseArray.fill_value`` directly.
 1745:         """
 1746:         return self._fill_value
 1747: 
 1748:     def _check_fill_value(self) -> None:
 1749:         if not lib.is_scalar(self._fill_value):
 1750:             raise ValueError(
 1751:                 f"fill_value must be a scalar. Got {self._fill_value} instead"
 1752:             )
 1753: 
 1754:         from pandas.core.dtypes.cast import can_hold_element
 1755:         from pandas.core.dtypes.missing import (
 1756:             is_valid_na_for_dtype,
 1757:             isna,
 1758:         )
 1759: 
 1760:         from pandas.core.construction import ensure_wrapped_if_datetimelike
 1761: 
 1762:         # GH#23124 require fill_value and subtype to match
 1763:         val = self._fill_value
 1764:         if isna(val):
 1765:             if not is_valid_na_for_dtype(val, self.subtype):
 1766:                 warnings.warn(
 1767:                     "Allowing arbitrary scalar fill_value in SparseDtype is "
 1768:                     "deprecated. In a future version, the fill_value must be "
 1769:                     "a valid value for the SparseDtype.subtype.",
 1770:                     FutureWarning,
 1771:                     stacklevel=find_stack_level(),
 1772:                 )
 1773:         else:
 1774:             dummy = np.empty(0, dtype=self.subtype)
 1775:             dummy = ensure_wrapped_if_datetimelike(dummy)
 1776: 
 1777:             if not can_hold_element(dummy, val):
 1778:                 warnings.warn(
 1779:                     "Allowing arbitrary scalar fill_value in SparseDtype is "
 1780:                     "deprecated. In a future version, the fill_value must be "
 1781:                     "a valid value for the SparseDtype.subtype.",
 1782:                     FutureWarning,
 1783:                     stacklevel=find_stack_level(),
 1784:                 )
 1785: 
 1786:     @property
 1787:     def _is_na_fill_value(self) -> bool:
 1788:         from pandas import isna
 1789: 
 1790:         return isna(self.fill_value)
 1791: 
 1792:     @property
 1793:     def _is_numeric(self) -> bool:
 1794:         return not self.subtype == object
 1795: 
 1796:     @property
 1797:     def _is_boolean(self) -> bool:
 1798:         return self.subtype.kind == "b"
 1799: 
 1800:     @property
 1801:     def kind(self) -> str:
 1802:         """
 1803:         The sparse kind. Either 'integer', or 'block'.
 1804:         """
 1805:         return self.subtype.kind
 1806: 
 1807:     @property
 1808:     def type(self):
 1809:         return self.subtype.type
 1810: 
 1811:     @property
 1812:     def subtype(self):
 1813:         return self._dtype
 1814: 
 1815:     @property
 1816:     def name(self) -> str:
 1817:         return f"Sparse[{self.subtype.name}, {repr(self.fill_value)}]"
 1818: 
 1819:     def __repr__(self) -> str:
 1820:         return self.name
 1821: 
 1822:     @classmethod
 1823:     def construct_array_type(cls) -> type_t[SparseArray]:
 1824:         """
 1825:         Return the array type associated with this dtype.
 1826: 
 1827:         Returns
 1828:         -------
 1829:         type
 1830:         """
 1831:         from pandas.core.arrays.sparse.array import SparseArray
 1832: 
 1833:         return SparseArray
 1834: 
 1835:     @classmethod
 1836:     def construct_from_string(cls, string: str) -> SparseDtype:
 1837:         """
 1838:         Construct a SparseDtype from a string form.
 1839: 
 1840:         Parameters
 1841:         ----------
 1842:         string : str
 1843:             Can take the following forms.
 1844: 
 1845:             string           dtype
 1846:             ================ ============================
 1847:             'int'            SparseDtype[np.int64, 0]
 1848:             'Sparse'         SparseDtype[np.float64, nan]
 1849:             'Sparse[int]'    SparseDtype[np.int64, 0]
 1850:             'Sparse[int, 0]' SparseDtype[np.int64, 0]
 1851:             ================ ============================
 1852: 
 1853:             It is not possible to specify non-default fill values
 1854:             with a string. An argument like ``'Sparse[int, 1]'``
 1855:             will raise a ``TypeError`` because the default fill value
 1856:             for integers is 0.
 1857: 
 1858:         Returns
 1859:         -------
 1860:         SparseDtype
 1861:         """
 1862:         if not isinstance(string, str):
 1863:             raise TypeError(
 1864:                 f"'construct_from_string' expects a string, got {type(string)}"
 1865:             )
 1866:         msg = f"Cannot construct a 'SparseDtype' from '{string}'"
 1867:         if string.startswith("Sparse"):
 1868:             try:
 1869:                 sub_type, has_fill_value = cls._parse_subtype(string)
 1870:             except ValueError as err:
 1871:                 raise TypeError(msg) from err
 1872:             else:
 1873:                 result = SparseDtype(sub_type)
 1874:                 msg = (
 1875:                     f"Cannot construct a 'SparseDtype' from '{string}'.\n\nIt "
 1876:                     "looks like the fill_value in the string is not "
 1877:                     "the default for the dtype. Non-default fill_values "
 1878:                     "are not supported. Use the 'SparseDtype()' "
 1879:                     "constructor instead."
 1880:                 )
 1881:                 if has_fill_value and str(result) != string:
 1882:                     raise TypeError(msg)
 1883:                 return result
 1884:         else:
 1885:             raise TypeError(msg)
 1886: 
 1887:     @staticmethod
 1888:     def _parse_subtype(dtype: str) -> tuple[str, bool]:
 1889:         """
 1890:         Parse a string to get the subtype
 1891: 
 1892:         Parameters
 1893:         ----------
 1894:         dtype : str
 1895:             A string like
 1896: 
 1897:             * Sparse[subtype]
 1898:             * Sparse[subtype, fill_value]
 1899: 
 1900:         Returns
 1901:         -------
 1902:         subtype : str
 1903: 
 1904:         Raises
 1905:         ------
 1906:         ValueError
 1907:             When the subtype cannot be extracted.
 1908:         """
 1909:         xpr = re.compile(r"Sparse\[(?P<subtype>[^,]*)(, )?(?P<fill_value>.*?)?\]$")
 1910:         m = xpr.match(dtype)
 1911:         has_fill_value = False
 1912:         if m:
 1913:             subtype = m.groupdict()["subtype"]
 1914:             has_fill_value = bool(m.groupdict()["fill_value"])
 1915:         elif dtype == "Sparse":
 1916:             subtype = "float64"
 1917:         else:
 1918:             raise ValueError(f"Cannot parse {dtype}")
 1919:         return subtype, has_fill_value
 1920: 
 1921:     @classmethod
 1922:     def is_dtype(cls, dtype: object) -> bool:
 1923:         dtype = getattr(dtype, "dtype", dtype)
 1924:         if isinstance(dtype, str) and dtype.startswith("Sparse"):
 1925:             sub_type, _ = cls._parse_subtype(dtype)
 1926:             dtype = np.dtype(sub_type)
 1927:         elif isinstance(dtype, cls):
 1928:             return True
 1929:         return isinstance(dtype, np.dtype) or dtype == "Sparse"
 1930: 
 1931:     def update_dtype(self, dtype) -> SparseDtype:
 1932:         """
 1933:         Convert the SparseDtype to a new dtype.
 1934: 
 1935:         This takes care of converting the ``fill_value``.
 1936: 
 1937:         Parameters
 1938:         ----------
 1939:         dtype : Union[str, numpy.dtype, SparseDtype]
 1940:             The new dtype to use.
 1941: 
 1942:             * For a SparseDtype, it is simply returned
 1943:             * For a NumPy dtype (or str), the current fill value
 1944:               is converted to the new dtype, and a SparseDtype
 1945:               with `dtype` and the new fill value is returned.
 1946: 
 1947:         Returns
 1948:         -------
 1949:         SparseDtype
 1950:             A new SparseDtype with the correct `dtype` and fill value
 1951:             for that `dtype`.
 1952: 
 1953:         Raises
 1954:         ------
 1955:         ValueError
 1956:             When the current fill value cannot be converted to the
 1957:             new `dtype` (e.g. trying to convert ``np.nan`` to an
 1958:             integer dtype).
 1959: 
 1960: 
 1961:         Examples
 1962:         --------
 1963:         >>> SparseDtype(int, 0).update_dtype(float)
 1964:         Sparse[float64, 0.0]
 1965: 
 1966:         >>> SparseDtype(int, 1).update_dtype(SparseDtype(float, np.nan))
 1967:         Sparse[float64, nan]
 1968:         """
 1969:         from pandas.core.dtypes.astype import astype_array
 1970:         from pandas.core.dtypes.common import pandas_dtype
 1971: 
 1972:         cls = type(self)
 1973:         dtype = pandas_dtype(dtype)
 1974: 
 1975:         if not isinstance(dtype, cls):
 1976:             if not isinstance(dtype, np.dtype):
 1977:                 raise TypeError("sparse arrays of extension dtypes not supported")
 1978: 
 1979:             fv_asarray = np.atleast_1d(np.array(self.fill_value))
 1980:             fvarr = astype_array(fv_asarray, dtype)
 1981:             # NB: not fv_0d.item(), as that casts dt64->int
 1982:             fill_value = fvarr[0]
 1983:             dtype = cls(dtype, fill_value=fill_value)
 1984: 
 1985:         return dtype
 1986: 
 1987:     @property
 1988:     def _subtype_with_str(self):
 1989:         """
 1990:         Whether the SparseDtype's subtype should be considered ``str``.
 1991: 
 1992:         Typically, pandas will store string data in an object-dtype array.
 1993:         When converting values to a dtype, e.g. in ``.astype``, we need to
 1994:         be more specific, we need the actual underlying type.
 1995: 
 1996:         Returns
 1997:         -------
 1998:         >>> SparseDtype(int, 1)._subtype_with_str
 1999:         dtype('int64')
 2000: 
 2001:         >>> SparseDtype(object, 1)._subtype_with_str
 2002:         dtype('O')
 2003: 
 2004:         >>> dtype = SparseDtype(str, '')
 2005:         >>> dtype.subtype
 2006:         dtype('O')
 2007: 
 2008:         >>> dtype._subtype_with_str
 2009:         <class 'str'>
 2010:         """
 2011:         if isinstance(self.fill_value, str):
 2012:             return type(self.fill_value)
 2013:         return self.subtype
 2014: 
 2015:     def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:
 2016:         # TODO for now only handle SparseDtypes and numpy dtypes => extend
 2017:         # with other compatible extension dtypes
 2018:         from pandas.core.dtypes.cast import np_find_common_type
 2019: 
 2020:         if any(
 2021:             isinstance(x, ExtensionDtype) and not isinstance(x, SparseDtype)
 2022:             for x in dtypes
 2023:         ):
 2024:             return None
 2025: 
 2026:         fill_values = [x.fill_value for x in dtypes if isinstance(x, SparseDtype)]
 2027:         fill_value = fill_values[0]
 2028: 
 2029:         from pandas import isna
 2030: 
 2031:         # np.nan isn't a singleton, so we may end up with multiple
 2032:         # NaNs here, so we ignore the all NA case too.
 2033:         if not (len(set(fill_values)) == 1 or isna(fill_values).all()):
 2034:             warnings.warn(
 2035:                 "Concatenating sparse arrays with multiple fill "
 2036:                 f"values: '{fill_values}'. Picking the first and "
 2037:                 "converting the rest.",
 2038:                 PerformanceWarning,
 2039:                 stacklevel=find_stack_level(),
 2040:             )
 2041: 
 2042:         np_dtypes = (x.subtype if isinstance(x, SparseDtype) else x for x in dtypes)
 2043:         return SparseDtype(np_find_common_type(*np_dtypes), fill_value=fill_value)
 2044: 
 2045: 
 2046: @register_extension_dtype
 2047: class ArrowDtype(StorageExtensionDtype):
 2048:     """
 2049:     An ExtensionDtype for PyArrow data types.
 2050: 
 2051:     .. warning::
 2052: 
 2053:        ArrowDtype is considered experimental. The implementation and
 2054:        parts of the API may change without warning.
 2055: 
 2056:     While most ``dtype`` arguments can accept the "string"
 2057:     constructor, e.g. ``"int64[pyarrow]"``, ArrowDtype is useful
 2058:     if the data type contains parameters like ``pyarrow.timestamp``.
 2059: 
 2060:     Parameters
 2061:     ----------
 2062:     pyarrow_dtype : pa.DataType
 2063:         An instance of a `pyarrow.DataType <https://arrow.apache.org/docs/python/api/datatypes.html#factory-functions>`__.
 2064: 
 2065:     Attributes
 2066:     ----------
 2067:     pyarrow_dtype
 2068: 
 2069:     Methods
 2070:     -------
 2071:     None
 2072: 
 2073:     Returns
 2074:     -------
 2075:     ArrowDtype
 2076: 
 2077:     Examples
 2078:     --------
 2079:     >>> import pyarrow as pa
 2080:     >>> pd.ArrowDtype(pa.int64())
 2081:     int64[pyarrow]
 2082: 
 2083:     Types with parameters must be constructed with ArrowDtype.
 2084: 
 2085:     >>> pd.ArrowDtype(pa.timestamp("s", tz="America/New_York"))
 2086:     timestamp[s, tz=America/New_York][pyarrow]
 2087:     >>> pd.ArrowDtype(pa.list_(pa.int64()))
 2088:     list<item: int64>[pyarrow]
 2089:     """
 2090: 
 2091:     _metadata = ("storage", "pyarrow_dtype")  # type: ignore[assignment]
 2092: 
 2093:     def __init__(self, pyarrow_dtype: pa.DataType) -> None:
 2094:         super().__init__("pyarrow")
 2095:         if pa_version_under10p1:
 2096:             raise ImportError("pyarrow>=10.0.1 is required for ArrowDtype")
 2097:         if not isinstance(pyarrow_dtype, pa.DataType):
 2098:             raise ValueError(
 2099:                 f"pyarrow_dtype ({pyarrow_dtype}) must be an instance "
 2100:                 f"of a pyarrow.DataType. Got {type(pyarrow_dtype)} instead."
 2101:             )
 2102:         self.pyarrow_dtype = pyarrow_dtype
 2103: 
 2104:     def __repr__(self) -> str:
 2105:         return self.name
 2106: 
 2107:     def __hash__(self) -> int:
 2108:         # make myself hashable
 2109:         return hash(str(self))
 2110: 
 2111:     def __eq__(self, other: object) -> bool:
 2112:         if not isinstance(other, type(self)):
 2113:             return super().__eq__(other)
 2114:         return self.pyarrow_dtype == other.pyarrow_dtype
 2115: 
 2116:     @property
 2117:     def type(self):
 2118:         """
 2119:         Returns associated scalar type.
 2120:         """
 2121:         pa_type = self.pyarrow_dtype
 2122:         if pa.types.is_integer(pa_type):
 2123:             return int
 2124:         elif pa.types.is_floating(pa_type):
 2125:             return float
 2126:         elif pa.types.is_string(pa_type) or pa.types.is_large_string(pa_type):
 2127:             return str
 2128:         elif (
 2129:             pa.types.is_binary(pa_type)
 2130:             or pa.types.is_fixed_size_binary(pa_type)
 2131:             or pa.types.is_large_binary(pa_type)
 2132:         ):
 2133:             return bytes
 2134:         elif pa.types.is_boolean(pa_type):
 2135:             return bool
 2136:         elif pa.types.is_duration(pa_type):
 2137:             if pa_type.unit == "ns":
 2138:                 return Timedelta
 2139:             else:
 2140:                 return timedelta
 2141:         elif pa.types.is_timestamp(pa_type):
 2142:             if pa_type.unit == "ns":
 2143:                 return Timestamp
 2144:             else:
 2145:                 return datetime
 2146:         elif pa.types.is_date(pa_type):
 2147:             return date
 2148:         elif pa.types.is_time(pa_type):
 2149:             return time
 2150:         elif pa.types.is_decimal(pa_type):
 2151:             return Decimal
 2152:         elif pa.types.is_dictionary(pa_type):
 2153:             # TODO: Potentially change this & CategoricalDtype.type to
 2154:             #  something more representative of the scalar
 2155:             return CategoricalDtypeType
 2156:         elif pa.types.is_list(pa_type) or pa.types.is_large_list(pa_type):
 2157:             return list
 2158:         elif pa.types.is_fixed_size_list(pa_type):
 2159:             return list
 2160:         elif pa.types.is_map(pa_type):
 2161:             return list
 2162:         elif pa.types.is_struct(pa_type):
 2163:             return dict
 2164:         elif pa.types.is_null(pa_type):
 2165:             # TODO: None? pd.NA? pa.null?
 2166:             return type(pa_type)
 2167:         elif isinstance(pa_type, pa.ExtensionType):
 2168:             return type(self)(pa_type.storage_type).type
 2169:         raise NotImplementedError(pa_type)
 2170: 
 2171:     @property
 2172:     def name(self) -> str:  # type: ignore[override]
 2173:         """
 2174:         A string identifying the data type.
 2175:         """
 2176:         return f"{str(self.pyarrow_dtype)}[{self.storage}]"
 2177: 
 2178:     @cache_readonly
 2179:     def numpy_dtype(self) -> np.dtype:
 2180:         """Return an instance of the related numpy dtype"""
 2181:         if pa.types.is_timestamp(self.pyarrow_dtype):
 2182:             # pa.timestamp(unit).to_pandas_dtype() returns ns units
 2183:             # regardless of the pyarrow timestamp units.
 2184:             # This can be removed if/when pyarrow addresses it:
 2185:             # https://github.com/apache/arrow/issues/34462
 2186:             return np.dtype(f"datetime64[{self.pyarrow_dtype.unit}]")
 2187:         if pa.types.is_duration(self.pyarrow_dtype):
 2188:             # pa.duration(unit).to_pandas_dtype() returns ns units
 2189:             # regardless of the pyarrow duration units
 2190:             # This can be removed if/when pyarrow addresses it:
 2191:             # https://github.com/apache/arrow/issues/34462
 2192:             return np.dtype(f"timedelta64[{self.pyarrow_dtype.unit}]")
 2193:         if pa.types.is_string(self.pyarrow_dtype) or pa.types.is_large_string(
 2194:             self.pyarrow_dtype
 2195:         ):
 2196:             # pa.string().to_pandas_dtype() = object which we don't want
 2197:             return np.dtype(str)
 2198:         try:
 2199:             return np.dtype(self.pyarrow_dtype.to_pandas_dtype())
 2200:         except (NotImplementedError, TypeError):
 2201:             return np.dtype(object)
 2202: 
 2203:     @cache_readonly
 2204:     def kind(self) -> str:
 2205:         if pa.types.is_timestamp(self.pyarrow_dtype):
 2206:             # To mirror DatetimeTZDtype
 2207:             return "M"
 2208:         return self.numpy_dtype.kind
 2209: 
 2210:     @cache_readonly
 2211:     def itemsize(self) -> int:
 2212:         """Return the number of bytes in this dtype"""
 2213:         return self.numpy_dtype.itemsize
 2214: 
 2215:     @classmethod
 2216:     def construct_array_type(cls) -> type_t[ArrowExtensionArray]:
 2217:         """
 2218:         Return the array type associated with this dtype.
 2219: 
 2220:         Returns
 2221:         -------
 2222:         type
 2223:         """
 2224:         from pandas.core.arrays.arrow import ArrowExtensionArray
 2225: 
 2226:         return ArrowExtensionArray
 2227: 
 2228:     @classmethod
 2229:     def construct_from_string(cls, string: str) -> ArrowDtype:
 2230:         """
 2231:         Construct this type from a string.
 2232: 
 2233:         Parameters
 2234:         ----------
 2235:         string : str
 2236:             string should follow the format f"{pyarrow_type}[pyarrow]"
 2237:             e.g. int64[pyarrow]
 2238:         """
 2239:         if not isinstance(string, str):
 2240:             raise TypeError(
 2241:                 f"'construct_from_string' expects a string, got {type(string)}"
 2242:             )
 2243:         if not string.endswith("[pyarrow]"):
 2244:             raise TypeError(f"'{string}' must end with '[pyarrow]'")
 2245:         if string == "string[pyarrow]":
 2246:             # Ensure Registry.find skips ArrowDtype to use StringDtype instead
 2247:             raise TypeError("string[pyarrow] should be constructed by StringDtype")
 2248: 
 2249:         base_type = string[:-9]  # get rid of "[pyarrow]"
 2250:         try:
 2251:             pa_dtype = pa.type_for_alias(base_type)
 2252:         except ValueError as err:
 2253:             has_parameters = re.search(r"[\[\(].*[\]\)]", base_type)
 2254:             if has_parameters:
 2255:                 # Fallback to try common temporal types
 2256:                 try:
 2257:                     return cls._parse_temporal_dtype_string(base_type)
 2258:                 except (NotImplementedError, ValueError):
 2259:                     # Fall through to raise with nice exception message below
 2260:                     pass
 2261: 
 2262:                 raise NotImplementedError(
 2263:                     "Passing pyarrow type specific parameters "
 2264:                     f"({has_parameters.group()}) in the string is not supported. "
 2265:                     "Please construct an ArrowDtype object with a pyarrow_dtype "
 2266:                     "instance with specific parameters."
 2267:                 ) from err
 2268:             raise TypeError(f"'{base_type}' is not a valid pyarrow data type.") from err
 2269:         return cls(pa_dtype)
 2270: 
 2271:     # TODO(arrow#33642): This can be removed once supported by pyarrow
 2272:     @classmethod
 2273:     def _parse_temporal_dtype_string(cls, string: str) -> ArrowDtype:
 2274:         """
 2275:         Construct a temporal ArrowDtype from string.
 2276:         """
 2277:         # we assume
 2278:         #  1) "[pyarrow]" has already been stripped from the end of our string.
 2279:         #  2) we know "[" is present
 2280:         head, tail = string.split("[", 1)
 2281: 
 2282:         if not tail.endswith("]"):
 2283:             raise ValueError
 2284:         tail = tail[:-1]
 2285: 
 2286:         if head == "timestamp":
 2287:             assert "," in tail  # otherwise type_for_alias should work
 2288:             unit, tz = tail.split(",", 1)
 2289:             unit = unit.strip()
 2290:             tz = tz.strip()
 2291:             if tz.startswith("tz="):
 2292:                 tz = tz[3:]
 2293: 
 2294:             pa_type = pa.timestamp(unit, tz=tz)
 2295:             dtype = cls(pa_type)
 2296:             return dtype
 2297: 
 2298:         raise NotImplementedError(string)
 2299: 
 2300:     @property
 2301:     def _is_numeric(self) -> bool:
 2302:         """
 2303:         Whether columns with this dtype should be considered numeric.
 2304:         """
 2305:         # TODO: pa.types.is_boolean?
 2306:         return (
 2307:             pa.types.is_integer(self.pyarrow_dtype)
 2308:             or pa.types.is_floating(self.pyarrow_dtype)
 2309:             or pa.types.is_decimal(self.pyarrow_dtype)
 2310:         )
 2311: 
 2312:     @property
 2313:     def _is_boolean(self) -> bool:
 2314:         """
 2315:         Whether this dtype should be considered boolean.
 2316:         """
 2317:         return pa.types.is_boolean(self.pyarrow_dtype)
 2318: 
 2319:     def _get_common_dtype(self, dtypes: list[DtypeObj]) -> DtypeObj | None:
 2320:         # We unwrap any masked dtypes, find the common dtype we would use
 2321:         #  for that, then re-mask the result.
 2322:         # Mirrors BaseMaskedDtype
 2323:         from pandas.core.dtypes.cast import find_common_type
 2324: 
 2325:         null_dtype = type(self)(pa.null())
 2326: 
 2327:         new_dtype = find_common_type(
 2328:             [
 2329:                 dtype.numpy_dtype if isinstance(dtype, ArrowDtype) else dtype
 2330:                 for dtype in dtypes
 2331:                 if dtype != null_dtype
 2332:             ]
 2333:         )
 2334:         if not isinstance(new_dtype, np.dtype):
 2335:             return None
 2336:         try:
 2337:             pa_dtype = pa.from_numpy_dtype(new_dtype)
 2338:             return type(self)(pa_dtype)
 2339:         except NotImplementedError:
 2340:             return None
 2341: 
 2342:     def __from_arrow__(self, array: pa.Array | pa.ChunkedArray):
 2343:         """
 2344:         Construct IntegerArray/FloatingArray from pyarrow Array/ChunkedArray.
 2345:         """
 2346:         array_class = self.construct_array_type()
 2347:         arr = array.cast(self.pyarrow_dtype, safe=True)
 2348:         return array_class(arr)
