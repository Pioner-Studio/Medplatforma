    1: """
    2: Read a SAS XPort format file into a Pandas DataFrame.
    3: 
    4: Based on code from Jack Cushman (github.com/jcushman/xport).
    5: 
    6: The file format is defined here:
    7: 
    8: https://support.sas.com/content/dam/SAS/support/en/technical-papers/record-layout-of-a-sas-version-5-or-6-data-set-in-sas-transport-xport-format.pdf
    9: """
   10: from __future__ import annotations
   11: 
   12: from collections import abc
   13: from datetime import datetime
   14: import struct
   15: from typing import TYPE_CHECKING
   16: import warnings
   17: 
   18: import numpy as np
   19: 
   20: from pandas.util._decorators import Appender
   21: from pandas.util._exceptions import find_stack_level
   22: 
   23: import pandas as pd
   24: 
   25: from pandas.io.common import get_handle
   26: from pandas.io.sas.sasreader import ReaderBase
   27: 
   28: if TYPE_CHECKING:
   29:     from pandas._typing import (
   30:         CompressionOptions,
   31:         DatetimeNaTType,
   32:         FilePath,
   33:         ReadBuffer,
   34:     )
   35: _correct_line1 = (
   36:     "HEADER RECORD*******LIBRARY HEADER RECORD!!!!!!!"
   37:     "000000000000000000000000000000  "
   38: )
   39: _correct_header1 = (
   40:     "HEADER RECORD*******MEMBER  HEADER RECORD!!!!!!!000000000000000001600000000"
   41: )
   42: _correct_header2 = (
   43:     "HEADER RECORD*******DSCRPTR HEADER RECORD!!!!!!!"
   44:     "000000000000000000000000000000  "
   45: )
   46: _correct_obs_header = (
   47:     "HEADER RECORD*******OBS     HEADER RECORD!!!!!!!"
   48:     "000000000000000000000000000000  "
   49: )
   50: _fieldkeys = [
   51:     "ntype",
   52:     "nhfun",
   53:     "field_length",
   54:     "nvar0",
   55:     "name",
   56:     "label",
   57:     "nform",
   58:     "nfl",
   59:     "num_decimals",
   60:     "nfj",
   61:     "nfill",
   62:     "niform",
   63:     "nifl",
   64:     "nifd",
   65:     "npos",
   66:     "_",
   67: ]
   68: 
   69: 
   70: _base_params_doc = """\
   71: Parameters
   72: ----------
   73: filepath_or_buffer : str or file-like object
   74:     Path to SAS file or object implementing binary read method."""
   75: 
   76: _params2_doc = """\
   77: index : identifier of index column
   78:     Identifier of column that should be used as index of the DataFrame.
   79: encoding : str
   80:     Encoding for text data.
   81: chunksize : int
   82:     Read file `chunksize` lines at a time, returns iterator."""
   83: 
   84: _format_params_doc = """\
   85: format : str
   86:     File format, only `xport` is currently supported."""
   87: 
   88: _iterator_doc = """\
   89: iterator : bool, default False
   90:     Return XportReader object for reading file incrementally."""
   91: 
   92: 
   93: _read_sas_doc = f"""Read a SAS file into a DataFrame.
   94: 
   95: {_base_params_doc}
   96: {_format_params_doc}
   97: {_params2_doc}
   98: {_iterator_doc}
   99: 
  100: Returns
  101: -------
  102: DataFrame or XportReader
  103: 
  104: Examples
  105: --------
  106: Read a SAS Xport file:
  107: 
  108: >>> df = pd.read_sas('filename.XPT')
  109: 
  110: Read a Xport file in 10,000 line chunks:
  111: 
  112: >>> itr = pd.read_sas('filename.XPT', chunksize=10000)
  113: >>> for chunk in itr:
  114: >>>     do_something(chunk)
  115: 
  116: """
  117: 
  118: _xport_reader_doc = f"""\
  119: Class for reading SAS Xport files.
  120: 
  121: {_base_params_doc}
  122: {_params2_doc}
  123: 
  124: Attributes
  125: ----------
  126: member_info : list
  127:     Contains information about the file
  128: fields : list
  129:     Contains information about the variables in the file
  130: """
  131: 
  132: _read_method_doc = """\
  133: Read observations from SAS Xport file, returning as data frame.
  134: 
  135: Parameters
  136: ----------
  137: nrows : int
  138:     Number of rows to read from data file; if None, read whole
  139:     file.
  140: 
  141: Returns
  142: -------
  143: A DataFrame.
  144: """
  145: 
  146: 
  147: def _parse_date(datestr: str) -> DatetimeNaTType:
  148:     """Given a date in xport format, return Python date."""
  149:     try:
  150:         # e.g. "16FEB11:10:07:55"
  151:         return datetime.strptime(datestr, "%d%b%y:%H:%M:%S")
  152:     except ValueError:
  153:         return pd.NaT
  154: 
  155: 
  156: def _split_line(s: str, parts):
  157:     """
  158:     Parameters
  159:     ----------
  160:     s: str
  161:         Fixed-length string to split
  162:     parts: list of (name, length) pairs
  163:         Used to break up string, name '_' will be filtered from output.
  164: 
  165:     Returns
  166:     -------
  167:     Dict of name:contents of string at given location.
  168:     """
  169:     out = {}
  170:     start = 0
  171:     for name, length in parts:
  172:         out[name] = s[start : start + length].strip()
  173:         start += length
  174:     del out["_"]
  175:     return out
  176: 
  177: 
  178: def _handle_truncated_float_vec(vec, nbytes):
  179:     # This feature is not well documented, but some SAS XPORT files
  180:     # have 2-7 byte "truncated" floats.  To read these truncated
  181:     # floats, pad them with zeros on the right to make 8 byte floats.
  182:     #
  183:     # References:
  184:     # https://github.com/jcushman/xport/pull/3
  185:     # The R "foreign" library
  186: 
  187:     if nbytes != 8:
  188:         vec1 = np.zeros(len(vec), np.dtype("S8"))
  189:         dtype = np.dtype(f"S{nbytes},S{8 - nbytes}")
  190:         vec2 = vec1.view(dtype=dtype)
  191:         vec2["f0"] = vec
  192:         return vec2
  193: 
  194:     return vec
  195: 
  196: 
  197: def _parse_float_vec(vec):
  198:     """
  199:     Parse a vector of float values representing IBM 8 byte floats into
  200:     native 8 byte floats.
  201:     """
  202:     dtype = np.dtype(">u4,>u4")
  203:     vec1 = vec.view(dtype=dtype)
  204:     xport1 = vec1["f0"]
  205:     xport2 = vec1["f1"]
  206: 
  207:     # Start by setting first half of ieee number to first half of IBM
  208:     # number sans exponent
  209:     ieee1 = xport1 & 0x00FFFFFF
  210: 
  211:     # The fraction bit to the left of the binary point in the ieee
  212:     # format was set and the number was shifted 0, 1, 2, or 3
  213:     # places. This will tell us how to adjust the ibm exponent to be a
  214:     # power of 2 ieee exponent and how to shift the fraction bits to
  215:     # restore the correct magnitude.
  216:     shift = np.zeros(len(vec), dtype=np.uint8)
  217:     shift[np.where(xport1 & 0x00200000)] = 1
  218:     shift[np.where(xport1 & 0x00400000)] = 2
  219:     shift[np.where(xport1 & 0x00800000)] = 3
  220: 
  221:     # shift the ieee number down the correct number of places then
  222:     # set the second half of the ieee number to be the second half
  223:     # of the ibm number shifted appropriately, ored with the bits
  224:     # from the first half that would have been shifted in if we
  225:     # could shift a double. All we are worried about are the low
  226:     # order 3 bits of the first half since we're only shifting by
  227:     # 1, 2, or 3.
  228:     ieee1 >>= shift
  229:     ieee2 = (xport2 >> shift) | ((xport1 & 0x00000007) << (29 + (3 - shift)))
  230: 
  231:     # clear the 1 bit to the left of the binary point
  232:     ieee1 &= 0xFFEFFFFF
  233: 
  234:     # set the exponent of the ieee number to be the actual exponent
  235:     # plus the shift count + 1023. Or this into the first half of the
  236:     # ieee number. The ibm exponent is excess 64 but is adjusted by 65
  237:     # since during conversion to ibm format the exponent is
  238:     # incremented by 1 and the fraction bits left 4 positions to the
  239:     # right of the radix point.  (had to add >> 24 because C treats &
  240:     # 0x7f as 0x7f000000 and Python doesn't)
  241:     ieee1 |= ((((((xport1 >> 24) & 0x7F) - 65) << 2) + shift + 1023) << 20) | (
  242:         xport1 & 0x80000000
  243:     )
  244: 
  245:     ieee = np.empty((len(ieee1),), dtype=">u4,>u4")
  246:     ieee["f0"] = ieee1
  247:     ieee["f1"] = ieee2
  248:     ieee = ieee.view(dtype=">f8")
  249:     ieee = ieee.astype("f8")
  250: 
  251:     return ieee
  252: 
  253: 
  254: class XportReader(ReaderBase, abc.Iterator):
  255:     __doc__ = _xport_reader_doc
  256: 
  257:     def __init__(
  258:         self,
  259:         filepath_or_buffer: FilePath | ReadBuffer[bytes],
  260:         index=None,
  261:         encoding: str | None = "ISO-8859-1",
  262:         chunksize: int | None = None,
  263:         compression: CompressionOptions = "infer",
  264:     ) -> None:
  265:         self._encoding = encoding
  266:         self._lines_read = 0
  267:         self._index = index
  268:         self._chunksize = chunksize
  269: 
  270:         self.handles = get_handle(
  271:             filepath_or_buffer,
  272:             "rb",
  273:             encoding=encoding,
  274:             is_text=False,
  275:             compression=compression,
  276:         )
  277:         self.filepath_or_buffer = self.handles.handle
  278: 
  279:         try:
  280:             self._read_header()
  281:         except Exception:
  282:             self.close()
  283:             raise
  284: 
  285:     def close(self) -> None:
  286:         self.handles.close()
  287: 
  288:     def _get_row(self):
  289:         return self.filepath_or_buffer.read(80).decode()
  290: 
  291:     def _read_header(self) -> None:
  292:         self.filepath_or_buffer.seek(0)
  293: 
  294:         # read file header
  295:         line1 = self._get_row()
  296:         if line1 != _correct_line1:
  297:             if "**COMPRESSED**" in line1:
  298:                 # this was created with the PROC CPORT method and can't be read
  299:                 # https://documentation.sas.com/doc/en/pgmsascdc/9.4_3.5/movefile/p1bm6aqp3fw4uin1hucwh718f6kp.htm
  300:                 raise ValueError(
  301:                     "Header record indicates a CPORT file, which is not readable."
  302:                 )
  303:             raise ValueError("Header record is not an XPORT file.")
  304: 
  305:         line2 = self._get_row()
  306:         fif = [["prefix", 24], ["version", 8], ["OS", 8], ["_", 24], ["created", 16]]
  307:         file_info = _split_line(line2, fif)
  308:         if file_info["prefix"] != "SAS     SAS     SASLIB":
  309:             raise ValueError("Header record has invalid prefix.")
  310:         file_info["created"] = _parse_date(file_info["created"])
  311:         self.file_info = file_info
  312: 
  313:         line3 = self._get_row()
  314:         file_info["modified"] = _parse_date(line3[:16])
  315: 
  316:         # read member header
  317:         header1 = self._get_row()
  318:         header2 = self._get_row()
  319:         headflag1 = header1.startswith(_correct_header1)
  320:         headflag2 = header2 == _correct_header2
  321:         if not (headflag1 and headflag2):
  322:             raise ValueError("Member header not found")
  323:         # usually 140, could be 135
  324:         fieldnamelength = int(header1[-5:-2])
  325: 
  326:         # member info
  327:         mem = [
  328:             ["prefix", 8],
  329:             ["set_name", 8],
  330:             ["sasdata", 8],
  331:             ["version", 8],
  332:             ["OS", 8],
  333:             ["_", 24],
  334:             ["created", 16],
  335:         ]
  336:         member_info = _split_line(self._get_row(), mem)
  337:         mem = [["modified", 16], ["_", 16], ["label", 40], ["type", 8]]
  338:         member_info.update(_split_line(self._get_row(), mem))
  339:         member_info["modified"] = _parse_date(member_info["modified"])
  340:         member_info["created"] = _parse_date(member_info["created"])
  341:         self.member_info = member_info
  342: 
  343:         # read field names
  344:         types = {1: "numeric", 2: "char"}
  345:         fieldcount = int(self._get_row()[54:58])
  346:         datalength = fieldnamelength * fieldcount
  347:         # round up to nearest 80
  348:         if datalength % 80:
  349:             datalength += 80 - datalength % 80
  350:         fielddata = self.filepath_or_buffer.read(datalength)
  351:         fields = []
  352:         obs_length = 0
  353:         while len(fielddata) >= fieldnamelength:
  354:             # pull data for one field
  355:             fieldbytes, fielddata = (
  356:                 fielddata[:fieldnamelength],
  357:                 fielddata[fieldnamelength:],
  358:             )
  359: 
  360:             # rest at end gets ignored, so if field is short, pad out
  361:             # to match struct pattern below
  362:             fieldbytes = fieldbytes.ljust(140)
  363: 
  364:             fieldstruct = struct.unpack(">hhhh8s40s8shhh2s8shhl52s", fieldbytes)
  365:             field = dict(zip(_fieldkeys, fieldstruct))
  366:             del field["_"]
  367:             field["ntype"] = types[field["ntype"]]
  368:             fl = field["field_length"]
  369:             if field["ntype"] == "numeric" and ((fl < 2) or (fl > 8)):
  370:                 msg = f"Floating field width {fl} is not between 2 and 8."
  371:                 raise TypeError(msg)
  372: 
  373:             for k, v in field.items():
  374:                 try:
  375:                     field[k] = v.strip()
  376:                 except AttributeError:
  377:                     pass
  378: 
  379:             obs_length += field["field_length"]
  380:             fields += [field]
  381: 
  382:         header = self._get_row()
  383:         if not header == _correct_obs_header:
  384:             raise ValueError("Observation header not found.")
  385: 
  386:         self.fields = fields
  387:         self.record_length = obs_length
  388:         self.record_start = self.filepath_or_buffer.tell()
  389: 
  390:         self.nobs = self._record_count()
  391:         self.columns = [x["name"].decode() for x in self.fields]
  392: 
  393:         # Setup the dtype.
  394:         dtypel = [
  395:             ("s" + str(i), "S" + str(field["field_length"]))
  396:             for i, field in enumerate(self.fields)
  397:         ]
  398:         dtype = np.dtype(dtypel)
  399:         self._dtype = dtype
  400: 
  401:     def __next__(self) -> pd.DataFrame:
  402:         return self.read(nrows=self._chunksize or 1)
  403: 
  404:     def _record_count(self) -> int:
  405:         """
  406:         Get number of records in file.
  407: 
  408:         This is maybe suboptimal because we have to seek to the end of
  409:         the file.
  410: 
  411:         Side effect: returns file position to record_start.
  412:         """
  413:         self.filepath_or_buffer.seek(0, 2)
  414:         total_records_length = self.filepath_or_buffer.tell() - self.record_start
  415: 
  416:         if total_records_length % 80 != 0:
  417:             warnings.warn(
  418:                 "xport file may be corrupted.",
  419:                 stacklevel=find_stack_level(),
  420:             )
  421: 
  422:         if self.record_length > 80:
  423:             self.filepath_or_buffer.seek(self.record_start)
  424:             return total_records_length // self.record_length
  425: 
  426:         self.filepath_or_buffer.seek(-80, 2)
  427:         last_card_bytes = self.filepath_or_buffer.read(80)
  428:         last_card = np.frombuffer(last_card_bytes, dtype=np.uint64)
  429: 
  430:         # 8 byte blank
  431:         ix = np.flatnonzero(last_card == 2314885530818453536)
  432: 
  433:         if len(ix) == 0:
  434:             tail_pad = 0
  435:         else:
  436:             tail_pad = 8 * len(ix)
  437: 
  438:         self.filepath_or_buffer.seek(self.record_start)
  439: 
  440:         return (total_records_length - tail_pad) // self.record_length
  441: 
  442:     def get_chunk(self, size: int | None = None) -> pd.DataFrame:
  443:         """
  444:         Reads lines from Xport file and returns as dataframe
  445: 
  446:         Parameters
  447:         ----------
  448:         size : int, defaults to None
  449:             Number of lines to read.  If None, reads whole file.
  450: 
  451:         Returns
  452:         -------
  453:         DataFrame
  454:         """
  455:         if size is None:
  456:             size = self._chunksize
  457:         return self.read(nrows=size)
  458: 
  459:     def _missing_double(self, vec):
  460:         v = vec.view(dtype="u1,u1,u2,u4")
  461:         miss = (v["f1"] == 0) & (v["f2"] == 0) & (v["f3"] == 0)
  462:         miss1 = (
  463:             ((v["f0"] >= 0x41) & (v["f0"] <= 0x5A))
  464:             | (v["f0"] == 0x5F)
  465:             | (v["f0"] == 0x2E)
  466:         )
  467:         miss &= miss1
  468:         return miss
  469: 
  470:     @Appender(_read_method_doc)
  471:     def read(self, nrows: int | None = None) -> pd.DataFrame:
  472:         if nrows is None:
  473:             nrows = self.nobs
  474: 
  475:         read_lines = min(nrows, self.nobs - self._lines_read)
  476:         read_len = read_lines * self.record_length
  477:         if read_len <= 0:
  478:             self.close()
  479:             raise StopIteration
  480:         raw = self.filepath_or_buffer.read(read_len)
  481:         data = np.frombuffer(raw, dtype=self._dtype, count=read_lines)
  482: 
  483:         df_data = {}
  484:         for j, x in enumerate(self.columns):
  485:             vec = data["s" + str(j)]
  486:             ntype = self.fields[j]["ntype"]
  487:             if ntype == "numeric":
  488:                 vec = _handle_truncated_float_vec(vec, self.fields[j]["field_length"])
  489:                 miss = self._missing_double(vec)
  490:                 v = _parse_float_vec(vec)
  491:                 v[miss] = np.nan
  492:             elif self.fields[j]["ntype"] == "char":
  493:                 v = [y.rstrip() for y in vec]
  494: 
  495:                 if self._encoding is not None:
  496:                     v = [y.decode(self._encoding) for y in v]
  497: 
  498:             df_data.update({x: v})
  499:         df = pd.DataFrame(df_data)
  500: 
  501:         if self._index is None:
  502:             df.index = pd.Index(range(self._lines_read, self._lines_read + read_lines))
  503:         else:
  504:             df = df.set_index(self._index)
  505: 
  506:         self._lines_read += read_lines
  507: 
  508:         return df
