    1: """
    2: Read SAS7BDAT files
    3: 
    4: Based on code written by Jared Hobbs:
    5:   https://bitbucket.org/jaredhobbs/sas7bdat
    6: 
    7: See also:
    8:   https://github.com/BioStatMatt/sas7bdat
    9: 
   10: Partial documentation of the file format:
   11:   https://cran.r-project.org/package=sas7bdat/vignettes/sas7bdat.pdf
   12: 
   13: Reference for binary data compression:
   14:   http://collaboration.cmc.ec.gc.ca/science/rpn/biblio/ddj/Website/articles/CUJ/1992/9210/ross/ross.htm
   15: """
   16: from __future__ import annotations
   17: 
   18: from collections import abc
   19: from datetime import (
   20:     datetime,
   21:     timedelta,
   22: )
   23: import sys
   24: from typing import TYPE_CHECKING
   25: 
   26: import numpy as np
   27: 
   28: from pandas._libs.byteswap import (
   29:     read_double_with_byteswap,
   30:     read_float_with_byteswap,
   31:     read_uint16_with_byteswap,
   32:     read_uint32_with_byteswap,
   33:     read_uint64_with_byteswap,
   34: )
   35: from pandas._libs.sas import (
   36:     Parser,
   37:     get_subheader_index,
   38: )
   39: from pandas._libs.tslibs.conversion import cast_from_unit_vectorized
   40: from pandas.errors import EmptyDataError
   41: 
   42: import pandas as pd
   43: from pandas import (
   44:     DataFrame,
   45:     Timestamp,
   46:     isna,
   47: )
   48: 
   49: from pandas.io.common import get_handle
   50: import pandas.io.sas.sas_constants as const
   51: from pandas.io.sas.sasreader import ReaderBase
   52: 
   53: if TYPE_CHECKING:
   54:     from pandas._typing import (
   55:         CompressionOptions,
   56:         FilePath,
   57:         ReadBuffer,
   58:     )
   59: 
   60: 
   61: _unix_origin = Timestamp("1970-01-01")
   62: _sas_origin = Timestamp("1960-01-01")
   63: 
   64: 
   65: def _parse_datetime(sas_datetime: float, unit: str):
   66:     if isna(sas_datetime):
   67:         return pd.NaT
   68: 
   69:     if unit == "s":
   70:         return datetime(1960, 1, 1) + timedelta(seconds=sas_datetime)
   71: 
   72:     elif unit == "d":
   73:         return datetime(1960, 1, 1) + timedelta(days=sas_datetime)
   74: 
   75:     else:
   76:         raise ValueError("unit must be 'd' or 's'")
   77: 
   78: 
   79: def _convert_datetimes(sas_datetimes: pd.Series, unit: str) -> pd.Series:
   80:     """
   81:     Convert to Timestamp if possible, otherwise to datetime.datetime.
   82:     SAS float64 lacks precision for more than ms resolution so the fit
   83:     to datetime.datetime is ok.
   84: 
   85:     Parameters
   86:     ----------
   87:     sas_datetimes : {Series, Sequence[float]}
   88:        Dates or datetimes in SAS
   89:     unit : {'d', 's'}
   90:        "d" if the floats represent dates, "s" for datetimes
   91: 
   92:     Returns
   93:     -------
   94:     Series
   95:        Series of datetime64 dtype or datetime.datetime.
   96:     """
   97:     td = (_sas_origin - _unix_origin).as_unit("s")
   98:     if unit == "s":
   99:         millis = cast_from_unit_vectorized(
  100:             sas_datetimes._values, unit="s", out_unit="ms"
  101:         )
  102:         dt64ms = millis.view("M8[ms]") + td
  103:         return pd.Series(dt64ms, index=sas_datetimes.index, copy=False)
  104:     else:
  105:         vals = np.array(sas_datetimes, dtype="M8[D]") + td
  106:         return pd.Series(vals, dtype="M8[s]", index=sas_datetimes.index, copy=False)
  107: 
  108: 
  109: class _Column:
  110:     col_id: int
  111:     name: str | bytes
  112:     label: str | bytes
  113:     format: str | bytes
  114:     ctype: bytes
  115:     length: int
  116: 
  117:     def __init__(
  118:         self,
  119:         col_id: int,
  120:         # These can be bytes when convert_header_text is False
  121:         name: str | bytes,
  122:         label: str | bytes,
  123:         format: str | bytes,
  124:         ctype: bytes,
  125:         length: int,
  126:     ) -> None:
  127:         self.col_id = col_id
  128:         self.name = name
  129:         self.label = label
  130:         self.format = format
  131:         self.ctype = ctype
  132:         self.length = length
  133: 
  134: 
  135: # SAS7BDAT represents a SAS data file in SAS7BDAT format.
  136: class SAS7BDATReader(ReaderBase, abc.Iterator):
  137:     """
  138:     Read SAS files in SAS7BDAT format.
  139: 
  140:     Parameters
  141:     ----------
  142:     path_or_buf : path name or buffer
  143:         Name of SAS file or file-like object pointing to SAS file
  144:         contents.
  145:     index : column identifier, defaults to None
  146:         Column to use as index.
  147:     convert_dates : bool, defaults to True
  148:         Attempt to convert dates to Pandas datetime values.  Note that
  149:         some rarely used SAS date formats may be unsupported.
  150:     blank_missing : bool, defaults to True
  151:         Convert empty strings to missing values (SAS uses blanks to
  152:         indicate missing character variables).
  153:     chunksize : int, defaults to None
  154:         Return SAS7BDATReader object for iterations, returns chunks
  155:         with given number of lines.
  156:     encoding : str, 'infer', defaults to None
  157:         String encoding acc. to Python standard encodings,
  158:         encoding='infer' tries to detect the encoding from the file header,
  159:         encoding=None will leave the data in binary format.
  160:     convert_text : bool, defaults to True
  161:         If False, text variables are left as raw bytes.
  162:     convert_header_text : bool, defaults to True
  163:         If False, header text, including column names, are left as raw
  164:         bytes.
  165:     """
  166: 
  167:     _int_length: int
  168:     _cached_page: bytes | None
  169: 
  170:     def __init__(
  171:         self,
  172:         path_or_buf: FilePath | ReadBuffer[bytes],
  173:         index=None,
  174:         convert_dates: bool = True,
  175:         blank_missing: bool = True,
  176:         chunksize: int | None = None,
  177:         encoding: str | None = None,
  178:         convert_text: bool = True,
  179:         convert_header_text: bool = True,
  180:         compression: CompressionOptions = "infer",
  181:     ) -> None:
  182:         self.index = index
  183:         self.convert_dates = convert_dates
  184:         self.blank_missing = blank_missing
  185:         self.chunksize = chunksize
  186:         self.encoding = encoding
  187:         self.convert_text = convert_text
  188:         self.convert_header_text = convert_header_text
  189: 
  190:         self.default_encoding = "latin-1"
  191:         self.compression = b""
  192:         self.column_names_raw: list[bytes] = []
  193:         self.column_names: list[str | bytes] = []
  194:         self.column_formats: list[str | bytes] = []
  195:         self.columns: list[_Column] = []
  196: 
  197:         self._current_page_data_subheader_pointers: list[tuple[int, int]] = []
  198:         self._cached_page = None
  199:         self._column_data_lengths: list[int] = []
  200:         self._column_data_offsets: list[int] = []
  201:         self._column_types: list[bytes] = []
  202: 
  203:         self._current_row_in_file_index = 0
  204:         self._current_row_on_page_index = 0
  205:         self._current_row_in_file_index = 0
  206: 
  207:         self.handles = get_handle(
  208:             path_or_buf, "rb", is_text=False, compression=compression
  209:         )
  210: 
  211:         self._path_or_buf = self.handles.handle
  212: 
  213:         # Same order as const.SASIndex
  214:         self._subheader_processors = [
  215:             self._process_rowsize_subheader,
  216:             self._process_columnsize_subheader,
  217:             self._process_subheader_counts,
  218:             self._process_columntext_subheader,
  219:             self._process_columnname_subheader,
  220:             self._process_columnattributes_subheader,
  221:             self._process_format_subheader,
  222:             self._process_columnlist_subheader,
  223:             None,  # Data
  224:         ]
  225: 
  226:         try:
  227:             self._get_properties()
  228:             self._parse_metadata()
  229:         except Exception:
  230:             self.close()
  231:             raise
  232: 
  233:     def column_data_lengths(self) -> np.ndarray:
  234:         """Return a numpy int64 array of the column data lengths"""
  235:         return np.asarray(self._column_data_lengths, dtype=np.int64)
  236: 
  237:     def column_data_offsets(self) -> np.ndarray:
  238:         """Return a numpy int64 array of the column offsets"""
  239:         return np.asarray(self._column_data_offsets, dtype=np.int64)
  240: 
  241:     def column_types(self) -> np.ndarray:
  242:         """
  243:         Returns a numpy character array of the column types:
  244:            s (string) or d (double)
  245:         """
  246:         return np.asarray(self._column_types, dtype=np.dtype("S1"))
  247: 
  248:     def close(self) -> None:
  249:         self.handles.close()
  250: 
  251:     def _get_properties(self) -> None:
  252:         # Check magic number
  253:         self._path_or_buf.seek(0)
  254:         self._cached_page = self._path_or_buf.read(288)
  255:         if self._cached_page[0 : len(const.magic)] != const.magic:
  256:             raise ValueError("magic number mismatch (not a SAS file?)")
  257: 
  258:         # Get alignment information
  259:         buf = self._read_bytes(const.align_1_offset, const.align_1_length)
  260:         if buf == const.u64_byte_checker_value:
  261:             self.U64 = True
  262:             self._int_length = 8
  263:             self._page_bit_offset = const.page_bit_offset_x64
  264:             self._subheader_pointer_length = const.subheader_pointer_length_x64
  265:         else:
  266:             self.U64 = False
  267:             self._page_bit_offset = const.page_bit_offset_x86
  268:             self._subheader_pointer_length = const.subheader_pointer_length_x86
  269:             self._int_length = 4
  270:         buf = self._read_bytes(const.align_2_offset, const.align_2_length)
  271:         if buf == const.align_1_checker_value:
  272:             align1 = const.align_2_value
  273:         else:
  274:             align1 = 0
  275: 
  276:         # Get endianness information
  277:         buf = self._read_bytes(const.endianness_offset, const.endianness_length)
  278:         if buf == b"\x01":
  279:             self.byte_order = "<"
  280:             self.need_byteswap = sys.byteorder == "big"
  281:         else:
  282:             self.byte_order = ">"
  283:             self.need_byteswap = sys.byteorder == "little"
  284: 
  285:         # Get encoding information
  286:         buf = self._read_bytes(const.encoding_offset, const.encoding_length)[0]
  287:         if buf in const.encoding_names:
  288:             self.inferred_encoding = const.encoding_names[buf]
  289:             if self.encoding == "infer":
  290:                 self.encoding = self.inferred_encoding
  291:         else:
  292:             self.inferred_encoding = f"unknown (code={buf})"
  293: 
  294:         # Timestamp is epoch 01/01/1960
  295:         epoch = datetime(1960, 1, 1)
  296:         x = self._read_float(
  297:             const.date_created_offset + align1, const.date_created_length
  298:         )
  299:         self.date_created = epoch + pd.to_timedelta(x, unit="s")
  300:         x = self._read_float(
  301:             const.date_modified_offset + align1, const.date_modified_length
  302:         )
  303:         self.date_modified = epoch + pd.to_timedelta(x, unit="s")
  304: 
  305:         self.header_length = self._read_uint(
  306:             const.header_size_offset + align1, const.header_size_length
  307:         )
  308: 
  309:         # Read the rest of the header into cached_page.
  310:         buf = self._path_or_buf.read(self.header_length - 288)
  311:         self._cached_page += buf
  312:         # error: Argument 1 to "len" has incompatible type "Optional[bytes]";
  313:         #  expected "Sized"
  314:         if len(self._cached_page) != self.header_length:  # type: ignore[arg-type]
  315:             raise ValueError("The SAS7BDAT file appears to be truncated.")
  316: 
  317:         self._page_length = self._read_uint(
  318:             const.page_size_offset + align1, const.page_size_length
  319:         )
  320: 
  321:     def __next__(self) -> DataFrame:
  322:         da = self.read(nrows=self.chunksize or 1)
  323:         if da.empty:
  324:             self.close()
  325:             raise StopIteration
  326:         return da
  327: 
  328:     # Read a single float of the given width (4 or 8).
  329:     def _read_float(self, offset: int, width: int):
  330:         assert self._cached_page is not None
  331:         if width == 4:
  332:             return read_float_with_byteswap(
  333:                 self._cached_page, offset, self.need_byteswap
  334:             )
  335:         elif width == 8:
  336:             return read_double_with_byteswap(
  337:                 self._cached_page, offset, self.need_byteswap
  338:             )
  339:         else:
  340:             self.close()
  341:             raise ValueError("invalid float width")
  342: 
  343:     # Read a single unsigned integer of the given width (1, 2, 4 or 8).
  344:     def _read_uint(self, offset: int, width: int) -> int:
  345:         assert self._cached_page is not None
  346:         if width == 1:
  347:             return self._read_bytes(offset, 1)[0]
  348:         elif width == 2:
  349:             return read_uint16_with_byteswap(
  350:                 self._cached_page, offset, self.need_byteswap
  351:             )
  352:         elif width == 4:
  353:             return read_uint32_with_byteswap(
  354:                 self._cached_page, offset, self.need_byteswap
  355:             )
  356:         elif width == 8:
  357:             return read_uint64_with_byteswap(
  358:                 self._cached_page, offset, self.need_byteswap
  359:             )
  360:         else:
  361:             self.close()
  362:             raise ValueError("invalid int width")
  363: 
  364:     def _read_bytes(self, offset: int, length: int):
  365:         assert self._cached_page is not None
  366:         if offset + length > len(self._cached_page):
  367:             self.close()
  368:             raise ValueError("The cached page is too small.")
  369:         return self._cached_page[offset : offset + length]
  370: 
  371:     def _read_and_convert_header_text(self, offset: int, length: int) -> str | bytes:
  372:         return self._convert_header_text(
  373:             self._read_bytes(offset, length).rstrip(b"\x00 ")
  374:         )
  375: 
  376:     def _parse_metadata(self) -> None:
  377:         done = False
  378:         while not done:
  379:             self._cached_page = self._path_or_buf.read(self._page_length)
  380:             if len(self._cached_page) <= 0:
  381:                 break
  382:             if len(self._cached_page) != self._page_length:
  383:                 raise ValueError("Failed to read a meta data page from the SAS file.")
  384:             done = self._process_page_meta()
  385: 
  386:     def _process_page_meta(self) -> bool:
  387:         self._read_page_header()
  388:         pt = const.page_meta_types + [const.page_amd_type, const.page_mix_type]
  389:         if self._current_page_type in pt:
  390:             self._process_page_metadata()
  391:         is_data_page = self._current_page_type == const.page_data_type
  392:         is_mix_page = self._current_page_type == const.page_mix_type
  393:         return bool(
  394:             is_data_page
  395:             or is_mix_page
  396:             or self._current_page_data_subheader_pointers != []
  397:         )
  398: 
  399:     def _read_page_header(self) -> None:
  400:         bit_offset = self._page_bit_offset
  401:         tx = const.page_type_offset + bit_offset
  402:         self._current_page_type = (
  403:             self._read_uint(tx, const.page_type_length) & const.page_type_mask2
  404:         )
  405:         tx = const.block_count_offset + bit_offset
  406:         self._current_page_block_count = self._read_uint(tx, const.block_count_length)
  407:         tx = const.subheader_count_offset + bit_offset
  408:         self._current_page_subheaders_count = self._read_uint(
  409:             tx, const.subheader_count_length
  410:         )
  411: 
  412:     def _process_page_metadata(self) -> None:
  413:         bit_offset = self._page_bit_offset
  414: 
  415:         for i in range(self._current_page_subheaders_count):
  416:             offset = const.subheader_pointers_offset + bit_offset
  417:             total_offset = offset + self._subheader_pointer_length * i
  418: 
  419:             subheader_offset = self._read_uint(total_offset, self._int_length)
  420:             total_offset += self._int_length
  421: 
  422:             subheader_length = self._read_uint(total_offset, self._int_length)
  423:             total_offset += self._int_length
  424: 
  425:             subheader_compression = self._read_uint(total_offset, 1)
  426:             total_offset += 1
  427: 
  428:             subheader_type = self._read_uint(total_offset, 1)
  429: 
  430:             if (
  431:                 subheader_length == 0
  432:                 or subheader_compression == const.truncated_subheader_id
  433:             ):
  434:                 continue
  435: 
  436:             subheader_signature = self._read_bytes(subheader_offset, self._int_length)
  437:             subheader_index = get_subheader_index(subheader_signature)
  438:             subheader_processor = self._subheader_processors[subheader_index]
  439: 
  440:             if subheader_processor is None:
  441:                 f1 = subheader_compression in (const.compressed_subheader_id, 0)
  442:                 f2 = subheader_type == const.compressed_subheader_type
  443:                 if self.compression and f1 and f2:
  444:                     self._current_page_data_subheader_pointers.append(
  445:                         (subheader_offset, subheader_length)
  446:                     )
  447:                 else:
  448:                     self.close()
  449:                     raise ValueError(
  450:                         f"Unknown subheader signature {subheader_signature}"
  451:                     )
  452:             else:
  453:                 subheader_processor(subheader_offset, subheader_length)
  454: 
  455:     def _process_rowsize_subheader(self, offset: int, length: int) -> None:
  456:         int_len = self._int_length
  457:         lcs_offset = offset
  458:         lcp_offset = offset
  459:         if self.U64:
  460:             lcs_offset += 682
  461:             lcp_offset += 706
  462:         else:
  463:             lcs_offset += 354
  464:             lcp_offset += 378
  465: 
  466:         self.row_length = self._read_uint(
  467:             offset + const.row_length_offset_multiplier * int_len,
  468:             int_len,
  469:         )
  470:         self.row_count = self._read_uint(
  471:             offset + const.row_count_offset_multiplier * int_len,
  472:             int_len,
  473:         )
  474:         self.col_count_p1 = self._read_uint(
  475:             offset + const.col_count_p1_multiplier * int_len, int_len
  476:         )
  477:         self.col_count_p2 = self._read_uint(
  478:             offset + const.col_count_p2_multiplier * int_len, int_len
  479:         )
  480:         mx = const.row_count_on_mix_page_offset_multiplier * int_len
  481:         self._mix_page_row_count = self._read_uint(offset + mx, int_len)
  482:         self._lcs = self._read_uint(lcs_offset, 2)
  483:         self._lcp = self._read_uint(lcp_offset, 2)
  484: 
  485:     def _process_columnsize_subheader(self, offset: int, length: int) -> None:
  486:         int_len = self._int_length
  487:         offset += int_len
  488:         self.column_count = self._read_uint(offset, int_len)
  489:         if self.col_count_p1 + self.col_count_p2 != self.column_count:
  490:             print(
  491:                 f"Warning: column count mismatch ({self.col_count_p1} + "
  492:                 f"{self.col_count_p2} != {self.column_count})\n"
  493:             )
  494: 
  495:     # Unknown purpose
  496:     def _process_subheader_counts(self, offset: int, length: int) -> None:
  497:         pass
  498: 
  499:     def _process_columntext_subheader(self, offset: int, length: int) -> None:
  500:         offset += self._int_length
  501:         text_block_size = self._read_uint(offset, const.text_block_size_length)
  502: 
  503:         buf = self._read_bytes(offset, text_block_size)
  504:         cname_raw = buf[0:text_block_size].rstrip(b"\x00 ")
  505:         self.column_names_raw.append(cname_raw)
  506: 
  507:         if len(self.column_names_raw) == 1:
  508:             compression_literal = b""
  509:             for cl in const.compression_literals:
  510:                 if cl in cname_raw:
  511:                     compression_literal = cl
  512:             self.compression = compression_literal
  513:             offset -= self._int_length
  514: 
  515:             offset1 = offset + 16
  516:             if self.U64:
  517:                 offset1 += 4
  518: 
  519:             buf = self._read_bytes(offset1, self._lcp)
  520:             compression_literal = buf.rstrip(b"\x00")
  521:             if compression_literal == b"":
  522:                 self._lcs = 0
  523:                 offset1 = offset + 32
  524:                 if self.U64:
  525:                     offset1 += 4
  526:                 buf = self._read_bytes(offset1, self._lcp)
  527:                 self.creator_proc = buf[0 : self._lcp]
  528:             elif compression_literal == const.rle_compression:
  529:                 offset1 = offset + 40
  530:                 if self.U64:
  531:                     offset1 += 4
  532:                 buf = self._read_bytes(offset1, self._lcp)
  533:                 self.creator_proc = buf[0 : self._lcp]
  534:             elif self._lcs > 0:
  535:                 self._lcp = 0
  536:                 offset1 = offset + 16
  537:                 if self.U64:
  538:                     offset1 += 4
  539:                 buf = self._read_bytes(offset1, self._lcs)
  540:                 self.creator_proc = buf[0 : self._lcp]
  541:             if hasattr(self, "creator_proc"):
  542:                 self.creator_proc = self._convert_header_text(self.creator_proc)
  543: 
  544:     def _process_columnname_subheader(self, offset: int, length: int) -> None:
  545:         int_len = self._int_length
  546:         offset += int_len
  547:         column_name_pointers_count = (length - 2 * int_len - 12) // 8
  548:         for i in range(column_name_pointers_count):
  549:             text_subheader = (
  550:                 offset
  551:                 + const.column_name_pointer_length * (i + 1)
  552:                 + const.column_name_text_subheader_offset
  553:             )
  554:             col_name_offset = (
  555:                 offset
  556:                 + const.column_name_pointer_length * (i + 1)
  557:                 + const.column_name_offset_offset
  558:             )
  559:             col_name_length = (
  560:                 offset
  561:                 + const.column_name_pointer_length * (i + 1)
  562:                 + const.column_name_length_offset
  563:             )
  564: 
  565:             idx = self._read_uint(
  566:                 text_subheader, const.column_name_text_subheader_length
  567:             )
  568:             col_offset = self._read_uint(
  569:                 col_name_offset, const.column_name_offset_length
  570:             )
  571:             col_len = self._read_uint(col_name_length, const.column_name_length_length)
  572: 
  573:             name_raw = self.column_names_raw[idx]
  574:             cname = name_raw[col_offset : col_offset + col_len]
  575:             self.column_names.append(self._convert_header_text(cname))
  576: 
  577:     def _process_columnattributes_subheader(self, offset: int, length: int) -> None:
  578:         int_len = self._int_length
  579:         column_attributes_vectors_count = (length - 2 * int_len - 12) // (int_len + 8)
  580:         for i in range(column_attributes_vectors_count):
  581:             col_data_offset = (
  582:                 offset + int_len + const.column_data_offset_offset + i * (int_len + 8)
  583:             )
  584:             col_data_len = (
  585:                 offset
  586:                 + 2 * int_len
  587:                 + const.column_data_length_offset
  588:                 + i * (int_len + 8)
  589:             )
  590:             col_types = (
  591:                 offset + 2 * int_len + const.column_type_offset + i * (int_len + 8)
  592:             )
  593: 
  594:             x = self._read_uint(col_data_offset, int_len)
  595:             self._column_data_offsets.append(x)
  596: 
  597:             x = self._read_uint(col_data_len, const.column_data_length_length)
  598:             self._column_data_lengths.append(x)
  599: 
  600:             x = self._read_uint(col_types, const.column_type_length)
  601:             self._column_types.append(b"d" if x == 1 else b"s")
  602: 
  603:     def _process_columnlist_subheader(self, offset: int, length: int) -> None:
  604:         # unknown purpose
  605:         pass
  606: 
  607:     def _process_format_subheader(self, offset: int, length: int) -> None:
  608:         int_len = self._int_length
  609:         text_subheader_format = (
  610:             offset + const.column_format_text_subheader_index_offset + 3 * int_len
  611:         )
  612:         col_format_offset = offset + const.column_format_offset_offset + 3 * int_len
  613:         col_format_len = offset + const.column_format_length_offset + 3 * int_len
  614:         text_subheader_label = (
  615:             offset + const.column_label_text_subheader_index_offset + 3 * int_len
  616:         )
  617:         col_label_offset = offset + const.column_label_offset_offset + 3 * int_len
  618:         col_label_len = offset + const.column_label_length_offset + 3 * int_len
  619: 
  620:         x = self._read_uint(
  621:             text_subheader_format, const.column_format_text_subheader_index_length
  622:         )
  623:         format_idx = min(x, len(self.column_names_raw) - 1)
  624: 
  625:         format_start = self._read_uint(
  626:             col_format_offset, const.column_format_offset_length
  627:         )
  628:         format_len = self._read_uint(col_format_len, const.column_format_length_length)
  629: 
  630:         label_idx = self._read_uint(
  631:             text_subheader_label, const.column_label_text_subheader_index_length
  632:         )
  633:         label_idx = min(label_idx, len(self.column_names_raw) - 1)
  634: 
  635:         label_start = self._read_uint(
  636:             col_label_offset, const.column_label_offset_length
  637:         )
  638:         label_len = self._read_uint(col_label_len, const.column_label_length_length)
  639: 
  640:         label_names = self.column_names_raw[label_idx]
  641:         column_label = self._convert_header_text(
  642:             label_names[label_start : label_start + label_len]
  643:         )
  644:         format_names = self.column_names_raw[format_idx]
  645:         column_format = self._convert_header_text(
  646:             format_names[format_start : format_start + format_len]
  647:         )
  648:         current_column_number = len(self.columns)
  649: 
  650:         col = _Column(
  651:             current_column_number,
  652:             self.column_names[current_column_number],
  653:             column_label,
  654:             column_format,
  655:             self._column_types[current_column_number],
  656:             self._column_data_lengths[current_column_number],
  657:         )
  658: 
  659:         self.column_formats.append(column_format)
  660:         self.columns.append(col)
  661: 
  662:     def read(self, nrows: int | None = None) -> DataFrame:
  663:         if (nrows is None) and (self.chunksize is not None):
  664:             nrows = self.chunksize
  665:         elif nrows is None:
  666:             nrows = self.row_count
  667: 
  668:         if len(self._column_types) == 0:
  669:             self.close()
  670:             raise EmptyDataError("No columns to parse from file")
  671: 
  672:         if nrows > 0 and self._current_row_in_file_index >= self.row_count:
  673:             return DataFrame()
  674: 
  675:         nrows = min(nrows, self.row_count - self._current_row_in_file_index)
  676: 
  677:         nd = self._column_types.count(b"d")
  678:         ns = self._column_types.count(b"s")
  679: 
  680:         self._string_chunk = np.empty((ns, nrows), dtype=object)
  681:         self._byte_chunk = np.zeros((nd, 8 * nrows), dtype=np.uint8)
  682: 
  683:         self._current_row_in_chunk_index = 0
  684:         p = Parser(self)
  685:         p.read(nrows)
  686: 
  687:         rslt = self._chunk_to_dataframe()
  688:         if self.index is not None:
  689:             rslt = rslt.set_index(self.index)
  690: 
  691:         return rslt
  692: 
  693:     def _read_next_page(self):
  694:         self._current_page_data_subheader_pointers = []
  695:         self._cached_page = self._path_or_buf.read(self._page_length)
  696:         if len(self._cached_page) <= 0:
  697:             return True
  698:         elif len(self._cached_page) != self._page_length:
  699:             self.close()
  700:             msg = (
  701:                 "failed to read complete page from file (read "
  702:                 f"{len(self._cached_page):d} of {self._page_length:d} bytes)"
  703:             )
  704:             raise ValueError(msg)
  705: 
  706:         self._read_page_header()
  707:         if self._current_page_type in const.page_meta_types:
  708:             self._process_page_metadata()
  709: 
  710:         if self._current_page_type not in const.page_meta_types + [
  711:             const.page_data_type,
  712:             const.page_mix_type,
  713:         ]:
  714:             return self._read_next_page()
  715: 
  716:         return False
  717: 
  718:     def _chunk_to_dataframe(self) -> DataFrame:
  719:         n = self._current_row_in_chunk_index
  720:         m = self._current_row_in_file_index
  721:         ix = range(m - n, m)
  722:         rslt = {}
  723: 
  724:         js, jb = 0, 0
  725:         for j in range(self.column_count):
  726:             name = self.column_names[j]
  727: 
  728:             if self._column_types[j] == b"d":
  729:                 col_arr = self._byte_chunk[jb, :].view(dtype=self.byte_order + "d")
  730:                 rslt[name] = pd.Series(col_arr, dtype=np.float64, index=ix, copy=False)
  731:                 if self.convert_dates:
  732:                     if self.column_formats[j] in const.sas_date_formats:
  733:                         rslt[name] = _convert_datetimes(rslt[name], "d")
  734:                     elif self.column_formats[j] in const.sas_datetime_formats:
  735:                         rslt[name] = _convert_datetimes(rslt[name], "s")
  736:                 jb += 1
  737:             elif self._column_types[j] == b"s":
  738:                 rslt[name] = pd.Series(self._string_chunk[js, :], index=ix, copy=False)
  739:                 if self.convert_text and (self.encoding is not None):
  740:                     rslt[name] = self._decode_string(rslt[name].str)
  741:                 js += 1
  742:             else:
  743:                 self.close()
  744:                 raise ValueError(f"unknown column type {repr(self._column_types[j])}")
  745: 
  746:         df = DataFrame(rslt, columns=self.column_names, index=ix, copy=False)
  747:         return df
  748: 
  749:     def _decode_string(self, b):
  750:         return b.decode(self.encoding or self.default_encoding)
  751: 
  752:     def _convert_header_text(self, b: bytes) -> str | bytes:
  753:         if self.convert_header_text:
  754:             return self._decode_string(b)
  755:         else:
  756:             return b
