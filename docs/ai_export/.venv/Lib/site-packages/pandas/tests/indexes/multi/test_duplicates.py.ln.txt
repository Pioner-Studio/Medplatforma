    1: from itertools import product
    2: 
    3: import numpy as np
    4: import pytest
    5: 
    6: from pandas._libs import (
    7:     hashtable,
    8:     index as libindex,
    9: )
   10: 
   11: from pandas import (
   12:     NA,
   13:     DatetimeIndex,
   14:     Index,
   15:     MultiIndex,
   16:     Series,
   17: )
   18: import pandas._testing as tm
   19: 
   20: 
   21: @pytest.fixture
   22: def idx_dup():
   23:     # compare tests/indexes/multi/conftest.py
   24:     major_axis = Index(["foo", "bar", "baz", "qux"])
   25:     minor_axis = Index(["one", "two"])
   26: 
   27:     major_codes = np.array([0, 0, 1, 0, 1, 1])
   28:     minor_codes = np.array([0, 1, 0, 1, 0, 1])
   29:     index_names = ["first", "second"]
   30:     mi = MultiIndex(
   31:         levels=[major_axis, minor_axis],
   32:         codes=[major_codes, minor_codes],
   33:         names=index_names,
   34:         verify_integrity=False,
   35:     )
   36:     return mi
   37: 
   38: 
   39: @pytest.mark.parametrize("names", [None, ["first", "second"]])
   40: def test_unique(names):
   41:     mi = MultiIndex.from_arrays([[1, 2, 1, 2], [1, 1, 1, 2]], names=names)
   42: 
   43:     res = mi.unique()
   44:     exp = MultiIndex.from_arrays([[1, 2, 2], [1, 1, 2]], names=mi.names)
   45:     tm.assert_index_equal(res, exp)
   46: 
   47:     mi = MultiIndex.from_arrays([list("aaaa"), list("abab")], names=names)
   48:     res = mi.unique()
   49:     exp = MultiIndex.from_arrays([list("aa"), list("ab")], names=mi.names)
   50:     tm.assert_index_equal(res, exp)
   51: 
   52:     mi = MultiIndex.from_arrays([list("aaaa"), list("aaaa")], names=names)
   53:     res = mi.unique()
   54:     exp = MultiIndex.from_arrays([["a"], ["a"]], names=mi.names)
   55:     tm.assert_index_equal(res, exp)
   56: 
   57:     # GH #20568 - empty MI
   58:     mi = MultiIndex.from_arrays([[], []], names=names)
   59:     res = mi.unique()
   60:     tm.assert_index_equal(mi, res)
   61: 
   62: 
   63: def test_unique_datetimelike():
   64:     idx1 = DatetimeIndex(
   65:         ["2015-01-01", "2015-01-01", "2015-01-01", "2015-01-01", "NaT", "NaT"]
   66:     )
   67:     idx2 = DatetimeIndex(
   68:         ["2015-01-01", "2015-01-01", "2015-01-02", "2015-01-02", "NaT", "2015-01-01"],
   69:         tz="Asia/Tokyo",
   70:     )
   71:     result = MultiIndex.from_arrays([idx1, idx2]).unique()
   72: 
   73:     eidx1 = DatetimeIndex(["2015-01-01", "2015-01-01", "NaT", "NaT"])
   74:     eidx2 = DatetimeIndex(
   75:         ["2015-01-01", "2015-01-02", "NaT", "2015-01-01"], tz="Asia/Tokyo"
   76:     )
   77:     exp = MultiIndex.from_arrays([eidx1, eidx2])
   78:     tm.assert_index_equal(result, exp)
   79: 
   80: 
   81: @pytest.mark.parametrize("level", [0, "first", 1, "second"])
   82: def test_unique_level(idx, level):
   83:     # GH #17896 - with level= argument
   84:     result = idx.unique(level=level)
   85:     expected = idx.get_level_values(level).unique()
   86:     tm.assert_index_equal(result, expected)
   87: 
   88:     # With already unique level
   89:     mi = MultiIndex.from_arrays([[1, 3, 2, 4], [1, 3, 2, 5]], names=["first", "second"])
   90:     result = mi.unique(level=level)
   91:     expected = mi.get_level_values(level)
   92:     tm.assert_index_equal(result, expected)
   93: 
   94:     # With empty MI
   95:     mi = MultiIndex.from_arrays([[], []], names=["first", "second"])
   96:     result = mi.unique(level=level)
   97:     expected = mi.get_level_values(level)
   98:     tm.assert_index_equal(result, expected)
   99: 
  100: 
  101: def test_duplicate_multiindex_codes():
  102:     # GH 17464
  103:     # Make sure that a MultiIndex with duplicate levels throws a ValueError
  104:     msg = r"Level values must be unique: \[[A', ]+\] on level 0"
  105:     with pytest.raises(ValueError, match=msg):
  106:         mi = MultiIndex([["A"] * 10, range(10)], [[0] * 10, range(10)])
  107: 
  108:     # And that using set_levels with duplicate levels fails
  109:     mi = MultiIndex.from_arrays([["A", "A", "B", "B", "B"], [1, 2, 1, 2, 3]])
  110:     msg = r"Level values must be unique: \[[AB', ]+\] on level 0"
  111:     with pytest.raises(ValueError, match=msg):
  112:         mi.set_levels([["A", "B", "A", "A", "B"], [2, 1, 3, -2, 5]])
  113: 
  114: 
  115: @pytest.mark.parametrize("names", [["a", "b", "a"], [1, 1, 2], [1, "a", 1]])
  116: def test_duplicate_level_names(names):
  117:     # GH18872, GH19029
  118:     mi = MultiIndex.from_product([[0, 1]] * 3, names=names)
  119:     assert mi.names == names
  120: 
  121:     # With .rename()
  122:     mi = MultiIndex.from_product([[0, 1]] * 3)
  123:     mi = mi.rename(names)
  124:     assert mi.names == names
  125: 
  126:     # With .rename(., level=)
  127:     mi.rename(names[1], level=1, inplace=True)
  128:     mi = mi.rename([names[0], names[2]], level=[0, 2])
  129:     assert mi.names == names
  130: 
  131: 
  132: def test_duplicate_meta_data():
  133:     # GH 10115
  134:     mi = MultiIndex(
  135:         levels=[[0, 1], [0, 1, 2]], codes=[[0, 0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 0, 1, 2]]
  136:     )
  137: 
  138:     for idx in [
  139:         mi,
  140:         mi.set_names([None, None]),
  141:         mi.set_names([None, "Num"]),
  142:         mi.set_names(["Upper", "Num"]),
  143:     ]:
  144:         assert idx.has_duplicates
  145:         assert idx.drop_duplicates().names == idx.names
  146: 
  147: 
  148: def test_has_duplicates(idx, idx_dup):
  149:     # see fixtures
  150:     assert idx.is_unique is True
  151:     assert idx.has_duplicates is False
  152:     assert idx_dup.is_unique is False
  153:     assert idx_dup.has_duplicates is True
  154: 
  155:     mi = MultiIndex(
  156:         levels=[[0, 1], [0, 1, 2]], codes=[[0, 0, 0, 0, 1, 1, 1], [0, 1, 2, 0, 0, 1, 2]]
  157:     )
  158:     assert mi.is_unique is False
  159:     assert mi.has_duplicates is True
  160: 
  161:     # single instance of NaN
  162:     mi_nan = MultiIndex(
  163:         levels=[["a", "b"], [0, 1]], codes=[[-1, 0, 0, 1, 1], [-1, 0, 1, 0, 1]]
  164:     )
  165:     assert mi_nan.is_unique is True
  166:     assert mi_nan.has_duplicates is False
  167: 
  168:     # multiple instances of NaN
  169:     mi_nan_dup = MultiIndex(
  170:         levels=[["a", "b"], [0, 1]], codes=[[-1, -1, 0, 0, 1, 1], [-1, -1, 0, 1, 0, 1]]
  171:     )
  172:     assert mi_nan_dup.is_unique is False
  173:     assert mi_nan_dup.has_duplicates is True
  174: 
  175: 
  176: def test_has_duplicates_from_tuples():
  177:     # GH 9075
  178:     t = [
  179:         ("x", "out", "z", 5, "y", "in", "z", 169),
  180:         ("x", "out", "z", 7, "y", "in", "z", 119),
  181:         ("x", "out", "z", 9, "y", "in", "z", 135),
  182:         ("x", "out", "z", 13, "y", "in", "z", 145),
  183:         ("x", "out", "z", 14, "y", "in", "z", 158),
  184:         ("x", "out", "z", 16, "y", "in", "z", 122),
  185:         ("x", "out", "z", 17, "y", "in", "z", 160),
  186:         ("x", "out", "z", 18, "y", "in", "z", 180),
  187:         ("x", "out", "z", 20, "y", "in", "z", 143),
  188:         ("x", "out", "z", 21, "y", "in", "z", 128),
  189:         ("x", "out", "z", 22, "y", "in", "z", 129),
  190:         ("x", "out", "z", 25, "y", "in", "z", 111),
  191:         ("x", "out", "z", 28, "y", "in", "z", 114),
  192:         ("x", "out", "z", 29, "y", "in", "z", 121),
  193:         ("x", "out", "z", 31, "y", "in", "z", 126),
  194:         ("x", "out", "z", 32, "y", "in", "z", 155),
  195:         ("x", "out", "z", 33, "y", "in", "z", 123),
  196:         ("x", "out", "z", 12, "y", "in", "z", 144),
  197:     ]
  198: 
  199:     mi = MultiIndex.from_tuples(t)
  200:     assert not mi.has_duplicates
  201: 
  202: 
  203: @pytest.mark.parametrize("nlevels", [4, 8])
  204: @pytest.mark.parametrize("with_nulls", [True, False])
  205: def test_has_duplicates_overflow(nlevels, with_nulls):
  206:     # handle int64 overflow if possible
  207:     # no overflow with 4
  208:     # overflow possible with 8
  209:     codes = np.tile(np.arange(500), 2)
  210:     level = np.arange(500)
  211: 
  212:     if with_nulls:  # inject some null values
  213:         codes[500] = -1  # common nan value
  214:         codes = [codes.copy() for i in range(nlevels)]
  215:         for i in range(nlevels):
  216:             codes[i][500 + i - nlevels // 2] = -1
  217: 
  218:         codes += [np.array([-1, 1]).repeat(500)]
  219:     else:
  220:         codes = [codes] * nlevels + [np.arange(2).repeat(500)]
  221: 
  222:     levels = [level] * nlevels + [[0, 1]]
  223: 
  224:     # no dups
  225:     mi = MultiIndex(levels=levels, codes=codes)
  226:     assert not mi.has_duplicates
  227: 
  228:     # with a dup
  229:     if with_nulls:
  230: 
  231:         def f(a):
  232:             return np.insert(a, 1000, a[0])
  233: 
  234:         codes = list(map(f, codes))
  235:         mi = MultiIndex(levels=levels, codes=codes)
  236:     else:
  237:         values = mi.values.tolist()
  238:         mi = MultiIndex.from_tuples(values + [values[0]])
  239: 
  240:     assert mi.has_duplicates
  241: 
  242: 
  243: @pytest.mark.parametrize(
  244:     "keep, expected",
  245:     [
  246:         ("first", np.array([False, False, False, True, True, False])),
  247:         ("last", np.array([False, True, True, False, False, False])),
  248:         (False, np.array([False, True, True, True, True, False])),
  249:     ],
  250: )
  251: def test_duplicated(idx_dup, keep, expected):
  252:     result = idx_dup.duplicated(keep=keep)
  253:     tm.assert_numpy_array_equal(result, expected)
  254: 
  255: 
  256: @pytest.mark.arm_slow
  257: def test_duplicated_hashtable_impl(keep, monkeypatch):
  258:     # GH 9125
  259:     n, k = 6, 10
  260:     levels = [np.arange(n), [str(i) for i in range(n)], 1000 + np.arange(n)]
  261:     codes = [np.random.default_rng(2).choice(n, k * n) for _ in levels]
  262:     with monkeypatch.context() as m:
  263:         m.setattr(libindex, "_SIZE_CUTOFF", 50)
  264:         mi = MultiIndex(levels=levels, codes=codes)
  265: 
  266:         result = mi.duplicated(keep=keep)
  267:         expected = hashtable.duplicated(mi.values, keep=keep)
  268:     tm.assert_numpy_array_equal(result, expected)
  269: 
  270: 
  271: @pytest.mark.parametrize("val", [101, 102])
  272: def test_duplicated_with_nan(val):
  273:     # GH5873
  274:     mi = MultiIndex.from_arrays([[101, val], [3.5, np.nan]])
  275:     assert not mi.has_duplicates
  276: 
  277:     tm.assert_numpy_array_equal(mi.duplicated(), np.zeros(2, dtype="bool"))
  278: 
  279: 
  280: @pytest.mark.parametrize("n", range(1, 6))
  281: @pytest.mark.parametrize("m", range(1, 5))
  282: def test_duplicated_with_nan_multi_shape(n, m):
  283:     # GH5873
  284:     # all possible unique combinations, including nan
  285:     codes = product(range(-1, n), range(-1, m))
  286:     mi = MultiIndex(
  287:         levels=[list("abcde")[:n], list("WXYZ")[:m]],
  288:         codes=np.random.default_rng(2).permutation(list(codes)).T,
  289:     )
  290:     assert len(mi) == (n + 1) * (m + 1)
  291:     assert not mi.has_duplicates
  292: 
  293:     tm.assert_numpy_array_equal(mi.duplicated(), np.zeros(len(mi), dtype="bool"))
  294: 
  295: 
  296: def test_duplicated_drop_duplicates():
  297:     # GH#4060
  298:     idx = MultiIndex.from_arrays(([1, 2, 3, 1, 2, 3], [1, 1, 1, 1, 2, 2]))
  299: 
  300:     expected = np.array([False, False, False, True, False, False], dtype=bool)
  301:     duplicated = idx.duplicated()
  302:     tm.assert_numpy_array_equal(duplicated, expected)
  303:     assert duplicated.dtype == bool
  304:     expected = MultiIndex.from_arrays(([1, 2, 3, 2, 3], [1, 1, 1, 2, 2]))
  305:     tm.assert_index_equal(idx.drop_duplicates(), expected)
  306: 
  307:     expected = np.array([True, False, False, False, False, False])
  308:     duplicated = idx.duplicated(keep="last")
  309:     tm.assert_numpy_array_equal(duplicated, expected)
  310:     assert duplicated.dtype == bool
  311:     expected = MultiIndex.from_arrays(([2, 3, 1, 2, 3], [1, 1, 1, 2, 2]))
  312:     tm.assert_index_equal(idx.drop_duplicates(keep="last"), expected)
  313: 
  314:     expected = np.array([True, False, False, True, False, False])
  315:     duplicated = idx.duplicated(keep=False)
  316:     tm.assert_numpy_array_equal(duplicated, expected)
  317:     assert duplicated.dtype == bool
  318:     expected = MultiIndex.from_arrays(([2, 3, 2, 3], [1, 1, 2, 2]))
  319:     tm.assert_index_equal(idx.drop_duplicates(keep=False), expected)
  320: 
  321: 
  322: @pytest.mark.parametrize(
  323:     "dtype",
  324:     [
  325:         np.complex64,
  326:         np.complex128,
  327:     ],
  328: )
  329: def test_duplicated_series_complex_numbers(dtype):
  330:     # GH 17927
  331:     expected = Series(
  332:         [False, False, False, True, False, False, False, True, False, True],
  333:         dtype=bool,
  334:     )
  335:     result = Series(
  336:         [
  337:             np.nan + np.nan * 1j,
  338:             0,
  339:             1j,
  340:             1j,
  341:             1,
  342:             1 + 1j,
  343:             1 + 2j,
  344:             1 + 1j,
  345:             np.nan,
  346:             np.nan + np.nan * 1j,
  347:         ],
  348:         dtype=dtype,
  349:     ).duplicated()
  350:     tm.assert_series_equal(result, expected)
  351: 
  352: 
  353: def test_midx_unique_ea_dtype():
  354:     # GH#48335
  355:     vals_a = Series([1, 2, NA, NA], dtype="Int64")
  356:     vals_b = np.array([1, 2, 3, 3])
  357:     midx = MultiIndex.from_arrays([vals_a, vals_b], names=["a", "b"])
  358:     result = midx.unique()
  359: 
  360:     exp_vals_a = Series([1, 2, NA], dtype="Int64")
  361:     exp_vals_b = np.array([1, 2, 3])
  362:     expected = MultiIndex.from_arrays([exp_vals_a, exp_vals_b], names=["a", "b"])
  363:     tm.assert_index_equal(result, expected)
