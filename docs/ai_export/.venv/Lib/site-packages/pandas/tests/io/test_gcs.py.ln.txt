    1: from io import BytesIO
    2: import os
    3: import pathlib
    4: import tarfile
    5: import zipfile
    6: 
    7: import numpy as np
    8: import pytest
    9: 
   10: from pandas import (
   11:     DataFrame,
   12:     Index,
   13:     date_range,
   14:     read_csv,
   15:     read_excel,
   16:     read_json,
   17:     read_parquet,
   18: )
   19: import pandas._testing as tm
   20: from pandas.util import _test_decorators as td
   21: 
   22: pytestmark = pytest.mark.filterwarnings(
   23:     "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   24: )
   25: 
   26: 
   27: @pytest.fixture
   28: def gcs_buffer():
   29:     """Emulate GCS using a binary buffer."""
   30:     pytest.importorskip("gcsfs")
   31:     fsspec = pytest.importorskip("fsspec")
   32: 
   33:     gcs_buffer = BytesIO()
   34:     gcs_buffer.close = lambda: True
   35: 
   36:     class MockGCSFileSystem(fsspec.AbstractFileSystem):
   37:         @staticmethod
   38:         def open(*args, **kwargs):
   39:             gcs_buffer.seek(0)
   40:             return gcs_buffer
   41: 
   42:         def ls(self, path, **kwargs):
   43:             # needed for pyarrow
   44:             return [{"name": path, "type": "file"}]
   45: 
   46:     # Overwrites the default implementation from gcsfs to our mock class
   47:     fsspec.register_implementation("gs", MockGCSFileSystem, clobber=True)
   48: 
   49:     return gcs_buffer
   50: 
   51: 
   52: # Patches pyarrow; other processes should not pick up change
   53: @pytest.mark.single_cpu
   54: @pytest.mark.parametrize("format", ["csv", "json", "parquet", "excel", "markdown"])
   55: def test_to_read_gcs(gcs_buffer, format, monkeypatch, capsys):
   56:     """
   57:     Test that many to/read functions support GCS.
   58: 
   59:     GH 33987
   60:     """
   61: 
   62:     df1 = DataFrame(
   63:         {
   64:             "int": [1, 3],
   65:             "float": [2.0, np.nan],
   66:             "str": ["t", "s"],
   67:             "dt": date_range("2018-06-18", periods=2),
   68:         }
   69:     )
   70: 
   71:     path = f"gs://test/test.{format}"
   72: 
   73:     if format == "csv":
   74:         df1.to_csv(path, index=True)
   75:         df2 = read_csv(path, parse_dates=["dt"], index_col=0)
   76:     elif format == "excel":
   77:         path = "gs://test/test.xlsx"
   78:         df1.to_excel(path)
   79:         df2 = read_excel(path, parse_dates=["dt"], index_col=0)
   80:     elif format == "json":
   81:         df1.to_json(path)
   82:         df2 = read_json(path, convert_dates=["dt"])
   83:     elif format == "parquet":
   84:         pytest.importorskip("pyarrow")
   85:         pa_fs = pytest.importorskip("pyarrow.fs")
   86: 
   87:         class MockFileSystem(pa_fs.FileSystem):
   88:             @staticmethod
   89:             def from_uri(path):
   90:                 print("Using pyarrow filesystem")
   91:                 to_local = pathlib.Path(path.replace("gs://", "")).absolute().as_uri()
   92:                 return pa_fs.LocalFileSystem(to_local)
   93: 
   94:         with monkeypatch.context() as m:
   95:             m.setattr(pa_fs, "FileSystem", MockFileSystem)
   96:             df1.to_parquet(path)
   97:             df2 = read_parquet(path)
   98:         captured = capsys.readouterr()
   99:         assert captured.out == "Using pyarrow filesystem\nUsing pyarrow filesystem\n"
  100:     elif format == "markdown":
  101:         pytest.importorskip("tabulate")
  102:         df1.to_markdown(path)
  103:         df2 = df1
  104: 
  105:     tm.assert_frame_equal(df1, df2)
  106: 
  107: 
  108: def assert_equal_zip_safe(result: bytes, expected: bytes, compression: str):
  109:     """
  110:     For zip compression, only compare the CRC-32 checksum of the file contents
  111:     to avoid checking the time-dependent last-modified timestamp which
  112:     in some CI builds is off-by-one
  113: 
  114:     See https://en.wikipedia.org/wiki/ZIP_(file_format)#File_headers
  115:     """
  116:     if compression == "zip":
  117:         # Only compare the CRC checksum of the file contents
  118:         with zipfile.ZipFile(BytesIO(result)) as exp, zipfile.ZipFile(
  119:             BytesIO(expected)
  120:         ) as res:
  121:             for res_info, exp_info in zip(res.infolist(), exp.infolist()):
  122:                 assert res_info.CRC == exp_info.CRC
  123:     elif compression == "tar":
  124:         with tarfile.open(fileobj=BytesIO(result)) as tar_exp, tarfile.open(
  125:             fileobj=BytesIO(expected)
  126:         ) as tar_res:
  127:             for tar_res_info, tar_exp_info in zip(
  128:                 tar_res.getmembers(), tar_exp.getmembers()
  129:             ):
  130:                 actual_file = tar_res.extractfile(tar_res_info)
  131:                 expected_file = tar_exp.extractfile(tar_exp_info)
  132:                 assert (actual_file is None) == (expected_file is None)
  133:                 if actual_file is not None and expected_file is not None:
  134:                     assert actual_file.read() == expected_file.read()
  135:     else:
  136:         assert result == expected
  137: 
  138: 
  139: @pytest.mark.parametrize("encoding", ["utf-8", "cp1251"])
  140: def test_to_csv_compression_encoding_gcs(
  141:     gcs_buffer, compression_only, encoding, compression_to_extension
  142: ):
  143:     """
  144:     Compression and encoding should with GCS.
  145: 
  146:     GH 35677 (to_csv, compression), GH 26124 (to_csv, encoding), and
  147:     GH 32392 (read_csv, encoding)
  148:     """
  149:     df = DataFrame(
  150:         1.1 * np.arange(120).reshape((30, 4)),
  151:         columns=Index(list("ABCD"), dtype=object),
  152:         index=Index([f"i-{i}" for i in range(30)], dtype=object),
  153:     )
  154: 
  155:     # reference of compressed and encoded file
  156:     compression = {"method": compression_only}
  157:     if compression_only == "gzip":
  158:         compression["mtime"] = 1  # be reproducible
  159:     buffer = BytesIO()
  160:     df.to_csv(buffer, compression=compression, encoding=encoding, mode="wb")
  161: 
  162:     # write compressed file with explicit compression
  163:     path_gcs = "gs://test/test.csv"
  164:     df.to_csv(path_gcs, compression=compression, encoding=encoding)
  165:     res = gcs_buffer.getvalue()
  166:     expected = buffer.getvalue()
  167:     assert_equal_zip_safe(res, expected, compression_only)
  168: 
  169:     read_df = read_csv(
  170:         path_gcs, index_col=0, compression=compression_only, encoding=encoding
  171:     )
  172:     tm.assert_frame_equal(df, read_df)
  173: 
  174:     # write compressed file with implicit compression
  175:     file_ext = compression_to_extension[compression_only]
  176:     compression["method"] = "infer"
  177:     path_gcs += f".{file_ext}"
  178:     df.to_csv(path_gcs, compression=compression, encoding=encoding)
  179: 
  180:     res = gcs_buffer.getvalue()
  181:     expected = buffer.getvalue()
  182:     assert_equal_zip_safe(res, expected, compression_only)
  183: 
  184:     read_df = read_csv(path_gcs, index_col=0, compression="infer", encoding=encoding)
  185:     tm.assert_frame_equal(df, read_df)
  186: 
  187: 
  188: def test_to_parquet_gcs_new_file(monkeypatch, tmpdir):
  189:     """Regression test for writing to a not-yet-existent GCS Parquet file."""
  190:     pytest.importorskip("fastparquet")
  191:     pytest.importorskip("gcsfs")
  192: 
  193:     from fsspec import AbstractFileSystem
  194: 
  195:     df1 = DataFrame(
  196:         {
  197:             "int": [1, 3],
  198:             "float": [2.0, np.nan],
  199:             "str": ["t", "s"],
  200:             "dt": date_range("2018-06-18", periods=2),
  201:         }
  202:     )
  203: 
  204:     class MockGCSFileSystem(AbstractFileSystem):
  205:         def open(self, path, mode="r", *args):
  206:             if "w" not in mode:
  207:                 raise FileNotFoundError
  208:             return open(os.path.join(tmpdir, "test.parquet"), mode, encoding="utf-8")
  209: 
  210:     monkeypatch.setattr("gcsfs.GCSFileSystem", MockGCSFileSystem)
  211:     df1.to_parquet(
  212:         "gs://test/test.csv", index=True, engine="fastparquet", compression=None
  213:     )
  214: 
  215: 
  216: @td.skip_if_installed("gcsfs")
  217: def test_gcs_not_present_exception():
  218:     with tm.external_error_raised(ImportError):
  219:         read_csv("gs://test/test.csv")
