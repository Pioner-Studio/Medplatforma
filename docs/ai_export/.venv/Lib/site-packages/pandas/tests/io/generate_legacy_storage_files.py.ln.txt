    1: """
    2: self-contained to write legacy storage pickle files
    3: 
    4: To use this script. Create an environment where you want
    5: generate pickles, say its for 0.20.3, with your pandas clone
    6: in ~/pandas
    7: 
    8: . activate pandas_0.20.3
    9: cd ~/pandas/pandas
   10: 
   11: $ python -m tests.io.generate_legacy_storage_files \
   12:     tests/io/data/legacy_pickle/0.20.3/ pickle
   13: 
   14: This script generates a storage file for the current arch, system,
   15: and python version
   16:   pandas version: 0.20.3
   17:   output dir    : pandas/pandas/tests/io/data/legacy_pickle/0.20.3/
   18:   storage format: pickle
   19: created pickle file: 0.20.3_x86_64_darwin_3.5.2.pickle
   20: 
   21: The idea here is you are using the *current* version of the
   22: generate_legacy_storage_files with an *older* version of pandas to
   23: generate a pickle file. We will then check this file into a current
   24: branch, and test using test_pickle.py. This will load the *older*
   25: pickles and test versus the current data that is generated
   26: (with main). These are then compared.
   27: 
   28: If we have cases where we changed the signature (e.g. we renamed
   29: offset -> freq in Timestamp). Then we have to conditionally execute
   30: in the generate_legacy_storage_files.py to make it
   31: run under the older AND the newer version.
   32: 
   33: """
   34: 
   35: from datetime import timedelta
   36: import os
   37: import pickle
   38: import platform as pl
   39: import sys
   40: 
   41: # Remove script directory from path, otherwise Python will try to
   42: # import the JSON test directory as the json module
   43: sys.path.pop(0)
   44: 
   45: import numpy as np
   46: 
   47: import pandas
   48: from pandas import (
   49:     Categorical,
   50:     DataFrame,
   51:     Index,
   52:     MultiIndex,
   53:     NaT,
   54:     Period,
   55:     RangeIndex,
   56:     Series,
   57:     Timestamp,
   58:     bdate_range,
   59:     date_range,
   60:     interval_range,
   61:     period_range,
   62:     timedelta_range,
   63: )
   64: from pandas.arrays import SparseArray
   65: 
   66: from pandas.tseries.offsets import (
   67:     FY5253,
   68:     BusinessDay,
   69:     BusinessHour,
   70:     CustomBusinessDay,
   71:     DateOffset,
   72:     Day,
   73:     Easter,
   74:     Hour,
   75:     LastWeekOfMonth,
   76:     Minute,
   77:     MonthBegin,
   78:     MonthEnd,
   79:     QuarterBegin,
   80:     QuarterEnd,
   81:     SemiMonthBegin,
   82:     SemiMonthEnd,
   83:     Week,
   84:     WeekOfMonth,
   85:     YearBegin,
   86:     YearEnd,
   87: )
   88: 
   89: 
   90: def _create_sp_series():
   91:     nan = np.nan
   92: 
   93:     # nan-based
   94:     arr = np.arange(15, dtype=np.float64)
   95:     arr[7:12] = nan
   96:     arr[-1:] = nan
   97: 
   98:     bseries = Series(SparseArray(arr, kind="block"))
   99:     bseries.name = "bseries"
  100:     return bseries
  101: 
  102: 
  103: def _create_sp_tsseries():
  104:     nan = np.nan
  105: 
  106:     # nan-based
  107:     arr = np.arange(15, dtype=np.float64)
  108:     arr[7:12] = nan
  109:     arr[-1:] = nan
  110: 
  111:     date_index = bdate_range("1/1/2011", periods=len(arr))
  112:     bseries = Series(SparseArray(arr, kind="block"), index=date_index)
  113:     bseries.name = "btsseries"
  114:     return bseries
  115: 
  116: 
  117: def _create_sp_frame():
  118:     nan = np.nan
  119: 
  120:     data = {
  121:         "A": [nan, nan, nan, 0, 1, 2, 3, 4, 5, 6],
  122:         "B": [0, 1, 2, nan, nan, nan, 3, 4, 5, 6],
  123:         "C": np.arange(10).astype(np.int64),
  124:         "D": [0, 1, 2, 3, 4, 5, nan, nan, nan, nan],
  125:     }
  126: 
  127:     dates = bdate_range("1/1/2011", periods=10)
  128:     return DataFrame(data, index=dates).apply(SparseArray)
  129: 
  130: 
  131: def create_pickle_data():
  132:     """create the pickle data"""
  133:     data = {
  134:         "A": [0.0, 1.0, 2.0, 3.0, np.nan],
  135:         "B": [0, 1, 0, 1, 0],
  136:         "C": ["foo1", "foo2", "foo3", "foo4", "foo5"],
  137:         "D": date_range("1/1/2009", periods=5),
  138:         "E": [0.0, 1, Timestamp("20100101"), "foo", 2.0],
  139:     }
  140: 
  141:     scalars = {"timestamp": Timestamp("20130101"), "period": Period("2012", "M")}
  142: 
  143:     index = {
  144:         "int": Index(np.arange(10)),
  145:         "date": date_range("20130101", periods=10),
  146:         "period": period_range("2013-01-01", freq="M", periods=10),
  147:         "float": Index(np.arange(10, dtype=np.float64)),
  148:         "uint": Index(np.arange(10, dtype=np.uint64)),
  149:         "timedelta": timedelta_range("00:00:00", freq="30min", periods=10),
  150:     }
  151: 
  152:     index["range"] = RangeIndex(10)
  153: 
  154:     index["interval"] = interval_range(0, periods=10)
  155: 
  156:     mi = {
  157:         "reg2": MultiIndex.from_tuples(
  158:             tuple(
  159:                 zip(
  160:                     *[
  161:                         ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
  162:                         ["one", "two", "one", "two", "one", "two", "one", "two"],
  163:                     ]
  164:                 )
  165:             ),
  166:             names=["first", "second"],
  167:         )
  168:     }
  169: 
  170:     series = {
  171:         "float": Series(data["A"]),
  172:         "int": Series(data["B"]),
  173:         "mixed": Series(data["E"]),
  174:         "ts": Series(
  175:             np.arange(10).astype(np.int64), index=date_range("20130101", periods=10)
  176:         ),
  177:         "mi": Series(
  178:             np.arange(5).astype(np.float64),
  179:             index=MultiIndex.from_tuples(
  180:                 tuple(zip(*[[1, 1, 2, 2, 2], [3, 4, 3, 4, 5]])), names=["one", "two"]
  181:             ),
  182:         ),
  183:         "dup": Series(np.arange(5).astype(np.float64), index=["A", "B", "C", "D", "A"]),
  184:         "cat": Series(Categorical(["foo", "bar", "baz"])),
  185:         "dt": Series(date_range("20130101", periods=5)),
  186:         "dt_tz": Series(date_range("20130101", periods=5, tz="US/Eastern")),
  187:         "period": Series([Period("2000Q1")] * 5),
  188:     }
  189: 
  190:     mixed_dup_df = DataFrame(data)
  191:     mixed_dup_df.columns = list("ABCDA")
  192:     frame = {
  193:         "float": DataFrame({"A": series["float"], "B": series["float"] + 1}),
  194:         "int": DataFrame({"A": series["int"], "B": series["int"] + 1}),
  195:         "mixed": DataFrame({k: data[k] for k in ["A", "B", "C", "D"]}),
  196:         "mi": DataFrame(
  197:             {"A": np.arange(5).astype(np.float64), "B": np.arange(5).astype(np.int64)},
  198:             index=MultiIndex.from_tuples(
  199:                 tuple(
  200:                     zip(
  201:                         *[
  202:                             ["bar", "bar", "baz", "baz", "baz"],
  203:                             ["one", "two", "one", "two", "three"],
  204:                         ]
  205:                     )
  206:                 ),
  207:                 names=["first", "second"],
  208:             ),
  209:         ),
  210:         "dup": DataFrame(
  211:             np.arange(15).reshape(5, 3).astype(np.float64), columns=["A", "B", "A"]
  212:         ),
  213:         "cat_onecol": DataFrame({"A": Categorical(["foo", "bar"])}),
  214:         "cat_and_float": DataFrame(
  215:             {
  216:                 "A": Categorical(["foo", "bar", "baz"]),
  217:                 "B": np.arange(3).astype(np.int64),
  218:             }
  219:         ),
  220:         "mixed_dup": mixed_dup_df,
  221:         "dt_mixed_tzs": DataFrame(
  222:             {
  223:                 "A": Timestamp("20130102", tz="US/Eastern"),
  224:                 "B": Timestamp("20130603", tz="CET"),
  225:             },
  226:             index=range(5),
  227:         ),
  228:         "dt_mixed2_tzs": DataFrame(
  229:             {
  230:                 "A": Timestamp("20130102", tz="US/Eastern"),
  231:                 "B": Timestamp("20130603", tz="CET"),
  232:                 "C": Timestamp("20130603", tz="UTC"),
  233:             },
  234:             index=range(5),
  235:         ),
  236:     }
  237: 
  238:     cat = {
  239:         "int8": Categorical(list("abcdefg")),
  240:         "int16": Categorical(np.arange(1000)),
  241:         "int32": Categorical(np.arange(10000)),
  242:     }
  243: 
  244:     timestamp = {
  245:         "normal": Timestamp("2011-01-01"),
  246:         "nat": NaT,
  247:         "tz": Timestamp("2011-01-01", tz="US/Eastern"),
  248:     }
  249: 
  250:     off = {
  251:         "DateOffset": DateOffset(years=1),
  252:         "DateOffset_h_ns": DateOffset(hour=6, nanoseconds=5824),
  253:         "BusinessDay": BusinessDay(offset=timedelta(seconds=9)),
  254:         "BusinessHour": BusinessHour(normalize=True, n=6, end="15:14"),
  255:         "CustomBusinessDay": CustomBusinessDay(weekmask="Mon Fri"),
  256:         "SemiMonthBegin": SemiMonthBegin(day_of_month=9),
  257:         "SemiMonthEnd": SemiMonthEnd(day_of_month=24),
  258:         "MonthBegin": MonthBegin(1),
  259:         "MonthEnd": MonthEnd(1),
  260:         "QuarterBegin": QuarterBegin(1),
  261:         "QuarterEnd": QuarterEnd(1),
  262:         "Day": Day(1),
  263:         "YearBegin": YearBegin(1),
  264:         "YearEnd": YearEnd(1),
  265:         "Week": Week(1),
  266:         "Week_Tues": Week(2, normalize=False, weekday=1),
  267:         "WeekOfMonth": WeekOfMonth(week=3, weekday=4),
  268:         "LastWeekOfMonth": LastWeekOfMonth(n=1, weekday=3),
  269:         "FY5253": FY5253(n=2, weekday=6, startingMonth=7, variation="last"),
  270:         "Easter": Easter(),
  271:         "Hour": Hour(1),
  272:         "Minute": Minute(1),
  273:     }
  274: 
  275:     return {
  276:         "series": series,
  277:         "frame": frame,
  278:         "index": index,
  279:         "scalars": scalars,
  280:         "mi": mi,
  281:         "sp_series": {"float": _create_sp_series(), "ts": _create_sp_tsseries()},
  282:         "sp_frame": {"float": _create_sp_frame()},
  283:         "cat": cat,
  284:         "timestamp": timestamp,
  285:         "offsets": off,
  286:     }
  287: 
  288: 
  289: def platform_name():
  290:     return "_".join(
  291:         [
  292:             str(pandas.__version__),
  293:             str(pl.machine()),
  294:             str(pl.system().lower()),
  295:             str(pl.python_version()),
  296:         ]
  297:     )
  298: 
  299: 
  300: def write_legacy_pickles(output_dir):
  301:     version = pandas.__version__
  302: 
  303:     print(
  304:         "This script generates a storage file for the current arch, system, "
  305:         "and python version"
  306:     )
  307:     print(f"  pandas version: {version}")
  308:     print(f"  output dir    : {output_dir}")
  309:     print("  storage format: pickle")
  310: 
  311:     pth = f"{platform_name()}.pickle"
  312: 
  313:     with open(os.path.join(output_dir, pth), "wb") as fh:
  314:         pickle.dump(create_pickle_data(), fh, pickle.DEFAULT_PROTOCOL)
  315: 
  316:     print(f"created pickle file: {pth}")
  317: 
  318: 
  319: def write_legacy_file():
  320:     # force our cwd to be the first searched
  321:     sys.path.insert(0, "")
  322: 
  323:     if not 3 <= len(sys.argv) <= 4:
  324:         sys.exit(
  325:             "Specify output directory and storage type: generate_legacy_"
  326:             "storage_files.py <output_dir> <storage_type> "
  327:         )
  328: 
  329:     output_dir = str(sys.argv[1])
  330:     storage_type = str(sys.argv[2])
  331: 
  332:     if not os.path.exists(output_dir):
  333:         os.mkdir(output_dir)
  334: 
  335:     if storage_type == "pickle":
  336:         write_legacy_pickles(output_dir=output_dir)
  337:     else:
  338:         sys.exit("storage_type must be one of {'pickle'}")
  339: 
  340: 
  341: if __name__ == "__main__":
  342:     write_legacy_file()
