    1: """
    2: Tests that apply specifically to the CParser. Unless specifically stated
    3: as a CParser-specific issue, the goal is to eventually move as many of
    4: these tests out of this module as soon as the Python parser can accept
    5: further arguments when parsing.
    6: """
    7: from decimal import Decimal
    8: from io import (
    9:     BytesIO,
   10:     StringIO,
   11:     TextIOWrapper,
   12: )
   13: import mmap
   14: import os
   15: import tarfile
   16: 
   17: import numpy as np
   18: import pytest
   19: 
   20: from pandas.compat.numpy import np_version_gte1p24
   21: from pandas.errors import (
   22:     ParserError,
   23:     ParserWarning,
   24: )
   25: import pandas.util._test_decorators as td
   26: 
   27: from pandas import (
   28:     DataFrame,
   29:     concat,
   30: )
   31: import pandas._testing as tm
   32: 
   33: 
   34: @pytest.mark.parametrize(
   35:     "malformed",
   36:     ["1\r1\r1\r 1\r 1\r", "1\r1\r1\r 1\r 1\r11\r", "1\r1\r1\r 1\r 1\r11\r1\r"],
   37:     ids=["words pointer", "stream pointer", "lines pointer"],
   38: )
   39: def test_buffer_overflow(c_parser_only, malformed):
   40:     # see gh-9205: test certain malformed input files that cause
   41:     # buffer overflows in tokenizer.c
   42:     msg = "Buffer overflow caught - possible malformed input file."
   43:     parser = c_parser_only
   44: 
   45:     with pytest.raises(ParserError, match=msg):
   46:         parser.read_csv(StringIO(malformed))
   47: 
   48: 
   49: def test_delim_whitespace_custom_terminator(c_parser_only):
   50:     # See gh-12912
   51:     data = "a b c~1 2 3~4 5 6~7 8 9"
   52:     parser = c_parser_only
   53: 
   54:     depr_msg = "The 'delim_whitespace' keyword in pd.read_csv is deprecated"
   55:     with tm.assert_produces_warning(
   56:         FutureWarning, match=depr_msg, check_stacklevel=False
   57:     ):
   58:         df = parser.read_csv(StringIO(data), lineterminator="~", delim_whitespace=True)
   59:     expected = DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=["a", "b", "c"])
   60:     tm.assert_frame_equal(df, expected)
   61: 
   62: 
   63: def test_dtype_and_names_error(c_parser_only):
   64:     # see gh-8833: passing both dtype and names
   65:     # resulting in an error reporting issue
   66:     parser = c_parser_only
   67:     data = """
   68: 1.0 1
   69: 2.0 2
   70: 3.0 3
   71: """
   72:     # base cases
   73:     result = parser.read_csv(StringIO(data), sep=r"\s+", header=None)
   74:     expected = DataFrame([[1.0, 1], [2.0, 2], [3.0, 3]])
   75:     tm.assert_frame_equal(result, expected)
   76: 
   77:     result = parser.read_csv(StringIO(data), sep=r"\s+", header=None, names=["a", "b"])
   78:     expected = DataFrame([[1.0, 1], [2.0, 2], [3.0, 3]], columns=["a", "b"])
   79:     tm.assert_frame_equal(result, expected)
   80: 
   81:     # fallback casting
   82:     result = parser.read_csv(
   83:         StringIO(data), sep=r"\s+", header=None, names=["a", "b"], dtype={"a": np.int32}
   84:     )
   85:     expected = DataFrame([[1, 1], [2, 2], [3, 3]], columns=["a", "b"])
   86:     expected["a"] = expected["a"].astype(np.int32)
   87:     tm.assert_frame_equal(result, expected)
   88: 
   89:     data = """
   90: 1.0 1
   91: nan 2
   92: 3.0 3
   93: """
   94:     # fallback casting, but not castable
   95:     warning = RuntimeWarning if np_version_gte1p24 else None
   96:     with pytest.raises(ValueError, match="cannot safely convert"):
   97:         with tm.assert_produces_warning(warning, check_stacklevel=False):
   98:             parser.read_csv(
   99:                 StringIO(data),
  100:                 sep=r"\s+",
  101:                 header=None,
  102:                 names=["a", "b"],
  103:                 dtype={"a": np.int32},
  104:             )
  105: 
  106: 
  107: @pytest.mark.parametrize(
  108:     "match,kwargs",
  109:     [
  110:         # For each of these cases, all of the dtypes are valid, just unsupported.
  111:         (
  112:             (
  113:                 "the dtype datetime64 is not supported for parsing, "
  114:                 "pass this column using parse_dates instead"
  115:             ),
  116:             {"dtype": {"A": "datetime64", "B": "float64"}},
  117:         ),
  118:         (
  119:             (
  120:                 "the dtype datetime64 is not supported for parsing, "
  121:                 "pass this column using parse_dates instead"
  122:             ),
  123:             {"dtype": {"A": "datetime64", "B": "float64"}, "parse_dates": ["B"]},
  124:         ),
  125:         (
  126:             "the dtype timedelta64 is not supported for parsing",
  127:             {"dtype": {"A": "timedelta64", "B": "float64"}},
  128:         ),
  129:         (
  130:             f"the dtype {tm.ENDIAN}U8 is not supported for parsing",
  131:             {"dtype": {"A": "U8"}},
  132:         ),
  133:     ],
  134:     ids=["dt64-0", "dt64-1", "td64", f"{tm.ENDIAN}U8"],
  135: )
  136: def test_unsupported_dtype(c_parser_only, match, kwargs):
  137:     parser = c_parser_only
  138:     df = DataFrame(
  139:         np.random.default_rng(2).random((5, 2)),
  140:         columns=list("AB"),
  141:         index=["1A", "1B", "1C", "1D", "1E"],
  142:     )
  143: 
  144:     with tm.ensure_clean("__unsupported_dtype__.csv") as path:
  145:         df.to_csv(path)
  146: 
  147:         with pytest.raises(TypeError, match=match):
  148:             parser.read_csv(path, index_col=0, **kwargs)
  149: 
  150: 
  151: @td.skip_if_32bit
  152: @pytest.mark.slow
  153: # test numbers between 1 and 2
  154: @pytest.mark.parametrize("num", np.linspace(1.0, 2.0, num=21))
  155: def test_precise_conversion(c_parser_only, num):
  156:     parser = c_parser_only
  157: 
  158:     normal_errors = []
  159:     precise_errors = []
  160: 
  161:     def error(val: float, actual_val: Decimal) -> Decimal:
  162:         return abs(Decimal(f"{val:.100}") - actual_val)
  163: 
  164:     # 25 decimal digits of precision
  165:     text = f"a\n{num:.25}"
  166: 
  167:     normal_val = float(
  168:         parser.read_csv(StringIO(text), float_precision="legacy")["a"][0]
  169:     )
  170:     precise_val = float(parser.read_csv(StringIO(text), float_precision="high")["a"][0])
  171:     roundtrip_val = float(
  172:         parser.read_csv(StringIO(text), float_precision="round_trip")["a"][0]
  173:     )
  174:     actual_val = Decimal(text[2:])
  175: 
  176:     normal_errors.append(error(normal_val, actual_val))
  177:     precise_errors.append(error(precise_val, actual_val))
  178: 
  179:     # round-trip should match float()
  180:     assert roundtrip_val == float(text[2:])
  181: 
  182:     assert sum(precise_errors) <= sum(normal_errors)
  183:     assert max(precise_errors) <= max(normal_errors)
  184: 
  185: 
  186: def test_usecols_dtypes(c_parser_only):
  187:     parser = c_parser_only
  188:     data = """\
  189: 1,2,3
  190: 4,5,6
  191: 7,8,9
  192: 10,11,12"""
  193: 
  194:     result = parser.read_csv(
  195:         StringIO(data),
  196:         usecols=(0, 1, 2),
  197:         names=("a", "b", "c"),
  198:         header=None,
  199:         converters={"a": str},
  200:         dtype={"b": int, "c": float},
  201:     )
  202:     result2 = parser.read_csv(
  203:         StringIO(data),
  204:         usecols=(0, 2),
  205:         names=("a", "b", "c"),
  206:         header=None,
  207:         converters={"a": str},
  208:         dtype={"b": int, "c": float},
  209:     )
  210: 
  211:     assert (result.dtypes == [object, int, float]).all()
  212:     assert (result2.dtypes == [object, float]).all()
  213: 
  214: 
  215: def test_disable_bool_parsing(c_parser_only):
  216:     # see gh-2090
  217: 
  218:     parser = c_parser_only
  219:     data = """A,B,C
  220: Yes,No,Yes
  221: No,Yes,Yes
  222: Yes,,Yes
  223: No,No,No"""
  224: 
  225:     result = parser.read_csv(StringIO(data), dtype=object)
  226:     assert (result.dtypes == object).all()
  227: 
  228:     result = parser.read_csv(StringIO(data), dtype=object, na_filter=False)
  229:     assert result["B"][2] == ""
  230: 
  231: 
  232: def test_custom_lineterminator(c_parser_only):
  233:     parser = c_parser_only
  234:     data = "a,b,c~1,2,3~4,5,6"
  235: 
  236:     result = parser.read_csv(StringIO(data), lineterminator="~")
  237:     expected = parser.read_csv(StringIO(data.replace("~", "\n")))
  238: 
  239:     tm.assert_frame_equal(result, expected)
  240: 
  241: 
  242: def test_parse_ragged_csv(c_parser_only):
  243:     parser = c_parser_only
  244:     data = """1,2,3
  245: 1,2,3,4
  246: 1,2,3,4,5
  247: 1,2
  248: 1,2,3,4"""
  249: 
  250:     nice_data = """1,2,3,,
  251: 1,2,3,4,
  252: 1,2,3,4,5
  253: 1,2,,,
  254: 1,2,3,4,"""
  255:     result = parser.read_csv(
  256:         StringIO(data), header=None, names=["a", "b", "c", "d", "e"]
  257:     )
  258: 
  259:     expected = parser.read_csv(
  260:         StringIO(nice_data), header=None, names=["a", "b", "c", "d", "e"]
  261:     )
  262: 
  263:     tm.assert_frame_equal(result, expected)
  264: 
  265:     # too many columns, cause segfault if not careful
  266:     data = "1,2\n3,4,5"
  267: 
  268:     result = parser.read_csv(StringIO(data), header=None, names=range(50))
  269:     expected = parser.read_csv(StringIO(data), header=None, names=range(3)).reindex(
  270:         columns=range(50)
  271:     )
  272: 
  273:     tm.assert_frame_equal(result, expected)
  274: 
  275: 
  276: def test_tokenize_CR_with_quoting(c_parser_only):
  277:     # see gh-3453
  278:     parser = c_parser_only
  279:     data = ' a,b,c\r"a,b","e,d","f,f"'
  280: 
  281:     result = parser.read_csv(StringIO(data), header=None)
  282:     expected = parser.read_csv(StringIO(data.replace("\r", "\n")), header=None)
  283:     tm.assert_frame_equal(result, expected)
  284: 
  285:     result = parser.read_csv(StringIO(data))
  286:     expected = parser.read_csv(StringIO(data.replace("\r", "\n")))
  287:     tm.assert_frame_equal(result, expected)
  288: 
  289: 
  290: @pytest.mark.slow
  291: @pytest.mark.parametrize("count", [3 * 2**n for n in range(6)])
  292: def test_grow_boundary_at_cap(c_parser_only, count):
  293:     # See gh-12494
  294:     #
  295:     # Cause of error was that the C parser
  296:     # was not increasing the buffer size when
  297:     # the desired space would fill the buffer
  298:     # to capacity, which would later cause a
  299:     # buffer overflow error when checking the
  300:     # EOF terminator of the CSV stream.
  301:     # 3 * 2^n commas was observed to break the parser
  302:     parser = c_parser_only
  303: 
  304:     with StringIO("," * count) as s:
  305:         expected = DataFrame(columns=[f"Unnamed: {i}" for i in range(count + 1)])
  306:         df = parser.read_csv(s)
  307:     tm.assert_frame_equal(df, expected)
  308: 
  309: 
  310: @pytest.mark.slow
  311: @pytest.mark.parametrize("encoding", [None, "utf-8"])
  312: def test_parse_trim_buffers(c_parser_only, encoding):
  313:     # This test is part of a bugfix for gh-13703. It attempts to
  314:     # to stress the system memory allocator, to cause it to move the
  315:     # stream buffer and either let the OS reclaim the region, or let
  316:     # other memory requests of parser otherwise modify the contents
  317:     # of memory space, where it was formally located.
  318:     # This test is designed to cause a `segfault` with unpatched
  319:     # `tokenizer.c`. Sometimes the test fails on `segfault`, other
  320:     # times it fails due to memory corruption, which causes the
  321:     # loaded DataFrame to differ from the expected one.
  322: 
  323:     # Also force 'utf-8' encoding, so that `_string_convert` would take
  324:     # a different execution branch.
  325: 
  326:     parser = c_parser_only
  327: 
  328:     # Generate a large mixed-type CSV file on-the-fly (one record is
  329:     # approx 1.5KiB).
  330:     record_ = (
  331:         """9999-9,99:99,,,,ZZ,ZZ,,,ZZZ-ZZZZ,.Z-ZZZZ,-9.99,,,9.99,Z"""
  332:         """ZZZZ,,-99,9,ZZZ-ZZZZ,ZZ-ZZZZ,,9.99,ZZZ-ZZZZZ,ZZZ-ZZZZZ,"""
  333:         """ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,9"""
  334:         """99,ZZZ-ZZZZ,,ZZ-ZZZZ,,,,,ZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,,,9,9,"""
  335:         """9,9,99,99,999,999,ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZ,9,ZZ-ZZZZ,9."""
  336:         """99,ZZ-ZZZZ,ZZ-ZZZZ,,,,ZZZZ,,,ZZ,ZZ,,,,,,,,,,,,,9,,,999."""
  337:         """99,999.99,,,ZZZZZ,,,Z9,,,,,,,ZZZ,ZZZ,,,,,,,,,,,ZZZZZ,ZZ"""
  338:         """ZZZ,ZZZ-ZZZZZZ,ZZZ-ZZZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,ZZ-ZZZZ,ZZ-ZZ"""
  339:         """ZZ,,,999999,999999,ZZZ,ZZZ,,,ZZZ,ZZZ,999.99,999.99,,,,Z"""
  340:         """ZZ-ZZZ,ZZZ-ZZZ,-9.99,-9.99,9,9,,99,,9.99,9.99,9,9,9.99,"""
  341:         """9.99,,,,9.99,9.99,,99,,99,9.99,9.99,,,ZZZ,ZZZ,,999.99,,"""
  342:         """999.99,ZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,,,ZZZZZ,ZZZZZ,ZZZ,ZZZ,9,9,"""
  343:         """,,,,,ZZZ-ZZZZ,ZZZ999Z,,,999.99,,999.99,ZZZ-ZZZZ,,,9.999"""
  344:         """,9.999,9.999,9.999,-9.999,-9.999,-9.999,-9.999,9.999,9."""
  345:         """999,9.999,9.999,9.999,9.999,9.999,9.999,99999,ZZZ-ZZZZ,"""
  346:         """,9.99,ZZZ,,,,,,,,ZZZ,,,,,9,,,,9,,,,,,,,,,ZZZ-ZZZZ,ZZZ-Z"""
  347:         """ZZZ,,ZZZZZ,ZZZZZ,ZZZZZ,ZZZZZ,,,9.99,,ZZ-ZZZZ,ZZ-ZZZZ,ZZ"""
  348:         """,999,,,,ZZ-ZZZZ,ZZZ,ZZZ,ZZZ-ZZZZ,ZZZ-ZZZZ,,,99.99,99.99"""
  349:         """,,,9.99,9.99,9.99,9.99,ZZZ-ZZZZ,,,ZZZ-ZZZZZ,,,,,-9.99,-"""
  350:         """9.99,-9.99,-9.99,,,,,,,,,ZZZ-ZZZZ,,9,9.99,9.99,99ZZ,,-9"""
  351:         """.99,-9.99,ZZZ-ZZZZ,,,,,,,ZZZ-ZZZZ,9.99,9.99,9999,,,,,,,"""
  352:         """,,,-9.9,Z/Z-ZZZZ,999.99,9.99,,999.99,ZZ-ZZZZ,ZZ-ZZZZ,9."""
  353:         """99,9.99,9.99,9.99,9.99,9.99,,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ-ZZ"""
  354:         """ZZZ,ZZZ-ZZZZZ,ZZZ-ZZZZZ,ZZZ,ZZZ,ZZZ,ZZZ,9.99,,,-9.99,ZZ"""
  355:         """-ZZZZ,-999.99,,-9999,,999.99,,,,999.99,99.99,,,ZZ-ZZZZZ"""
  356:         """ZZZ,ZZ-ZZZZ-ZZZZZZZ,,,,ZZ-ZZ-ZZZZZZZZ,ZZZZZZZZ,ZZZ-ZZZZ"""
  357:         """,9999,999.99,ZZZ-ZZZZ,-9.99,-9.99,ZZZ-ZZZZ,99:99:99,,99"""
  358:         """,99,,9.99,,-99.99,,,,,,9.99,ZZZ-ZZZZ,-9.99,-9.99,9.99,9"""
  359:         """.99,,ZZZ,,,,,,,ZZZ,ZZZ,,,,,"""
  360:     )
  361: 
  362:     # Set the number of lines so that a call to `parser_trim_buffers`
  363:     # is triggered: after a couple of full chunks are consumed a
  364:     # relatively small 'residual' chunk would cause reallocation
  365:     # within the parser.
  366:     chunksize, n_lines = 128, 2 * 128 + 15
  367:     csv_data = "\n".join([record_] * n_lines) + "\n"
  368: 
  369:     # We will use StringIO to load the CSV from this text buffer.
  370:     # pd.read_csv() will iterate over the file in chunks and will
  371:     # finally read a residual chunk of really small size.
  372: 
  373:     # Generate the expected output: manually create the dataframe
  374:     # by splitting by comma and repeating the `n_lines` times.
  375:     row = tuple(val_ if val_ else np.nan for val_ in record_.split(","))
  376:     expected = DataFrame(
  377:         [row for _ in range(n_lines)], dtype=object, columns=None, index=None
  378:     )
  379: 
  380:     # Iterate over the CSV file in chunks of `chunksize` lines
  381:     with parser.read_csv(
  382:         StringIO(csv_data),
  383:         header=None,
  384:         dtype=object,
  385:         chunksize=chunksize,
  386:         encoding=encoding,
  387:     ) as chunks_:
  388:         result = concat(chunks_, axis=0, ignore_index=True)
  389: 
  390:     # Check for data corruption if there was no segfault
  391:     tm.assert_frame_equal(result, expected)
  392: 
  393: 
  394: def test_internal_null_byte(c_parser_only):
  395:     # see gh-14012
  396:     #
  397:     # The null byte ('\x00') should not be used as a
  398:     # true line terminator, escape character, or comment
  399:     # character, only as a placeholder to indicate that
  400:     # none was specified.
  401:     #
  402:     # This test should be moved to test_common.py ONLY when
  403:     # Python's csv class supports parsing '\x00'.
  404:     parser = c_parser_only
  405: 
  406:     names = ["a", "b", "c"]
  407:     data = "1,2,3\n4,\x00,6\n7,8,9"
  408:     expected = DataFrame([[1, 2.0, 3], [4, np.nan, 6], [7, 8, 9]], columns=names)
  409: 
  410:     result = parser.read_csv(StringIO(data), names=names)
  411:     tm.assert_frame_equal(result, expected)
  412: 
  413: 
  414: def test_read_nrows_large(c_parser_only):
  415:     # gh-7626 - Read only nrows of data in for large inputs (>262144b)
  416:     parser = c_parser_only
  417:     header_narrow = "\t".join(["COL_HEADER_" + str(i) for i in range(10)]) + "\n"
  418:     data_narrow = "\t".join(["somedatasomedatasomedata1" for _ in range(10)]) + "\n"
  419:     header_wide = "\t".join(["COL_HEADER_" + str(i) for i in range(15)]) + "\n"
  420:     data_wide = "\t".join(["somedatasomedatasomedata2" for _ in range(15)]) + "\n"
  421:     test_input = header_narrow + data_narrow * 1050 + header_wide + data_wide * 2
  422: 
  423:     df = parser.read_csv(StringIO(test_input), sep="\t", nrows=1010)
  424: 
  425:     assert df.size == 1010 * 10
  426: 
  427: 
  428: def test_float_precision_round_trip_with_text(c_parser_only):
  429:     # see gh-15140
  430:     parser = c_parser_only
  431:     df = parser.read_csv(StringIO("a"), header=None, float_precision="round_trip")
  432:     tm.assert_frame_equal(df, DataFrame({0: ["a"]}))
  433: 
  434: 
  435: def test_large_difference_in_columns(c_parser_only):
  436:     # see gh-14125
  437:     parser = c_parser_only
  438: 
  439:     count = 10000
  440:     large_row = ("X," * count)[:-1] + "\n"
  441:     normal_row = "XXXXXX XXXXXX,111111111111111\n"
  442:     test_input = (large_row + normal_row * 6)[:-1]
  443: 
  444:     result = parser.read_csv(StringIO(test_input), header=None, usecols=[0])
  445:     rows = test_input.split("\n")
  446: 
  447:     expected = DataFrame([row.split(",")[0] for row in rows])
  448:     tm.assert_frame_equal(result, expected)
  449: 
  450: 
  451: def test_data_after_quote(c_parser_only):
  452:     # see gh-15910
  453:     parser = c_parser_only
  454: 
  455:     data = 'a\n1\n"b"a'
  456:     result = parser.read_csv(StringIO(data))
  457: 
  458:     expected = DataFrame({"a": ["1", "ba"]})
  459:     tm.assert_frame_equal(result, expected)
  460: 
  461: 
  462: def test_comment_whitespace_delimited(c_parser_only):
  463:     parser = c_parser_only
  464:     test_input = """\
  465: 1 2
  466: 2 2 3
  467: 3 2 3 # 3 fields
  468: 4 2 3# 3 fields
  469: 5 2 # 2 fields
  470: 6 2# 2 fields
  471: 7 # 1 field, NaN
  472: 8# 1 field, NaN
  473: 9 2 3 # skipped line
  474: # comment"""
  475:     with tm.assert_produces_warning(
  476:         ParserWarning, match="Skipping line", check_stacklevel=False
  477:     ):
  478:         df = parser.read_csv(
  479:             StringIO(test_input),
  480:             comment="#",
  481:             header=None,
  482:             delimiter="\\s+",
  483:             skiprows=0,
  484:             on_bad_lines="warn",
  485:         )
  486:     expected = DataFrame([[1, 2], [5, 2], [6, 2], [7, np.nan], [8, np.nan]])
  487:     tm.assert_frame_equal(df, expected)
  488: 
  489: 
  490: def test_file_like_no_next(c_parser_only):
  491:     # gh-16530: the file-like need not have a "next" or "__next__"
  492:     # attribute despite having an "__iter__" attribute.
  493:     #
  494:     # NOTE: This is only true for the C engine, not Python engine.
  495:     class NoNextBuffer(StringIO):
  496:         def __next__(self):
  497:             raise AttributeError("No next method")
  498: 
  499:         next = __next__
  500: 
  501:     parser = c_parser_only
  502:     data = "a\n1"
  503: 
  504:     expected = DataFrame({"a": [1]})
  505:     result = parser.read_csv(NoNextBuffer(data))
  506: 
  507:     tm.assert_frame_equal(result, expected)
  508: 
  509: 
  510: def test_buffer_rd_bytes_bad_unicode(c_parser_only):
  511:     # see gh-22748
  512:     t = BytesIO(b"\xB0")
  513:     t = TextIOWrapper(t, encoding="ascii", errors="surrogateescape")
  514:     msg = "'utf-8' codec can't encode character"
  515:     with pytest.raises(UnicodeError, match=msg):
  516:         c_parser_only.read_csv(t, encoding="UTF-8")
  517: 
  518: 
  519: @pytest.mark.parametrize("tar_suffix", [".tar", ".tar.gz"])
  520: def test_read_tarfile(c_parser_only, csv_dir_path, tar_suffix):
  521:     # see gh-16530
  522:     #
  523:     # Unfortunately, Python's CSV library can't handle
  524:     # tarfile objects (expects string, not bytes when
  525:     # iterating through a file-like).
  526:     parser = c_parser_only
  527:     tar_path = os.path.join(csv_dir_path, "tar_csv" + tar_suffix)
  528: 
  529:     with tarfile.open(tar_path, "r") as tar:
  530:         data_file = tar.extractfile("tar_data.csv")
  531: 
  532:         out = parser.read_csv(data_file)
  533:         expected = DataFrame({"a": [1]})
  534:         tm.assert_frame_equal(out, expected)
  535: 
  536: 
  537: def test_chunk_whitespace_on_boundary(c_parser_only):
  538:     # see gh-9735: this issue is C parser-specific (bug when
  539:     # parsing whitespace and characters at chunk boundary)
  540:     #
  541:     # This test case has a field too large for the Python parser / CSV library.
  542:     parser = c_parser_only
  543: 
  544:     chunk1 = "a" * (1024 * 256 - 2) + "\na"
  545:     chunk2 = "\n a"
  546:     result = parser.read_csv(StringIO(chunk1 + chunk2), header=None)
  547: 
  548:     expected = DataFrame(["a" * (1024 * 256 - 2), "a", " a"])
  549:     tm.assert_frame_equal(result, expected)
  550: 
  551: 
  552: def test_file_handles_mmap(c_parser_only, csv1):
  553:     # gh-14418
  554:     #
  555:     # Don't close user provided file handles.
  556:     parser = c_parser_only
  557: 
  558:     with open(csv1, encoding="utf-8") as f:
  559:         with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as m:
  560:             parser.read_csv(m)
  561:             assert not m.closed
  562: 
  563: 
  564: def test_file_binary_mode(c_parser_only):
  565:     # see gh-23779
  566:     parser = c_parser_only
  567:     expected = DataFrame([[1, 2, 3], [4, 5, 6]])
  568: 
  569:     with tm.ensure_clean() as path:
  570:         with open(path, "w", encoding="utf-8") as f:
  571:             f.write("1,2,3\n4,5,6")
  572: 
  573:         with open(path, "rb") as f:
  574:             result = parser.read_csv(f, header=None)
  575:             tm.assert_frame_equal(result, expected)
  576: 
  577: 
  578: def test_unix_style_breaks(c_parser_only):
  579:     # GH 11020
  580:     parser = c_parser_only
  581:     with tm.ensure_clean() as path:
  582:         with open(path, "w", newline="\n", encoding="utf-8") as f:
  583:             f.write("blah\n\ncol_1,col_2,col_3\n\n")
  584:         result = parser.read_csv(path, skiprows=2, encoding="utf-8", engine="c")
  585:     expected = DataFrame(columns=["col_1", "col_2", "col_3"])
  586:     tm.assert_frame_equal(result, expected)
  587: 
  588: 
  589: @pytest.mark.parametrize("float_precision", [None, "legacy", "high", "round_trip"])
  590: @pytest.mark.parametrize(
  591:     "data,thousands,decimal",
  592:     [
  593:         (
  594:             """A|B|C
  595: 1|2,334.01|5
  596: 10|13|10.
  597: """,
  598:             ",",
  599:             ".",
  600:         ),
  601:         (
  602:             """A|B|C
  603: 1|2.334,01|5
  604: 10|13|10,
  605: """,
  606:             ".",
  607:             ",",
  608:         ),
  609:     ],
  610: )
  611: def test_1000_sep_with_decimal(
  612:     c_parser_only, data, thousands, decimal, float_precision
  613: ):
  614:     parser = c_parser_only
  615:     expected = DataFrame({"A": [1, 10], "B": [2334.01, 13], "C": [5, 10.0]})
  616: 
  617:     result = parser.read_csv(
  618:         StringIO(data),
  619:         sep="|",
  620:         thousands=thousands,
  621:         decimal=decimal,
  622:         float_precision=float_precision,
  623:     )
  624:     tm.assert_frame_equal(result, expected)
  625: 
  626: 
  627: def test_float_precision_options(c_parser_only):
  628:     # GH 17154, 36228
  629:     parser = c_parser_only
  630:     s = "foo\n243.164\n"
  631:     df = parser.read_csv(StringIO(s))
  632:     df2 = parser.read_csv(StringIO(s), float_precision="high")
  633: 
  634:     tm.assert_frame_equal(df, df2)
  635: 
  636:     df3 = parser.read_csv(StringIO(s), float_precision="legacy")
  637: 
  638:     assert not df.iloc[0, 0] == df3.iloc[0, 0]
  639: 
  640:     msg = "Unrecognized float_precision option: junk"
  641: 
  642:     with pytest.raises(ValueError, match=msg):
  643:         parser.read_csv(StringIO(s), float_precision="junk")
