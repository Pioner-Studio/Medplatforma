    1: from __future__ import annotations
    2: 
    3: import contextlib
    4: from contextlib import closing
    5: import csv
    6: from datetime import (
    7:     date,
    8:     datetime,
    9:     time,
   10:     timedelta,
   11: )
   12: from io import StringIO
   13: from pathlib import Path
   14: import sqlite3
   15: from typing import TYPE_CHECKING
   16: import uuid
   17: 
   18: import numpy as np
   19: import pytest
   20: 
   21: from pandas._libs import lib
   22: from pandas.compat import (
   23:     pa_version_under13p0,
   24:     pa_version_under14p1,
   25: )
   26: from pandas.compat._optional import import_optional_dependency
   27: import pandas.util._test_decorators as td
   28: 
   29: import pandas as pd
   30: from pandas import (
   31:     DataFrame,
   32:     Index,
   33:     MultiIndex,
   34:     Series,
   35:     Timestamp,
   36:     concat,
   37:     date_range,
   38:     isna,
   39:     to_datetime,
   40:     to_timedelta,
   41: )
   42: import pandas._testing as tm
   43: from pandas.core.arrays import (
   44:     ArrowStringArray,
   45:     StringArray,
   46: )
   47: from pandas.util.version import Version
   48: 
   49: from pandas.io import sql
   50: from pandas.io.sql import (
   51:     SQLAlchemyEngine,
   52:     SQLDatabase,
   53:     SQLiteDatabase,
   54:     get_engine,
   55:     pandasSQL_builder,
   56:     read_sql_query,
   57:     read_sql_table,
   58: )
   59: 
   60: if TYPE_CHECKING:
   61:     import sqlalchemy
   62: 
   63: 
   64: pytestmark = pytest.mark.filterwarnings(
   65:     "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   66: )
   67: 
   68: 
   69: @pytest.fixture
   70: def sql_strings():
   71:     return {
   72:         "read_parameters": {
   73:             "sqlite": "SELECT * FROM iris WHERE Name=? AND SepalLength=?",
   74:             "mysql": "SELECT * FROM iris WHERE `Name`=%s AND `SepalLength`=%s",
   75:             "postgresql": 'SELECT * FROM iris WHERE "Name"=%s AND "SepalLength"=%s',
   76:         },
   77:         "read_named_parameters": {
   78:             "sqlite": """
   79:                 SELECT * FROM iris WHERE Name=:name AND SepalLength=:length
   80:                 """,
   81:             "mysql": """
   82:                 SELECT * FROM iris WHERE
   83:                 `Name`=%(name)s AND `SepalLength`=%(length)s
   84:                 """,
   85:             "postgresql": """
   86:                 SELECT * FROM iris WHERE
   87:                 "Name"=%(name)s AND "SepalLength"=%(length)s
   88:                 """,
   89:         },
   90:         "read_no_parameters_with_percent": {
   91:             "sqlite": "SELECT * FROM iris WHERE Name LIKE '%'",
   92:             "mysql": "SELECT * FROM iris WHERE `Name` LIKE '%'",
   93:             "postgresql": "SELECT * FROM iris WHERE \"Name\" LIKE '%'",
   94:         },
   95:     }
   96: 
   97: 
   98: def iris_table_metadata():
   99:     import sqlalchemy
  100:     from sqlalchemy import (
  101:         Column,
  102:         Double,
  103:         Float,
  104:         MetaData,
  105:         String,
  106:         Table,
  107:     )
  108: 
  109:     dtype = Double if Version(sqlalchemy.__version__) >= Version("2.0.0") else Float
  110:     metadata = MetaData()
  111:     iris = Table(
  112:         "iris",
  113:         metadata,
  114:         Column("SepalLength", dtype),
  115:         Column("SepalWidth", dtype),
  116:         Column("PetalLength", dtype),
  117:         Column("PetalWidth", dtype),
  118:         Column("Name", String(200)),
  119:     )
  120:     return iris
  121: 
  122: 
  123: def create_and_load_iris_sqlite3(conn, iris_file: Path):
  124:     stmt = """CREATE TABLE iris (
  125:             "SepalLength" REAL,
  126:             "SepalWidth" REAL,
  127:             "PetalLength" REAL,
  128:             "PetalWidth" REAL,
  129:             "Name" TEXT
  130:         )"""
  131: 
  132:     cur = conn.cursor()
  133:     cur.execute(stmt)
  134:     with iris_file.open(newline=None, encoding="utf-8") as csvfile:
  135:         reader = csv.reader(csvfile)
  136:         next(reader)
  137:         stmt = "INSERT INTO iris VALUES(?, ?, ?, ?, ?)"
  138:         # ADBC requires explicit types - no implicit str -> float conversion
  139:         records = []
  140:         records = [
  141:             (
  142:                 float(row[0]),
  143:                 float(row[1]),
  144:                 float(row[2]),
  145:                 float(row[3]),
  146:                 row[4],
  147:             )
  148:             for row in reader
  149:         ]
  150: 
  151:         cur.executemany(stmt, records)
  152:     cur.close()
  153: 
  154:     conn.commit()
  155: 
  156: 
  157: def create_and_load_iris_postgresql(conn, iris_file: Path):
  158:     stmt = """CREATE TABLE iris (
  159:             "SepalLength" DOUBLE PRECISION,
  160:             "SepalWidth" DOUBLE PRECISION,
  161:             "PetalLength" DOUBLE PRECISION,
  162:             "PetalWidth" DOUBLE PRECISION,
  163:             "Name" TEXT
  164:         )"""
  165:     with conn.cursor() as cur:
  166:         cur.execute(stmt)
  167:         with iris_file.open(newline=None, encoding="utf-8") as csvfile:
  168:             reader = csv.reader(csvfile)
  169:             next(reader)
  170:             stmt = "INSERT INTO iris VALUES($1, $2, $3, $4, $5)"
  171:             # ADBC requires explicit types - no implicit str -> float conversion
  172:             records = [
  173:                 (
  174:                     float(row[0]),
  175:                     float(row[1]),
  176:                     float(row[2]),
  177:                     float(row[3]),
  178:                     row[4],
  179:                 )
  180:                 for row in reader
  181:             ]
  182: 
  183:             cur.executemany(stmt, records)
  184: 
  185:     conn.commit()
  186: 
  187: 
  188: def create_and_load_iris(conn, iris_file: Path):
  189:     from sqlalchemy import insert
  190: 
  191:     iris = iris_table_metadata()
  192: 
  193:     with iris_file.open(newline=None, encoding="utf-8") as csvfile:
  194:         reader = csv.reader(csvfile)
  195:         header = next(reader)
  196:         params = [dict(zip(header, row)) for row in reader]
  197:         stmt = insert(iris).values(params)
  198:         with conn.begin() as con:
  199:             iris.drop(con, checkfirst=True)
  200:             iris.create(bind=con)
  201:             con.execute(stmt)
  202: 
  203: 
  204: def create_and_load_iris_view(conn):
  205:     stmt = "CREATE VIEW iris_view AS SELECT * FROM iris"
  206:     if isinstance(conn, sqlite3.Connection):
  207:         cur = conn.cursor()
  208:         cur.execute(stmt)
  209:     else:
  210:         adbc = import_optional_dependency("adbc_driver_manager.dbapi", errors="ignore")
  211:         if adbc and isinstance(conn, adbc.Connection):
  212:             with conn.cursor() as cur:
  213:                 cur.execute(stmt)
  214:             conn.commit()
  215:         else:
  216:             from sqlalchemy import text
  217: 
  218:             stmt = text(stmt)
  219:             with conn.begin() as con:
  220:                 con.execute(stmt)
  221: 
  222: 
  223: def types_table_metadata(dialect: str):
  224:     from sqlalchemy import (
  225:         TEXT,
  226:         Boolean,
  227:         Column,
  228:         DateTime,
  229:         Float,
  230:         Integer,
  231:         MetaData,
  232:         Table,
  233:     )
  234: 
  235:     date_type = TEXT if dialect == "sqlite" else DateTime
  236:     bool_type = Integer if dialect == "sqlite" else Boolean
  237:     metadata = MetaData()
  238:     types = Table(
  239:         "types",
  240:         metadata,
  241:         Column("TextCol", TEXT),
  242:         Column("DateCol", date_type),
  243:         Column("IntDateCol", Integer),
  244:         Column("IntDateOnlyCol", Integer),
  245:         Column("FloatCol", Float),
  246:         Column("IntCol", Integer),
  247:         Column("BoolCol", bool_type),
  248:         Column("IntColWithNull", Integer),
  249:         Column("BoolColWithNull", bool_type),
  250:     )
  251:     return types
  252: 
  253: 
  254: def create_and_load_types_sqlite3(conn, types_data: list[dict]):
  255:     stmt = """CREATE TABLE types (
  256:                     "TextCol" TEXT,
  257:                     "DateCol" TEXT,
  258:                     "IntDateCol" INTEGER,
  259:                     "IntDateOnlyCol" INTEGER,
  260:                     "FloatCol" REAL,
  261:                     "IntCol" INTEGER,
  262:                     "BoolCol" INTEGER,
  263:                     "IntColWithNull" INTEGER,
  264:                     "BoolColWithNull" INTEGER
  265:                 )"""
  266: 
  267:     ins_stmt = """
  268:                 INSERT INTO types
  269:                 VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)
  270:                 """
  271: 
  272:     if isinstance(conn, sqlite3.Connection):
  273:         cur = conn.cursor()
  274:         cur.execute(stmt)
  275:         cur.executemany(ins_stmt, types_data)
  276:     else:
  277:         with conn.cursor() as cur:
  278:             cur.execute(stmt)
  279:             cur.executemany(ins_stmt, types_data)
  280: 
  281:         conn.commit()
  282: 
  283: 
  284: def create_and_load_types_postgresql(conn, types_data: list[dict]):
  285:     with conn.cursor() as cur:
  286:         stmt = """CREATE TABLE types (
  287:                         "TextCol" TEXT,
  288:                         "DateCol" TIMESTAMP,
  289:                         "IntDateCol" INTEGER,
  290:                         "IntDateOnlyCol" INTEGER,
  291:                         "FloatCol" DOUBLE PRECISION,
  292:                         "IntCol" INTEGER,
  293:                         "BoolCol" BOOLEAN,
  294:                         "IntColWithNull" INTEGER,
  295:                         "BoolColWithNull" BOOLEAN
  296:                     )"""
  297:         cur.execute(stmt)
  298: 
  299:         stmt = """
  300:                 INSERT INTO types
  301:                 VALUES($1, $2::timestamp, $3, $4, $5, $6, $7, $8, $9)
  302:                 """
  303: 
  304:         cur.executemany(stmt, types_data)
  305: 
  306:     conn.commit()
  307: 
  308: 
  309: def create_and_load_types(conn, types_data: list[dict], dialect: str):
  310:     from sqlalchemy import insert
  311:     from sqlalchemy.engine import Engine
  312: 
  313:     types = types_table_metadata(dialect)
  314: 
  315:     stmt = insert(types).values(types_data)
  316:     if isinstance(conn, Engine):
  317:         with conn.connect() as conn:
  318:             with conn.begin():
  319:                 types.drop(conn, checkfirst=True)
  320:                 types.create(bind=conn)
  321:                 conn.execute(stmt)
  322:     else:
  323:         with conn.begin():
  324:             types.drop(conn, checkfirst=True)
  325:             types.create(bind=conn)
  326:             conn.execute(stmt)
  327: 
  328: 
  329: def create_and_load_postgres_datetz(conn):
  330:     from sqlalchemy import (
  331:         Column,
  332:         DateTime,
  333:         MetaData,
  334:         Table,
  335:         insert,
  336:     )
  337:     from sqlalchemy.engine import Engine
  338: 
  339:     metadata = MetaData()
  340:     datetz = Table("datetz", metadata, Column("DateColWithTz", DateTime(timezone=True)))
  341:     datetz_data = [
  342:         {
  343:             "DateColWithTz": "2000-01-01 00:00:00-08:00",
  344:         },
  345:         {
  346:             "DateColWithTz": "2000-06-01 00:00:00-07:00",
  347:         },
  348:     ]
  349:     stmt = insert(datetz).values(datetz_data)
  350:     if isinstance(conn, Engine):
  351:         with conn.connect() as conn:
  352:             with conn.begin():
  353:                 datetz.drop(conn, checkfirst=True)
  354:                 datetz.create(bind=conn)
  355:                 conn.execute(stmt)
  356:     else:
  357:         with conn.begin():
  358:             datetz.drop(conn, checkfirst=True)
  359:             datetz.create(bind=conn)
  360:             conn.execute(stmt)
  361: 
  362:     # "2000-01-01 00:00:00-08:00" should convert to
  363:     # "2000-01-01 08:00:00"
  364:     # "2000-06-01 00:00:00-07:00" should convert to
  365:     # "2000-06-01 07:00:00"
  366:     # GH 6415
  367:     expected_data = [
  368:         Timestamp("2000-01-01 08:00:00", tz="UTC"),
  369:         Timestamp("2000-06-01 07:00:00", tz="UTC"),
  370:     ]
  371:     return Series(expected_data, name="DateColWithTz")
  372: 
  373: 
  374: def check_iris_frame(frame: DataFrame):
  375:     pytype = frame.dtypes.iloc[0].type
  376:     row = frame.iloc[0]
  377:     assert issubclass(pytype, np.floating)
  378:     tm.assert_series_equal(
  379:         row, Series([5.1, 3.5, 1.4, 0.2, "Iris-setosa"], index=frame.columns, name=0)
  380:     )
  381:     assert frame.shape in ((150, 5), (8, 5))
  382: 
  383: 
  384: def count_rows(conn, table_name: str):
  385:     stmt = f"SELECT count(*) AS count_1 FROM {table_name}"
  386:     adbc = import_optional_dependency("adbc_driver_manager.dbapi", errors="ignore")
  387:     if isinstance(conn, sqlite3.Connection):
  388:         cur = conn.cursor()
  389:         return cur.execute(stmt).fetchone()[0]
  390:     elif adbc and isinstance(conn, adbc.Connection):
  391:         with conn.cursor() as cur:
  392:             cur.execute(stmt)
  393:             return cur.fetchone()[0]
  394:     else:
  395:         from sqlalchemy import create_engine
  396:         from sqlalchemy.engine import Engine
  397: 
  398:         if isinstance(conn, str):
  399:             try:
  400:                 engine = create_engine(conn)
  401:                 with engine.connect() as conn:
  402:                     return conn.exec_driver_sql(stmt).scalar_one()
  403:             finally:
  404:                 engine.dispose()
  405:         elif isinstance(conn, Engine):
  406:             with conn.connect() as conn:
  407:                 return conn.exec_driver_sql(stmt).scalar_one()
  408:         else:
  409:             return conn.exec_driver_sql(stmt).scalar_one()
  410: 
  411: 
  412: @pytest.fixture
  413: def iris_path(datapath):
  414:     iris_path = datapath("io", "data", "csv", "iris.csv")
  415:     return Path(iris_path)
  416: 
  417: 
  418: @pytest.fixture
  419: def types_data():
  420:     return [
  421:         {
  422:             "TextCol": "first",
  423:             "DateCol": "2000-01-03 00:00:00",
  424:             "IntDateCol": 535852800,
  425:             "IntDateOnlyCol": 20101010,
  426:             "FloatCol": 10.10,
  427:             "IntCol": 1,
  428:             "BoolCol": False,
  429:             "IntColWithNull": 1,
  430:             "BoolColWithNull": False,
  431:         },
  432:         {
  433:             "TextCol": "first",
  434:             "DateCol": "2000-01-04 00:00:00",
  435:             "IntDateCol": 1356998400,
  436:             "IntDateOnlyCol": 20101212,
  437:             "FloatCol": 10.10,
  438:             "IntCol": 1,
  439:             "BoolCol": False,
  440:             "IntColWithNull": None,
  441:             "BoolColWithNull": None,
  442:         },
  443:     ]
  444: 
  445: 
  446: @pytest.fixture
  447: def types_data_frame(types_data):
  448:     dtypes = {
  449:         "TextCol": "str",
  450:         "DateCol": "str",
  451:         "IntDateCol": "int64",
  452:         "IntDateOnlyCol": "int64",
  453:         "FloatCol": "float",
  454:         "IntCol": "int64",
  455:         "BoolCol": "int64",
  456:         "IntColWithNull": "float",
  457:         "BoolColWithNull": "float",
  458:     }
  459:     df = DataFrame(types_data)
  460:     return df[dtypes.keys()].astype(dtypes)
  461: 
  462: 
  463: @pytest.fixture
  464: def test_frame1():
  465:     columns = ["index", "A", "B", "C", "D"]
  466:     data = [
  467:         (
  468:             "2000-01-03 00:00:00",
  469:             0.980268513777,
  470:             3.68573087906,
  471:             -0.364216805298,
  472:             -1.15973806169,
  473:         ),
  474:         (
  475:             "2000-01-04 00:00:00",
  476:             1.04791624281,
  477:             -0.0412318367011,
  478:             -0.16181208307,
  479:             0.212549316967,
  480:         ),
  481:         (
  482:             "2000-01-05 00:00:00",
  483:             0.498580885705,
  484:             0.731167677815,
  485:             -0.537677223318,
  486:             1.34627041952,
  487:         ),
  488:         (
  489:             "2000-01-06 00:00:00",
  490:             1.12020151869,
  491:             1.56762092543,
  492:             0.00364077397681,
  493:             0.67525259227,
  494:         ),
  495:     ]
  496:     return DataFrame(data, columns=columns)
  497: 
  498: 
  499: @pytest.fixture
  500: def test_frame3():
  501:     columns = ["index", "A", "B"]
  502:     data = [
  503:         ("2000-01-03 00:00:00", 2**31 - 1, -1.987670),
  504:         ("2000-01-04 00:00:00", -29, -0.0412318367011),
  505:         ("2000-01-05 00:00:00", 20000, 0.731167677815),
  506:         ("2000-01-06 00:00:00", -290867, 1.56762092543),
  507:     ]
  508:     return DataFrame(data, columns=columns)
  509: 
  510: 
  511: def get_all_views(conn):
  512:     if isinstance(conn, sqlite3.Connection):
  513:         c = conn.execute("SELECT name FROM sqlite_master WHERE type='view'")
  514:         return [view[0] for view in c.fetchall()]
  515:     else:
  516:         adbc = import_optional_dependency("adbc_driver_manager.dbapi", errors="ignore")
  517:         if adbc and isinstance(conn, adbc.Connection):
  518:             results = []
  519:             info = conn.adbc_get_objects().read_all().to_pylist()
  520:             for catalog in info:
  521:                 catalog["catalog_name"]
  522:                 for schema in catalog["catalog_db_schemas"]:
  523:                     schema["db_schema_name"]
  524:                     for table in schema["db_schema_tables"]:
  525:                         if table["table_type"] == "view":
  526:                             view_name = table["table_name"]
  527:                             results.append(view_name)
  528: 
  529:             return results
  530:         else:
  531:             from sqlalchemy import inspect
  532: 
  533:             return inspect(conn).get_view_names()
  534: 
  535: 
  536: def get_all_tables(conn):
  537:     if isinstance(conn, sqlite3.Connection):
  538:         c = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
  539:         return [table[0] for table in c.fetchall()]
  540:     else:
  541:         adbc = import_optional_dependency("adbc_driver_manager.dbapi", errors="ignore")
  542: 
  543:         if adbc and isinstance(conn, adbc.Connection):
  544:             results = []
  545:             info = conn.adbc_get_objects().read_all().to_pylist()
  546:             for catalog in info:
  547:                 for schema in catalog["catalog_db_schemas"]:
  548:                     for table in schema["db_schema_tables"]:
  549:                         if table["table_type"] == "table":
  550:                             table_name = table["table_name"]
  551:                             results.append(table_name)
  552: 
  553:             return results
  554:         else:
  555:             from sqlalchemy import inspect
  556: 
  557:             return inspect(conn).get_table_names()
  558: 
  559: 
  560: def drop_table(
  561:     table_name: str,
  562:     conn: sqlite3.Connection | sqlalchemy.engine.Engine | sqlalchemy.engine.Connection,
  563: ):
  564:     if isinstance(conn, sqlite3.Connection):
  565:         conn.execute(f"DROP TABLE IF EXISTS {sql._get_valid_sqlite_name(table_name)}")
  566:         conn.commit()
  567: 
  568:     else:
  569:         adbc = import_optional_dependency("adbc_driver_manager.dbapi", errors="ignore")
  570:         if adbc and isinstance(conn, adbc.Connection):
  571:             with conn.cursor() as cur:
  572:                 cur.execute(f'DROP TABLE IF EXISTS "{table_name}"')
  573:         else:
  574:             with conn.begin() as con:
  575:                 with sql.SQLDatabase(con) as db:
  576:                     db.drop_table(table_name)
  577: 
  578: 
  579: def drop_view(
  580:     view_name: str,
  581:     conn: sqlite3.Connection | sqlalchemy.engine.Engine | sqlalchemy.engine.Connection,
  582: ):
  583:     import sqlalchemy
  584: 
  585:     if isinstance(conn, sqlite3.Connection):
  586:         conn.execute(f"DROP VIEW IF EXISTS {sql._get_valid_sqlite_name(view_name)}")
  587:         conn.commit()
  588:     else:
  589:         adbc = import_optional_dependency("adbc_driver_manager.dbapi", errors="ignore")
  590:         if adbc and isinstance(conn, adbc.Connection):
  591:             with conn.cursor() as cur:
  592:                 cur.execute(f'DROP VIEW IF EXISTS "{view_name}"')
  593:         else:
  594:             quoted_view = conn.engine.dialect.identifier_preparer.quote_identifier(
  595:                 view_name
  596:             )
  597:             stmt = sqlalchemy.text(f"DROP VIEW IF EXISTS {quoted_view}")
  598:             with conn.begin() as con:
  599:                 con.execute(stmt)  # type: ignore[union-attr]
  600: 
  601: 
  602: @pytest.fixture
  603: def mysql_pymysql_engine():
  604:     sqlalchemy = pytest.importorskip("sqlalchemy")
  605:     pymysql = pytest.importorskip("pymysql")
  606:     engine = sqlalchemy.create_engine(
  607:         "mysql+pymysql://root@localhost:3306/pandas",
  608:         connect_args={"client_flag": pymysql.constants.CLIENT.MULTI_STATEMENTS},
  609:         poolclass=sqlalchemy.pool.NullPool,
  610:     )
  611:     yield engine
  612:     for view in get_all_views(engine):
  613:         drop_view(view, engine)
  614:     for tbl in get_all_tables(engine):
  615:         drop_table(tbl, engine)
  616:     engine.dispose()
  617: 
  618: 
  619: @pytest.fixture
  620: def mysql_pymysql_engine_iris(mysql_pymysql_engine, iris_path):
  621:     create_and_load_iris(mysql_pymysql_engine, iris_path)
  622:     create_and_load_iris_view(mysql_pymysql_engine)
  623:     yield mysql_pymysql_engine
  624: 
  625: 
  626: @pytest.fixture
  627: def mysql_pymysql_engine_types(mysql_pymysql_engine, types_data):
  628:     create_and_load_types(mysql_pymysql_engine, types_data, "mysql")
  629:     yield mysql_pymysql_engine
  630: 
  631: 
  632: @pytest.fixture
  633: def mysql_pymysql_conn(mysql_pymysql_engine):
  634:     with mysql_pymysql_engine.connect() as conn:
  635:         yield conn
  636: 
  637: 
  638: @pytest.fixture
  639: def mysql_pymysql_conn_iris(mysql_pymysql_engine_iris):
  640:     with mysql_pymysql_engine_iris.connect() as conn:
  641:         yield conn
  642: 
  643: 
  644: @pytest.fixture
  645: def mysql_pymysql_conn_types(mysql_pymysql_engine_types):
  646:     with mysql_pymysql_engine_types.connect() as conn:
  647:         yield conn
  648: 
  649: 
  650: @pytest.fixture
  651: def postgresql_psycopg2_engine():
  652:     sqlalchemy = pytest.importorskip("sqlalchemy")
  653:     pytest.importorskip("psycopg2")
  654:     engine = sqlalchemy.create_engine(
  655:         "postgresql+psycopg2://postgres:postgres@localhost:5432/pandas",
  656:         poolclass=sqlalchemy.pool.NullPool,
  657:     )
  658:     yield engine
  659:     for view in get_all_views(engine):
  660:         drop_view(view, engine)
  661:     for tbl in get_all_tables(engine):
  662:         drop_table(tbl, engine)
  663:     engine.dispose()
  664: 
  665: 
  666: @pytest.fixture
  667: def postgresql_psycopg2_engine_iris(postgresql_psycopg2_engine, iris_path):
  668:     create_and_load_iris(postgresql_psycopg2_engine, iris_path)
  669:     create_and_load_iris_view(postgresql_psycopg2_engine)
  670:     yield postgresql_psycopg2_engine
  671: 
  672: 
  673: @pytest.fixture
  674: def postgresql_psycopg2_engine_types(postgresql_psycopg2_engine, types_data):
  675:     create_and_load_types(postgresql_psycopg2_engine, types_data, "postgres")
  676:     yield postgresql_psycopg2_engine
  677: 
  678: 
  679: @pytest.fixture
  680: def postgresql_psycopg2_conn(postgresql_psycopg2_engine):
  681:     with postgresql_psycopg2_engine.connect() as conn:
  682:         yield conn
  683: 
  684: 
  685: @pytest.fixture
  686: def postgresql_adbc_conn():
  687:     pytest.importorskip("adbc_driver_postgresql")
  688:     from adbc_driver_postgresql import dbapi
  689: 
  690:     uri = "postgresql://postgres:postgres@localhost:5432/pandas"
  691:     with dbapi.connect(uri) as conn:
  692:         yield conn
  693:         for view in get_all_views(conn):
  694:             drop_view(view, conn)
  695:         for tbl in get_all_tables(conn):
  696:             drop_table(tbl, conn)
  697:         conn.commit()
  698: 
  699: 
  700: @pytest.fixture
  701: def postgresql_adbc_iris(postgresql_adbc_conn, iris_path):
  702:     import adbc_driver_manager as mgr
  703: 
  704:     conn = postgresql_adbc_conn
  705: 
  706:     try:
  707:         conn.adbc_get_table_schema("iris")
  708:     except mgr.ProgrammingError:
  709:         conn.rollback()
  710:         create_and_load_iris_postgresql(conn, iris_path)
  711:     try:
  712:         conn.adbc_get_table_schema("iris_view")
  713:     except mgr.ProgrammingError:  # note arrow-adbc issue 1022
  714:         conn.rollback()
  715:         create_and_load_iris_view(conn)
  716:     yield conn
  717: 
  718: 
  719: @pytest.fixture
  720: def postgresql_adbc_types(postgresql_adbc_conn, types_data):
  721:     import adbc_driver_manager as mgr
  722: 
  723:     conn = postgresql_adbc_conn
  724: 
  725:     try:
  726:         conn.adbc_get_table_schema("types")
  727:     except mgr.ProgrammingError:
  728:         conn.rollback()
  729:         new_data = [tuple(entry.values()) for entry in types_data]
  730: 
  731:         create_and_load_types_postgresql(conn, new_data)
  732: 
  733:     yield conn
  734: 
  735: 
  736: @pytest.fixture
  737: def postgresql_psycopg2_conn_iris(postgresql_psycopg2_engine_iris):
  738:     with postgresql_psycopg2_engine_iris.connect() as conn:
  739:         yield conn
  740: 
  741: 
  742: @pytest.fixture
  743: def postgresql_psycopg2_conn_types(postgresql_psycopg2_engine_types):
  744:     with postgresql_psycopg2_engine_types.connect() as conn:
  745:         yield conn
  746: 
  747: 
  748: @pytest.fixture
  749: def sqlite_str():
  750:     pytest.importorskip("sqlalchemy")
  751:     with tm.ensure_clean() as name:
  752:         yield f"sqlite:///{name}"
  753: 
  754: 
  755: @pytest.fixture
  756: def sqlite_engine(sqlite_str):
  757:     sqlalchemy = pytest.importorskip("sqlalchemy")
  758:     engine = sqlalchemy.create_engine(sqlite_str, poolclass=sqlalchemy.pool.NullPool)
  759:     yield engine
  760:     for view in get_all_views(engine):
  761:         drop_view(view, engine)
  762:     for tbl in get_all_tables(engine):
  763:         drop_table(tbl, engine)
  764:     engine.dispose()
  765: 
  766: 
  767: @pytest.fixture
  768: def sqlite_conn(sqlite_engine):
  769:     with sqlite_engine.connect() as conn:
  770:         yield conn
  771: 
  772: 
  773: @pytest.fixture
  774: def sqlite_str_iris(sqlite_str, iris_path):
  775:     sqlalchemy = pytest.importorskip("sqlalchemy")
  776:     engine = sqlalchemy.create_engine(sqlite_str)
  777:     create_and_load_iris(engine, iris_path)
  778:     create_and_load_iris_view(engine)
  779:     engine.dispose()
  780:     return sqlite_str
  781: 
  782: 
  783: @pytest.fixture
  784: def sqlite_engine_iris(sqlite_engine, iris_path):
  785:     create_and_load_iris(sqlite_engine, iris_path)
  786:     create_and_load_iris_view(sqlite_engine)
  787:     yield sqlite_engine
  788: 
  789: 
  790: @pytest.fixture
  791: def sqlite_conn_iris(sqlite_engine_iris):
  792:     with sqlite_engine_iris.connect() as conn:
  793:         yield conn
  794: 
  795: 
  796: @pytest.fixture
  797: def sqlite_str_types(sqlite_str, types_data):
  798:     sqlalchemy = pytest.importorskip("sqlalchemy")
  799:     engine = sqlalchemy.create_engine(sqlite_str)
  800:     create_and_load_types(engine, types_data, "sqlite")
  801:     engine.dispose()
  802:     return sqlite_str
  803: 
  804: 
  805: @pytest.fixture
  806: def sqlite_engine_types(sqlite_engine, types_data):
  807:     create_and_load_types(sqlite_engine, types_data, "sqlite")
  808:     yield sqlite_engine
  809: 
  810: 
  811: @pytest.fixture
  812: def sqlite_conn_types(sqlite_engine_types):
  813:     with sqlite_engine_types.connect() as conn:
  814:         yield conn
  815: 
  816: 
  817: @pytest.fixture
  818: def sqlite_adbc_conn():
  819:     pytest.importorskip("adbc_driver_sqlite")
  820:     from adbc_driver_sqlite import dbapi
  821: 
  822:     with tm.ensure_clean() as name:
  823:         uri = f"file:{name}"
  824:         with dbapi.connect(uri) as conn:
  825:             yield conn
  826:             for view in get_all_views(conn):
  827:                 drop_view(view, conn)
  828:             for tbl in get_all_tables(conn):
  829:                 drop_table(tbl, conn)
  830:             conn.commit()
  831: 
  832: 
  833: @pytest.fixture
  834: def sqlite_adbc_iris(sqlite_adbc_conn, iris_path):
  835:     import adbc_driver_manager as mgr
  836: 
  837:     conn = sqlite_adbc_conn
  838:     try:
  839:         conn.adbc_get_table_schema("iris")
  840:     except mgr.ProgrammingError:
  841:         conn.rollback()
  842:         create_and_load_iris_sqlite3(conn, iris_path)
  843:     try:
  844:         conn.adbc_get_table_schema("iris_view")
  845:     except mgr.ProgrammingError:
  846:         conn.rollback()
  847:         create_and_load_iris_view(conn)
  848:     yield conn
  849: 
  850: 
  851: @pytest.fixture
  852: def sqlite_adbc_types(sqlite_adbc_conn, types_data):
  853:     import adbc_driver_manager as mgr
  854: 
  855:     conn = sqlite_adbc_conn
  856:     try:
  857:         conn.adbc_get_table_schema("types")
  858:     except mgr.ProgrammingError:
  859:         conn.rollback()
  860:         new_data = []
  861:         for entry in types_data:
  862:             entry["BoolCol"] = int(entry["BoolCol"])
  863:             if entry["BoolColWithNull"] is not None:
  864:                 entry["BoolColWithNull"] = int(entry["BoolColWithNull"])
  865:             new_data.append(tuple(entry.values()))
  866: 
  867:         create_and_load_types_sqlite3(conn, new_data)
  868:         conn.commit()
  869: 
  870:     yield conn
  871: 
  872: 
  873: @pytest.fixture
  874: def sqlite_buildin():
  875:     with contextlib.closing(sqlite3.connect(":memory:")) as closing_conn:
  876:         with closing_conn as conn:
  877:             yield conn
  878: 
  879: 
  880: @pytest.fixture
  881: def sqlite_buildin_iris(sqlite_buildin, iris_path):
  882:     create_and_load_iris_sqlite3(sqlite_buildin, iris_path)
  883:     create_and_load_iris_view(sqlite_buildin)
  884:     yield sqlite_buildin
  885: 
  886: 
  887: @pytest.fixture
  888: def sqlite_buildin_types(sqlite_buildin, types_data):
  889:     types_data = [tuple(entry.values()) for entry in types_data]
  890:     create_and_load_types_sqlite3(sqlite_buildin, types_data)
  891:     yield sqlite_buildin
  892: 
  893: 
  894: mysql_connectable = [
  895:     pytest.param("mysql_pymysql_engine", marks=pytest.mark.db),
  896:     pytest.param("mysql_pymysql_conn", marks=pytest.mark.db),
  897: ]
  898: 
  899: mysql_connectable_iris = [
  900:     pytest.param("mysql_pymysql_engine_iris", marks=pytest.mark.db),
  901:     pytest.param("mysql_pymysql_conn_iris", marks=pytest.mark.db),
  902: ]
  903: 
  904: mysql_connectable_types = [
  905:     pytest.param("mysql_pymysql_engine_types", marks=pytest.mark.db),
  906:     pytest.param("mysql_pymysql_conn_types", marks=pytest.mark.db),
  907: ]
  908: 
  909: postgresql_connectable = [
  910:     pytest.param("postgresql_psycopg2_engine", marks=pytest.mark.db),
  911:     pytest.param("postgresql_psycopg2_conn", marks=pytest.mark.db),
  912: ]
  913: 
  914: postgresql_connectable_iris = [
  915:     pytest.param("postgresql_psycopg2_engine_iris", marks=pytest.mark.db),
  916:     pytest.param("postgresql_psycopg2_conn_iris", marks=pytest.mark.db),
  917: ]
  918: 
  919: postgresql_connectable_types = [
  920:     pytest.param("postgresql_psycopg2_engine_types", marks=pytest.mark.db),
  921:     pytest.param("postgresql_psycopg2_conn_types", marks=pytest.mark.db),
  922: ]
  923: 
  924: sqlite_connectable = [
  925:     "sqlite_engine",
  926:     "sqlite_conn",
  927:     "sqlite_str",
  928: ]
  929: 
  930: sqlite_connectable_iris = [
  931:     "sqlite_engine_iris",
  932:     "sqlite_conn_iris",
  933:     "sqlite_str_iris",
  934: ]
  935: 
  936: sqlite_connectable_types = [
  937:     "sqlite_engine_types",
  938:     "sqlite_conn_types",
  939:     "sqlite_str_types",
  940: ]
  941: 
  942: sqlalchemy_connectable = mysql_connectable + postgresql_connectable + sqlite_connectable
  943: 
  944: sqlalchemy_connectable_iris = (
  945:     mysql_connectable_iris + postgresql_connectable_iris + sqlite_connectable_iris
  946: )
  947: 
  948: sqlalchemy_connectable_types = (
  949:     mysql_connectable_types + postgresql_connectable_types + sqlite_connectable_types
  950: )
  951: 
  952: adbc_connectable = [
  953:     "sqlite_adbc_conn",
  954:     pytest.param("postgresql_adbc_conn", marks=pytest.mark.db),
  955: ]
  956: 
  957: adbc_connectable_iris = [
  958:     pytest.param("postgresql_adbc_iris", marks=pytest.mark.db),
  959:     pytest.param("sqlite_adbc_iris", marks=pytest.mark.db),
  960: ]
  961: 
  962: adbc_connectable_types = [
  963:     pytest.param("postgresql_adbc_types", marks=pytest.mark.db),
  964:     pytest.param("sqlite_adbc_types", marks=pytest.mark.db),
  965: ]
  966: 
  967: 
  968: all_connectable = sqlalchemy_connectable + ["sqlite_buildin"] + adbc_connectable
  969: 
  970: all_connectable_iris = (
  971:     sqlalchemy_connectable_iris + ["sqlite_buildin_iris"] + adbc_connectable_iris
  972: )
  973: 
  974: all_connectable_types = (
  975:     sqlalchemy_connectable_types + ["sqlite_buildin_types"] + adbc_connectable_types
  976: )
  977: 
  978: 
  979: @pytest.mark.parametrize("conn", all_connectable)
  980: def test_dataframe_to_sql(conn, test_frame1, request):
  981:     # GH 51086 if conn is sqlite_engine
  982:     conn = request.getfixturevalue(conn)
  983:     test_frame1.to_sql(name="test", con=conn, if_exists="append", index=False)
  984: 
  985: 
  986: @pytest.mark.parametrize("conn", all_connectable)
  987: def test_dataframe_to_sql_empty(conn, test_frame1, request):
  988:     if conn == "postgresql_adbc_conn":
  989:         request.node.add_marker(
  990:             pytest.mark.xfail(
  991:                 reason="postgres ADBC driver cannot insert index with null type",
  992:                 strict=True,
  993:             )
  994:         )
  995:     # GH 51086 if conn is sqlite_engine
  996:     conn = request.getfixturevalue(conn)
  997:     empty_df = test_frame1.iloc[:0]
  998:     empty_df.to_sql(name="test", con=conn, if_exists="append", index=False)
  999: 
 1000: 
 1001: @pytest.mark.parametrize("conn", all_connectable)
 1002: def test_dataframe_to_sql_arrow_dtypes(conn, request):
 1003:     # GH 52046
 1004:     pytest.importorskip("pyarrow")
 1005:     df = DataFrame(
 1006:         {
 1007:             "int": pd.array([1], dtype="int8[pyarrow]"),
 1008:             "datetime": pd.array(
 1009:                 [datetime(2023, 1, 1)], dtype="timestamp[ns][pyarrow]"
 1010:             ),
 1011:             "date": pd.array([date(2023, 1, 1)], dtype="date32[day][pyarrow]"),
 1012:             "timedelta": pd.array([timedelta(1)], dtype="duration[ns][pyarrow]"),
 1013:             "string": pd.array(["a"], dtype="string[pyarrow]"),
 1014:         }
 1015:     )
 1016: 
 1017:     if "adbc" in conn:
 1018:         if conn == "sqlite_adbc_conn":
 1019:             df = df.drop(columns=["timedelta"])
 1020:         if pa_version_under14p1:
 1021:             exp_warning = DeprecationWarning
 1022:             msg = "is_sparse is deprecated"
 1023:         else:
 1024:             exp_warning = None
 1025:             msg = ""
 1026:     else:
 1027:         exp_warning = UserWarning
 1028:         msg = "the 'timedelta'"
 1029: 
 1030:     conn = request.getfixturevalue(conn)
 1031:     with tm.assert_produces_warning(exp_warning, match=msg, check_stacklevel=False):
 1032:         df.to_sql(name="test_arrow", con=conn, if_exists="replace", index=False)
 1033: 
 1034: 
 1035: @pytest.mark.parametrize("conn", all_connectable)
 1036: def test_dataframe_to_sql_arrow_dtypes_missing(conn, request, nulls_fixture):
 1037:     # GH 52046
 1038:     pytest.importorskip("pyarrow")
 1039:     df = DataFrame(
 1040:         {
 1041:             "datetime": pd.array(
 1042:                 [datetime(2023, 1, 1), nulls_fixture], dtype="timestamp[ns][pyarrow]"
 1043:             ),
 1044:         }
 1045:     )
 1046:     conn = request.getfixturevalue(conn)
 1047:     df.to_sql(name="test_arrow", con=conn, if_exists="replace", index=False)
 1048: 
 1049: 
 1050: @pytest.mark.parametrize("conn", all_connectable)
 1051: @pytest.mark.parametrize("method", [None, "multi"])
 1052: def test_to_sql(conn, method, test_frame1, request):
 1053:     if method == "multi" and "adbc" in conn:
 1054:         request.node.add_marker(
 1055:             pytest.mark.xfail(
 1056:                 reason="'method' not implemented for ADBC drivers", strict=True
 1057:             )
 1058:         )
 1059: 
 1060:     conn = request.getfixturevalue(conn)
 1061:     with pandasSQL_builder(conn, need_transaction=True) as pandasSQL:
 1062:         pandasSQL.to_sql(test_frame1, "test_frame", method=method)
 1063:         assert pandasSQL.has_table("test_frame")
 1064:     assert count_rows(conn, "test_frame") == len(test_frame1)
 1065: 
 1066: 
 1067: @pytest.mark.parametrize("conn", all_connectable)
 1068: @pytest.mark.parametrize("mode, num_row_coef", [("replace", 1), ("append", 2)])
 1069: def test_to_sql_exist(conn, mode, num_row_coef, test_frame1, request):
 1070:     conn = request.getfixturevalue(conn)
 1071:     with pandasSQL_builder(conn, need_transaction=True) as pandasSQL:
 1072:         pandasSQL.to_sql(test_frame1, "test_frame", if_exists="fail")
 1073:         pandasSQL.to_sql(test_frame1, "test_frame", if_exists=mode)
 1074:         assert pandasSQL.has_table("test_frame")
 1075:     assert count_rows(conn, "test_frame") == num_row_coef * len(test_frame1)
 1076: 
 1077: 
 1078: @pytest.mark.parametrize("conn", all_connectable)
 1079: def test_to_sql_exist_fail(conn, test_frame1, request):
 1080:     conn = request.getfixturevalue(conn)
 1081:     with pandasSQL_builder(conn, need_transaction=True) as pandasSQL:
 1082:         pandasSQL.to_sql(test_frame1, "test_frame", if_exists="fail")
 1083:         assert pandasSQL.has_table("test_frame")
 1084: 
 1085:         msg = "Table 'test_frame' already exists"
 1086:         with pytest.raises(ValueError, match=msg):
 1087:             pandasSQL.to_sql(test_frame1, "test_frame", if_exists="fail")
 1088: 
 1089: 
 1090: @pytest.mark.parametrize("conn", all_connectable_iris)
 1091: def test_read_iris_query(conn, request):
 1092:     conn = request.getfixturevalue(conn)
 1093:     iris_frame = read_sql_query("SELECT * FROM iris", conn)
 1094:     check_iris_frame(iris_frame)
 1095:     iris_frame = pd.read_sql("SELECT * FROM iris", conn)
 1096:     check_iris_frame(iris_frame)
 1097:     iris_frame = pd.read_sql("SELECT * FROM iris where 0=1", conn)
 1098:     assert iris_frame.shape == (0, 5)
 1099:     assert "SepalWidth" in iris_frame.columns
 1100: 
 1101: 
 1102: @pytest.mark.parametrize("conn", all_connectable_iris)
 1103: def test_read_iris_query_chunksize(conn, request):
 1104:     if "adbc" in conn:
 1105:         request.node.add_marker(
 1106:             pytest.mark.xfail(
 1107:                 reason="'chunksize' not implemented for ADBC drivers",
 1108:                 strict=True,
 1109:             )
 1110:         )
 1111:     conn = request.getfixturevalue(conn)
 1112:     iris_frame = concat(read_sql_query("SELECT * FROM iris", conn, chunksize=7))
 1113:     check_iris_frame(iris_frame)
 1114:     iris_frame = concat(pd.read_sql("SELECT * FROM iris", conn, chunksize=7))
 1115:     check_iris_frame(iris_frame)
 1116:     iris_frame = concat(pd.read_sql("SELECT * FROM iris where 0=1", conn, chunksize=7))
 1117:     assert iris_frame.shape == (0, 5)
 1118:     assert "SepalWidth" in iris_frame.columns
 1119: 
 1120: 
 1121: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 1122: def test_read_iris_query_expression_with_parameter(conn, request):
 1123:     if "adbc" in conn:
 1124:         request.node.add_marker(
 1125:             pytest.mark.xfail(
 1126:                 reason="'chunksize' not implemented for ADBC drivers",
 1127:                 strict=True,
 1128:             )
 1129:         )
 1130:     conn = request.getfixturevalue(conn)
 1131:     from sqlalchemy import (
 1132:         MetaData,
 1133:         Table,
 1134:         create_engine,
 1135:         select,
 1136:     )
 1137: 
 1138:     metadata = MetaData()
 1139:     autoload_con = create_engine(conn) if isinstance(conn, str) else conn
 1140:     iris = Table("iris", metadata, autoload_with=autoload_con)
 1141:     iris_frame = read_sql_query(
 1142:         select(iris), conn, params={"name": "Iris-setosa", "length": 5.1}
 1143:     )
 1144:     check_iris_frame(iris_frame)
 1145:     if isinstance(conn, str):
 1146:         autoload_con.dispose()
 1147: 
 1148: 
 1149: @pytest.mark.parametrize("conn", all_connectable_iris)
 1150: def test_read_iris_query_string_with_parameter(conn, request, sql_strings):
 1151:     if "adbc" in conn:
 1152:         request.node.add_marker(
 1153:             pytest.mark.xfail(
 1154:                 reason="'chunksize' not implemented for ADBC drivers",
 1155:                 strict=True,
 1156:             )
 1157:         )
 1158: 
 1159:     for db, query in sql_strings["read_parameters"].items():
 1160:         if db in conn:
 1161:             break
 1162:     else:
 1163:         raise KeyError(f"No part of {conn} found in sql_strings['read_parameters']")
 1164:     conn = request.getfixturevalue(conn)
 1165:     iris_frame = read_sql_query(query, conn, params=("Iris-setosa", 5.1))
 1166:     check_iris_frame(iris_frame)
 1167: 
 1168: 
 1169: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 1170: def test_read_iris_table(conn, request):
 1171:     # GH 51015 if conn = sqlite_iris_str
 1172:     conn = request.getfixturevalue(conn)
 1173:     iris_frame = read_sql_table("iris", conn)
 1174:     check_iris_frame(iris_frame)
 1175:     iris_frame = pd.read_sql("iris", conn)
 1176:     check_iris_frame(iris_frame)
 1177: 
 1178: 
 1179: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 1180: def test_read_iris_table_chunksize(conn, request):
 1181:     if "adbc" in conn:
 1182:         request.node.add_marker(
 1183:             pytest.mark.xfail(reason="chunksize argument NotImplemented with ADBC")
 1184:         )
 1185:     conn = request.getfixturevalue(conn)
 1186:     iris_frame = concat(read_sql_table("iris", conn, chunksize=7))
 1187:     check_iris_frame(iris_frame)
 1188:     iris_frame = concat(pd.read_sql("iris", conn, chunksize=7))
 1189:     check_iris_frame(iris_frame)
 1190: 
 1191: 
 1192: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 1193: def test_to_sql_callable(conn, test_frame1, request):
 1194:     conn = request.getfixturevalue(conn)
 1195: 
 1196:     check = []  # used to double check function below is really being used
 1197: 
 1198:     def sample(pd_table, conn, keys, data_iter):
 1199:         check.append(1)
 1200:         data = [dict(zip(keys, row)) for row in data_iter]
 1201:         conn.execute(pd_table.table.insert(), data)
 1202: 
 1203:     with pandasSQL_builder(conn, need_transaction=True) as pandasSQL:
 1204:         pandasSQL.to_sql(test_frame1, "test_frame", method=sample)
 1205:         assert pandasSQL.has_table("test_frame")
 1206:     assert check == [1]
 1207:     assert count_rows(conn, "test_frame") == len(test_frame1)
 1208: 
 1209: 
 1210: @pytest.mark.parametrize("conn", all_connectable_types)
 1211: def test_default_type_conversion(conn, request):
 1212:     conn_name = conn
 1213:     if conn_name == "sqlite_buildin_types":
 1214:         request.applymarker(
 1215:             pytest.mark.xfail(
 1216:                 reason="sqlite_buildin connection does not implement read_sql_table"
 1217:             )
 1218:         )
 1219: 
 1220:     conn = request.getfixturevalue(conn)
 1221:     df = sql.read_sql_table("types", conn)
 1222: 
 1223:     assert issubclass(df.FloatCol.dtype.type, np.floating)
 1224:     assert issubclass(df.IntCol.dtype.type, np.integer)
 1225: 
 1226:     # MySQL/sqlite has no real BOOL type
 1227:     if "postgresql" in conn_name:
 1228:         assert issubclass(df.BoolCol.dtype.type, np.bool_)
 1229:     else:
 1230:         assert issubclass(df.BoolCol.dtype.type, np.integer)
 1231: 
 1232:     # Int column with NA values stays as float
 1233:     assert issubclass(df.IntColWithNull.dtype.type, np.floating)
 1234: 
 1235:     # Bool column with NA = int column with NA values => becomes float
 1236:     if "postgresql" in conn_name:
 1237:         assert issubclass(df.BoolColWithNull.dtype.type, object)
 1238:     else:
 1239:         assert issubclass(df.BoolColWithNull.dtype.type, np.floating)
 1240: 
 1241: 
 1242: @pytest.mark.parametrize("conn", mysql_connectable)
 1243: def test_read_procedure(conn, request):
 1244:     conn = request.getfixturevalue(conn)
 1245: 
 1246:     # GH 7324
 1247:     # Although it is more an api test, it is added to the
 1248:     # mysql tests as sqlite does not have stored procedures
 1249:     from sqlalchemy import text
 1250:     from sqlalchemy.engine import Engine
 1251: 
 1252:     df = DataFrame({"a": [1, 2, 3], "b": [0.1, 0.2, 0.3]})
 1253:     df.to_sql(name="test_frame", con=conn, index=False)
 1254: 
 1255:     proc = """DROP PROCEDURE IF EXISTS get_testdb;
 1256: 
 1257:     CREATE PROCEDURE get_testdb ()
 1258: 
 1259:     BEGIN
 1260:         SELECT * FROM test_frame;
 1261:     END"""
 1262:     proc = text(proc)
 1263:     if isinstance(conn, Engine):
 1264:         with conn.connect() as engine_conn:
 1265:             with engine_conn.begin():
 1266:                 engine_conn.execute(proc)
 1267:     else:
 1268:         with conn.begin():
 1269:             conn.execute(proc)
 1270: 
 1271:     res1 = sql.read_sql_query("CALL get_testdb();", conn)
 1272:     tm.assert_frame_equal(df, res1)
 1273: 
 1274:     # test delegation to read_sql_query
 1275:     res2 = sql.read_sql("CALL get_testdb();", conn)
 1276:     tm.assert_frame_equal(df, res2)
 1277: 
 1278: 
 1279: @pytest.mark.parametrize("conn", postgresql_connectable)
 1280: @pytest.mark.parametrize("expected_count", [2, "Success!"])
 1281: def test_copy_from_callable_insertion_method(conn, expected_count, request):
 1282:     # GH 8953
 1283:     # Example in io.rst found under _io.sql.method
 1284:     # not available in sqlite, mysql
 1285:     def psql_insert_copy(table, conn, keys, data_iter):
 1286:         # gets a DBAPI connection that can provide a cursor
 1287:         dbapi_conn = conn.connection
 1288:         with dbapi_conn.cursor() as cur:
 1289:             s_buf = StringIO()
 1290:             writer = csv.writer(s_buf)
 1291:             writer.writerows(data_iter)
 1292:             s_buf.seek(0)
 1293: 
 1294:             columns = ", ".join([f'"{k}"' for k in keys])
 1295:             if table.schema:
 1296:                 table_name = f"{table.schema}.{table.name}"
 1297:             else:
 1298:                 table_name = table.name
 1299: 
 1300:             sql_query = f"COPY {table_name} ({columns}) FROM STDIN WITH CSV"
 1301:             cur.copy_expert(sql=sql_query, file=s_buf)
 1302:         return expected_count
 1303: 
 1304:     conn = request.getfixturevalue(conn)
 1305:     expected = DataFrame({"col1": [1, 2], "col2": [0.1, 0.2], "col3": ["a", "n"]})
 1306:     result_count = expected.to_sql(
 1307:         name="test_frame", con=conn, index=False, method=psql_insert_copy
 1308:     )
 1309:     # GH 46891
 1310:     if expected_count is None:
 1311:         assert result_count is None
 1312:     else:
 1313:         assert result_count == expected_count
 1314:     result = sql.read_sql_table("test_frame", conn)
 1315:     tm.assert_frame_equal(result, expected)
 1316: 
 1317: 
 1318: @pytest.mark.parametrize("conn", postgresql_connectable)
 1319: def test_insertion_method_on_conflict_do_nothing(conn, request):
 1320:     # GH 15988: Example in to_sql docstring
 1321:     conn = request.getfixturevalue(conn)
 1322: 
 1323:     from sqlalchemy.dialects.postgresql import insert
 1324:     from sqlalchemy.engine import Engine
 1325:     from sqlalchemy.sql import text
 1326: 
 1327:     def insert_on_conflict(table, conn, keys, data_iter):
 1328:         data = [dict(zip(keys, row)) for row in data_iter]
 1329:         stmt = (
 1330:             insert(table.table)
 1331:             .values(data)
 1332:             .on_conflict_do_nothing(index_elements=["a"])
 1333:         )
 1334:         result = conn.execute(stmt)
 1335:         return result.rowcount
 1336: 
 1337:     create_sql = text(
 1338:         """
 1339:     CREATE TABLE test_insert_conflict (
 1340:         a  integer PRIMARY KEY,
 1341:         b  numeric,
 1342:         c  text
 1343:     );
 1344:     """
 1345:     )
 1346:     if isinstance(conn, Engine):
 1347:         with conn.connect() as con:
 1348:             with con.begin():
 1349:                 con.execute(create_sql)
 1350:     else:
 1351:         with conn.begin():
 1352:             conn.execute(create_sql)
 1353: 
 1354:     expected = DataFrame([[1, 2.1, "a"]], columns=list("abc"))
 1355:     expected.to_sql(
 1356:         name="test_insert_conflict", con=conn, if_exists="append", index=False
 1357:     )
 1358: 
 1359:     df_insert = DataFrame([[1, 3.2, "b"]], columns=list("abc"))
 1360:     inserted = df_insert.to_sql(
 1361:         name="test_insert_conflict",
 1362:         con=conn,
 1363:         index=False,
 1364:         if_exists="append",
 1365:         method=insert_on_conflict,
 1366:     )
 1367:     result = sql.read_sql_table("test_insert_conflict", conn)
 1368:     tm.assert_frame_equal(result, expected)
 1369:     assert inserted == 0
 1370: 
 1371:     # Cleanup
 1372:     with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1373:         pandasSQL.drop_table("test_insert_conflict")
 1374: 
 1375: 
 1376: @pytest.mark.parametrize("conn", all_connectable)
 1377: def test_to_sql_on_public_schema(conn, request):
 1378:     if "sqlite" in conn or "mysql" in conn:
 1379:         request.applymarker(
 1380:             pytest.mark.xfail(
 1381:                 reason="test for public schema only specific to postgresql"
 1382:             )
 1383:         )
 1384: 
 1385:     conn = request.getfixturevalue(conn)
 1386: 
 1387:     test_data = DataFrame([[1, 2.1, "a"], [2, 3.1, "b"]], columns=list("abc"))
 1388:     test_data.to_sql(
 1389:         name="test_public_schema",
 1390:         con=conn,
 1391:         if_exists="append",
 1392:         index=False,
 1393:         schema="public",
 1394:     )
 1395: 
 1396:     df_out = sql.read_sql_table("test_public_schema", conn, schema="public")
 1397:     tm.assert_frame_equal(test_data, df_out)
 1398: 
 1399: 
 1400: @pytest.mark.parametrize("conn", mysql_connectable)
 1401: def test_insertion_method_on_conflict_update(conn, request):
 1402:     # GH 14553: Example in to_sql docstring
 1403:     conn = request.getfixturevalue(conn)
 1404: 
 1405:     from sqlalchemy.dialects.mysql import insert
 1406:     from sqlalchemy.engine import Engine
 1407:     from sqlalchemy.sql import text
 1408: 
 1409:     def insert_on_conflict(table, conn, keys, data_iter):
 1410:         data = [dict(zip(keys, row)) for row in data_iter]
 1411:         stmt = insert(table.table).values(data)
 1412:         stmt = stmt.on_duplicate_key_update(b=stmt.inserted.b, c=stmt.inserted.c)
 1413:         result = conn.execute(stmt)
 1414:         return result.rowcount
 1415: 
 1416:     create_sql = text(
 1417:         """
 1418:     CREATE TABLE test_insert_conflict (
 1419:         a INT PRIMARY KEY,
 1420:         b FLOAT,
 1421:         c VARCHAR(10)
 1422:     );
 1423:     """
 1424:     )
 1425:     if isinstance(conn, Engine):
 1426:         with conn.connect() as con:
 1427:             with con.begin():
 1428:                 con.execute(create_sql)
 1429:     else:
 1430:         with conn.begin():
 1431:             conn.execute(create_sql)
 1432: 
 1433:     df = DataFrame([[1, 2.1, "a"]], columns=list("abc"))
 1434:     df.to_sql(name="test_insert_conflict", con=conn, if_exists="append", index=False)
 1435: 
 1436:     expected = DataFrame([[1, 3.2, "b"]], columns=list("abc"))
 1437:     inserted = expected.to_sql(
 1438:         name="test_insert_conflict",
 1439:         con=conn,
 1440:         index=False,
 1441:         if_exists="append",
 1442:         method=insert_on_conflict,
 1443:     )
 1444:     result = sql.read_sql_table("test_insert_conflict", conn)
 1445:     tm.assert_frame_equal(result, expected)
 1446:     assert inserted == 2
 1447: 
 1448:     # Cleanup
 1449:     with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1450:         pandasSQL.drop_table("test_insert_conflict")
 1451: 
 1452: 
 1453: @pytest.mark.parametrize("conn", postgresql_connectable)
 1454: def test_read_view_postgres(conn, request):
 1455:     # GH 52969
 1456:     conn = request.getfixturevalue(conn)
 1457: 
 1458:     from sqlalchemy.engine import Engine
 1459:     from sqlalchemy.sql import text
 1460: 
 1461:     table_name = f"group_{uuid.uuid4().hex}"
 1462:     view_name = f"group_view_{uuid.uuid4().hex}"
 1463: 
 1464:     sql_stmt = text(
 1465:         f"""
 1466:     CREATE TABLE {table_name} (
 1467:         group_id INTEGER,
 1468:         name TEXT
 1469:     );
 1470:     INSERT INTO {table_name} VALUES
 1471:         (1, 'name');
 1472:     CREATE VIEW {view_name}
 1473:     AS
 1474:     SELECT * FROM {table_name};
 1475:     """
 1476:     )
 1477:     if isinstance(conn, Engine):
 1478:         with conn.connect() as con:
 1479:             with con.begin():
 1480:                 con.execute(sql_stmt)
 1481:     else:
 1482:         with conn.begin():
 1483:             conn.execute(sql_stmt)
 1484:     result = read_sql_table(view_name, conn)
 1485:     expected = DataFrame({"group_id": [1], "name": "name"})
 1486:     tm.assert_frame_equal(result, expected)
 1487: 
 1488: 
 1489: def test_read_view_sqlite(sqlite_buildin):
 1490:     # GH 52969
 1491:     create_table = """
 1492: CREATE TABLE groups (
 1493:    group_id INTEGER,
 1494:    name TEXT
 1495: );
 1496: """
 1497:     insert_into = """
 1498: INSERT INTO groups VALUES
 1499:     (1, 'name');
 1500: """
 1501:     create_view = """
 1502: CREATE VIEW group_view
 1503: AS
 1504: SELECT * FROM groups;
 1505: """
 1506:     sqlite_buildin.execute(create_table)
 1507:     sqlite_buildin.execute(insert_into)
 1508:     sqlite_buildin.execute(create_view)
 1509:     result = pd.read_sql("SELECT * FROM group_view", sqlite_buildin)
 1510:     expected = DataFrame({"group_id": [1], "name": "name"})
 1511:     tm.assert_frame_equal(result, expected)
 1512: 
 1513: 
 1514: def test_execute_typeerror(sqlite_engine_iris):
 1515:     with pytest.raises(TypeError, match="pandas.io.sql.execute requires a connection"):
 1516:         with tm.assert_produces_warning(
 1517:             FutureWarning,
 1518:             match="`pandas.io.sql.execute` is deprecated and "
 1519:             "will be removed in the future version.",
 1520:         ):
 1521:             sql.execute("select * from iris", sqlite_engine_iris)
 1522: 
 1523: 
 1524: def test_execute_deprecated(sqlite_conn_iris):
 1525:     # GH50185
 1526:     with tm.assert_produces_warning(
 1527:         FutureWarning,
 1528:         match="`pandas.io.sql.execute` is deprecated and "
 1529:         "will be removed in the future version.",
 1530:     ):
 1531:         sql.execute("select * from iris", sqlite_conn_iris)
 1532: 
 1533: 
 1534: def flavor(conn_name):
 1535:     if "postgresql" in conn_name:
 1536:         return "postgresql"
 1537:     elif "sqlite" in conn_name:
 1538:         return "sqlite"
 1539:     elif "mysql" in conn_name:
 1540:         return "mysql"
 1541: 
 1542:     raise ValueError(f"unsupported connection: {conn_name}")
 1543: 
 1544: 
 1545: @pytest.mark.parametrize("conn", all_connectable_iris)
 1546: def test_read_sql_iris_parameter(conn, request, sql_strings):
 1547:     if "adbc" in conn:
 1548:         request.node.add_marker(
 1549:             pytest.mark.xfail(
 1550:                 reason="'params' not implemented for ADBC drivers",
 1551:                 strict=True,
 1552:             )
 1553:         )
 1554:     conn_name = conn
 1555:     conn = request.getfixturevalue(conn)
 1556:     query = sql_strings["read_parameters"][flavor(conn_name)]
 1557:     params = ("Iris-setosa", 5.1)
 1558:     with pandasSQL_builder(conn) as pandasSQL:
 1559:         with pandasSQL.run_transaction():
 1560:             iris_frame = pandasSQL.read_query(query, params=params)
 1561:     check_iris_frame(iris_frame)
 1562: 
 1563: 
 1564: @pytest.mark.parametrize("conn", all_connectable_iris)
 1565: def test_read_sql_iris_named_parameter(conn, request, sql_strings):
 1566:     if "adbc" in conn:
 1567:         request.node.add_marker(
 1568:             pytest.mark.xfail(
 1569:                 reason="'params' not implemented for ADBC drivers",
 1570:                 strict=True,
 1571:             )
 1572:         )
 1573: 
 1574:     conn_name = conn
 1575:     conn = request.getfixturevalue(conn)
 1576:     query = sql_strings["read_named_parameters"][flavor(conn_name)]
 1577:     params = {"name": "Iris-setosa", "length": 5.1}
 1578:     with pandasSQL_builder(conn) as pandasSQL:
 1579:         with pandasSQL.run_transaction():
 1580:             iris_frame = pandasSQL.read_query(query, params=params)
 1581:     check_iris_frame(iris_frame)
 1582: 
 1583: 
 1584: @pytest.mark.parametrize("conn", all_connectable_iris)
 1585: def test_read_sql_iris_no_parameter_with_percent(conn, request, sql_strings):
 1586:     if "mysql" in conn or ("postgresql" in conn and "adbc" not in conn):
 1587:         request.applymarker(pytest.mark.xfail(reason="broken test"))
 1588: 
 1589:     conn_name = conn
 1590:     conn = request.getfixturevalue(conn)
 1591: 
 1592:     query = sql_strings["read_no_parameters_with_percent"][flavor(conn_name)]
 1593:     with pandasSQL_builder(conn) as pandasSQL:
 1594:         with pandasSQL.run_transaction():
 1595:             iris_frame = pandasSQL.read_query(query, params=None)
 1596:     check_iris_frame(iris_frame)
 1597: 
 1598: 
 1599: # -----------------------------------------------------------------------------
 1600: # -- Testing the public API
 1601: 
 1602: 
 1603: @pytest.mark.parametrize("conn", all_connectable_iris)
 1604: def test_api_read_sql_view(conn, request):
 1605:     conn = request.getfixturevalue(conn)
 1606:     iris_frame = sql.read_sql_query("SELECT * FROM iris_view", conn)
 1607:     check_iris_frame(iris_frame)
 1608: 
 1609: 
 1610: @pytest.mark.parametrize("conn", all_connectable_iris)
 1611: def test_api_read_sql_with_chunksize_no_result(conn, request):
 1612:     if "adbc" in conn:
 1613:         request.node.add_marker(
 1614:             pytest.mark.xfail(reason="chunksize argument NotImplemented with ADBC")
 1615:         )
 1616:     conn = request.getfixturevalue(conn)
 1617:     query = 'SELECT * FROM iris_view WHERE "SepalLength" < 0.0'
 1618:     with_batch = sql.read_sql_query(query, conn, chunksize=5)
 1619:     without_batch = sql.read_sql_query(query, conn)
 1620:     tm.assert_frame_equal(concat(with_batch), without_batch)
 1621: 
 1622: 
 1623: @pytest.mark.parametrize("conn", all_connectable)
 1624: def test_api_to_sql(conn, request, test_frame1):
 1625:     conn = request.getfixturevalue(conn)
 1626:     if sql.has_table("test_frame1", conn):
 1627:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1628:             pandasSQL.drop_table("test_frame1")
 1629: 
 1630:     sql.to_sql(test_frame1, "test_frame1", conn)
 1631:     assert sql.has_table("test_frame1", conn)
 1632: 
 1633: 
 1634: @pytest.mark.parametrize("conn", all_connectable)
 1635: def test_api_to_sql_fail(conn, request, test_frame1):
 1636:     conn = request.getfixturevalue(conn)
 1637:     if sql.has_table("test_frame2", conn):
 1638:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1639:             pandasSQL.drop_table("test_frame2")
 1640: 
 1641:     sql.to_sql(test_frame1, "test_frame2", conn, if_exists="fail")
 1642:     assert sql.has_table("test_frame2", conn)
 1643: 
 1644:     msg = "Table 'test_frame2' already exists"
 1645:     with pytest.raises(ValueError, match=msg):
 1646:         sql.to_sql(test_frame1, "test_frame2", conn, if_exists="fail")
 1647: 
 1648: 
 1649: @pytest.mark.parametrize("conn", all_connectable)
 1650: def test_api_to_sql_replace(conn, request, test_frame1):
 1651:     conn = request.getfixturevalue(conn)
 1652:     if sql.has_table("test_frame3", conn):
 1653:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1654:             pandasSQL.drop_table("test_frame3")
 1655: 
 1656:     sql.to_sql(test_frame1, "test_frame3", conn, if_exists="fail")
 1657:     # Add to table again
 1658:     sql.to_sql(test_frame1, "test_frame3", conn, if_exists="replace")
 1659:     assert sql.has_table("test_frame3", conn)
 1660: 
 1661:     num_entries = len(test_frame1)
 1662:     num_rows = count_rows(conn, "test_frame3")
 1663: 
 1664:     assert num_rows == num_entries
 1665: 
 1666: 
 1667: @pytest.mark.parametrize("conn", all_connectable)
 1668: def test_api_to_sql_append(conn, request, test_frame1):
 1669:     conn = request.getfixturevalue(conn)
 1670:     if sql.has_table("test_frame4", conn):
 1671:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1672:             pandasSQL.drop_table("test_frame4")
 1673: 
 1674:     assert sql.to_sql(test_frame1, "test_frame4", conn, if_exists="fail") == 4
 1675: 
 1676:     # Add to table again
 1677:     assert sql.to_sql(test_frame1, "test_frame4", conn, if_exists="append") == 4
 1678:     assert sql.has_table("test_frame4", conn)
 1679: 
 1680:     num_entries = 2 * len(test_frame1)
 1681:     num_rows = count_rows(conn, "test_frame4")
 1682: 
 1683:     assert num_rows == num_entries
 1684: 
 1685: 
 1686: @pytest.mark.parametrize("conn", all_connectable)
 1687: def test_api_to_sql_type_mapping(conn, request, test_frame3):
 1688:     conn = request.getfixturevalue(conn)
 1689:     if sql.has_table("test_frame5", conn):
 1690:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1691:             pandasSQL.drop_table("test_frame5")
 1692: 
 1693:     sql.to_sql(test_frame3, "test_frame5", conn, index=False)
 1694:     result = sql.read_sql("SELECT * FROM test_frame5", conn)
 1695: 
 1696:     tm.assert_frame_equal(test_frame3, result)
 1697: 
 1698: 
 1699: @pytest.mark.parametrize("conn", all_connectable)
 1700: def test_api_to_sql_series(conn, request):
 1701:     conn = request.getfixturevalue(conn)
 1702:     if sql.has_table("test_series", conn):
 1703:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1704:             pandasSQL.drop_table("test_series")
 1705: 
 1706:     s = Series(np.arange(5, dtype="int64"), name="series")
 1707:     sql.to_sql(s, "test_series", conn, index=False)
 1708:     s2 = sql.read_sql_query("SELECT * FROM test_series", conn)
 1709:     tm.assert_frame_equal(s.to_frame(), s2)
 1710: 
 1711: 
 1712: @pytest.mark.parametrize("conn", all_connectable)
 1713: def test_api_roundtrip(conn, request, test_frame1):
 1714:     conn_name = conn
 1715:     conn = request.getfixturevalue(conn)
 1716:     if sql.has_table("test_frame_roundtrip", conn):
 1717:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1718:             pandasSQL.drop_table("test_frame_roundtrip")
 1719: 
 1720:     sql.to_sql(test_frame1, "test_frame_roundtrip", con=conn)
 1721:     result = sql.read_sql_query("SELECT * FROM test_frame_roundtrip", con=conn)
 1722: 
 1723:     # HACK!
 1724:     if "adbc" in conn_name:
 1725:         result = result.rename(columns={"__index_level_0__": "level_0"})
 1726:     result.index = test_frame1.index
 1727:     result.set_index("level_0", inplace=True)
 1728:     result.index.astype(int)
 1729:     result.index.name = None
 1730:     tm.assert_frame_equal(result, test_frame1)
 1731: 
 1732: 
 1733: @pytest.mark.parametrize("conn", all_connectable)
 1734: def test_api_roundtrip_chunksize(conn, request, test_frame1):
 1735:     if "adbc" in conn:
 1736:         request.node.add_marker(
 1737:             pytest.mark.xfail(reason="chunksize argument NotImplemented with ADBC")
 1738:         )
 1739:     conn = request.getfixturevalue(conn)
 1740:     if sql.has_table("test_frame_roundtrip", conn):
 1741:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1742:             pandasSQL.drop_table("test_frame_roundtrip")
 1743: 
 1744:     sql.to_sql(
 1745:         test_frame1,
 1746:         "test_frame_roundtrip",
 1747:         con=conn,
 1748:         index=False,
 1749:         chunksize=2,
 1750:     )
 1751:     result = sql.read_sql_query("SELECT * FROM test_frame_roundtrip", con=conn)
 1752:     tm.assert_frame_equal(result, test_frame1)
 1753: 
 1754: 
 1755: @pytest.mark.parametrize("conn", all_connectable_iris)
 1756: def test_api_execute_sql(conn, request):
 1757:     # drop_sql = "DROP TABLE IF EXISTS test"  # should already be done
 1758:     conn = request.getfixturevalue(conn)
 1759:     with sql.pandasSQL_builder(conn) as pandas_sql:
 1760:         iris_results = pandas_sql.execute("SELECT * FROM iris")
 1761:         row = iris_results.fetchone()
 1762:         iris_results.close()
 1763:     assert list(row) == [5.1, 3.5, 1.4, 0.2, "Iris-setosa"]
 1764: 
 1765: 
 1766: @pytest.mark.parametrize("conn", all_connectable_types)
 1767: def test_api_date_parsing(conn, request):
 1768:     conn_name = conn
 1769:     conn = request.getfixturevalue(conn)
 1770:     # Test date parsing in read_sql
 1771:     # No Parsing
 1772:     df = sql.read_sql_query("SELECT * FROM types", conn)
 1773:     if not ("mysql" in conn_name or "postgres" in conn_name):
 1774:         assert not issubclass(df.DateCol.dtype.type, np.datetime64)
 1775: 
 1776:     df = sql.read_sql_query("SELECT * FROM types", conn, parse_dates=["DateCol"])
 1777:     assert issubclass(df.DateCol.dtype.type, np.datetime64)
 1778:     assert df.DateCol.tolist() == [
 1779:         Timestamp(2000, 1, 3, 0, 0, 0),
 1780:         Timestamp(2000, 1, 4, 0, 0, 0),
 1781:     ]
 1782: 
 1783:     df = sql.read_sql_query(
 1784:         "SELECT * FROM types",
 1785:         conn,
 1786:         parse_dates={"DateCol": "%Y-%m-%d %H:%M:%S"},
 1787:     )
 1788:     assert issubclass(df.DateCol.dtype.type, np.datetime64)
 1789:     assert df.DateCol.tolist() == [
 1790:         Timestamp(2000, 1, 3, 0, 0, 0),
 1791:         Timestamp(2000, 1, 4, 0, 0, 0),
 1792:     ]
 1793: 
 1794:     df = sql.read_sql_query("SELECT * FROM types", conn, parse_dates=["IntDateCol"])
 1795:     assert issubclass(df.IntDateCol.dtype.type, np.datetime64)
 1796:     assert df.IntDateCol.tolist() == [
 1797:         Timestamp(1986, 12, 25, 0, 0, 0),
 1798:         Timestamp(2013, 1, 1, 0, 0, 0),
 1799:     ]
 1800: 
 1801:     df = sql.read_sql_query(
 1802:         "SELECT * FROM types", conn, parse_dates={"IntDateCol": "s"}
 1803:     )
 1804:     assert issubclass(df.IntDateCol.dtype.type, np.datetime64)
 1805:     assert df.IntDateCol.tolist() == [
 1806:         Timestamp(1986, 12, 25, 0, 0, 0),
 1807:         Timestamp(2013, 1, 1, 0, 0, 0),
 1808:     ]
 1809: 
 1810:     df = sql.read_sql_query(
 1811:         "SELECT * FROM types",
 1812:         conn,
 1813:         parse_dates={"IntDateOnlyCol": "%Y%m%d"},
 1814:     )
 1815:     assert issubclass(df.IntDateOnlyCol.dtype.type, np.datetime64)
 1816:     assert df.IntDateOnlyCol.tolist() == [
 1817:         Timestamp("2010-10-10"),
 1818:         Timestamp("2010-12-12"),
 1819:     ]
 1820: 
 1821: 
 1822: @pytest.mark.parametrize("conn", all_connectable_types)
 1823: @pytest.mark.parametrize("error", ["ignore", "raise", "coerce"])
 1824: @pytest.mark.parametrize(
 1825:     "read_sql, text, mode",
 1826:     [
 1827:         (sql.read_sql, "SELECT * FROM types", ("sqlalchemy", "fallback")),
 1828:         (sql.read_sql, "types", ("sqlalchemy")),
 1829:         (
 1830:             sql.read_sql_query,
 1831:             "SELECT * FROM types",
 1832:             ("sqlalchemy", "fallback"),
 1833:         ),
 1834:         (sql.read_sql_table, "types", ("sqlalchemy")),
 1835:     ],
 1836: )
 1837: def test_api_custom_dateparsing_error(
 1838:     conn, request, read_sql, text, mode, error, types_data_frame
 1839: ):
 1840:     conn_name = conn
 1841:     conn = request.getfixturevalue(conn)
 1842:     if text == "types" and conn_name == "sqlite_buildin_types":
 1843:         request.applymarker(
 1844:             pytest.mark.xfail(reason="failing combination of arguments")
 1845:         )
 1846: 
 1847:     expected = types_data_frame.astype({"DateCol": "datetime64[ns]"})
 1848: 
 1849:     result = read_sql(
 1850:         text,
 1851:         con=conn,
 1852:         parse_dates={
 1853:             "DateCol": {"errors": error},
 1854:         },
 1855:     )
 1856:     if "postgres" in conn_name:
 1857:         # TODO: clean up types_data_frame fixture
 1858:         result["BoolCol"] = result["BoolCol"].astype(int)
 1859:         result["BoolColWithNull"] = result["BoolColWithNull"].astype(float)
 1860: 
 1861:     if conn_name == "postgresql_adbc_types":
 1862:         expected = expected.astype(
 1863:             {
 1864:                 "IntDateCol": "int32",
 1865:                 "IntDateOnlyCol": "int32",
 1866:                 "IntCol": "int32",
 1867:             }
 1868:         )
 1869: 
 1870:         if not pa_version_under13p0:
 1871:             # TODO: is this astype safe?
 1872:             expected["DateCol"] = expected["DateCol"].astype("datetime64[us]")
 1873: 
 1874:     tm.assert_frame_equal(result, expected)
 1875: 
 1876: 
 1877: @pytest.mark.parametrize("conn", all_connectable_types)
 1878: def test_api_date_and_index(conn, request):
 1879:     # Test case where same column appears in parse_date and index_col
 1880:     conn = request.getfixturevalue(conn)
 1881:     df = sql.read_sql_query(
 1882:         "SELECT * FROM types",
 1883:         conn,
 1884:         index_col="DateCol",
 1885:         parse_dates=["DateCol", "IntDateCol"],
 1886:     )
 1887: 
 1888:     assert issubclass(df.index.dtype.type, np.datetime64)
 1889:     assert issubclass(df.IntDateCol.dtype.type, np.datetime64)
 1890: 
 1891: 
 1892: @pytest.mark.parametrize("conn", all_connectable)
 1893: def test_api_timedelta(conn, request):
 1894:     # see #6921
 1895:     conn_name = conn
 1896:     conn = request.getfixturevalue(conn)
 1897:     if sql.has_table("test_timedelta", conn):
 1898:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1899:             pandasSQL.drop_table("test_timedelta")
 1900: 
 1901:     df = to_timedelta(Series(["00:00:01", "00:00:03"], name="foo")).to_frame()
 1902: 
 1903:     if conn_name == "sqlite_adbc_conn":
 1904:         request.node.add_marker(
 1905:             pytest.mark.xfail(
 1906:                 reason="sqlite ADBC driver doesn't implement timedelta",
 1907:             )
 1908:         )
 1909: 
 1910:     if "adbc" in conn_name:
 1911:         if pa_version_under14p1:
 1912:             exp_warning = DeprecationWarning
 1913:         else:
 1914:             exp_warning = None
 1915:     else:
 1916:         exp_warning = UserWarning
 1917: 
 1918:     with tm.assert_produces_warning(exp_warning, check_stacklevel=False):
 1919:         result_count = df.to_sql(name="test_timedelta", con=conn)
 1920:     assert result_count == 2
 1921:     result = sql.read_sql_query("SELECT * FROM test_timedelta", conn)
 1922: 
 1923:     if conn_name == "postgresql_adbc_conn":
 1924:         # TODO: Postgres stores an INTERVAL, which ADBC reads as a Month-Day-Nano
 1925:         # Interval; the default pandas type mapper maps this to a DateOffset
 1926:         # but maybe we should try and restore the timedelta here?
 1927:         expected = Series(
 1928:             [
 1929:                 pd.DateOffset(months=0, days=0, microseconds=1000000, nanoseconds=0),
 1930:                 pd.DateOffset(months=0, days=0, microseconds=3000000, nanoseconds=0),
 1931:             ],
 1932:             name="foo",
 1933:         )
 1934:     else:
 1935:         expected = df["foo"].astype("int64")
 1936:     tm.assert_series_equal(result["foo"], expected)
 1937: 
 1938: 
 1939: @pytest.mark.parametrize("conn", all_connectable)
 1940: def test_api_complex_raises(conn, request):
 1941:     conn_name = conn
 1942:     conn = request.getfixturevalue(conn)
 1943:     df = DataFrame({"a": [1 + 1j, 2j]})
 1944: 
 1945:     if "adbc" in conn_name:
 1946:         msg = "datatypes not supported"
 1947:     else:
 1948:         msg = "Complex datatypes not supported"
 1949:     with pytest.raises(ValueError, match=msg):
 1950:         assert df.to_sql("test_complex", con=conn) is None
 1951: 
 1952: 
 1953: @pytest.mark.parametrize("conn", all_connectable)
 1954: @pytest.mark.parametrize(
 1955:     "index_name,index_label,expected",
 1956:     [
 1957:         # no index name, defaults to 'index'
 1958:         (None, None, "index"),
 1959:         # specifying index_label
 1960:         (None, "other_label", "other_label"),
 1961:         # using the index name
 1962:         ("index_name", None, "index_name"),
 1963:         # has index name, but specifying index_label
 1964:         ("index_name", "other_label", "other_label"),
 1965:         # index name is integer
 1966:         (0, None, "0"),
 1967:         # index name is None but index label is integer
 1968:         (None, 0, "0"),
 1969:     ],
 1970: )
 1971: def test_api_to_sql_index_label(conn, request, index_name, index_label, expected):
 1972:     if "adbc" in conn:
 1973:         request.node.add_marker(
 1974:             pytest.mark.xfail(reason="index_label argument NotImplemented with ADBC")
 1975:         )
 1976:     conn = request.getfixturevalue(conn)
 1977:     if sql.has_table("test_index_label", conn):
 1978:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 1979:             pandasSQL.drop_table("test_index_label")
 1980: 
 1981:     temp_frame = DataFrame({"col1": range(4)})
 1982:     temp_frame.index.name = index_name
 1983:     query = "SELECT * FROM test_index_label"
 1984:     sql.to_sql(temp_frame, "test_index_label", conn, index_label=index_label)
 1985:     frame = sql.read_sql_query(query, conn)
 1986:     assert frame.columns[0] == expected
 1987: 
 1988: 
 1989: @pytest.mark.parametrize("conn", all_connectable)
 1990: def test_api_to_sql_index_label_multiindex(conn, request):
 1991:     conn_name = conn
 1992:     if "mysql" in conn_name:
 1993:         request.applymarker(
 1994:             pytest.mark.xfail(
 1995:                 reason="MySQL can fail using TEXT without length as key", strict=False
 1996:             )
 1997:         )
 1998:     elif "adbc" in conn_name:
 1999:         request.node.add_marker(
 2000:             pytest.mark.xfail(reason="index_label argument NotImplemented with ADBC")
 2001:         )
 2002: 
 2003:     conn = request.getfixturevalue(conn)
 2004:     if sql.has_table("test_index_label", conn):
 2005:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2006:             pandasSQL.drop_table("test_index_label")
 2007: 
 2008:     expected_row_count = 4
 2009:     temp_frame = DataFrame(
 2010:         {"col1": range(4)},
 2011:         index=MultiIndex.from_product([("A0", "A1"), ("B0", "B1")]),
 2012:     )
 2013: 
 2014:     # no index name, defaults to 'level_0' and 'level_1'
 2015:     result = sql.to_sql(temp_frame, "test_index_label", conn)
 2016:     assert result == expected_row_count
 2017:     frame = sql.read_sql_query("SELECT * FROM test_index_label", conn)
 2018:     assert frame.columns[0] == "level_0"
 2019:     assert frame.columns[1] == "level_1"
 2020: 
 2021:     # specifying index_label
 2022:     result = sql.to_sql(
 2023:         temp_frame,
 2024:         "test_index_label",
 2025:         conn,
 2026:         if_exists="replace",
 2027:         index_label=["A", "B"],
 2028:     )
 2029:     assert result == expected_row_count
 2030:     frame = sql.read_sql_query("SELECT * FROM test_index_label", conn)
 2031:     assert frame.columns[:2].tolist() == ["A", "B"]
 2032: 
 2033:     # using the index name
 2034:     temp_frame.index.names = ["A", "B"]
 2035:     result = sql.to_sql(temp_frame, "test_index_label", conn, if_exists="replace")
 2036:     assert result == expected_row_count
 2037:     frame = sql.read_sql_query("SELECT * FROM test_index_label", conn)
 2038:     assert frame.columns[:2].tolist() == ["A", "B"]
 2039: 
 2040:     # has index name, but specifying index_label
 2041:     result = sql.to_sql(
 2042:         temp_frame,
 2043:         "test_index_label",
 2044:         conn,
 2045:         if_exists="replace",
 2046:         index_label=["C", "D"],
 2047:     )
 2048:     assert result == expected_row_count
 2049:     frame = sql.read_sql_query("SELECT * FROM test_index_label", conn)
 2050:     assert frame.columns[:2].tolist() == ["C", "D"]
 2051: 
 2052:     msg = "Length of 'index_label' should match number of levels, which is 2"
 2053:     with pytest.raises(ValueError, match=msg):
 2054:         sql.to_sql(
 2055:             temp_frame,
 2056:             "test_index_label",
 2057:             conn,
 2058:             if_exists="replace",
 2059:             index_label="C",
 2060:         )
 2061: 
 2062: 
 2063: @pytest.mark.parametrize("conn", all_connectable)
 2064: def test_api_multiindex_roundtrip(conn, request):
 2065:     conn = request.getfixturevalue(conn)
 2066:     if sql.has_table("test_multiindex_roundtrip", conn):
 2067:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2068:             pandasSQL.drop_table("test_multiindex_roundtrip")
 2069: 
 2070:     df = DataFrame.from_records(
 2071:         [(1, 2.1, "line1"), (2, 1.5, "line2")],
 2072:         columns=["A", "B", "C"],
 2073:         index=["A", "B"],
 2074:     )
 2075: 
 2076:     df.to_sql(name="test_multiindex_roundtrip", con=conn)
 2077:     result = sql.read_sql_query(
 2078:         "SELECT * FROM test_multiindex_roundtrip", conn, index_col=["A", "B"]
 2079:     )
 2080:     tm.assert_frame_equal(df, result, check_index_type=True)
 2081: 
 2082: 
 2083: @pytest.mark.parametrize("conn", all_connectable)
 2084: @pytest.mark.parametrize(
 2085:     "dtype",
 2086:     [
 2087:         None,
 2088:         int,
 2089:         float,
 2090:         {"A": int, "B": float},
 2091:     ],
 2092: )
 2093: def test_api_dtype_argument(conn, request, dtype):
 2094:     # GH10285 Add dtype argument to read_sql_query
 2095:     conn_name = conn
 2096:     conn = request.getfixturevalue(conn)
 2097:     if sql.has_table("test_dtype_argument", conn):
 2098:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2099:             pandasSQL.drop_table("test_dtype_argument")
 2100: 
 2101:     df = DataFrame([[1.2, 3.4], [5.6, 7.8]], columns=["A", "B"])
 2102:     assert df.to_sql(name="test_dtype_argument", con=conn) == 2
 2103: 
 2104:     expected = df.astype(dtype)
 2105: 
 2106:     if "postgres" in conn_name:
 2107:         query = 'SELECT "A", "B" FROM test_dtype_argument'
 2108:     else:
 2109:         query = "SELECT A, B FROM test_dtype_argument"
 2110:     result = sql.read_sql_query(query, con=conn, dtype=dtype)
 2111: 
 2112:     tm.assert_frame_equal(result, expected)
 2113: 
 2114: 
 2115: @pytest.mark.parametrize("conn", all_connectable)
 2116: def test_api_integer_col_names(conn, request):
 2117:     conn = request.getfixturevalue(conn)
 2118:     df = DataFrame([[1, 2], [3, 4]], columns=[0, 1])
 2119:     sql.to_sql(df, "test_frame_integer_col_names", conn, if_exists="replace")
 2120: 
 2121: 
 2122: @pytest.mark.parametrize("conn", all_connectable)
 2123: def test_api_get_schema(conn, request, test_frame1):
 2124:     if "adbc" in conn:
 2125:         request.node.add_marker(
 2126:             pytest.mark.xfail(
 2127:                 reason="'get_schema' not implemented for ADBC drivers",
 2128:                 strict=True,
 2129:             )
 2130:         )
 2131:     conn = request.getfixturevalue(conn)
 2132:     create_sql = sql.get_schema(test_frame1, "test", con=conn)
 2133:     assert "CREATE" in create_sql
 2134: 
 2135: 
 2136: @pytest.mark.parametrize("conn", all_connectable)
 2137: def test_api_get_schema_with_schema(conn, request, test_frame1):
 2138:     # GH28486
 2139:     if "adbc" in conn:
 2140:         request.node.add_marker(
 2141:             pytest.mark.xfail(
 2142:                 reason="'get_schema' not implemented for ADBC drivers",
 2143:                 strict=True,
 2144:             )
 2145:         )
 2146:     conn = request.getfixturevalue(conn)
 2147:     create_sql = sql.get_schema(test_frame1, "test", con=conn, schema="pypi")
 2148:     assert "CREATE TABLE pypi." in create_sql
 2149: 
 2150: 
 2151: @pytest.mark.parametrize("conn", all_connectable)
 2152: def test_api_get_schema_dtypes(conn, request):
 2153:     if "adbc" in conn:
 2154:         request.node.add_marker(
 2155:             pytest.mark.xfail(
 2156:                 reason="'get_schema' not implemented for ADBC drivers",
 2157:                 strict=True,
 2158:             )
 2159:         )
 2160:     conn_name = conn
 2161:     conn = request.getfixturevalue(conn)
 2162:     float_frame = DataFrame({"a": [1.1, 1.2], "b": [2.1, 2.2]})
 2163: 
 2164:     if conn_name == "sqlite_buildin":
 2165:         dtype = "INTEGER"
 2166:     else:
 2167:         from sqlalchemy import Integer
 2168: 
 2169:         dtype = Integer
 2170:     create_sql = sql.get_schema(float_frame, "test", con=conn, dtype={"b": dtype})
 2171:     assert "CREATE" in create_sql
 2172:     assert "INTEGER" in create_sql
 2173: 
 2174: 
 2175: @pytest.mark.parametrize("conn", all_connectable)
 2176: def test_api_get_schema_keys(conn, request, test_frame1):
 2177:     if "adbc" in conn:
 2178:         request.node.add_marker(
 2179:             pytest.mark.xfail(
 2180:                 reason="'get_schema' not implemented for ADBC drivers",
 2181:                 strict=True,
 2182:             )
 2183:         )
 2184:     conn_name = conn
 2185:     conn = request.getfixturevalue(conn)
 2186:     frame = DataFrame({"Col1": [1.1, 1.2], "Col2": [2.1, 2.2]})
 2187:     create_sql = sql.get_schema(frame, "test", con=conn, keys="Col1")
 2188: 
 2189:     if "mysql" in conn_name:
 2190:         constraint_sentence = "CONSTRAINT test_pk PRIMARY KEY (`Col1`)"
 2191:     else:
 2192:         constraint_sentence = 'CONSTRAINT test_pk PRIMARY KEY ("Col1")'
 2193:     assert constraint_sentence in create_sql
 2194: 
 2195:     # multiple columns as key (GH10385)
 2196:     create_sql = sql.get_schema(test_frame1, "test", con=conn, keys=["A", "B"])
 2197:     if "mysql" in conn_name:
 2198:         constraint_sentence = "CONSTRAINT test_pk PRIMARY KEY (`A`, `B`)"
 2199:     else:
 2200:         constraint_sentence = 'CONSTRAINT test_pk PRIMARY KEY ("A", "B")'
 2201:     assert constraint_sentence in create_sql
 2202: 
 2203: 
 2204: @pytest.mark.parametrize("conn", all_connectable)
 2205: def test_api_chunksize_read(conn, request):
 2206:     if "adbc" in conn:
 2207:         request.node.add_marker(
 2208:             pytest.mark.xfail(reason="chunksize argument NotImplemented with ADBC")
 2209:         )
 2210:     conn_name = conn
 2211:     conn = request.getfixturevalue(conn)
 2212:     if sql.has_table("test_chunksize", conn):
 2213:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2214:             pandasSQL.drop_table("test_chunksize")
 2215: 
 2216:     df = DataFrame(
 2217:         np.random.default_rng(2).standard_normal((22, 5)), columns=list("abcde")
 2218:     )
 2219:     df.to_sql(name="test_chunksize", con=conn, index=False)
 2220: 
 2221:     # reading the query in one time
 2222:     res1 = sql.read_sql_query("select * from test_chunksize", conn)
 2223: 
 2224:     # reading the query in chunks with read_sql_query
 2225:     res2 = DataFrame()
 2226:     i = 0
 2227:     sizes = [5, 5, 5, 5, 2]
 2228: 
 2229:     for chunk in sql.read_sql_query("select * from test_chunksize", conn, chunksize=5):
 2230:         res2 = concat([res2, chunk], ignore_index=True)
 2231:         assert len(chunk) == sizes[i]
 2232:         i += 1
 2233: 
 2234:     tm.assert_frame_equal(res1, res2)
 2235: 
 2236:     # reading the query in chunks with read_sql_query
 2237:     if conn_name == "sqlite_buildin":
 2238:         with pytest.raises(NotImplementedError, match=""):
 2239:             sql.read_sql_table("test_chunksize", conn, chunksize=5)
 2240:     else:
 2241:         res3 = DataFrame()
 2242:         i = 0
 2243:         sizes = [5, 5, 5, 5, 2]
 2244: 
 2245:         for chunk in sql.read_sql_table("test_chunksize", conn, chunksize=5):
 2246:             res3 = concat([res3, chunk], ignore_index=True)
 2247:             assert len(chunk) == sizes[i]
 2248:             i += 1
 2249: 
 2250:         tm.assert_frame_equal(res1, res3)
 2251: 
 2252: 
 2253: @pytest.mark.parametrize("conn", all_connectable)
 2254: def test_api_categorical(conn, request):
 2255:     if conn == "postgresql_adbc_conn":
 2256:         adbc = import_optional_dependency("adbc_driver_postgresql", errors="ignore")
 2257:         if adbc is not None and Version(adbc.__version__) < Version("0.9.0"):
 2258:             request.node.add_marker(
 2259:                 pytest.mark.xfail(
 2260:                     reason="categorical dtype not implemented for ADBC postgres driver",
 2261:                     strict=True,
 2262:                 )
 2263:             )
 2264:     # GH8624
 2265:     # test that categorical gets written correctly as dense column
 2266:     conn = request.getfixturevalue(conn)
 2267:     if sql.has_table("test_categorical", conn):
 2268:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2269:             pandasSQL.drop_table("test_categorical")
 2270: 
 2271:     df = DataFrame(
 2272:         {
 2273:             "person_id": [1, 2, 3],
 2274:             "person_name": ["John P. Doe", "Jane Dove", "John P. Doe"],
 2275:         }
 2276:     )
 2277:     df2 = df.copy()
 2278:     df2["person_name"] = df2["person_name"].astype("category")
 2279: 
 2280:     df2.to_sql(name="test_categorical", con=conn, index=False)
 2281:     res = sql.read_sql_query("SELECT * FROM test_categorical", conn)
 2282: 
 2283:     tm.assert_frame_equal(res, df)
 2284: 
 2285: 
 2286: @pytest.mark.parametrize("conn", all_connectable)
 2287: def test_api_unicode_column_name(conn, request):
 2288:     # GH 11431
 2289:     conn = request.getfixturevalue(conn)
 2290:     if sql.has_table("test_unicode", conn):
 2291:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2292:             pandasSQL.drop_table("test_unicode")
 2293: 
 2294:     df = DataFrame([[1, 2], [3, 4]], columns=["\xe9", "b"])
 2295:     df.to_sql(name="test_unicode", con=conn, index=False)
 2296: 
 2297: 
 2298: @pytest.mark.parametrize("conn", all_connectable)
 2299: def test_api_escaped_table_name(conn, request):
 2300:     # GH 13206
 2301:     conn_name = conn
 2302:     conn = request.getfixturevalue(conn)
 2303:     if sql.has_table("d1187b08-4943-4c8d-a7f6", conn):
 2304:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2305:             pandasSQL.drop_table("d1187b08-4943-4c8d-a7f6")
 2306: 
 2307:     df = DataFrame({"A": [0, 1, 2], "B": [0.2, np.nan, 5.6]})
 2308:     df.to_sql(name="d1187b08-4943-4c8d-a7f6", con=conn, index=False)
 2309: 
 2310:     if "postgres" in conn_name:
 2311:         query = 'SELECT * FROM "d1187b08-4943-4c8d-a7f6"'
 2312:     else:
 2313:         query = "SELECT * FROM `d1187b08-4943-4c8d-a7f6`"
 2314:     res = sql.read_sql_query(query, conn)
 2315: 
 2316:     tm.assert_frame_equal(res, df)
 2317: 
 2318: 
 2319: @pytest.mark.parametrize("conn", all_connectable)
 2320: def test_api_read_sql_duplicate_columns(conn, request):
 2321:     # GH#53117
 2322:     if "adbc" in conn:
 2323:         request.node.add_marker(
 2324:             pytest.mark.xfail(reason="pyarrow->pandas throws ValueError", strict=True)
 2325:         )
 2326:     conn = request.getfixturevalue(conn)
 2327:     if sql.has_table("test_table", conn):
 2328:         with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2329:             pandasSQL.drop_table("test_table")
 2330: 
 2331:     df = DataFrame({"a": [1, 2, 3], "b": [0.1, 0.2, 0.3], "c": 1})
 2332:     df.to_sql(name="test_table", con=conn, index=False)
 2333: 
 2334:     result = pd.read_sql("SELECT a, b, a +1 as a, c FROM test_table", conn)
 2335:     expected = DataFrame(
 2336:         [[1, 0.1, 2, 1], [2, 0.2, 3, 1], [3, 0.3, 4, 1]],
 2337:         columns=["a", "b", "a", "c"],
 2338:     )
 2339:     tm.assert_frame_equal(result, expected)
 2340: 
 2341: 
 2342: @pytest.mark.parametrize("conn", all_connectable)
 2343: def test_read_table_columns(conn, request, test_frame1):
 2344:     # test columns argument in read_table
 2345:     conn_name = conn
 2346:     if conn_name == "sqlite_buildin":
 2347:         request.applymarker(pytest.mark.xfail(reason="Not Implemented"))
 2348: 
 2349:     conn = request.getfixturevalue(conn)
 2350:     sql.to_sql(test_frame1, "test_frame", conn)
 2351: 
 2352:     cols = ["A", "B"]
 2353: 
 2354:     result = sql.read_sql_table("test_frame", conn, columns=cols)
 2355:     assert result.columns.tolist() == cols
 2356: 
 2357: 
 2358: @pytest.mark.parametrize("conn", all_connectable)
 2359: def test_read_table_index_col(conn, request, test_frame1):
 2360:     # test columns argument in read_table
 2361:     conn_name = conn
 2362:     if conn_name == "sqlite_buildin":
 2363:         request.applymarker(pytest.mark.xfail(reason="Not Implemented"))
 2364: 
 2365:     conn = request.getfixturevalue(conn)
 2366:     sql.to_sql(test_frame1, "test_frame", conn)
 2367: 
 2368:     result = sql.read_sql_table("test_frame", conn, index_col="index")
 2369:     assert result.index.names == ["index"]
 2370: 
 2371:     result = sql.read_sql_table("test_frame", conn, index_col=["A", "B"])
 2372:     assert result.index.names == ["A", "B"]
 2373: 
 2374:     result = sql.read_sql_table(
 2375:         "test_frame", conn, index_col=["A", "B"], columns=["C", "D"]
 2376:     )
 2377:     assert result.index.names == ["A", "B"]
 2378:     assert result.columns.tolist() == ["C", "D"]
 2379: 
 2380: 
 2381: @pytest.mark.parametrize("conn", all_connectable_iris)
 2382: def test_read_sql_delegate(conn, request):
 2383:     if conn == "sqlite_buildin_iris":
 2384:         request.applymarker(
 2385:             pytest.mark.xfail(
 2386:                 reason="sqlite_buildin connection does not implement read_sql_table"
 2387:             )
 2388:         )
 2389: 
 2390:     conn = request.getfixturevalue(conn)
 2391:     iris_frame1 = sql.read_sql_query("SELECT * FROM iris", conn)
 2392:     iris_frame2 = sql.read_sql("SELECT * FROM iris", conn)
 2393:     tm.assert_frame_equal(iris_frame1, iris_frame2)
 2394: 
 2395:     iris_frame1 = sql.read_sql_table("iris", conn)
 2396:     iris_frame2 = sql.read_sql("iris", conn)
 2397:     tm.assert_frame_equal(iris_frame1, iris_frame2)
 2398: 
 2399: 
 2400: def test_not_reflect_all_tables(sqlite_conn):
 2401:     conn = sqlite_conn
 2402:     from sqlalchemy import text
 2403:     from sqlalchemy.engine import Engine
 2404: 
 2405:     # create invalid table
 2406:     query_list = [
 2407:         text("CREATE TABLE invalid (x INTEGER, y UNKNOWN);"),
 2408:         text("CREATE TABLE other_table (x INTEGER, y INTEGER);"),
 2409:     ]
 2410: 
 2411:     for query in query_list:
 2412:         if isinstance(conn, Engine):
 2413:             with conn.connect() as conn:
 2414:                 with conn.begin():
 2415:                     conn.execute(query)
 2416:         else:
 2417:             with conn.begin():
 2418:                 conn.execute(query)
 2419: 
 2420:     with tm.assert_produces_warning(None):
 2421:         sql.read_sql_table("other_table", conn)
 2422:         sql.read_sql_query("SELECT * FROM other_table", conn)
 2423: 
 2424: 
 2425: @pytest.mark.parametrize("conn", all_connectable)
 2426: def test_warning_case_insensitive_table_name(conn, request, test_frame1):
 2427:     conn_name = conn
 2428:     if conn_name == "sqlite_buildin" or "adbc" in conn_name:
 2429:         request.applymarker(pytest.mark.xfail(reason="Does not raise warning"))
 2430: 
 2431:     conn = request.getfixturevalue(conn)
 2432:     # see gh-7815
 2433:     with tm.assert_produces_warning(
 2434:         UserWarning,
 2435:         match=(
 2436:             r"The provided table name 'TABLE1' is not found exactly as such in "
 2437:             r"the database after writing the table, possibly due to case "
 2438:             r"sensitivity issues. Consider using lower case table names."
 2439:         ),
 2440:     ):
 2441:         with sql.SQLDatabase(conn) as db:
 2442:             db.check_case_sensitive("TABLE1", "")
 2443: 
 2444:     # Test that the warning is certainly NOT triggered in a normal case.
 2445:     with tm.assert_produces_warning(None):
 2446:         test_frame1.to_sql(name="CaseSensitive", con=conn)
 2447: 
 2448: 
 2449: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2450: def test_sqlalchemy_type_mapping(conn, request):
 2451:     conn = request.getfixturevalue(conn)
 2452:     from sqlalchemy import TIMESTAMP
 2453: 
 2454:     # Test Timestamp objects (no datetime64 because of timezone) (GH9085)
 2455:     df = DataFrame(
 2456:         {"time": to_datetime(["2014-12-12 01:54", "2014-12-11 02:54"], utc=True)}
 2457:     )
 2458:     with sql.SQLDatabase(conn) as db:
 2459:         table = sql.SQLTable("test_type", db, frame=df)
 2460:         # GH 9086: TIMESTAMP is the suggested type for datetimes with timezones
 2461:         assert isinstance(table.table.c["time"].type, TIMESTAMP)
 2462: 
 2463: 
 2464: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2465: @pytest.mark.parametrize(
 2466:     "integer, expected",
 2467:     [
 2468:         ("int8", "SMALLINT"),
 2469:         ("Int8", "SMALLINT"),
 2470:         ("uint8", "SMALLINT"),
 2471:         ("UInt8", "SMALLINT"),
 2472:         ("int16", "SMALLINT"),
 2473:         ("Int16", "SMALLINT"),
 2474:         ("uint16", "INTEGER"),
 2475:         ("UInt16", "INTEGER"),
 2476:         ("int32", "INTEGER"),
 2477:         ("Int32", "INTEGER"),
 2478:         ("uint32", "BIGINT"),
 2479:         ("UInt32", "BIGINT"),
 2480:         ("int64", "BIGINT"),
 2481:         ("Int64", "BIGINT"),
 2482:         (int, "BIGINT" if np.dtype(int).name == "int64" else "INTEGER"),
 2483:     ],
 2484: )
 2485: def test_sqlalchemy_integer_mapping(conn, request, integer, expected):
 2486:     # GH35076 Map pandas integer to optimal SQLAlchemy integer type
 2487:     conn = request.getfixturevalue(conn)
 2488:     df = DataFrame([0, 1], columns=["a"], dtype=integer)
 2489:     with sql.SQLDatabase(conn) as db:
 2490:         table = sql.SQLTable("test_type", db, frame=df)
 2491: 
 2492:         result = str(table.table.c.a.type)
 2493:     assert result == expected
 2494: 
 2495: 
 2496: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2497: @pytest.mark.parametrize("integer", ["uint64", "UInt64"])
 2498: def test_sqlalchemy_integer_overload_mapping(conn, request, integer):
 2499:     conn = request.getfixturevalue(conn)
 2500:     # GH35076 Map pandas integer to optimal SQLAlchemy integer type
 2501:     df = DataFrame([0, 1], columns=["a"], dtype=integer)
 2502:     with sql.SQLDatabase(conn) as db:
 2503:         with pytest.raises(
 2504:             ValueError, match="Unsigned 64 bit integer datatype is not supported"
 2505:         ):
 2506:             sql.SQLTable("test_type", db, frame=df)
 2507: 
 2508: 
 2509: @pytest.mark.parametrize("conn", all_connectable)
 2510: def test_database_uri_string(conn, request, test_frame1):
 2511:     pytest.importorskip("sqlalchemy")
 2512:     conn = request.getfixturevalue(conn)
 2513:     # Test read_sql and .to_sql method with a database URI (GH10654)
 2514:     # db_uri = 'sqlite:///:memory:' # raises
 2515:     # sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) near
 2516:     # "iris": syntax error [SQL: 'iris']
 2517:     with tm.ensure_clean() as name:
 2518:         db_uri = "sqlite:///" + name
 2519:         table = "iris"
 2520:         test_frame1.to_sql(name=table, con=db_uri, if_exists="replace", index=False)
 2521:         test_frame2 = sql.read_sql(table, db_uri)
 2522:         test_frame3 = sql.read_sql_table(table, db_uri)
 2523:         query = "SELECT * FROM iris"
 2524:         test_frame4 = sql.read_sql_query(query, db_uri)
 2525:     tm.assert_frame_equal(test_frame1, test_frame2)
 2526:     tm.assert_frame_equal(test_frame1, test_frame3)
 2527:     tm.assert_frame_equal(test_frame1, test_frame4)
 2528: 
 2529: 
 2530: @td.skip_if_installed("pg8000")
 2531: @pytest.mark.parametrize("conn", all_connectable)
 2532: def test_pg8000_sqlalchemy_passthrough_error(conn, request):
 2533:     pytest.importorskip("sqlalchemy")
 2534:     conn = request.getfixturevalue(conn)
 2535:     # using driver that will not be installed on CI to trigger error
 2536:     # in sqlalchemy.create_engine -> test passing of this error to user
 2537:     db_uri = "postgresql+pg8000://user:pass@host/dbname"
 2538:     with pytest.raises(ImportError, match="pg8000"):
 2539:         sql.read_sql("select * from table", db_uri)
 2540: 
 2541: 
 2542: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 2543: def test_query_by_text_obj(conn, request):
 2544:     # WIP : GH10846
 2545:     conn_name = conn
 2546:     conn = request.getfixturevalue(conn)
 2547:     from sqlalchemy import text
 2548: 
 2549:     if "postgres" in conn_name:
 2550:         name_text = text('select * from iris where "Name"=:name')
 2551:     else:
 2552:         name_text = text("select * from iris where name=:name")
 2553:     iris_df = sql.read_sql(name_text, conn, params={"name": "Iris-versicolor"})
 2554:     all_names = set(iris_df["Name"])
 2555:     assert all_names == {"Iris-versicolor"}
 2556: 
 2557: 
 2558: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 2559: def test_query_by_select_obj(conn, request):
 2560:     conn = request.getfixturevalue(conn)
 2561:     # WIP : GH10846
 2562:     from sqlalchemy import (
 2563:         bindparam,
 2564:         select,
 2565:     )
 2566: 
 2567:     iris = iris_table_metadata()
 2568:     name_select = select(iris).where(iris.c.Name == bindparam("name"))
 2569:     iris_df = sql.read_sql(name_select, conn, params={"name": "Iris-setosa"})
 2570:     all_names = set(iris_df["Name"])
 2571:     assert all_names == {"Iris-setosa"}
 2572: 
 2573: 
 2574: @pytest.mark.parametrize("conn", all_connectable)
 2575: def test_column_with_percentage(conn, request):
 2576:     # GH 37157
 2577:     conn_name = conn
 2578:     if conn_name == "sqlite_buildin":
 2579:         request.applymarker(pytest.mark.xfail(reason="Not Implemented"))
 2580: 
 2581:     conn = request.getfixturevalue(conn)
 2582:     df = DataFrame({"A": [0, 1, 2], "%_variation": [3, 4, 5]})
 2583:     df.to_sql(name="test_column_percentage", con=conn, index=False)
 2584: 
 2585:     res = sql.read_sql_table("test_column_percentage", conn)
 2586: 
 2587:     tm.assert_frame_equal(res, df)
 2588: 
 2589: 
 2590: def test_sql_open_close(test_frame3):
 2591:     # Test if the IO in the database still work if the connection closed
 2592:     # between the writing and reading (as in many real situations).
 2593: 
 2594:     with tm.ensure_clean() as name:
 2595:         with closing(sqlite3.connect(name)) as conn:
 2596:             assert sql.to_sql(test_frame3, "test_frame3_legacy", conn, index=False) == 4
 2597: 
 2598:         with closing(sqlite3.connect(name)) as conn:
 2599:             result = sql.read_sql_query("SELECT * FROM test_frame3_legacy;", conn)
 2600: 
 2601:     tm.assert_frame_equal(test_frame3, result)
 2602: 
 2603: 
 2604: @td.skip_if_installed("sqlalchemy")
 2605: def test_con_string_import_error():
 2606:     conn = "mysql://root@localhost/pandas"
 2607:     msg = "Using URI string without sqlalchemy installed"
 2608:     with pytest.raises(ImportError, match=msg):
 2609:         sql.read_sql("SELECT * FROM iris", conn)
 2610: 
 2611: 
 2612: @td.skip_if_installed("sqlalchemy")
 2613: def test_con_unknown_dbapi2_class_does_not_error_without_sql_alchemy_installed():
 2614:     class MockSqliteConnection:
 2615:         def __init__(self, *args, **kwargs) -> None:
 2616:             self.conn = sqlite3.Connection(*args, **kwargs)
 2617: 
 2618:         def __getattr__(self, name):
 2619:             return getattr(self.conn, name)
 2620: 
 2621:         def close(self):
 2622:             self.conn.close()
 2623: 
 2624:     with contextlib.closing(MockSqliteConnection(":memory:")) as conn:
 2625:         with tm.assert_produces_warning(UserWarning):
 2626:             sql.read_sql("SELECT 1", conn)
 2627: 
 2628: 
 2629: def test_sqlite_read_sql_delegate(sqlite_buildin_iris):
 2630:     conn = sqlite_buildin_iris
 2631:     iris_frame1 = sql.read_sql_query("SELECT * FROM iris", conn)
 2632:     iris_frame2 = sql.read_sql("SELECT * FROM iris", conn)
 2633:     tm.assert_frame_equal(iris_frame1, iris_frame2)
 2634: 
 2635:     msg = "Execution failed on sql 'iris': near \"iris\": syntax error"
 2636:     with pytest.raises(sql.DatabaseError, match=msg):
 2637:         sql.read_sql("iris", conn)
 2638: 
 2639: 
 2640: def test_get_schema2(test_frame1):
 2641:     # without providing a connection object (available for backwards comp)
 2642:     create_sql = sql.get_schema(test_frame1, "test")
 2643:     assert "CREATE" in create_sql
 2644: 
 2645: 
 2646: def test_sqlite_type_mapping(sqlite_buildin):
 2647:     # Test Timestamp objects (no datetime64 because of timezone) (GH9085)
 2648:     conn = sqlite_buildin
 2649:     df = DataFrame(
 2650:         {"time": to_datetime(["2014-12-12 01:54", "2014-12-11 02:54"], utc=True)}
 2651:     )
 2652:     db = sql.SQLiteDatabase(conn)
 2653:     table = sql.SQLiteTable("test_type", db, frame=df)
 2654:     schema = table.sql_schema()
 2655:     for col in schema.split("\n"):
 2656:         if col.split()[0].strip('"') == "time":
 2657:             assert col.split()[1] == "TIMESTAMP"
 2658: 
 2659: 
 2660: # -----------------------------------------------------------------------------
 2661: # -- Database flavor specific tests
 2662: 
 2663: 
 2664: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2665: def test_create_table(conn, request):
 2666:     if conn == "sqlite_str":
 2667:         pytest.skip("sqlite_str has no inspection system")
 2668: 
 2669:     conn = request.getfixturevalue(conn)
 2670: 
 2671:     from sqlalchemy import inspect
 2672: 
 2673:     temp_frame = DataFrame({"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]})
 2674:     with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2675:         assert pandasSQL.to_sql(temp_frame, "temp_frame") == 4
 2676: 
 2677:     insp = inspect(conn)
 2678:     assert insp.has_table("temp_frame")
 2679: 
 2680:     # Cleanup
 2681:     with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 2682:         pandasSQL.drop_table("temp_frame")
 2683: 
 2684: 
 2685: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2686: def test_drop_table(conn, request):
 2687:     if conn == "sqlite_str":
 2688:         pytest.skip("sqlite_str has no inspection system")
 2689: 
 2690:     conn = request.getfixturevalue(conn)
 2691: 
 2692:     from sqlalchemy import inspect
 2693: 
 2694:     temp_frame = DataFrame({"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]})
 2695:     with sql.SQLDatabase(conn) as pandasSQL:
 2696:         with pandasSQL.run_transaction():
 2697:             assert pandasSQL.to_sql(temp_frame, "temp_frame") == 4
 2698: 
 2699:         insp = inspect(conn)
 2700:         assert insp.has_table("temp_frame")
 2701: 
 2702:         with pandasSQL.run_transaction():
 2703:             pandasSQL.drop_table("temp_frame")
 2704:         try:
 2705:             insp.clear_cache()  # needed with SQLAlchemy 2.0, unavailable prior
 2706:         except AttributeError:
 2707:             pass
 2708:         assert not insp.has_table("temp_frame")
 2709: 
 2710: 
 2711: @pytest.mark.parametrize("conn", all_connectable)
 2712: def test_roundtrip(conn, request, test_frame1):
 2713:     if conn == "sqlite_str":
 2714:         pytest.skip("sqlite_str has no inspection system")
 2715: 
 2716:     conn_name = conn
 2717:     conn = request.getfixturevalue(conn)
 2718:     pandasSQL = pandasSQL_builder(conn)
 2719:     with pandasSQL.run_transaction():
 2720:         assert pandasSQL.to_sql(test_frame1, "test_frame_roundtrip") == 4
 2721:         result = pandasSQL.read_query("SELECT * FROM test_frame_roundtrip")
 2722: 
 2723:     if "adbc" in conn_name:
 2724:         result = result.rename(columns={"__index_level_0__": "level_0"})
 2725:     result.set_index("level_0", inplace=True)
 2726:     # result.index.astype(int)
 2727: 
 2728:     result.index.name = None
 2729: 
 2730:     tm.assert_frame_equal(result, test_frame1)
 2731: 
 2732: 
 2733: @pytest.mark.parametrize("conn", all_connectable_iris)
 2734: def test_execute_sql(conn, request):
 2735:     conn = request.getfixturevalue(conn)
 2736:     with pandasSQL_builder(conn) as pandasSQL:
 2737:         with pandasSQL.run_transaction():
 2738:             iris_results = pandasSQL.execute("SELECT * FROM iris")
 2739:             row = iris_results.fetchone()
 2740:             iris_results.close()
 2741:     assert list(row) == [5.1, 3.5, 1.4, 0.2, "Iris-setosa"]
 2742: 
 2743: 
 2744: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 2745: def test_sqlalchemy_read_table(conn, request):
 2746:     conn = request.getfixturevalue(conn)
 2747:     iris_frame = sql.read_sql_table("iris", con=conn)
 2748:     check_iris_frame(iris_frame)
 2749: 
 2750: 
 2751: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 2752: def test_sqlalchemy_read_table_columns(conn, request):
 2753:     conn = request.getfixturevalue(conn)
 2754:     iris_frame = sql.read_sql_table(
 2755:         "iris", con=conn, columns=["SepalLength", "SepalLength"]
 2756:     )
 2757:     tm.assert_index_equal(iris_frame.columns, Index(["SepalLength", "SepalLength__1"]))
 2758: 
 2759: 
 2760: @pytest.mark.parametrize("conn", sqlalchemy_connectable_iris)
 2761: def test_read_table_absent_raises(conn, request):
 2762:     conn = request.getfixturevalue(conn)
 2763:     msg = "Table this_doesnt_exist not found"
 2764:     with pytest.raises(ValueError, match=msg):
 2765:         sql.read_sql_table("this_doesnt_exist", con=conn)
 2766: 
 2767: 
 2768: @pytest.mark.parametrize("conn", sqlalchemy_connectable_types)
 2769: def test_sqlalchemy_default_type_conversion(conn, request):
 2770:     conn_name = conn
 2771:     if conn_name == "sqlite_str":
 2772:         pytest.skip("types tables not created in sqlite_str fixture")
 2773:     elif "mysql" in conn_name or "sqlite" in conn_name:
 2774:         request.applymarker(
 2775:             pytest.mark.xfail(reason="boolean dtype not inferred properly")
 2776:         )
 2777: 
 2778:     conn = request.getfixturevalue(conn)
 2779:     df = sql.read_sql_table("types", conn)
 2780: 
 2781:     assert issubclass(df.FloatCol.dtype.type, np.floating)
 2782:     assert issubclass(df.IntCol.dtype.type, np.integer)
 2783:     assert issubclass(df.BoolCol.dtype.type, np.bool_)
 2784: 
 2785:     # Int column with NA values stays as float
 2786:     assert issubclass(df.IntColWithNull.dtype.type, np.floating)
 2787:     # Bool column with NA values becomes object
 2788:     assert issubclass(df.BoolColWithNull.dtype.type, object)
 2789: 
 2790: 
 2791: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2792: def test_bigint(conn, request):
 2793:     # int64 should be converted to BigInteger, GH7433
 2794:     conn = request.getfixturevalue(conn)
 2795:     df = DataFrame(data={"i64": [2**62]})
 2796:     assert df.to_sql(name="test_bigint", con=conn, index=False) == 1
 2797:     result = sql.read_sql_table("test_bigint", conn)
 2798: 
 2799:     tm.assert_frame_equal(df, result)
 2800: 
 2801: 
 2802: @pytest.mark.parametrize("conn", sqlalchemy_connectable_types)
 2803: def test_default_date_load(conn, request):
 2804:     conn_name = conn
 2805:     if conn_name == "sqlite_str":
 2806:         pytest.skip("types tables not created in sqlite_str fixture")
 2807:     elif "sqlite" in conn_name:
 2808:         request.applymarker(
 2809:             pytest.mark.xfail(reason="sqlite does not read date properly")
 2810:         )
 2811: 
 2812:     conn = request.getfixturevalue(conn)
 2813:     df = sql.read_sql_table("types", conn)
 2814: 
 2815:     assert issubclass(df.DateCol.dtype.type, np.datetime64)
 2816: 
 2817: 
 2818: @pytest.mark.parametrize("conn", postgresql_connectable)
 2819: @pytest.mark.parametrize("parse_dates", [None, ["DateColWithTz"]])
 2820: def test_datetime_with_timezone_query(conn, request, parse_dates):
 2821:     # edge case that converts postgresql datetime with time zone types
 2822:     # to datetime64[ns,psycopg2.tz.FixedOffsetTimezone..], which is ok
 2823:     # but should be more natural, so coerce to datetime64[ns] for now
 2824:     conn = request.getfixturevalue(conn)
 2825:     expected = create_and_load_postgres_datetz(conn)
 2826: 
 2827:     # GH11216
 2828:     df = read_sql_query("select * from datetz", conn, parse_dates=parse_dates)
 2829:     col = df.DateColWithTz
 2830:     tm.assert_series_equal(col, expected)
 2831: 
 2832: 
 2833: @pytest.mark.parametrize("conn", postgresql_connectable)
 2834: def test_datetime_with_timezone_query_chunksize(conn, request):
 2835:     conn = request.getfixturevalue(conn)
 2836:     expected = create_and_load_postgres_datetz(conn)
 2837: 
 2838:     df = concat(
 2839:         list(read_sql_query("select * from datetz", conn, chunksize=1)),
 2840:         ignore_index=True,
 2841:     )
 2842:     col = df.DateColWithTz
 2843:     tm.assert_series_equal(col, expected)
 2844: 
 2845: 
 2846: @pytest.mark.parametrize("conn", postgresql_connectable)
 2847: def test_datetime_with_timezone_table(conn, request):
 2848:     conn = request.getfixturevalue(conn)
 2849:     expected = create_and_load_postgres_datetz(conn)
 2850:     result = sql.read_sql_table("datetz", conn)
 2851:     tm.assert_frame_equal(result, expected.to_frame())
 2852: 
 2853: 
 2854: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2855: def test_datetime_with_timezone_roundtrip(conn, request):
 2856:     conn_name = conn
 2857:     conn = request.getfixturevalue(conn)
 2858:     # GH 9086
 2859:     # Write datetimetz data to a db and read it back
 2860:     # For dbs that support timestamps with timezones, should get back UTC
 2861:     # otherwise naive data should be returned
 2862:     expected = DataFrame(
 2863:         {"A": date_range("2013-01-01 09:00:00", periods=3, tz="US/Pacific")}
 2864:     )
 2865:     assert expected.to_sql(name="test_datetime_tz", con=conn, index=False) == 3
 2866: 
 2867:     if "postgresql" in conn_name:
 2868:         # SQLAlchemy "timezones" (i.e. offsets) are coerced to UTC
 2869:         expected["A"] = expected["A"].dt.tz_convert("UTC")
 2870:     else:
 2871:         # Otherwise, timestamps are returned as local, naive
 2872:         expected["A"] = expected["A"].dt.tz_localize(None)
 2873: 
 2874:     result = sql.read_sql_table("test_datetime_tz", conn)
 2875:     tm.assert_frame_equal(result, expected)
 2876: 
 2877:     result = sql.read_sql_query("SELECT * FROM test_datetime_tz", conn)
 2878:     if "sqlite" in conn_name:
 2879:         # read_sql_query does not return datetime type like read_sql_table
 2880:         assert isinstance(result.loc[0, "A"], str)
 2881:         result["A"] = to_datetime(result["A"])
 2882:     tm.assert_frame_equal(result, expected)
 2883: 
 2884: 
 2885: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2886: def test_out_of_bounds_datetime(conn, request):
 2887:     # GH 26761
 2888:     conn = request.getfixturevalue(conn)
 2889:     data = DataFrame({"date": datetime(9999, 1, 1)}, index=[0])
 2890:     assert data.to_sql(name="test_datetime_obb", con=conn, index=False) == 1
 2891:     result = sql.read_sql_table("test_datetime_obb", conn)
 2892:     expected = DataFrame([pd.NaT], columns=["date"])
 2893:     tm.assert_frame_equal(result, expected)
 2894: 
 2895: 
 2896: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2897: def test_naive_datetimeindex_roundtrip(conn, request):
 2898:     # GH 23510
 2899:     # Ensure that a naive DatetimeIndex isn't converted to UTC
 2900:     conn = request.getfixturevalue(conn)
 2901:     dates = date_range("2018-01-01", periods=5, freq="6h")._with_freq(None)
 2902:     expected = DataFrame({"nums": range(5)}, index=dates)
 2903:     assert expected.to_sql(name="foo_table", con=conn, index_label="info_date") == 5
 2904:     result = sql.read_sql_table("foo_table", conn, index_col="info_date")
 2905:     # result index with gain a name from a set_index operation; expected
 2906:     tm.assert_frame_equal(result, expected, check_names=False)
 2907: 
 2908: 
 2909: @pytest.mark.parametrize("conn", sqlalchemy_connectable_types)
 2910: def test_date_parsing(conn, request):
 2911:     # No Parsing
 2912:     conn_name = conn
 2913:     conn = request.getfixturevalue(conn)
 2914:     df = sql.read_sql_table("types", conn)
 2915:     expected_type = object if "sqlite" in conn_name else np.datetime64
 2916:     assert issubclass(df.DateCol.dtype.type, expected_type)
 2917: 
 2918:     df = sql.read_sql_table("types", conn, parse_dates=["DateCol"])
 2919:     assert issubclass(df.DateCol.dtype.type, np.datetime64)
 2920: 
 2921:     df = sql.read_sql_table("types", conn, parse_dates={"DateCol": "%Y-%m-%d %H:%M:%S"})
 2922:     assert issubclass(df.DateCol.dtype.type, np.datetime64)
 2923: 
 2924:     df = sql.read_sql_table(
 2925:         "types",
 2926:         conn,
 2927:         parse_dates={"DateCol": {"format": "%Y-%m-%d %H:%M:%S"}},
 2928:     )
 2929:     assert issubclass(df.DateCol.dtype.type, np.datetime64)
 2930: 
 2931:     df = sql.read_sql_table("types", conn, parse_dates=["IntDateCol"])
 2932:     assert issubclass(df.IntDateCol.dtype.type, np.datetime64)
 2933: 
 2934:     df = sql.read_sql_table("types", conn, parse_dates={"IntDateCol": "s"})
 2935:     assert issubclass(df.IntDateCol.dtype.type, np.datetime64)
 2936: 
 2937:     df = sql.read_sql_table("types", conn, parse_dates={"IntDateCol": {"unit": "s"}})
 2938:     assert issubclass(df.IntDateCol.dtype.type, np.datetime64)
 2939: 
 2940: 
 2941: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2942: def test_datetime(conn, request):
 2943:     conn_name = conn
 2944:     conn = request.getfixturevalue(conn)
 2945:     df = DataFrame(
 2946:         {"A": date_range("2013-01-01 09:00:00", periods=3), "B": np.arange(3.0)}
 2947:     )
 2948:     assert df.to_sql(name="test_datetime", con=conn) == 3
 2949: 
 2950:     # with read_table -> type information from schema used
 2951:     result = sql.read_sql_table("test_datetime", conn)
 2952:     result = result.drop("index", axis=1)
 2953:     tm.assert_frame_equal(result, df)
 2954: 
 2955:     # with read_sql -> no type information -> sqlite has no native
 2956:     result = sql.read_sql_query("SELECT * FROM test_datetime", conn)
 2957:     result = result.drop("index", axis=1)
 2958:     if "sqlite" in conn_name:
 2959:         assert isinstance(result.loc[0, "A"], str)
 2960:         result["A"] = to_datetime(result["A"])
 2961:         tm.assert_frame_equal(result, df)
 2962:     else:
 2963:         tm.assert_frame_equal(result, df)
 2964: 
 2965: 
 2966: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2967: def test_datetime_NaT(conn, request):
 2968:     conn_name = conn
 2969:     conn = request.getfixturevalue(conn)
 2970:     df = DataFrame(
 2971:         {"A": date_range("2013-01-01 09:00:00", periods=3), "B": np.arange(3.0)}
 2972:     )
 2973:     df.loc[1, "A"] = np.nan
 2974:     assert df.to_sql(name="test_datetime", con=conn, index=False) == 3
 2975: 
 2976:     # with read_table -> type information from schema used
 2977:     result = sql.read_sql_table("test_datetime", conn)
 2978:     tm.assert_frame_equal(result, df)
 2979: 
 2980:     # with read_sql -> no type information -> sqlite has no native
 2981:     result = sql.read_sql_query("SELECT * FROM test_datetime", conn)
 2982:     if "sqlite" in conn_name:
 2983:         assert isinstance(result.loc[0, "A"], str)
 2984:         result["A"] = to_datetime(result["A"], errors="coerce")
 2985:         tm.assert_frame_equal(result, df)
 2986:     else:
 2987:         tm.assert_frame_equal(result, df)
 2988: 
 2989: 
 2990: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 2991: def test_datetime_date(conn, request):
 2992:     # test support for datetime.date
 2993:     conn = request.getfixturevalue(conn)
 2994:     df = DataFrame([date(2014, 1, 1), date(2014, 1, 2)], columns=["a"])
 2995:     assert df.to_sql(name="test_date", con=conn, index=False) == 2
 2996:     res = read_sql_table("test_date", conn)
 2997:     result = res["a"]
 2998:     expected = to_datetime(df["a"])
 2999:     # comes back as datetime64
 3000:     tm.assert_series_equal(result, expected)
 3001: 
 3002: 
 3003: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3004: def test_datetime_time(conn, request, sqlite_buildin):
 3005:     # test support for datetime.time
 3006:     conn_name = conn
 3007:     conn = request.getfixturevalue(conn)
 3008:     df = DataFrame([time(9, 0, 0), time(9, 1, 30)], columns=["a"])
 3009:     assert df.to_sql(name="test_time", con=conn, index=False) == 2
 3010:     res = read_sql_table("test_time", conn)
 3011:     tm.assert_frame_equal(res, df)
 3012: 
 3013:     # GH8341
 3014:     # first, use the fallback to have the sqlite adapter put in place
 3015:     sqlite_conn = sqlite_buildin
 3016:     assert sql.to_sql(df, "test_time2", sqlite_conn, index=False) == 2
 3017:     res = sql.read_sql_query("SELECT * FROM test_time2", sqlite_conn)
 3018:     ref = df.map(lambda _: _.strftime("%H:%M:%S.%f"))
 3019:     tm.assert_frame_equal(ref, res)  # check if adapter is in place
 3020:     # then test if sqlalchemy is unaffected by the sqlite adapter
 3021:     assert sql.to_sql(df, "test_time3", conn, index=False) == 2
 3022:     if "sqlite" in conn_name:
 3023:         res = sql.read_sql_query("SELECT * FROM test_time3", conn)
 3024:         ref = df.map(lambda _: _.strftime("%H:%M:%S.%f"))
 3025:         tm.assert_frame_equal(ref, res)
 3026:     res = sql.read_sql_table("test_time3", conn)
 3027:     tm.assert_frame_equal(df, res)
 3028: 
 3029: 
 3030: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3031: def test_mixed_dtype_insert(conn, request):
 3032:     # see GH6509
 3033:     conn = request.getfixturevalue(conn)
 3034:     s1 = Series(2**25 + 1, dtype=np.int32)
 3035:     s2 = Series(0.0, dtype=np.float32)
 3036:     df = DataFrame({"s1": s1, "s2": s2})
 3037: 
 3038:     # write and read again
 3039:     assert df.to_sql(name="test_read_write", con=conn, index=False) == 1
 3040:     df2 = sql.read_sql_table("test_read_write", conn)
 3041: 
 3042:     tm.assert_frame_equal(df, df2, check_dtype=False, check_exact=True)
 3043: 
 3044: 
 3045: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3046: def test_nan_numeric(conn, request):
 3047:     # NaNs in numeric float column
 3048:     conn = request.getfixturevalue(conn)
 3049:     df = DataFrame({"A": [0, 1, 2], "B": [0.2, np.nan, 5.6]})
 3050:     assert df.to_sql(name="test_nan", con=conn, index=False) == 3
 3051: 
 3052:     # with read_table
 3053:     result = sql.read_sql_table("test_nan", conn)
 3054:     tm.assert_frame_equal(result, df)
 3055: 
 3056:     # with read_sql
 3057:     result = sql.read_sql_query("SELECT * FROM test_nan", conn)
 3058:     tm.assert_frame_equal(result, df)
 3059: 
 3060: 
 3061: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3062: def test_nan_fullcolumn(conn, request):
 3063:     # full NaN column (numeric float column)
 3064:     conn = request.getfixturevalue(conn)
 3065:     df = DataFrame({"A": [0, 1, 2], "B": [np.nan, np.nan, np.nan]})
 3066:     assert df.to_sql(name="test_nan", con=conn, index=False) == 3
 3067: 
 3068:     # with read_table
 3069:     result = sql.read_sql_table("test_nan", conn)
 3070:     tm.assert_frame_equal(result, df)
 3071: 
 3072:     # with read_sql -> not type info from table -> stays None
 3073:     df["B"] = df["B"].astype("object")
 3074:     df["B"] = None
 3075:     result = sql.read_sql_query("SELECT * FROM test_nan", conn)
 3076:     tm.assert_frame_equal(result, df)
 3077: 
 3078: 
 3079: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3080: def test_nan_string(conn, request):
 3081:     # NaNs in string column
 3082:     conn = request.getfixturevalue(conn)
 3083:     df = DataFrame({"A": [0, 1, 2], "B": ["a", "b", np.nan]})
 3084:     assert df.to_sql(name="test_nan", con=conn, index=False) == 3
 3085: 
 3086:     # NaNs are coming back as None
 3087:     df.loc[2, "B"] = None
 3088: 
 3089:     # with read_table
 3090:     result = sql.read_sql_table("test_nan", conn)
 3091:     tm.assert_frame_equal(result, df)
 3092: 
 3093:     # with read_sql
 3094:     result = sql.read_sql_query("SELECT * FROM test_nan", conn)
 3095:     tm.assert_frame_equal(result, df)
 3096: 
 3097: 
 3098: @pytest.mark.parametrize("conn", all_connectable)
 3099: def test_to_sql_save_index(conn, request):
 3100:     if "adbc" in conn:
 3101:         request.node.add_marker(
 3102:             pytest.mark.xfail(
 3103:                 reason="ADBC implementation does not create index", strict=True
 3104:             )
 3105:         )
 3106:     conn_name = conn
 3107:     conn = request.getfixturevalue(conn)
 3108:     df = DataFrame.from_records(
 3109:         [(1, 2.1, "line1"), (2, 1.5, "line2")], columns=["A", "B", "C"], index=["A"]
 3110:     )
 3111: 
 3112:     tbl_name = "test_to_sql_saves_index"
 3113:     with pandasSQL_builder(conn) as pandasSQL:
 3114:         with pandasSQL.run_transaction():
 3115:             assert pandasSQL.to_sql(df, tbl_name) == 2
 3116: 
 3117:     if conn_name in {"sqlite_buildin", "sqlite_str"}:
 3118:         ixs = sql.read_sql_query(
 3119:             "SELECT * FROM sqlite_master WHERE type = 'index' "
 3120:             f"AND tbl_name = '{tbl_name}'",
 3121:             conn,
 3122:         )
 3123:         ix_cols = []
 3124:         for ix_name in ixs.name:
 3125:             ix_info = sql.read_sql_query(f"PRAGMA index_info({ix_name})", conn)
 3126:             ix_cols.append(ix_info.name.tolist())
 3127:     else:
 3128:         from sqlalchemy import inspect
 3129: 
 3130:         insp = inspect(conn)
 3131: 
 3132:         ixs = insp.get_indexes(tbl_name)
 3133:         ix_cols = [i["column_names"] for i in ixs]
 3134: 
 3135:     assert ix_cols == [["A"]]
 3136: 
 3137: 
 3138: @pytest.mark.parametrize("conn", all_connectable)
 3139: def test_transactions(conn, request):
 3140:     conn_name = conn
 3141:     conn = request.getfixturevalue(conn)
 3142: 
 3143:     stmt = "CREATE TABLE test_trans (A INT, B TEXT)"
 3144:     if conn_name != "sqlite_buildin" and "adbc" not in conn_name:
 3145:         from sqlalchemy import text
 3146: 
 3147:         stmt = text(stmt)
 3148: 
 3149:     with pandasSQL_builder(conn) as pandasSQL:
 3150:         with pandasSQL.run_transaction() as trans:
 3151:             trans.execute(stmt)
 3152: 
 3153: 
 3154: @pytest.mark.parametrize("conn", all_connectable)
 3155: def test_transaction_rollback(conn, request):
 3156:     conn_name = conn
 3157:     conn = request.getfixturevalue(conn)
 3158:     with pandasSQL_builder(conn) as pandasSQL:
 3159:         with pandasSQL.run_transaction() as trans:
 3160:             stmt = "CREATE TABLE test_trans (A INT, B TEXT)"
 3161:             if "adbc" in conn_name or isinstance(pandasSQL, SQLiteDatabase):
 3162:                 trans.execute(stmt)
 3163:             else:
 3164:                 from sqlalchemy import text
 3165: 
 3166:                 stmt = text(stmt)
 3167:                 trans.execute(stmt)
 3168: 
 3169:         class DummyException(Exception):
 3170:             pass
 3171: 
 3172:         # Make sure when transaction is rolled back, no rows get inserted
 3173:         ins_sql = "INSERT INTO test_trans (A,B) VALUES (1, 'blah')"
 3174:         if isinstance(pandasSQL, SQLDatabase):
 3175:             from sqlalchemy import text
 3176: 
 3177:             ins_sql = text(ins_sql)
 3178:         try:
 3179:             with pandasSQL.run_transaction() as trans:
 3180:                 trans.execute(ins_sql)
 3181:                 raise DummyException("error")
 3182:         except DummyException:
 3183:             # ignore raised exception
 3184:             pass
 3185:         with pandasSQL.run_transaction():
 3186:             res = pandasSQL.read_query("SELECT * FROM test_trans")
 3187:         assert len(res) == 0
 3188: 
 3189:         # Make sure when transaction is committed, rows do get inserted
 3190:         with pandasSQL.run_transaction() as trans:
 3191:             trans.execute(ins_sql)
 3192:             res2 = pandasSQL.read_query("SELECT * FROM test_trans")
 3193:         assert len(res2) == 1
 3194: 
 3195: 
 3196: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3197: def test_get_schema_create_table(conn, request, test_frame3):
 3198:     # Use a dataframe without a bool column, since MySQL converts bool to
 3199:     # TINYINT (which read_sql_table returns as an int and causes a dtype
 3200:     # mismatch)
 3201:     if conn == "sqlite_str":
 3202:         request.applymarker(
 3203:             pytest.mark.xfail(reason="test does not support sqlite_str fixture")
 3204:         )
 3205: 
 3206:     conn = request.getfixturevalue(conn)
 3207: 
 3208:     from sqlalchemy import text
 3209:     from sqlalchemy.engine import Engine
 3210: 
 3211:     tbl = "test_get_schema_create_table"
 3212:     create_sql = sql.get_schema(test_frame3, tbl, con=conn)
 3213:     blank_test_df = test_frame3.iloc[:0]
 3214: 
 3215:     create_sql = text(create_sql)
 3216:     if isinstance(conn, Engine):
 3217:         with conn.connect() as newcon:
 3218:             with newcon.begin():
 3219:                 newcon.execute(create_sql)
 3220:     else:
 3221:         conn.execute(create_sql)
 3222:     returned_df = sql.read_sql_table(tbl, conn)
 3223:     tm.assert_frame_equal(returned_df, blank_test_df, check_index_type=False)
 3224: 
 3225: 
 3226: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3227: def test_dtype(conn, request):
 3228:     if conn == "sqlite_str":
 3229:         pytest.skip("sqlite_str has no inspection system")
 3230: 
 3231:     conn = request.getfixturevalue(conn)
 3232: 
 3233:     from sqlalchemy import (
 3234:         TEXT,
 3235:         String,
 3236:     )
 3237:     from sqlalchemy.schema import MetaData
 3238: 
 3239:     cols = ["A", "B"]
 3240:     data = [(0.8, True), (0.9, None)]
 3241:     df = DataFrame(data, columns=cols)
 3242:     assert df.to_sql(name="dtype_test", con=conn) == 2
 3243:     assert df.to_sql(name="dtype_test2", con=conn, dtype={"B": TEXT}) == 2
 3244:     meta = MetaData()
 3245:     meta.reflect(bind=conn)
 3246:     sqltype = meta.tables["dtype_test2"].columns["B"].type
 3247:     assert isinstance(sqltype, TEXT)
 3248:     msg = "The type of B is not a SQLAlchemy type"
 3249:     with pytest.raises(ValueError, match=msg):
 3250:         df.to_sql(name="error", con=conn, dtype={"B": str})
 3251: 
 3252:     # GH9083
 3253:     assert df.to_sql(name="dtype_test3", con=conn, dtype={"B": String(10)}) == 2
 3254:     meta.reflect(bind=conn)
 3255:     sqltype = meta.tables["dtype_test3"].columns["B"].type
 3256:     assert isinstance(sqltype, String)
 3257:     assert sqltype.length == 10
 3258: 
 3259:     # single dtype
 3260:     assert df.to_sql(name="single_dtype_test", con=conn, dtype=TEXT) == 2
 3261:     meta.reflect(bind=conn)
 3262:     sqltypea = meta.tables["single_dtype_test"].columns["A"].type
 3263:     sqltypeb = meta.tables["single_dtype_test"].columns["B"].type
 3264:     assert isinstance(sqltypea, TEXT)
 3265:     assert isinstance(sqltypeb, TEXT)
 3266: 
 3267: 
 3268: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3269: def test_notna_dtype(conn, request):
 3270:     if conn == "sqlite_str":
 3271:         pytest.skip("sqlite_str has no inspection system")
 3272: 
 3273:     conn_name = conn
 3274:     conn = request.getfixturevalue(conn)
 3275: 
 3276:     from sqlalchemy import (
 3277:         Boolean,
 3278:         DateTime,
 3279:         Float,
 3280:         Integer,
 3281:     )
 3282:     from sqlalchemy.schema import MetaData
 3283: 
 3284:     cols = {
 3285:         "Bool": Series([True, None]),
 3286:         "Date": Series([datetime(2012, 5, 1), None]),
 3287:         "Int": Series([1, None], dtype="object"),
 3288:         "Float": Series([1.1, None]),
 3289:     }
 3290:     df = DataFrame(cols)
 3291: 
 3292:     tbl = "notna_dtype_test"
 3293:     assert df.to_sql(name=tbl, con=conn) == 2
 3294:     _ = sql.read_sql_table(tbl, conn)
 3295:     meta = MetaData()
 3296:     meta.reflect(bind=conn)
 3297:     my_type = Integer if "mysql" in conn_name else Boolean
 3298:     col_dict = meta.tables[tbl].columns
 3299:     assert isinstance(col_dict["Bool"].type, my_type)
 3300:     assert isinstance(col_dict["Date"].type, DateTime)
 3301:     assert isinstance(col_dict["Int"].type, Integer)
 3302:     assert isinstance(col_dict["Float"].type, Float)
 3303: 
 3304: 
 3305: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3306: def test_double_precision(conn, request):
 3307:     if conn == "sqlite_str":
 3308:         pytest.skip("sqlite_str has no inspection system")
 3309: 
 3310:     conn = request.getfixturevalue(conn)
 3311: 
 3312:     from sqlalchemy import (
 3313:         BigInteger,
 3314:         Float,
 3315:         Integer,
 3316:     )
 3317:     from sqlalchemy.schema import MetaData
 3318: 
 3319:     V = 1.23456789101112131415
 3320: 
 3321:     df = DataFrame(
 3322:         {
 3323:             "f32": Series([V], dtype="float32"),
 3324:             "f64": Series([V], dtype="float64"),
 3325:             "f64_as_f32": Series([V], dtype="float64"),
 3326:             "i32": Series([5], dtype="int32"),
 3327:             "i64": Series([5], dtype="int64"),
 3328:         }
 3329:     )
 3330: 
 3331:     assert (
 3332:         df.to_sql(
 3333:             name="test_dtypes",
 3334:             con=conn,
 3335:             index=False,
 3336:             if_exists="replace",
 3337:             dtype={"f64_as_f32": Float(precision=23)},
 3338:         )
 3339:         == 1
 3340:     )
 3341:     res = sql.read_sql_table("test_dtypes", conn)
 3342: 
 3343:     # check precision of float64
 3344:     assert np.round(df["f64"].iloc[0], 14) == np.round(res["f64"].iloc[0], 14)
 3345: 
 3346:     # check sql types
 3347:     meta = MetaData()
 3348:     meta.reflect(bind=conn)
 3349:     col_dict = meta.tables["test_dtypes"].columns
 3350:     assert str(col_dict["f32"].type) == str(col_dict["f64_as_f32"].type)
 3351:     assert isinstance(col_dict["f32"].type, Float)
 3352:     assert isinstance(col_dict["f64"].type, Float)
 3353:     assert isinstance(col_dict["i32"].type, Integer)
 3354:     assert isinstance(col_dict["i64"].type, BigInteger)
 3355: 
 3356: 
 3357: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3358: def test_connectable_issue_example(conn, request):
 3359:     conn = request.getfixturevalue(conn)
 3360: 
 3361:     # This tests the example raised in issue
 3362:     # https://github.com/pandas-dev/pandas/issues/10104
 3363:     from sqlalchemy.engine import Engine
 3364: 
 3365:     def test_select(connection):
 3366:         query = "SELECT test_foo_data FROM test_foo_data"
 3367:         return sql.read_sql_query(query, con=connection)
 3368: 
 3369:     def test_append(connection, data):
 3370:         data.to_sql(name="test_foo_data", con=connection, if_exists="append")
 3371: 
 3372:     def test_connectable(conn):
 3373:         # https://github.com/sqlalchemy/sqlalchemy/commit/
 3374:         # 00b5c10846e800304caa86549ab9da373b42fa5d#r48323973
 3375:         foo_data = test_select(conn)
 3376:         test_append(conn, foo_data)
 3377: 
 3378:     def main(connectable):
 3379:         if isinstance(connectable, Engine):
 3380:             with connectable.connect() as conn:
 3381:                 with conn.begin():
 3382:                     test_connectable(conn)
 3383:         else:
 3384:             test_connectable(connectable)
 3385: 
 3386:     assert (
 3387:         DataFrame({"test_foo_data": [0, 1, 2]}).to_sql(name="test_foo_data", con=conn)
 3388:         == 3
 3389:     )
 3390:     main(conn)
 3391: 
 3392: 
 3393: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3394: @pytest.mark.parametrize(
 3395:     "input",
 3396:     [{"foo": [np.inf]}, {"foo": [-np.inf]}, {"foo": [-np.inf], "infe0": ["bar"]}],
 3397: )
 3398: def test_to_sql_with_negative_npinf(conn, request, input):
 3399:     # GH 34431
 3400: 
 3401:     df = DataFrame(input)
 3402:     conn_name = conn
 3403:     conn = request.getfixturevalue(conn)
 3404: 
 3405:     if "mysql" in conn_name:
 3406:         # GH 36465
 3407:         # The input {"foo": [-np.inf], "infe0": ["bar"]} does not raise any error
 3408:         # for pymysql version >= 0.10
 3409:         # TODO(GH#36465): remove this version check after GH 36465 is fixed
 3410:         pymysql = pytest.importorskip("pymysql")
 3411: 
 3412:         if Version(pymysql.__version__) < Version("1.0.3") and "infe0" in df.columns:
 3413:             mark = pytest.mark.xfail(reason="GH 36465")
 3414:             request.applymarker(mark)
 3415: 
 3416:         msg = "inf cannot be used with MySQL"
 3417:         with pytest.raises(ValueError, match=msg):
 3418:             df.to_sql(name="foobar", con=conn, index=False)
 3419:     else:
 3420:         assert df.to_sql(name="foobar", con=conn, index=False) == 1
 3421:         res = sql.read_sql_table("foobar", conn)
 3422:         tm.assert_equal(df, res)
 3423: 
 3424: 
 3425: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3426: def test_temporary_table(conn, request):
 3427:     if conn == "sqlite_str":
 3428:         pytest.skip("test does not work with str connection")
 3429: 
 3430:     conn = request.getfixturevalue(conn)
 3431: 
 3432:     from sqlalchemy import (
 3433:         Column,
 3434:         Integer,
 3435:         Unicode,
 3436:         select,
 3437:     )
 3438:     from sqlalchemy.orm import (
 3439:         Session,
 3440:         declarative_base,
 3441:     )
 3442: 
 3443:     test_data = "Hello, World!"
 3444:     expected = DataFrame({"spam": [test_data]})
 3445:     Base = declarative_base()
 3446: 
 3447:     class Temporary(Base):
 3448:         __tablename__ = "temp_test"
 3449:         __table_args__ = {"prefixes": ["TEMPORARY"]}
 3450:         id = Column(Integer, primary_key=True)
 3451:         spam = Column(Unicode(30), nullable=False)
 3452: 
 3453:     with Session(conn) as session:
 3454:         with session.begin():
 3455:             conn = session.connection()
 3456:             Temporary.__table__.create(conn)
 3457:             session.add(Temporary(spam=test_data))
 3458:             session.flush()
 3459:             df = sql.read_sql_query(sql=select(Temporary.spam), con=conn)
 3460:     tm.assert_frame_equal(df, expected)
 3461: 
 3462: 
 3463: @pytest.mark.parametrize("conn", all_connectable)
 3464: def test_invalid_engine(conn, request, test_frame1):
 3465:     if conn == "sqlite_buildin" or "adbc" in conn:
 3466:         request.applymarker(
 3467:             pytest.mark.xfail(
 3468:                 reason="SQLiteDatabase/ADBCDatabase does not raise for bad engine"
 3469:             )
 3470:         )
 3471: 
 3472:     conn = request.getfixturevalue(conn)
 3473:     msg = "engine must be one of 'auto', 'sqlalchemy'"
 3474:     with pandasSQL_builder(conn) as pandasSQL:
 3475:         with pytest.raises(ValueError, match=msg):
 3476:             pandasSQL.to_sql(test_frame1, "test_frame1", engine="bad_engine")
 3477: 
 3478: 
 3479: @pytest.mark.parametrize("conn", all_connectable)
 3480: def test_to_sql_with_sql_engine(conn, request, test_frame1):
 3481:     """`to_sql` with the `engine` param"""
 3482:     # mostly copied from this class's `_to_sql()` method
 3483:     conn = request.getfixturevalue(conn)
 3484:     with pandasSQL_builder(conn) as pandasSQL:
 3485:         with pandasSQL.run_transaction():
 3486:             assert pandasSQL.to_sql(test_frame1, "test_frame1", engine="auto") == 4
 3487:             assert pandasSQL.has_table("test_frame1")
 3488: 
 3489:     num_entries = len(test_frame1)
 3490:     num_rows = count_rows(conn, "test_frame1")
 3491:     assert num_rows == num_entries
 3492: 
 3493: 
 3494: @pytest.mark.parametrize("conn", sqlalchemy_connectable)
 3495: def test_options_sqlalchemy(conn, request, test_frame1):
 3496:     # use the set option
 3497:     conn = request.getfixturevalue(conn)
 3498:     with pd.option_context("io.sql.engine", "sqlalchemy"):
 3499:         with pandasSQL_builder(conn) as pandasSQL:
 3500:             with pandasSQL.run_transaction():
 3501:                 assert pandasSQL.to_sql(test_frame1, "test_frame1") == 4
 3502:                 assert pandasSQL.has_table("test_frame1")
 3503: 
 3504:         num_entries = len(test_frame1)
 3505:         num_rows = count_rows(conn, "test_frame1")
 3506:         assert num_rows == num_entries
 3507: 
 3508: 
 3509: @pytest.mark.parametrize("conn", all_connectable)
 3510: def test_options_auto(conn, request, test_frame1):
 3511:     # use the set option
 3512:     conn = request.getfixturevalue(conn)
 3513:     with pd.option_context("io.sql.engine", "auto"):
 3514:         with pandasSQL_builder(conn) as pandasSQL:
 3515:             with pandasSQL.run_transaction():
 3516:                 assert pandasSQL.to_sql(test_frame1, "test_frame1") == 4
 3517:                 assert pandasSQL.has_table("test_frame1")
 3518: 
 3519:         num_entries = len(test_frame1)
 3520:         num_rows = count_rows(conn, "test_frame1")
 3521:         assert num_rows == num_entries
 3522: 
 3523: 
 3524: def test_options_get_engine():
 3525:     pytest.importorskip("sqlalchemy")
 3526:     assert isinstance(get_engine("sqlalchemy"), SQLAlchemyEngine)
 3527: 
 3528:     with pd.option_context("io.sql.engine", "sqlalchemy"):
 3529:         assert isinstance(get_engine("auto"), SQLAlchemyEngine)
 3530:         assert isinstance(get_engine("sqlalchemy"), SQLAlchemyEngine)
 3531: 
 3532:     with pd.option_context("io.sql.engine", "auto"):
 3533:         assert isinstance(get_engine("auto"), SQLAlchemyEngine)
 3534:         assert isinstance(get_engine("sqlalchemy"), SQLAlchemyEngine)
 3535: 
 3536: 
 3537: def test_get_engine_auto_error_message():
 3538:     # Expect different error messages from get_engine(engine="auto")
 3539:     # if engines aren't installed vs. are installed but bad version
 3540:     pass
 3541:     # TODO(GH#36893) fill this in when we add more engines
 3542: 
 3543: 
 3544: @pytest.mark.parametrize("conn", all_connectable)
 3545: @pytest.mark.parametrize("func", ["read_sql", "read_sql_query"])
 3546: def test_read_sql_dtype_backend(
 3547:     conn,
 3548:     request,
 3549:     string_storage,
 3550:     func,
 3551:     dtype_backend,
 3552:     dtype_backend_data,
 3553:     dtype_backend_expected,
 3554: ):
 3555:     # GH#50048
 3556:     conn_name = conn
 3557:     conn = request.getfixturevalue(conn)
 3558:     table = "test"
 3559:     df = dtype_backend_data
 3560:     df.to_sql(name=table, con=conn, index=False, if_exists="replace")
 3561: 
 3562:     with pd.option_context("mode.string_storage", string_storage):
 3563:         result = getattr(pd, func)(
 3564:             f"Select * from {table}", conn, dtype_backend=dtype_backend
 3565:         )
 3566:     expected = dtype_backend_expected(string_storage, dtype_backend, conn_name)
 3567:     tm.assert_frame_equal(result, expected)
 3568: 
 3569:     if "adbc" in conn_name:
 3570:         # adbc does not support chunksize argument
 3571:         request.applymarker(
 3572:             pytest.mark.xfail(reason="adbc does not support chunksize argument")
 3573:         )
 3574: 
 3575:     with pd.option_context("mode.string_storage", string_storage):
 3576:         iterator = getattr(pd, func)(
 3577:             f"Select * from {table}",
 3578:             con=conn,
 3579:             dtype_backend=dtype_backend,
 3580:             chunksize=3,
 3581:         )
 3582:         expected = dtype_backend_expected(string_storage, dtype_backend, conn_name)
 3583:         for result in iterator:
 3584:             tm.assert_frame_equal(result, expected)
 3585: 
 3586: 
 3587: @pytest.mark.parametrize("conn", all_connectable)
 3588: @pytest.mark.parametrize("func", ["read_sql", "read_sql_table"])
 3589: def test_read_sql_dtype_backend_table(
 3590:     conn,
 3591:     request,
 3592:     string_storage,
 3593:     func,
 3594:     dtype_backend,
 3595:     dtype_backend_data,
 3596:     dtype_backend_expected,
 3597: ):
 3598:     if "sqlite" in conn and "adbc" not in conn:
 3599:         request.applymarker(
 3600:             pytest.mark.xfail(
 3601:                 reason=(
 3602:                     "SQLite actually returns proper boolean values via "
 3603:                     "read_sql_table, but before pytest refactor was skipped"
 3604:                 )
 3605:             )
 3606:         )
 3607:     # GH#50048
 3608:     conn_name = conn
 3609:     conn = request.getfixturevalue(conn)
 3610:     table = "test"
 3611:     df = dtype_backend_data
 3612:     df.to_sql(name=table, con=conn, index=False, if_exists="replace")
 3613: 
 3614:     with pd.option_context("mode.string_storage", string_storage):
 3615:         result = getattr(pd, func)(table, conn, dtype_backend=dtype_backend)
 3616:     expected = dtype_backend_expected(string_storage, dtype_backend, conn_name)
 3617:     tm.assert_frame_equal(result, expected)
 3618: 
 3619:     if "adbc" in conn_name:
 3620:         # adbc does not support chunksize argument
 3621:         return
 3622: 
 3623:     with pd.option_context("mode.string_storage", string_storage):
 3624:         iterator = getattr(pd, func)(
 3625:             table,
 3626:             conn,
 3627:             dtype_backend=dtype_backend,
 3628:             chunksize=3,
 3629:         )
 3630:         expected = dtype_backend_expected(string_storage, dtype_backend, conn_name)
 3631:         for result in iterator:
 3632:             tm.assert_frame_equal(result, expected)
 3633: 
 3634: 
 3635: @pytest.mark.parametrize("conn", all_connectable)
 3636: @pytest.mark.parametrize("func", ["read_sql", "read_sql_table", "read_sql_query"])
 3637: def test_read_sql_invalid_dtype_backend_table(conn, request, func, dtype_backend_data):
 3638:     conn = request.getfixturevalue(conn)
 3639:     table = "test"
 3640:     df = dtype_backend_data
 3641:     df.to_sql(name=table, con=conn, index=False, if_exists="replace")
 3642: 
 3643:     msg = (
 3644:         "dtype_backend numpy is invalid, only 'numpy_nullable' and "
 3645:         "'pyarrow' are allowed."
 3646:     )
 3647:     with pytest.raises(ValueError, match=msg):
 3648:         getattr(pd, func)(table, conn, dtype_backend="numpy")
 3649: 
 3650: 
 3651: @pytest.fixture
 3652: def dtype_backend_data() -> DataFrame:
 3653:     return DataFrame(
 3654:         {
 3655:             "a": Series([1, np.nan, 3], dtype="Int64"),
 3656:             "b": Series([1, 2, 3], dtype="Int64"),
 3657:             "c": Series([1.5, np.nan, 2.5], dtype="Float64"),
 3658:             "d": Series([1.5, 2.0, 2.5], dtype="Float64"),
 3659:             "e": [True, False, None],
 3660:             "f": [True, False, True],
 3661:             "g": ["a", "b", "c"],
 3662:             "h": ["a", "b", None],
 3663:         }
 3664:     )
 3665: 
 3666: 
 3667: @pytest.fixture
 3668: def dtype_backend_expected():
 3669:     def func(storage, dtype_backend, conn_name) -> DataFrame:
 3670:         string_array: StringArray | ArrowStringArray
 3671:         string_array_na: StringArray | ArrowStringArray
 3672:         if storage == "python":
 3673:             string_array = StringArray(np.array(["a", "b", "c"], dtype=np.object_))
 3674:             string_array_na = StringArray(np.array(["a", "b", pd.NA], dtype=np.object_))
 3675: 
 3676:         elif dtype_backend == "pyarrow":
 3677:             pa = pytest.importorskip("pyarrow")
 3678:             from pandas.arrays import ArrowExtensionArray
 3679: 
 3680:             string_array = ArrowExtensionArray(pa.array(["a", "b", "c"]))  # type: ignore[assignment]
 3681:             string_array_na = ArrowExtensionArray(pa.array(["a", "b", None]))  # type: ignore[assignment]
 3682: 
 3683:         else:
 3684:             pa = pytest.importorskip("pyarrow")
 3685:             string_array = ArrowStringArray(pa.array(["a", "b", "c"]))
 3686:             string_array_na = ArrowStringArray(pa.array(["a", "b", None]))
 3687: 
 3688:         df = DataFrame(
 3689:             {
 3690:                 "a": Series([1, np.nan, 3], dtype="Int64"),
 3691:                 "b": Series([1, 2, 3], dtype="Int64"),
 3692:                 "c": Series([1.5, np.nan, 2.5], dtype="Float64"),
 3693:                 "d": Series([1.5, 2.0, 2.5], dtype="Float64"),
 3694:                 "e": Series([True, False, pd.NA], dtype="boolean"),
 3695:                 "f": Series([True, False, True], dtype="boolean"),
 3696:                 "g": string_array,
 3697:                 "h": string_array_na,
 3698:             }
 3699:         )
 3700:         if dtype_backend == "pyarrow":
 3701:             pa = pytest.importorskip("pyarrow")
 3702: 
 3703:             from pandas.arrays import ArrowExtensionArray
 3704: 
 3705:             df = DataFrame(
 3706:                 {
 3707:                     col: ArrowExtensionArray(pa.array(df[col], from_pandas=True))
 3708:                     for col in df.columns
 3709:                 }
 3710:             )
 3711: 
 3712:         if "mysql" in conn_name or "sqlite" in conn_name:
 3713:             if dtype_backend == "numpy_nullable":
 3714:                 df = df.astype({"e": "Int64", "f": "Int64"})
 3715:             else:
 3716:                 df = df.astype({"e": "int64[pyarrow]", "f": "int64[pyarrow]"})
 3717: 
 3718:         return df
 3719: 
 3720:     return func
 3721: 
 3722: 
 3723: @pytest.mark.parametrize("conn", all_connectable)
 3724: def test_chunksize_empty_dtypes(conn, request):
 3725:     # GH#50245
 3726:     if "adbc" in conn:
 3727:         request.node.add_marker(
 3728:             pytest.mark.xfail(reason="chunksize argument NotImplemented with ADBC")
 3729:         )
 3730:     conn = request.getfixturevalue(conn)
 3731:     dtypes = {"a": "int64", "b": "object"}
 3732:     df = DataFrame(columns=["a", "b"]).astype(dtypes)
 3733:     expected = df.copy()
 3734:     df.to_sql(name="test", con=conn, index=False, if_exists="replace")
 3735: 
 3736:     for result in read_sql_query(
 3737:         "SELECT * FROM test",
 3738:         conn,
 3739:         dtype=dtypes,
 3740:         chunksize=1,
 3741:     ):
 3742:         tm.assert_frame_equal(result, expected)
 3743: 
 3744: 
 3745: @pytest.mark.parametrize("conn", all_connectable)
 3746: @pytest.mark.parametrize("dtype_backend", [lib.no_default, "numpy_nullable"])
 3747: @pytest.mark.parametrize("func", ["read_sql", "read_sql_query"])
 3748: def test_read_sql_dtype(conn, request, func, dtype_backend):
 3749:     # GH#50797
 3750:     conn = request.getfixturevalue(conn)
 3751:     table = "test"
 3752:     df = DataFrame({"a": [1, 2, 3], "b": 5})
 3753:     df.to_sql(name=table, con=conn, index=False, if_exists="replace")
 3754: 
 3755:     result = getattr(pd, func)(
 3756:         f"Select * from {table}",
 3757:         conn,
 3758:         dtype={"a": np.float64},
 3759:         dtype_backend=dtype_backend,
 3760:     )
 3761:     expected = DataFrame(
 3762:         {
 3763:             "a": Series([1, 2, 3], dtype=np.float64),
 3764:             "b": Series(
 3765:                 [5, 5, 5],
 3766:                 dtype="int64" if not dtype_backend == "numpy_nullable" else "Int64",
 3767:             ),
 3768:         }
 3769:     )
 3770:     tm.assert_frame_equal(result, expected)
 3771: 
 3772: 
 3773: def test_keyword_deprecation(sqlite_engine):
 3774:     conn = sqlite_engine
 3775:     # GH 54397
 3776:     msg = (
 3777:         "Starting with pandas version 3.0 all arguments of to_sql except for the "
 3778:         "arguments 'name' and 'con' will be keyword-only."
 3779:     )
 3780:     df = DataFrame([{"A": 1, "B": 2, "C": 3}, {"A": 1, "B": 2, "C": 3}])
 3781:     df.to_sql("example", conn)
 3782: 
 3783:     with tm.assert_produces_warning(FutureWarning, match=msg):
 3784:         df.to_sql("example", conn, None, if_exists="replace")
 3785: 
 3786: 
 3787: def test_bigint_warning(sqlite_engine):
 3788:     conn = sqlite_engine
 3789:     # test no warning for BIGINT (to support int64) is raised (GH7433)
 3790:     df = DataFrame({"a": [1, 2]}, dtype="int64")
 3791:     assert df.to_sql(name="test_bigintwarning", con=conn, index=False) == 2
 3792: 
 3793:     with tm.assert_produces_warning(None):
 3794:         sql.read_sql_table("test_bigintwarning", conn)
 3795: 
 3796: 
 3797: def test_valueerror_exception(sqlite_engine):
 3798:     conn = sqlite_engine
 3799:     df = DataFrame({"col1": [1, 2], "col2": [3, 4]})
 3800:     with pytest.raises(ValueError, match="Empty table name specified"):
 3801:         df.to_sql(name="", con=conn, if_exists="replace", index=False)
 3802: 
 3803: 
 3804: def test_row_object_is_named_tuple(sqlite_engine):
 3805:     conn = sqlite_engine
 3806:     # GH 40682
 3807:     # Test for the is_named_tuple() function
 3808:     # Placed here due to its usage of sqlalchemy
 3809: 
 3810:     from sqlalchemy import (
 3811:         Column,
 3812:         Integer,
 3813:         String,
 3814:     )
 3815:     from sqlalchemy.orm import (
 3816:         declarative_base,
 3817:         sessionmaker,
 3818:     )
 3819: 
 3820:     BaseModel = declarative_base()
 3821: 
 3822:     class Test(BaseModel):
 3823:         __tablename__ = "test_frame"
 3824:         id = Column(Integer, primary_key=True)
 3825:         string_column = Column(String(50))
 3826: 
 3827:     with conn.begin():
 3828:         BaseModel.metadata.create_all(conn)
 3829:     Session = sessionmaker(bind=conn)
 3830:     with Session() as session:
 3831:         df = DataFrame({"id": [0, 1], "string_column": ["hello", "world"]})
 3832:         assert (
 3833:             df.to_sql(name="test_frame", con=conn, index=False, if_exists="replace")
 3834:             == 2
 3835:         )
 3836:         session.commit()
 3837:         test_query = session.query(Test.id, Test.string_column)
 3838:         df = DataFrame(test_query)
 3839: 
 3840:     assert list(df.columns) == ["id", "string_column"]
 3841: 
 3842: 
 3843: def test_read_sql_string_inference(sqlite_engine):
 3844:     conn = sqlite_engine
 3845:     # GH#54430
 3846:     pytest.importorskip("pyarrow")
 3847:     table = "test"
 3848:     df = DataFrame({"a": ["x", "y"]})
 3849:     df.to_sql(table, con=conn, index=False, if_exists="replace")
 3850: 
 3851:     with pd.option_context("future.infer_string", True):
 3852:         result = read_sql_table(table, conn)
 3853: 
 3854:     dtype = "string[pyarrow_numpy]"
 3855:     expected = DataFrame(
 3856:         {"a": ["x", "y"]}, dtype=dtype, columns=Index(["a"], dtype=dtype)
 3857:     )
 3858: 
 3859:     tm.assert_frame_equal(result, expected)
 3860: 
 3861: 
 3862: def test_roundtripping_datetimes(sqlite_engine):
 3863:     conn = sqlite_engine
 3864:     # GH#54877
 3865:     df = DataFrame({"t": [datetime(2020, 12, 31, 12)]}, dtype="datetime64[ns]")
 3866:     df.to_sql("test", conn, if_exists="replace", index=False)
 3867:     result = pd.read_sql("select * from test", conn).iloc[0, 0]
 3868:     assert result == "2020-12-31 12:00:00.000000"
 3869: 
 3870: 
 3871: @pytest.fixture
 3872: def sqlite_builtin_detect_types():
 3873:     with contextlib.closing(
 3874:         sqlite3.connect(":memory:", detect_types=sqlite3.PARSE_DECLTYPES)
 3875:     ) as closing_conn:
 3876:         with closing_conn as conn:
 3877:             yield conn
 3878: 
 3879: 
 3880: def test_roundtripping_datetimes_detect_types(sqlite_builtin_detect_types):
 3881:     # https://github.com/pandas-dev/pandas/issues/55554
 3882:     conn = sqlite_builtin_detect_types
 3883:     df = DataFrame({"t": [datetime(2020, 12, 31, 12)]}, dtype="datetime64[ns]")
 3884:     df.to_sql("test", conn, if_exists="replace", index=False)
 3885:     result = pd.read_sql("select * from test", conn).iloc[0, 0]
 3886:     assert result == Timestamp("2020-12-31 12:00:00.000000")
 3887: 
 3888: 
 3889: @pytest.mark.db
 3890: def test_psycopg2_schema_support(postgresql_psycopg2_engine):
 3891:     conn = postgresql_psycopg2_engine
 3892: 
 3893:     # only test this for postgresql (schema's not supported in
 3894:     # mysql/sqlite)
 3895:     df = DataFrame({"col1": [1, 2], "col2": [0.1, 0.2], "col3": ["a", "n"]})
 3896: 
 3897:     # create a schema
 3898:     with conn.connect() as con:
 3899:         with con.begin():
 3900:             con.exec_driver_sql("DROP SCHEMA IF EXISTS other CASCADE;")
 3901:             con.exec_driver_sql("CREATE SCHEMA other;")
 3902: 
 3903:     # write dataframe to different schema's
 3904:     assert df.to_sql(name="test_schema_public", con=conn, index=False) == 2
 3905:     assert (
 3906:         df.to_sql(
 3907:             name="test_schema_public_explicit",
 3908:             con=conn,
 3909:             index=False,
 3910:             schema="public",
 3911:         )
 3912:         == 2
 3913:     )
 3914:     assert (
 3915:         df.to_sql(name="test_schema_other", con=conn, index=False, schema="other") == 2
 3916:     )
 3917: 
 3918:     # read dataframes back in
 3919:     res1 = sql.read_sql_table("test_schema_public", conn)
 3920:     tm.assert_frame_equal(df, res1)
 3921:     res2 = sql.read_sql_table("test_schema_public_explicit", conn)
 3922:     tm.assert_frame_equal(df, res2)
 3923:     res3 = sql.read_sql_table("test_schema_public_explicit", conn, schema="public")
 3924:     tm.assert_frame_equal(df, res3)
 3925:     res4 = sql.read_sql_table("test_schema_other", conn, schema="other")
 3926:     tm.assert_frame_equal(df, res4)
 3927:     msg = "Table test_schema_other not found"
 3928:     with pytest.raises(ValueError, match=msg):
 3929:         sql.read_sql_table("test_schema_other", conn, schema="public")
 3930: 
 3931:     # different if_exists options
 3932: 
 3933:     # create a schema
 3934:     with conn.connect() as con:
 3935:         with con.begin():
 3936:             con.exec_driver_sql("DROP SCHEMA IF EXISTS other CASCADE;")
 3937:             con.exec_driver_sql("CREATE SCHEMA other;")
 3938: 
 3939:     # write dataframe with different if_exists options
 3940:     assert (
 3941:         df.to_sql(name="test_schema_other", con=conn, schema="other", index=False) == 2
 3942:     )
 3943:     df.to_sql(
 3944:         name="test_schema_other",
 3945:         con=conn,
 3946:         schema="other",
 3947:         index=False,
 3948:         if_exists="replace",
 3949:     )
 3950:     assert (
 3951:         df.to_sql(
 3952:             name="test_schema_other",
 3953:             con=conn,
 3954:             schema="other",
 3955:             index=False,
 3956:             if_exists="append",
 3957:         )
 3958:         == 2
 3959:     )
 3960:     res = sql.read_sql_table("test_schema_other", conn, schema="other")
 3961:     tm.assert_frame_equal(concat([df, df], ignore_index=True), res)
 3962: 
 3963: 
 3964: @pytest.mark.db
 3965: def test_self_join_date_columns(postgresql_psycopg2_engine):
 3966:     # GH 44421
 3967:     conn = postgresql_psycopg2_engine
 3968:     from sqlalchemy.sql import text
 3969: 
 3970:     create_table = text(
 3971:         """
 3972:     CREATE TABLE person
 3973:     (
 3974:         id serial constraint person_pkey primary key,
 3975:         created_dt timestamp with time zone
 3976:     );
 3977: 
 3978:     INSERT INTO person
 3979:         VALUES (1, '2021-01-01T00:00:00Z');
 3980:     """
 3981:     )
 3982:     with conn.connect() as con:
 3983:         with con.begin():
 3984:             con.execute(create_table)
 3985: 
 3986:     sql_query = (
 3987:         'SELECT * FROM "person" AS p1 INNER JOIN "person" AS p2 ON p1.id = p2.id;'
 3988:     )
 3989:     result = pd.read_sql(sql_query, conn)
 3990:     expected = DataFrame(
 3991:         [[1, Timestamp("2021", tz="UTC")] * 2], columns=["id", "created_dt"] * 2
 3992:     )
 3993:     tm.assert_frame_equal(result, expected)
 3994: 
 3995:     # Cleanup
 3996:     with sql.SQLDatabase(conn, need_transaction=True) as pandasSQL:
 3997:         pandasSQL.drop_table("person")
 3998: 
 3999: 
 4000: def test_create_and_drop_table(sqlite_engine):
 4001:     conn = sqlite_engine
 4002:     temp_frame = DataFrame({"one": [1.0, 2.0, 3.0, 4.0], "two": [4.0, 3.0, 2.0, 1.0]})
 4003:     with sql.SQLDatabase(conn) as pandasSQL:
 4004:         with pandasSQL.run_transaction():
 4005:             assert pandasSQL.to_sql(temp_frame, "drop_test_frame") == 4
 4006: 
 4007:         assert pandasSQL.has_table("drop_test_frame")
 4008: 
 4009:         with pandasSQL.run_transaction():
 4010:             pandasSQL.drop_table("drop_test_frame")
 4011: 
 4012:         assert not pandasSQL.has_table("drop_test_frame")
 4013: 
 4014: 
 4015: def test_sqlite_datetime_date(sqlite_buildin):
 4016:     conn = sqlite_buildin
 4017:     df = DataFrame([date(2014, 1, 1), date(2014, 1, 2)], columns=["a"])
 4018:     assert df.to_sql(name="test_date", con=conn, index=False) == 2
 4019:     res = read_sql_query("SELECT * FROM test_date", conn)
 4020:     # comes back as strings
 4021:     tm.assert_frame_equal(res, df.astype(str))
 4022: 
 4023: 
 4024: @pytest.mark.parametrize("tz_aware", [False, True])
 4025: def test_sqlite_datetime_time(tz_aware, sqlite_buildin):
 4026:     conn = sqlite_buildin
 4027:     # test support for datetime.time, GH #8341
 4028:     if not tz_aware:
 4029:         tz_times = [time(9, 0, 0), time(9, 1, 30)]
 4030:     else:
 4031:         tz_dt = date_range("2013-01-01 09:00:00", periods=2, tz="US/Pacific")
 4032:         tz_times = Series(tz_dt.to_pydatetime()).map(lambda dt: dt.timetz())
 4033: 
 4034:     df = DataFrame(tz_times, columns=["a"])
 4035: 
 4036:     assert df.to_sql(name="test_time", con=conn, index=False) == 2
 4037:     res = read_sql_query("SELECT * FROM test_time", conn)
 4038:     # comes back as strings
 4039:     expected = df.map(lambda _: _.strftime("%H:%M:%S.%f"))
 4040:     tm.assert_frame_equal(res, expected)
 4041: 
 4042: 
 4043: def get_sqlite_column_type(conn, table, column):
 4044:     recs = conn.execute(f"PRAGMA table_info({table})")
 4045:     for cid, name, ctype, not_null, default, pk in recs:
 4046:         if name == column:
 4047:             return ctype
 4048:     raise ValueError(f"Table {table}, column {column} not found")
 4049: 
 4050: 
 4051: def test_sqlite_test_dtype(sqlite_buildin):
 4052:     conn = sqlite_buildin
 4053:     cols = ["A", "B"]
 4054:     data = [(0.8, True), (0.9, None)]
 4055:     df = DataFrame(data, columns=cols)
 4056:     assert df.to_sql(name="dtype_test", con=conn) == 2
 4057:     assert df.to_sql(name="dtype_test2", con=conn, dtype={"B": "STRING"}) == 2
 4058: 
 4059:     # sqlite stores Boolean values as INTEGER
 4060:     assert get_sqlite_column_type(conn, "dtype_test", "B") == "INTEGER"
 4061: 
 4062:     assert get_sqlite_column_type(conn, "dtype_test2", "B") == "STRING"
 4063:     msg = r"B \(<class 'bool'>\) not a string"
 4064:     with pytest.raises(ValueError, match=msg):
 4065:         df.to_sql(name="error", con=conn, dtype={"B": bool})
 4066: 
 4067:     # single dtype
 4068:     assert df.to_sql(name="single_dtype_test", con=conn, dtype="STRING") == 2
 4069:     assert get_sqlite_column_type(conn, "single_dtype_test", "A") == "STRING"
 4070:     assert get_sqlite_column_type(conn, "single_dtype_test", "B") == "STRING"
 4071: 
 4072: 
 4073: def test_sqlite_notna_dtype(sqlite_buildin):
 4074:     conn = sqlite_buildin
 4075:     cols = {
 4076:         "Bool": Series([True, None]),
 4077:         "Date": Series([datetime(2012, 5, 1), None]),
 4078:         "Int": Series([1, None], dtype="object"),
 4079:         "Float": Series([1.1, None]),
 4080:     }
 4081:     df = DataFrame(cols)
 4082: 
 4083:     tbl = "notna_dtype_test"
 4084:     assert df.to_sql(name=tbl, con=conn) == 2
 4085: 
 4086:     assert get_sqlite_column_type(conn, tbl, "Bool") == "INTEGER"
 4087:     assert get_sqlite_column_type(conn, tbl, "Date") == "TIMESTAMP"
 4088:     assert get_sqlite_column_type(conn, tbl, "Int") == "INTEGER"
 4089:     assert get_sqlite_column_type(conn, tbl, "Float") == "REAL"
 4090: 
 4091: 
 4092: def test_sqlite_illegal_names(sqlite_buildin):
 4093:     # For sqlite, these should work fine
 4094:     conn = sqlite_buildin
 4095:     df = DataFrame([[1, 2], [3, 4]], columns=["a", "b"])
 4096: 
 4097:     msg = "Empty table or column name specified"
 4098:     with pytest.raises(ValueError, match=msg):
 4099:         df.to_sql(name="", con=conn)
 4100: 
 4101:     for ndx, weird_name in enumerate(
 4102:         [
 4103:             "test_weird_name]",
 4104:             "test_weird_name[",
 4105:             "test_weird_name`",
 4106:             'test_weird_name"',
 4107:             "test_weird_name'",
 4108:             "_b.test_weird_name_01-30",
 4109:             '"_b.test_weird_name_01-30"',
 4110:             "99beginswithnumber",
 4111:             "12345",
 4112:             "\xe9",
 4113:         ]
 4114:     ):
 4115:         assert df.to_sql(name=weird_name, con=conn) == 2
 4116:         sql.table_exists(weird_name, conn)
 4117: 
 4118:         df2 = DataFrame([[1, 2], [3, 4]], columns=["a", weird_name])
 4119:         c_tbl = f"test_weird_col_name{ndx:d}"
 4120:         assert df2.to_sql(name=c_tbl, con=conn) == 2
 4121:         sql.table_exists(c_tbl, conn)
 4122: 
 4123: 
 4124: def format_query(sql, *args):
 4125:     _formatters = {
 4126:         datetime: "'{}'".format,
 4127:         str: "'{}'".format,
 4128:         np.str_: "'{}'".format,
 4129:         bytes: "'{}'".format,
 4130:         float: "{:.8f}".format,
 4131:         int: "{:d}".format,
 4132:         type(None): lambda x: "NULL",
 4133:         np.float64: "{:.10f}".format,
 4134:         bool: "'{!s}'".format,
 4135:     }
 4136:     processed_args = []
 4137:     for arg in args:
 4138:         if isinstance(arg, float) and isna(arg):
 4139:             arg = None
 4140: 
 4141:         formatter = _formatters[type(arg)]
 4142:         processed_args.append(formatter(arg))
 4143: 
 4144:     return sql % tuple(processed_args)
 4145: 
 4146: 
 4147: def tquery(query, con=None):
 4148:     """Replace removed sql.tquery function"""
 4149:     with sql.pandasSQL_builder(con) as pandas_sql:
 4150:         res = pandas_sql.execute(query).fetchall()
 4151:     return None if res is None else list(res)
 4152: 
 4153: 
 4154: def test_xsqlite_basic(sqlite_buildin):
 4155:     frame = DataFrame(
 4156:         np.random.default_rng(2).standard_normal((10, 4)),
 4157:         columns=Index(list("ABCD"), dtype=object),
 4158:         index=date_range("2000-01-01", periods=10, freq="B"),
 4159:     )
 4160:     assert sql.to_sql(frame, name="test_table", con=sqlite_buildin, index=False) == 10
 4161:     result = sql.read_sql("select * from test_table", sqlite_buildin)
 4162: 
 4163:     # HACK! Change this once indexes are handled properly.
 4164:     result.index = frame.index
 4165: 
 4166:     expected = frame
 4167:     tm.assert_frame_equal(result, frame)
 4168: 
 4169:     frame["txt"] = ["a"] * len(frame)
 4170:     frame2 = frame.copy()
 4171:     new_idx = Index(np.arange(len(frame2)), dtype=np.int64) + 10
 4172:     frame2["Idx"] = new_idx.copy()
 4173:     assert sql.to_sql(frame2, name="test_table2", con=sqlite_buildin, index=False) == 10
 4174:     result = sql.read_sql("select * from test_table2", sqlite_buildin, index_col="Idx")
 4175:     expected = frame.copy()
 4176:     expected.index = new_idx
 4177:     expected.index.name = "Idx"
 4178:     tm.assert_frame_equal(expected, result)
 4179: 
 4180: 
 4181: def test_xsqlite_write_row_by_row(sqlite_buildin):
 4182:     frame = DataFrame(
 4183:         np.random.default_rng(2).standard_normal((10, 4)),
 4184:         columns=Index(list("ABCD"), dtype=object),
 4185:         index=date_range("2000-01-01", periods=10, freq="B"),
 4186:     )
 4187:     frame.iloc[0, 0] = np.nan
 4188:     create_sql = sql.get_schema(frame, "test")
 4189:     cur = sqlite_buildin.cursor()
 4190:     cur.execute(create_sql)
 4191: 
 4192:     ins = "INSERT INTO test VALUES (%s, %s, %s, %s)"
 4193:     for _, row in frame.iterrows():
 4194:         fmt_sql = format_query(ins, *row)
 4195:         tquery(fmt_sql, con=sqlite_buildin)
 4196: 
 4197:     sqlite_buildin.commit()
 4198: 
 4199:     result = sql.read_sql("select * from test", con=sqlite_buildin)
 4200:     result.index = frame.index
 4201:     tm.assert_frame_equal(result, frame, rtol=1e-3)
 4202: 
 4203: 
 4204: def test_xsqlite_execute(sqlite_buildin):
 4205:     frame = DataFrame(
 4206:         np.random.default_rng(2).standard_normal((10, 4)),
 4207:         columns=Index(list("ABCD"), dtype=object),
 4208:         index=date_range("2000-01-01", periods=10, freq="B"),
 4209:     )
 4210:     create_sql = sql.get_schema(frame, "test")
 4211:     cur = sqlite_buildin.cursor()
 4212:     cur.execute(create_sql)
 4213:     ins = "INSERT INTO test VALUES (?, ?, ?, ?)"
 4214: 
 4215:     row = frame.iloc[0]
 4216:     with sql.pandasSQL_builder(sqlite_buildin) as pandas_sql:
 4217:         pandas_sql.execute(ins, tuple(row))
 4218:     sqlite_buildin.commit()
 4219: 
 4220:     result = sql.read_sql("select * from test", sqlite_buildin)
 4221:     result.index = frame.index[:1]
 4222:     tm.assert_frame_equal(result, frame[:1])
 4223: 
 4224: 
 4225: def test_xsqlite_schema(sqlite_buildin):
 4226:     frame = DataFrame(
 4227:         np.random.default_rng(2).standard_normal((10, 4)),
 4228:         columns=Index(list("ABCD"), dtype=object),
 4229:         index=date_range("2000-01-01", periods=10, freq="B"),
 4230:     )
 4231:     create_sql = sql.get_schema(frame, "test")
 4232:     lines = create_sql.splitlines()
 4233:     for line in lines:
 4234:         tokens = line.split(" ")
 4235:         if len(tokens) == 2 and tokens[0] == "A":
 4236:             assert tokens[1] == "DATETIME"
 4237: 
 4238:     create_sql = sql.get_schema(frame, "test", keys=["A", "B"])
 4239:     lines = create_sql.splitlines()
 4240:     assert 'PRIMARY KEY ("A", "B")' in create_sql
 4241:     cur = sqlite_buildin.cursor()
 4242:     cur.execute(create_sql)
 4243: 
 4244: 
 4245: def test_xsqlite_execute_fail(sqlite_buildin):
 4246:     create_sql = """
 4247:     CREATE TABLE test
 4248:     (
 4249:     a TEXT,
 4250:     b TEXT,
 4251:     c REAL,
 4252:     PRIMARY KEY (a, b)
 4253:     );
 4254:     """
 4255:     cur = sqlite_buildin.cursor()
 4256:     cur.execute(create_sql)
 4257: 
 4258:     with sql.pandasSQL_builder(sqlite_buildin) as pandas_sql:
 4259:         pandas_sql.execute('INSERT INTO test VALUES("foo", "bar", 1.234)')
 4260:         pandas_sql.execute('INSERT INTO test VALUES("foo", "baz", 2.567)')
 4261: 
 4262:         with pytest.raises(sql.DatabaseError, match="Execution failed on sql"):
 4263:             pandas_sql.execute('INSERT INTO test VALUES("foo", "bar", 7)')
 4264: 
 4265: 
 4266: def test_xsqlite_execute_closed_connection():
 4267:     create_sql = """
 4268:     CREATE TABLE test
 4269:     (
 4270:     a TEXT,
 4271:     b TEXT,
 4272:     c REAL,
 4273:     PRIMARY KEY (a, b)
 4274:     );
 4275:     """
 4276:     with contextlib.closing(sqlite3.connect(":memory:")) as conn:
 4277:         cur = conn.cursor()
 4278:         cur.execute(create_sql)
 4279: 
 4280:         with sql.pandasSQL_builder(conn) as pandas_sql:
 4281:             pandas_sql.execute('INSERT INTO test VALUES("foo", "bar", 1.234)')
 4282: 
 4283:     msg = "Cannot operate on a closed database."
 4284:     with pytest.raises(sqlite3.ProgrammingError, match=msg):
 4285:         tquery("select * from test", con=conn)
 4286: 
 4287: 
 4288: def test_xsqlite_keyword_as_column_names(sqlite_buildin):
 4289:     df = DataFrame({"From": np.ones(5)})
 4290:     assert sql.to_sql(df, con=sqlite_buildin, name="testkeywords", index=False) == 5
 4291: 
 4292: 
 4293: def test_xsqlite_onecolumn_of_integer(sqlite_buildin):
 4294:     # GH 3628
 4295:     # a column_of_integers dataframe should transfer well to sql
 4296: 
 4297:     mono_df = DataFrame([1, 2], columns=["c0"])
 4298:     assert sql.to_sql(mono_df, con=sqlite_buildin, name="mono_df", index=False) == 2
 4299:     # computing the sum via sql
 4300:     con_x = sqlite_buildin
 4301:     the_sum = sum(my_c0[0] for my_c0 in con_x.execute("select * from mono_df"))
 4302:     # it should not fail, and gives 3 ( Issue #3628 )
 4303:     assert the_sum == 3
 4304: 
 4305:     result = sql.read_sql("select * from mono_df", con_x)
 4306:     tm.assert_frame_equal(result, mono_df)
 4307: 
 4308: 
 4309: def test_xsqlite_if_exists(sqlite_buildin):
 4310:     df_if_exists_1 = DataFrame({"col1": [1, 2], "col2": ["A", "B"]})
 4311:     df_if_exists_2 = DataFrame({"col1": [3, 4, 5], "col2": ["C", "D", "E"]})
 4312:     table_name = "table_if_exists"
 4313:     sql_select = f"SELECT * FROM {table_name}"
 4314: 
 4315:     msg = "'notvalidvalue' is not valid for if_exists"
 4316:     with pytest.raises(ValueError, match=msg):
 4317:         sql.to_sql(
 4318:             frame=df_if_exists_1,
 4319:             con=sqlite_buildin,
 4320:             name=table_name,
 4321:             if_exists="notvalidvalue",
 4322:         )
 4323:     drop_table(table_name, sqlite_buildin)
 4324: 
 4325:     # test if_exists='fail'
 4326:     sql.to_sql(
 4327:         frame=df_if_exists_1, con=sqlite_buildin, name=table_name, if_exists="fail"
 4328:     )
 4329:     msg = "Table 'table_if_exists' already exists"
 4330:     with pytest.raises(ValueError, match=msg):
 4331:         sql.to_sql(
 4332:             frame=df_if_exists_1,
 4333:             con=sqlite_buildin,
 4334:             name=table_name,
 4335:             if_exists="fail",
 4336:         )
 4337:     # test if_exists='replace'
 4338:     sql.to_sql(
 4339:         frame=df_if_exists_1,
 4340:         con=sqlite_buildin,
 4341:         name=table_name,
 4342:         if_exists="replace",
 4343:         index=False,
 4344:     )
 4345:     assert tquery(sql_select, con=sqlite_buildin) == [(1, "A"), (2, "B")]
 4346:     assert (
 4347:         sql.to_sql(
 4348:             frame=df_if_exists_2,
 4349:             con=sqlite_buildin,
 4350:             name=table_name,
 4351:             if_exists="replace",
 4352:             index=False,
 4353:         )
 4354:         == 3
 4355:     )
 4356:     assert tquery(sql_select, con=sqlite_buildin) == [(3, "C"), (4, "D"), (5, "E")]
 4357:     drop_table(table_name, sqlite_buildin)
 4358: 
 4359:     # test if_exists='append'
 4360:     assert (
 4361:         sql.to_sql(
 4362:             frame=df_if_exists_1,
 4363:             con=sqlite_buildin,
 4364:             name=table_name,
 4365:             if_exists="fail",
 4366:             index=False,
 4367:         )
 4368:         == 2
 4369:     )
 4370:     assert tquery(sql_select, con=sqlite_buildin) == [(1, "A"), (2, "B")]
 4371:     assert (
 4372:         sql.to_sql(
 4373:             frame=df_if_exists_2,
 4374:             con=sqlite_buildin,
 4375:             name=table_name,
 4376:             if_exists="append",
 4377:             index=False,
 4378:         )
 4379:         == 3
 4380:     )
 4381:     assert tquery(sql_select, con=sqlite_buildin) == [
 4382:         (1, "A"),
 4383:         (2, "B"),
 4384:         (3, "C"),
 4385:         (4, "D"),
 4386:         (5, "E"),
 4387:     ]
 4388:     drop_table(table_name, sqlite_buildin)
