    1: import datetime
    2: from pathlib import Path
    3: 
    4: import numpy as np
    5: import pytest
    6: 
    7: import pandas as pd
    8: import pandas._testing as tm
    9: from pandas.util.version import Version
   10: 
   11: pyreadstat = pytest.importorskip("pyreadstat")
   12: 
   13: 
   14: # TODO(CoW) - detection of chained assignment in cython
   15: # https://github.com/pandas-dev/pandas/issues/51315
   16: @pytest.mark.filterwarnings("ignore::pandas.errors.ChainedAssignmentError")
   17: @pytest.mark.filterwarnings("ignore:ChainedAssignmentError:FutureWarning")
   18: @pytest.mark.parametrize("path_klass", [lambda p: p, Path])
   19: def test_spss_labelled_num(path_klass, datapath):
   20:     # test file from the Haven project (https://haven.tidyverse.org/)
   21:     # Licence at LICENSES/HAVEN_LICENSE, LICENSES/HAVEN_MIT
   22:     fname = path_klass(datapath("io", "data", "spss", "labelled-num.sav"))
   23: 
   24:     df = pd.read_spss(fname, convert_categoricals=True)
   25:     expected = pd.DataFrame({"VAR00002": "This is one"}, index=[0])
   26:     expected["VAR00002"] = pd.Categorical(expected["VAR00002"])
   27:     tm.assert_frame_equal(df, expected)
   28: 
   29:     df = pd.read_spss(fname, convert_categoricals=False)
   30:     expected = pd.DataFrame({"VAR00002": 1.0}, index=[0])
   31:     tm.assert_frame_equal(df, expected)
   32: 
   33: 
   34: @pytest.mark.filterwarnings("ignore::pandas.errors.ChainedAssignmentError")
   35: @pytest.mark.filterwarnings("ignore:ChainedAssignmentError:FutureWarning")
   36: def test_spss_labelled_num_na(datapath):
   37:     # test file from the Haven project (https://haven.tidyverse.org/)
   38:     # Licence at LICENSES/HAVEN_LICENSE, LICENSES/HAVEN_MIT
   39:     fname = datapath("io", "data", "spss", "labelled-num-na.sav")
   40: 
   41:     df = pd.read_spss(fname, convert_categoricals=True)
   42:     expected = pd.DataFrame({"VAR00002": ["This is one", None]})
   43:     expected["VAR00002"] = pd.Categorical(expected["VAR00002"])
   44:     tm.assert_frame_equal(df, expected)
   45: 
   46:     df = pd.read_spss(fname, convert_categoricals=False)
   47:     expected = pd.DataFrame({"VAR00002": [1.0, np.nan]})
   48:     tm.assert_frame_equal(df, expected)
   49: 
   50: 
   51: @pytest.mark.filterwarnings("ignore::pandas.errors.ChainedAssignmentError")
   52: @pytest.mark.filterwarnings("ignore:ChainedAssignmentError:FutureWarning")
   53: def test_spss_labelled_str(datapath):
   54:     # test file from the Haven project (https://haven.tidyverse.org/)
   55:     # Licence at LICENSES/HAVEN_LICENSE, LICENSES/HAVEN_MIT
   56:     fname = datapath("io", "data", "spss", "labelled-str.sav")
   57: 
   58:     df = pd.read_spss(fname, convert_categoricals=True)
   59:     expected = pd.DataFrame({"gender": ["Male", "Female"]})
   60:     expected["gender"] = pd.Categorical(expected["gender"])
   61:     tm.assert_frame_equal(df, expected)
   62: 
   63:     df = pd.read_spss(fname, convert_categoricals=False)
   64:     expected = pd.DataFrame({"gender": ["M", "F"]})
   65:     tm.assert_frame_equal(df, expected)
   66: 
   67: 
   68: @pytest.mark.filterwarnings("ignore::pandas.errors.ChainedAssignmentError")
   69: @pytest.mark.filterwarnings("ignore:ChainedAssignmentError:FutureWarning")
   70: def test_spss_umlauts(datapath):
   71:     # test file from the Haven project (https://haven.tidyverse.org/)
   72:     # Licence at LICENSES/HAVEN_LICENSE, LICENSES/HAVEN_MIT
   73:     fname = datapath("io", "data", "spss", "umlauts.sav")
   74: 
   75:     df = pd.read_spss(fname, convert_categoricals=True)
   76:     expected = pd.DataFrame(
   77:         {"var1": ["the Г¤ umlaut", "the Гј umlaut", "the Г¤ umlaut", "the Г¶ umlaut"]}
   78:     )
   79:     expected["var1"] = pd.Categorical(expected["var1"])
   80:     tm.assert_frame_equal(df, expected)
   81: 
   82:     df = pd.read_spss(fname, convert_categoricals=False)
   83:     expected = pd.DataFrame({"var1": [1.0, 2.0, 1.0, 3.0]})
   84:     tm.assert_frame_equal(df, expected)
   85: 
   86: 
   87: def test_spss_usecols(datapath):
   88:     # usecols must be list-like
   89:     fname = datapath("io", "data", "spss", "labelled-num.sav")
   90: 
   91:     with pytest.raises(TypeError, match="usecols must be list-like."):
   92:         pd.read_spss(fname, usecols="VAR00002")
   93: 
   94: 
   95: def test_spss_umlauts_dtype_backend(datapath, dtype_backend):
   96:     # test file from the Haven project (https://haven.tidyverse.org/)
   97:     # Licence at LICENSES/HAVEN_LICENSE, LICENSES/HAVEN_MIT
   98:     fname = datapath("io", "data", "spss", "umlauts.sav")
   99: 
  100:     df = pd.read_spss(fname, convert_categoricals=False, dtype_backend=dtype_backend)
  101:     expected = pd.DataFrame({"var1": [1.0, 2.0, 1.0, 3.0]}, dtype="Int64")
  102: 
  103:     if dtype_backend == "pyarrow":
  104:         pa = pytest.importorskip("pyarrow")
  105: 
  106:         from pandas.arrays import ArrowExtensionArray
  107: 
  108:         expected = pd.DataFrame(
  109:             {
  110:                 col: ArrowExtensionArray(pa.array(expected[col], from_pandas=True))
  111:                 for col in expected.columns
  112:             }
  113:         )
  114: 
  115:     tm.assert_frame_equal(df, expected)
  116: 
  117: 
  118: def test_invalid_dtype_backend():
  119:     msg = (
  120:         "dtype_backend numpy is invalid, only 'numpy_nullable' and "
  121:         "'pyarrow' are allowed."
  122:     )
  123:     with pytest.raises(ValueError, match=msg):
  124:         pd.read_spss("test", dtype_backend="numpy")
  125: 
  126: 
  127: @pytest.mark.filterwarnings("ignore::pandas.errors.ChainedAssignmentError")
  128: @pytest.mark.filterwarnings("ignore:ChainedAssignmentError:FutureWarning")
  129: def test_spss_metadata(datapath):
  130:     # GH 54264
  131:     fname = datapath("io", "data", "spss", "labelled-num.sav")
  132: 
  133:     df = pd.read_spss(fname)
  134:     metadata = {
  135:         "column_names": ["VAR00002"],
  136:         "column_labels": [None],
  137:         "column_names_to_labels": {"VAR00002": None},
  138:         "file_encoding": "UTF-8",
  139:         "number_columns": 1,
  140:         "number_rows": 1,
  141:         "variable_value_labels": {"VAR00002": {1.0: "This is one"}},
  142:         "value_labels": {"labels0": {1.0: "This is one"}},
  143:         "variable_to_label": {"VAR00002": "labels0"},
  144:         "notes": [],
  145:         "original_variable_types": {"VAR00002": "F8.0"},
  146:         "readstat_variable_types": {"VAR00002": "double"},
  147:         "table_name": None,
  148:         "missing_ranges": {},
  149:         "missing_user_values": {},
  150:         "variable_storage_width": {"VAR00002": 8},
  151:         "variable_display_width": {"VAR00002": 8},
  152:         "variable_alignment": {"VAR00002": "unknown"},
  153:         "variable_measure": {"VAR00002": "unknown"},
  154:         "file_label": None,
  155:         "file_format": "sav/zsav",
  156:     }
  157:     if Version(pyreadstat.__version__) >= Version("1.2.4"):
  158:         metadata.update(
  159:             {
  160:                 "creation_time": datetime.datetime(2015, 2, 6, 14, 33, 36),
  161:                 "modification_time": datetime.datetime(2015, 2, 6, 14, 33, 36),
  162:             }
  163:         )
  164:     assert df.attrs == metadata
