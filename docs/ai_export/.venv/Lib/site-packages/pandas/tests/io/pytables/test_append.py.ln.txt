    1: import datetime
    2: from datetime import timedelta
    3: import re
    4: 
    5: import numpy as np
    6: import pytest
    7: 
    8: from pandas._libs.tslibs import Timestamp
    9: import pandas.util._test_decorators as td
   10: 
   11: import pandas as pd
   12: from pandas import (
   13:     DataFrame,
   14:     Index,
   15:     Series,
   16:     _testing as tm,
   17:     concat,
   18:     date_range,
   19:     read_hdf,
   20: )
   21: from pandas.tests.io.pytables.common import (
   22:     _maybe_remove,
   23:     ensure_clean_store,
   24: )
   25: 
   26: pytestmark = pytest.mark.single_cpu
   27: 
   28: tables = pytest.importorskip("tables")
   29: 
   30: 
   31: @pytest.mark.filterwarnings("ignore::tables.NaturalNameWarning")
   32: def test_append(setup_path):
   33:     with ensure_clean_store(setup_path) as store:
   34:         # this is allowed by almost always don't want to do it
   35:         # tables.NaturalNameWarning):
   36:         df = DataFrame(
   37:             np.random.default_rng(2).standard_normal((20, 4)),
   38:             columns=Index(list("ABCD"), dtype=object),
   39:             index=date_range("2000-01-01", periods=20, freq="B"),
   40:         )
   41:         _maybe_remove(store, "df1")
   42:         store.append("df1", df[:10])
   43:         store.append("df1", df[10:])
   44:         tm.assert_frame_equal(store["df1"], df)
   45: 
   46:         _maybe_remove(store, "df2")
   47:         store.put("df2", df[:10], format="table")
   48:         store.append("df2", df[10:])
   49:         tm.assert_frame_equal(store["df2"], df)
   50: 
   51:         _maybe_remove(store, "df3")
   52:         store.append("/df3", df[:10])
   53:         store.append("/df3", df[10:])
   54:         tm.assert_frame_equal(store["df3"], df)
   55: 
   56:         # this is allowed by almost always don't want to do it
   57:         # tables.NaturalNameWarning
   58:         _maybe_remove(store, "/df3 foo")
   59:         store.append("/df3 foo", df[:10])
   60:         store.append("/df3 foo", df[10:])
   61:         tm.assert_frame_equal(store["df3 foo"], df)
   62: 
   63:         # dtype issues - mizxed type in a single object column
   64:         df = DataFrame(data=[[1, 2], [0, 1], [1, 2], [0, 0]])
   65:         df["mixed_column"] = "testing"
   66:         df.loc[2, "mixed_column"] = np.nan
   67:         _maybe_remove(store, "df")
   68:         store.append("df", df)
   69:         tm.assert_frame_equal(store["df"], df)
   70: 
   71:         # uints - test storage of uints
   72:         uint_data = DataFrame(
   73:             {
   74:                 "u08": Series(
   75:                     np.random.default_rng(2).integers(0, high=255, size=5),
   76:                     dtype=np.uint8,
   77:                 ),
   78:                 "u16": Series(
   79:                     np.random.default_rng(2).integers(0, high=65535, size=5),
   80:                     dtype=np.uint16,
   81:                 ),
   82:                 "u32": Series(
   83:                     np.random.default_rng(2).integers(0, high=2**30, size=5),
   84:                     dtype=np.uint32,
   85:                 ),
   86:                 "u64": Series(
   87:                     [2**58, 2**59, 2**60, 2**61, 2**62],
   88:                     dtype=np.uint64,
   89:                 ),
   90:             },
   91:             index=np.arange(5),
   92:         )
   93:         _maybe_remove(store, "uints")
   94:         store.append("uints", uint_data)
   95:         tm.assert_frame_equal(store["uints"], uint_data, check_index_type=True)
   96: 
   97:         # uints - test storage of uints in indexable columns
   98:         _maybe_remove(store, "uints")
   99:         # 64-bit indices not yet supported
  100:         store.append("uints", uint_data, data_columns=["u08", "u16", "u32"])
  101:         tm.assert_frame_equal(store["uints"], uint_data, check_index_type=True)
  102: 
  103: 
  104: def test_append_series(setup_path):
  105:     with ensure_clean_store(setup_path) as store:
  106:         # basic
  107:         ss = Series(range(20), dtype=np.float64, index=[f"i_{i}" for i in range(20)])
  108:         ts = Series(
  109:             np.arange(10, dtype=np.float64), index=date_range("2020-01-01", periods=10)
  110:         )
  111:         ns = Series(np.arange(100))
  112: 
  113:         store.append("ss", ss)
  114:         result = store["ss"]
  115:         tm.assert_series_equal(result, ss)
  116:         assert result.name is None
  117: 
  118:         store.append("ts", ts)
  119:         result = store["ts"]
  120:         tm.assert_series_equal(result, ts)
  121:         assert result.name is None
  122: 
  123:         ns.name = "foo"
  124:         store.append("ns", ns)
  125:         result = store["ns"]
  126:         tm.assert_series_equal(result, ns)
  127:         assert result.name == ns.name
  128: 
  129:         # select on the values
  130:         expected = ns[ns > 60]
  131:         result = store.select("ns", "foo>60")
  132:         tm.assert_series_equal(result, expected)
  133: 
  134:         # select on the index and values
  135:         expected = ns[(ns > 70) & (ns.index < 90)]
  136:         result = store.select("ns", "foo>70 and index<90")
  137:         tm.assert_series_equal(result, expected, check_index_type=True)
  138: 
  139:         # multi-index
  140:         mi = DataFrame(np.random.default_rng(2).standard_normal((5, 1)), columns=["A"])
  141:         mi["B"] = np.arange(len(mi))
  142:         mi["C"] = "foo"
  143:         mi.loc[3:5, "C"] = "bar"
  144:         mi.set_index(["C", "B"], inplace=True)
  145:         s = mi.stack(future_stack=True)
  146:         s.index = s.index.droplevel(2)
  147:         store.append("mi", s)
  148:         tm.assert_series_equal(store["mi"], s, check_index_type=True)
  149: 
  150: 
  151: def test_append_some_nans(setup_path):
  152:     with ensure_clean_store(setup_path) as store:
  153:         df = DataFrame(
  154:             {
  155:                 "A": Series(np.random.default_rng(2).standard_normal(20)).astype(
  156:                     "int32"
  157:                 ),
  158:                 "A1": np.random.default_rng(2).standard_normal(20),
  159:                 "A2": np.random.default_rng(2).standard_normal(20),
  160:                 "B": "foo",
  161:                 "C": "bar",
  162:                 "D": Timestamp("2001-01-01").as_unit("ns"),
  163:                 "E": Timestamp("2001-01-02").as_unit("ns"),
  164:             },
  165:             index=np.arange(20),
  166:         )
  167:         # some nans
  168:         _maybe_remove(store, "df1")
  169:         df.loc[0:15, ["A1", "B", "D", "E"]] = np.nan
  170:         store.append("df1", df[:10])
  171:         store.append("df1", df[10:])
  172:         tm.assert_frame_equal(store["df1"], df, check_index_type=True)
  173: 
  174:         # first column
  175:         df1 = df.copy()
  176:         df1["A1"] = np.nan
  177:         _maybe_remove(store, "df1")
  178:         store.append("df1", df1[:10])
  179:         store.append("df1", df1[10:])
  180:         tm.assert_frame_equal(store["df1"], df1, check_index_type=True)
  181: 
  182:         # 2nd column
  183:         df2 = df.copy()
  184:         df2["A2"] = np.nan
  185:         _maybe_remove(store, "df2")
  186:         store.append("df2", df2[:10])
  187:         store.append("df2", df2[10:])
  188:         tm.assert_frame_equal(store["df2"], df2, check_index_type=True)
  189: 
  190:         # datetimes
  191:         df3 = df.copy()
  192:         df3["E"] = np.nan
  193:         _maybe_remove(store, "df3")
  194:         store.append("df3", df3[:10])
  195:         store.append("df3", df3[10:])
  196:         tm.assert_frame_equal(store["df3"], df3, check_index_type=True)
  197: 
  198: 
  199: def test_append_all_nans(setup_path):
  200:     with ensure_clean_store(setup_path) as store:
  201:         df = DataFrame(
  202:             {
  203:                 "A1": np.random.default_rng(2).standard_normal(20),
  204:                 "A2": np.random.default_rng(2).standard_normal(20),
  205:             },
  206:             index=np.arange(20),
  207:         )
  208:         df.loc[0:15, :] = np.nan
  209: 
  210:         # nan some entire rows (dropna=True)
  211:         _maybe_remove(store, "df")
  212:         store.append("df", df[:10], dropna=True)
  213:         store.append("df", df[10:], dropna=True)
  214:         tm.assert_frame_equal(store["df"], df[-4:], check_index_type=True)
  215: 
  216:         # nan some entire rows (dropna=False)
  217:         _maybe_remove(store, "df2")
  218:         store.append("df2", df[:10], dropna=False)
  219:         store.append("df2", df[10:], dropna=False)
  220:         tm.assert_frame_equal(store["df2"], df, check_index_type=True)
  221: 
  222:         # tests the option io.hdf.dropna_table
  223:         with pd.option_context("io.hdf.dropna_table", False):
  224:             _maybe_remove(store, "df3")
  225:             store.append("df3", df[:10])
  226:             store.append("df3", df[10:])
  227:             tm.assert_frame_equal(store["df3"], df)
  228: 
  229:         with pd.option_context("io.hdf.dropna_table", True):
  230:             _maybe_remove(store, "df4")
  231:             store.append("df4", df[:10])
  232:             store.append("df4", df[10:])
  233:             tm.assert_frame_equal(store["df4"], df[-4:])
  234: 
  235:             # nan some entire rows (string are still written!)
  236:             df = DataFrame(
  237:                 {
  238:                     "A1": np.random.default_rng(2).standard_normal(20),
  239:                     "A2": np.random.default_rng(2).standard_normal(20),
  240:                     "B": "foo",
  241:                     "C": "bar",
  242:                 },
  243:                 index=np.arange(20),
  244:             )
  245: 
  246:             df.loc[0:15, :] = np.nan
  247: 
  248:             _maybe_remove(store, "df")
  249:             store.append("df", df[:10], dropna=True)
  250:             store.append("df", df[10:], dropna=True)
  251:             tm.assert_frame_equal(store["df"], df, check_index_type=True)
  252: 
  253:             _maybe_remove(store, "df2")
  254:             store.append("df2", df[:10], dropna=False)
  255:             store.append("df2", df[10:], dropna=False)
  256:             tm.assert_frame_equal(store["df2"], df, check_index_type=True)
  257: 
  258:             # nan some entire rows (but since we have dates they are still
  259:             # written!)
  260:             df = DataFrame(
  261:                 {
  262:                     "A1": np.random.default_rng(2).standard_normal(20),
  263:                     "A2": np.random.default_rng(2).standard_normal(20),
  264:                     "B": "foo",
  265:                     "C": "bar",
  266:                     "D": Timestamp("2001-01-01").as_unit("ns"),
  267:                     "E": Timestamp("2001-01-02").as_unit("ns"),
  268:                 },
  269:                 index=np.arange(20),
  270:             )
  271: 
  272:             df.loc[0:15, :] = np.nan
  273: 
  274:             _maybe_remove(store, "df")
  275:             store.append("df", df[:10], dropna=True)
  276:             store.append("df", df[10:], dropna=True)
  277:             tm.assert_frame_equal(store["df"], df, check_index_type=True)
  278: 
  279:             _maybe_remove(store, "df2")
  280:             store.append("df2", df[:10], dropna=False)
  281:             store.append("df2", df[10:], dropna=False)
  282:             tm.assert_frame_equal(store["df2"], df, check_index_type=True)
  283: 
  284: 
  285: def test_append_frame_column_oriented(setup_path):
  286:     with ensure_clean_store(setup_path) as store:
  287:         # column oriented
  288:         df = DataFrame(
  289:             np.random.default_rng(2).standard_normal((10, 4)),
  290:             columns=Index(list("ABCD"), dtype=object),
  291:             index=date_range("2000-01-01", periods=10, freq="B"),
  292:         )
  293:         df.index = df.index._with_freq(None)  # freq doesn't round-trip
  294: 
  295:         _maybe_remove(store, "df1")
  296:         store.append("df1", df.iloc[:, :2], axes=["columns"])
  297:         store.append("df1", df.iloc[:, 2:])
  298:         tm.assert_frame_equal(store["df1"], df)
  299: 
  300:         result = store.select("df1", "columns=A")
  301:         expected = df.reindex(columns=["A"])
  302:         tm.assert_frame_equal(expected, result)
  303: 
  304:         # selection on the non-indexable
  305:         result = store.select("df1", ("columns=A", "index=df.index[0:4]"))
  306:         expected = df.reindex(columns=["A"], index=df.index[0:4])
  307:         tm.assert_frame_equal(expected, result)
  308: 
  309:         # this isn't supported
  310:         msg = re.escape(
  311:             "passing a filterable condition to a non-table indexer "
  312:             "[Filter: Not Initialized]"
  313:         )
  314:         with pytest.raises(TypeError, match=msg):
  315:             store.select("df1", "columns=A and index>df.index[4]")
  316: 
  317: 
  318: def test_append_with_different_block_ordering(setup_path):
  319:     # GH 4096; using same frames, but different block orderings
  320:     with ensure_clean_store(setup_path) as store:
  321:         for i in range(10):
  322:             df = DataFrame(
  323:                 np.random.default_rng(2).standard_normal((10, 2)), columns=list("AB")
  324:             )
  325:             df["index"] = range(10)
  326:             df["index"] += i * 10
  327:             df["int64"] = Series([1] * len(df), dtype="int64")
  328:             df["int16"] = Series([1] * len(df), dtype="int16")
  329: 
  330:             if i % 2 == 0:
  331:                 del df["int64"]
  332:                 df["int64"] = Series([1] * len(df), dtype="int64")
  333:             if i % 3 == 0:
  334:                 a = df.pop("A")
  335:                 df["A"] = a
  336: 
  337:             df.set_index("index", inplace=True)
  338: 
  339:             store.append("df", df)
  340: 
  341:     # test a different ordering but with more fields (like invalid
  342:     # combinations)
  343:     with ensure_clean_store(setup_path) as store:
  344:         df = DataFrame(
  345:             np.random.default_rng(2).standard_normal((10, 2)),
  346:             columns=list("AB"),
  347:             dtype="float64",
  348:         )
  349:         df["int64"] = Series([1] * len(df), dtype="int64")
  350:         df["int16"] = Series([1] * len(df), dtype="int16")
  351:         store.append("df", df)
  352: 
  353:         # store additional fields in different blocks
  354:         df["int16_2"] = Series([1] * len(df), dtype="int16")
  355:         msg = re.escape(
  356:             "cannot match existing table structure for [int16] on appending data"
  357:         )
  358:         with pytest.raises(ValueError, match=msg):
  359:             store.append("df", df)
  360: 
  361:         # store multiple additional fields in different blocks
  362:         df["float_3"] = Series([1.0] * len(df), dtype="float64")
  363:         msg = re.escape(
  364:             "cannot match existing table structure for [A,B] on appending data"
  365:         )
  366:         with pytest.raises(ValueError, match=msg):
  367:             store.append("df", df)
  368: 
  369: 
  370: def test_append_with_strings(setup_path):
  371:     with ensure_clean_store(setup_path) as store:
  372: 
  373:         def check_col(key, name, size):
  374:             assert (
  375:                 getattr(store.get_storer(key).table.description, name).itemsize == size
  376:             )
  377: 
  378:         # avoid truncation on elements
  379:         df = DataFrame([[123, "asdqwerty"], [345, "dggnhebbsdfbdfb"]])
  380:         store.append("df_big", df)
  381:         tm.assert_frame_equal(store.select("df_big"), df)
  382:         check_col("df_big", "values_block_1", 15)
  383: 
  384:         # appending smaller string ok
  385:         df2 = DataFrame([[124, "asdqy"], [346, "dggnhefbdfb"]])
  386:         store.append("df_big", df2)
  387:         expected = concat([df, df2])
  388:         tm.assert_frame_equal(store.select("df_big"), expected)
  389:         check_col("df_big", "values_block_1", 15)
  390: 
  391:         # avoid truncation on elements
  392:         df = DataFrame([[123, "asdqwerty"], [345, "dggnhebbsdfbdfb"]])
  393:         store.append("df_big2", df, min_itemsize={"values": 50})
  394:         tm.assert_frame_equal(store.select("df_big2"), df)
  395:         check_col("df_big2", "values_block_1", 50)
  396: 
  397:         # bigger string on next append
  398:         store.append("df_new", df)
  399:         df_new = DataFrame([[124, "abcdefqhij"], [346, "abcdefghijklmnopqrtsuvwxyz"]])
  400:         msg = (
  401:             r"Trying to store a string with len \[26\] in "
  402:             r"\[values_block_1\] column but\n"
  403:             r"this column has a limit of \[15\]!\n"
  404:             "Consider using min_itemsize to preset the sizes on these "
  405:             "columns"
  406:         )
  407:         with pytest.raises(ValueError, match=msg):
  408:             store.append("df_new", df_new)
  409: 
  410:         # min_itemsize on Series index (GH 11412)
  411:         df = DataFrame(
  412:             {
  413:                 "A": [0.0, 1.0, 2.0, 3.0, 4.0],
  414:                 "B": [0.0, 1.0, 0.0, 1.0, 0.0],
  415:                 "C": Index(["foo1", "foo2", "foo3", "foo4", "foo5"], dtype=object),
  416:                 "D": date_range("20130101", periods=5),
  417:             }
  418:         ).set_index("C")
  419:         store.append("ss", df["B"], min_itemsize={"index": 4})
  420:         tm.assert_series_equal(store.select("ss"), df["B"])
  421: 
  422:         # same as above, with data_columns=True
  423:         store.append("ss2", df["B"], data_columns=True, min_itemsize={"index": 4})
  424:         tm.assert_series_equal(store.select("ss2"), df["B"])
  425: 
  426:         # min_itemsize in index without appending (GH 10381)
  427:         store.put("ss3", df, format="table", min_itemsize={"index": 6})
  428:         # just make sure there is a longer string:
  429:         df2 = df.copy().reset_index().assign(C="longer").set_index("C")
  430:         store.append("ss3", df2)
  431:         tm.assert_frame_equal(store.select("ss3"), concat([df, df2]))
  432: 
  433:         # same as above, with a Series
  434:         store.put("ss4", df["B"], format="table", min_itemsize={"index": 6})
  435:         store.append("ss4", df2["B"])
  436:         tm.assert_series_equal(store.select("ss4"), concat([df["B"], df2["B"]]))
  437: 
  438:         # with nans
  439:         _maybe_remove(store, "df")
  440:         df = DataFrame(
  441:             np.random.default_rng(2).standard_normal((10, 4)),
  442:             columns=Index(list("ABCD"), dtype=object),
  443:             index=date_range("2000-01-01", periods=10, freq="B"),
  444:         )
  445:         df["string"] = "foo"
  446:         df.loc[df.index[1:4], "string"] = np.nan
  447:         df["string2"] = "bar"
  448:         df.loc[df.index[4:8], "string2"] = np.nan
  449:         df["string3"] = "bah"
  450:         df.loc[df.index[1:], "string3"] = np.nan
  451:         store.append("df", df)
  452:         result = store.select("df")
  453:         tm.assert_frame_equal(result, df)
  454: 
  455:     with ensure_clean_store(setup_path) as store:
  456:         df = DataFrame({"A": "foo", "B": "bar"}, index=range(10))
  457: 
  458:         # a min_itemsize that creates a data_column
  459:         _maybe_remove(store, "df")
  460:         store.append("df", df, min_itemsize={"A": 200})
  461:         check_col("df", "A", 200)
  462:         assert store.get_storer("df").data_columns == ["A"]
  463: 
  464:         # a min_itemsize that creates a data_column2
  465:         _maybe_remove(store, "df")
  466:         store.append("df", df, data_columns=["B"], min_itemsize={"A": 200})
  467:         check_col("df", "A", 200)
  468:         assert store.get_storer("df").data_columns == ["B", "A"]
  469: 
  470:         # a min_itemsize that creates a data_column2
  471:         _maybe_remove(store, "df")
  472:         store.append("df", df, data_columns=["B"], min_itemsize={"values": 200})
  473:         check_col("df", "B", 200)
  474:         check_col("df", "values_block_0", 200)
  475:         assert store.get_storer("df").data_columns == ["B"]
  476: 
  477:         # infer the .typ on subsequent appends
  478:         _maybe_remove(store, "df")
  479:         store.append("df", df[:5], min_itemsize=200)
  480:         store.append("df", df[5:], min_itemsize=200)
  481:         tm.assert_frame_equal(store["df"], df)
  482: 
  483:         # invalid min_itemsize keys
  484:         df = DataFrame(["foo", "foo", "foo", "barh", "barh", "barh"], columns=["A"])
  485:         _maybe_remove(store, "df")
  486:         msg = re.escape(
  487:             "min_itemsize has the key [foo] which is not an axis or data_column"
  488:         )
  489:         with pytest.raises(ValueError, match=msg):
  490:             store.append("df", df, min_itemsize={"foo": 20, "foobar": 20})
  491: 
  492: 
  493: def test_append_with_empty_string(setup_path):
  494:     with ensure_clean_store(setup_path) as store:
  495:         # with all empty strings (GH 12242)
  496:         df = DataFrame({"x": ["a", "b", "c", "d", "e", "f", ""]})
  497:         store.append("df", df[:-1], min_itemsize={"x": 1})
  498:         store.append("df", df[-1:], min_itemsize={"x": 1})
  499:         tm.assert_frame_equal(store.select("df"), df)
  500: 
  501: 
  502: def test_append_with_data_columns(setup_path):
  503:     with ensure_clean_store(setup_path) as store:
  504:         df = DataFrame(
  505:             np.random.default_rng(2).standard_normal((10, 4)),
  506:             columns=Index(list("ABCD"), dtype=object),
  507:             index=date_range("2000-01-01", periods=10, freq="B"),
  508:         )
  509:         df.iloc[0, df.columns.get_loc("B")] = 1.0
  510:         _maybe_remove(store, "df")
  511:         store.append("df", df[:2], data_columns=["B"])
  512:         store.append("df", df[2:])
  513:         tm.assert_frame_equal(store["df"], df)
  514: 
  515:         # check that we have indices created
  516:         assert store._handle.root.df.table.cols.index.is_indexed is True
  517:         assert store._handle.root.df.table.cols.B.is_indexed is True
  518: 
  519:         # data column searching
  520:         result = store.select("df", "B>0")
  521:         expected = df[df.B > 0]
  522:         tm.assert_frame_equal(result, expected)
  523: 
  524:         # data column searching (with an indexable and a data_columns)
  525:         result = store.select("df", "B>0 and index>df.index[3]")
  526:         df_new = df.reindex(index=df.index[4:])
  527:         expected = df_new[df_new.B > 0]
  528:         tm.assert_frame_equal(result, expected)
  529: 
  530:         # data column selection with a string data_column
  531:         df_new = df.copy()
  532:         df_new["string"] = "foo"
  533:         df_new.loc[df_new.index[1:4], "string"] = np.nan
  534:         df_new.loc[df_new.index[5:6], "string"] = "bar"
  535:         _maybe_remove(store, "df")
  536:         store.append("df", df_new, data_columns=["string"])
  537:         result = store.select("df", "string='foo'")
  538:         expected = df_new[df_new.string == "foo"]
  539:         tm.assert_frame_equal(result, expected)
  540: 
  541:         # using min_itemsize and a data column
  542:         def check_col(key, name, size):
  543:             assert (
  544:                 getattr(store.get_storer(key).table.description, name).itemsize == size
  545:             )
  546: 
  547:     with ensure_clean_store(setup_path) as store:
  548:         _maybe_remove(store, "df")
  549:         store.append("df", df_new, data_columns=["string"], min_itemsize={"string": 30})
  550:         check_col("df", "string", 30)
  551:         _maybe_remove(store, "df")
  552:         store.append("df", df_new, data_columns=["string"], min_itemsize=30)
  553:         check_col("df", "string", 30)
  554:         _maybe_remove(store, "df")
  555:         store.append("df", df_new, data_columns=["string"], min_itemsize={"values": 30})
  556:         check_col("df", "string", 30)
  557: 
  558:     with ensure_clean_store(setup_path) as store:
  559:         df_new["string2"] = "foobarbah"
  560:         df_new["string_block1"] = "foobarbah1"
  561:         df_new["string_block2"] = "foobarbah2"
  562:         _maybe_remove(store, "df")
  563:         store.append(
  564:             "df",
  565:             df_new,
  566:             data_columns=["string", "string2"],
  567:             min_itemsize={"string": 30, "string2": 40, "values": 50},
  568:         )
  569:         check_col("df", "string", 30)
  570:         check_col("df", "string2", 40)
  571:         check_col("df", "values_block_1", 50)
  572: 
  573:     with ensure_clean_store(setup_path) as store:
  574:         # multiple data columns
  575:         df_new = df.copy()
  576:         df_new.iloc[0, df_new.columns.get_loc("A")] = 1.0
  577:         df_new.iloc[0, df_new.columns.get_loc("B")] = -1.0
  578:         df_new["string"] = "foo"
  579: 
  580:         sl = df_new.columns.get_loc("string")
  581:         df_new.iloc[1:4, sl] = np.nan
  582:         df_new.iloc[5:6, sl] = "bar"
  583: 
  584:         df_new["string2"] = "foo"
  585:         sl = df_new.columns.get_loc("string2")
  586:         df_new.iloc[2:5, sl] = np.nan
  587:         df_new.iloc[7:8, sl] = "bar"
  588:         _maybe_remove(store, "df")
  589:         store.append("df", df_new, data_columns=["A", "B", "string", "string2"])
  590:         result = store.select("df", "string='foo' and string2='foo' and A>0 and B<0")
  591:         expected = df_new[
  592:             (df_new.string == "foo")
  593:             & (df_new.string2 == "foo")
  594:             & (df_new.A > 0)
  595:             & (df_new.B < 0)
  596:         ]
  597:         tm.assert_frame_equal(result, expected, check_freq=False)
  598:         # FIXME: 2020-05-07 freq check randomly fails in the CI
  599: 
  600:         # yield an empty frame
  601:         result = store.select("df", "string='foo' and string2='cool'")
  602:         expected = df_new[(df_new.string == "foo") & (df_new.string2 == "cool")]
  603:         tm.assert_frame_equal(result, expected)
  604: 
  605:     with ensure_clean_store(setup_path) as store:
  606:         # doc example
  607:         df_dc = df.copy()
  608:         df_dc["string"] = "foo"
  609:         df_dc.loc[df_dc.index[4:6], "string"] = np.nan
  610:         df_dc.loc[df_dc.index[7:9], "string"] = "bar"
  611:         df_dc["string2"] = "cool"
  612:         df_dc["datetime"] = Timestamp("20010102").as_unit("ns")
  613:         df_dc.loc[df_dc.index[3:5], ["A", "B", "datetime"]] = np.nan
  614: 
  615:         _maybe_remove(store, "df_dc")
  616:         store.append(
  617:             "df_dc", df_dc, data_columns=["B", "C", "string", "string2", "datetime"]
  618:         )
  619:         result = store.select("df_dc", "B>0")
  620: 
  621:         expected = df_dc[df_dc.B > 0]
  622:         tm.assert_frame_equal(result, expected)
  623: 
  624:         result = store.select("df_dc", ["B > 0", "C > 0", "string == foo"])
  625:         expected = df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == "foo")]
  626:         tm.assert_frame_equal(result, expected, check_freq=False)
  627:         # FIXME: 2020-12-07 intermittent build failures here with freq of
  628:         #  None instead of BDay(4)
  629: 
  630:     with ensure_clean_store(setup_path) as store:
  631:         # doc example part 2
  632: 
  633:         index = date_range("1/1/2000", periods=8)
  634:         df_dc = DataFrame(
  635:             np.random.default_rng(2).standard_normal((8, 3)),
  636:             index=index,
  637:             columns=["A", "B", "C"],
  638:         )
  639:         df_dc["string"] = "foo"
  640:         df_dc.loc[df_dc.index[4:6], "string"] = np.nan
  641:         df_dc.loc[df_dc.index[7:9], "string"] = "bar"
  642:         df_dc[["B", "C"]] = df_dc[["B", "C"]].abs()
  643:         df_dc["string2"] = "cool"
  644: 
  645:         # on-disk operations
  646:         store.append("df_dc", df_dc, data_columns=["B", "C", "string", "string2"])
  647: 
  648:         result = store.select("df_dc", "B>0")
  649:         expected = df_dc[df_dc.B > 0]
  650:         tm.assert_frame_equal(result, expected)
  651: 
  652:         result = store.select("df_dc", ["B > 0", "C > 0", 'string == "foo"'])
  653:         expected = df_dc[(df_dc.B > 0) & (df_dc.C > 0) & (df_dc.string == "foo")]
  654:         tm.assert_frame_equal(result, expected)
  655: 
  656: 
  657: def test_append_hierarchical(tmp_path, setup_path, multiindex_dataframe_random_data):
  658:     df = multiindex_dataframe_random_data
  659:     df.columns.name = None
  660: 
  661:     with ensure_clean_store(setup_path) as store:
  662:         store.append("mi", df)
  663:         result = store.select("mi")
  664:         tm.assert_frame_equal(result, df)
  665: 
  666:         # GH 3748
  667:         result = store.select("mi", columns=["A", "B"])
  668:         expected = df.reindex(columns=["A", "B"])
  669:         tm.assert_frame_equal(result, expected)
  670: 
  671:     path = tmp_path / "test.hdf"
  672:     df.to_hdf(path, key="df", format="table")
  673:     result = read_hdf(path, "df", columns=["A", "B"])
  674:     expected = df.reindex(columns=["A", "B"])
  675:     tm.assert_frame_equal(result, expected)
  676: 
  677: 
  678: def test_append_misc(setup_path):
  679:     with ensure_clean_store(setup_path) as store:
  680:         df = DataFrame(
  681:             1.1 * np.arange(120).reshape((30, 4)),
  682:             columns=Index(list("ABCD"), dtype=object),
  683:             index=Index([f"i-{i}" for i in range(30)], dtype=object),
  684:         )
  685:         store.append("df", df, chunksize=1)
  686:         result = store.select("df")
  687:         tm.assert_frame_equal(result, df)
  688: 
  689:         store.append("df1", df, expectedrows=10)
  690:         result = store.select("df1")
  691:         tm.assert_frame_equal(result, df)
  692: 
  693: 
  694: @pytest.mark.parametrize("chunksize", [10, 200, 1000])
  695: def test_append_misc_chunksize(setup_path, chunksize):
  696:     # more chunksize in append tests
  697:     df = DataFrame(
  698:         1.1 * np.arange(120).reshape((30, 4)),
  699:         columns=Index(list("ABCD"), dtype=object),
  700:         index=Index([f"i-{i}" for i in range(30)], dtype=object),
  701:     )
  702:     df["string"] = "foo"
  703:     df["float322"] = 1.0
  704:     df["float322"] = df["float322"].astype("float32")
  705:     df["bool"] = df["float322"] > 0
  706:     df["time1"] = Timestamp("20130101").as_unit("ns")
  707:     df["time2"] = Timestamp("20130102").as_unit("ns")
  708:     with ensure_clean_store(setup_path, mode="w") as store:
  709:         store.append("obj", df, chunksize=chunksize)
  710:         result = store.select("obj")
  711:         tm.assert_frame_equal(result, df)
  712: 
  713: 
  714: def test_append_misc_empty_frame(setup_path):
  715:     # empty frame, GH4273
  716:     with ensure_clean_store(setup_path) as store:
  717:         # 0 len
  718:         df_empty = DataFrame(columns=list("ABC"))
  719:         store.append("df", df_empty)
  720:         with pytest.raises(KeyError, match="'No object named df in the file'"):
  721:             store.select("df")
  722: 
  723:         # repeated append of 0/non-zero frames
  724:         df = DataFrame(np.random.default_rng(2).random((10, 3)), columns=list("ABC"))
  725:         store.append("df", df)
  726:         tm.assert_frame_equal(store.select("df"), df)
  727:         store.append("df", df_empty)
  728:         tm.assert_frame_equal(store.select("df"), df)
  729: 
  730:         # store
  731:         df = DataFrame(columns=list("ABC"))
  732:         store.put("df2", df)
  733:         tm.assert_frame_equal(store.select("df2"), df)
  734: 
  735: 
  736: # TODO(ArrayManager) currently we rely on falling back to BlockManager, but
  737: # the conversion from AM->BM converts the invalid object dtype column into
  738: # a datetime64 column no longer raising an error
  739: @td.skip_array_manager_not_yet_implemented
  740: def test_append_raise(setup_path):
  741:     with ensure_clean_store(setup_path) as store:
  742:         # test append with invalid input to get good error messages
  743: 
  744:         # list in column
  745:         df = DataFrame(
  746:             1.1 * np.arange(120).reshape((30, 4)),
  747:             columns=Index(list("ABCD"), dtype=object),
  748:             index=Index([f"i-{i}" for i in range(30)], dtype=object),
  749:         )
  750:         df["invalid"] = [["a"]] * len(df)
  751:         assert df.dtypes["invalid"] == np.object_
  752:         msg = re.escape(
  753:             """Cannot serialize the column [invalid]
  754: because its data contents are not [string] but [mixed] object dtype"""
  755:         )
  756:         with pytest.raises(TypeError, match=msg):
  757:             store.append("df", df)
  758: 
  759:         # multiple invalid columns
  760:         df["invalid2"] = [["a"]] * len(df)
  761:         df["invalid3"] = [["a"]] * len(df)
  762:         with pytest.raises(TypeError, match=msg):
  763:             store.append("df", df)
  764: 
  765:         # datetime with embedded nans as object
  766:         df = DataFrame(
  767:             1.1 * np.arange(120).reshape((30, 4)),
  768:             columns=Index(list("ABCD"), dtype=object),
  769:             index=Index([f"i-{i}" for i in range(30)], dtype=object),
  770:         )
  771:         s = Series(datetime.datetime(2001, 1, 2), index=df.index)
  772:         s = s.astype(object)
  773:         s[0:5] = np.nan
  774:         df["invalid"] = s
  775:         assert df.dtypes["invalid"] == np.object_
  776:         msg = "too many timezones in this block, create separate data columns"
  777:         with pytest.raises(TypeError, match=msg):
  778:             store.append("df", df)
  779: 
  780:         # directly ndarray
  781:         msg = "value must be None, Series, or DataFrame"
  782:         with pytest.raises(TypeError, match=msg):
  783:             store.append("df", np.arange(10))
  784: 
  785:         # series directly
  786:         msg = re.escape(
  787:             "cannot properly create the storer for: "
  788:             "[group->df,value-><class 'pandas.core.series.Series'>]"
  789:         )
  790:         with pytest.raises(TypeError, match=msg):
  791:             store.append("df", Series(np.arange(10)))
  792: 
  793:         # appending an incompatible table
  794:         df = DataFrame(
  795:             1.1 * np.arange(120).reshape((30, 4)),
  796:             columns=Index(list("ABCD"), dtype=object),
  797:             index=Index([f"i-{i}" for i in range(30)], dtype=object),
  798:         )
  799:         store.append("df", df)
  800: 
  801:         df["foo"] = "foo"
  802:         msg = re.escape(
  803:             "invalid combination of [non_index_axes] on appending data "
  804:             "[(1, ['A', 'B', 'C', 'D', 'foo'])] vs current table "
  805:             "[(1, ['A', 'B', 'C', 'D'])]"
  806:         )
  807:         with pytest.raises(ValueError, match=msg):
  808:             store.append("df", df)
  809: 
  810:         # incompatible type (GH 41897)
  811:         _maybe_remove(store, "df")
  812:         df["foo"] = Timestamp("20130101")
  813:         store.append("df", df)
  814:         df["foo"] = "bar"
  815:         msg = re.escape(
  816:             "invalid combination of [values_axes] on appending data "
  817:             "[name->values_block_1,cname->values_block_1,"
  818:             "dtype->bytes24,kind->string,shape->(1, 30)] "
  819:             "vs current table "
  820:             "[name->values_block_1,cname->values_block_1,"
  821:             "dtype->datetime64[s],kind->datetime64[s],shape->None]"
  822:         )
  823:         with pytest.raises(ValueError, match=msg):
  824:             store.append("df", df)
  825: 
  826: 
  827: def test_append_with_timedelta(setup_path):
  828:     # GH 3577
  829:     # append timedelta
  830: 
  831:     ts = Timestamp("20130101").as_unit("ns")
  832:     df = DataFrame(
  833:         {
  834:             "A": ts,
  835:             "B": [ts + timedelta(days=i, seconds=10) for i in range(10)],
  836:         }
  837:     )
  838:     df["C"] = df["A"] - df["B"]
  839:     df.loc[3:5, "C"] = np.nan
  840: 
  841:     with ensure_clean_store(setup_path) as store:
  842:         # table
  843:         _maybe_remove(store, "df")
  844:         store.append("df", df, data_columns=True)
  845:         result = store.select("df")
  846:         tm.assert_frame_equal(result, df)
  847: 
  848:         result = store.select("df", where="C<100000")
  849:         tm.assert_frame_equal(result, df)
  850: 
  851:         result = store.select("df", where="C<pd.Timedelta('-3D')")
  852:         tm.assert_frame_equal(result, df.iloc[3:])
  853: 
  854:         result = store.select("df", "C<'-3D'")
  855:         tm.assert_frame_equal(result, df.iloc[3:])
  856: 
  857:         # a bit hacky here as we don't really deal with the NaT properly
  858: 
  859:         result = store.select("df", "C<'-500000s'")
  860:         result = result.dropna(subset=["C"])
  861:         tm.assert_frame_equal(result, df.iloc[6:])
  862: 
  863:         result = store.select("df", "C<'-3.5D'")
  864:         result = result.iloc[1:]
  865:         tm.assert_frame_equal(result, df.iloc[4:])
  866: 
  867:         # fixed
  868:         _maybe_remove(store, "df2")
  869:         store.put("df2", df)
  870:         result = store.select("df2")
  871:         tm.assert_frame_equal(result, df)
  872: 
  873: 
  874: def test_append_to_multiple(setup_path):
  875:     df1 = DataFrame(
  876:         np.random.default_rng(2).standard_normal((10, 4)),
  877:         columns=Index(list("ABCD"), dtype=object),
  878:         index=date_range("2000-01-01", periods=10, freq="B"),
  879:     )
  880:     df2 = df1.copy().rename(columns="{}_2".format)
  881:     df2["foo"] = "bar"
  882:     df = concat([df1, df2], axis=1)
  883: 
  884:     with ensure_clean_store(setup_path) as store:
  885:         # exceptions
  886:         msg = "append_to_multiple requires a selector that is in passed dict"
  887:         with pytest.raises(ValueError, match=msg):
  888:             store.append_to_multiple(
  889:                 {"df1": ["A", "B"], "df2": None}, df, selector="df3"
  890:             )
  891: 
  892:         with pytest.raises(ValueError, match=msg):
  893:             store.append_to_multiple({"df1": None, "df2": None}, df, selector="df3")
  894: 
  895:         msg = (
  896:             "append_to_multiple must have a dictionary specified as the way to "
  897:             "split the value"
  898:         )
  899:         with pytest.raises(ValueError, match=msg):
  900:             store.append_to_multiple("df1", df, "df1")
  901: 
  902:         # regular operation
  903:         store.append_to_multiple({"df1": ["A", "B"], "df2": None}, df, selector="df1")
  904:         result = store.select_as_multiple(
  905:             ["df1", "df2"], where=["A>0", "B>0"], selector="df1"
  906:         )
  907:         expected = df[(df.A > 0) & (df.B > 0)]
  908:         tm.assert_frame_equal(result, expected)
  909: 
  910: 
  911: def test_append_to_multiple_dropna(setup_path):
  912:     df1 = DataFrame(
  913:         np.random.default_rng(2).standard_normal((10, 4)),
  914:         columns=Index(list("ABCD"), dtype=object),
  915:         index=date_range("2000-01-01", periods=10, freq="B"),
  916:     )
  917:     df2 = DataFrame(
  918:         np.random.default_rng(2).standard_normal((10, 4)),
  919:         columns=Index(list("ABCD"), dtype=object),
  920:         index=date_range("2000-01-01", periods=10, freq="B"),
  921:     ).rename(columns="{}_2".format)
  922:     df1.iloc[1, df1.columns.get_indexer(["A", "B"])] = np.nan
  923:     df = concat([df1, df2], axis=1)
  924: 
  925:     with ensure_clean_store(setup_path) as store:
  926:         # dropna=True should guarantee rows are synchronized
  927:         store.append_to_multiple(
  928:             {"df1": ["A", "B"], "df2": None}, df, selector="df1", dropna=True
  929:         )
  930:         result = store.select_as_multiple(["df1", "df2"])
  931:         expected = df.dropna()
  932:         tm.assert_frame_equal(result, expected, check_index_type=True)
  933:         tm.assert_index_equal(store.select("df1").index, store.select("df2").index)
  934: 
  935: 
  936: def test_append_to_multiple_dropna_false(setup_path):
  937:     df1 = DataFrame(
  938:         np.random.default_rng(2).standard_normal((10, 4)),
  939:         columns=Index(list("ABCD"), dtype=object),
  940:         index=date_range("2000-01-01", periods=10, freq="B"),
  941:     )
  942:     df2 = df1.copy().rename(columns="{}_2".format)
  943:     df1.iloc[1, df1.columns.get_indexer(["A", "B"])] = np.nan
  944:     df = concat([df1, df2], axis=1)
  945: 
  946:     with ensure_clean_store(setup_path) as store, pd.option_context(
  947:         "io.hdf.dropna_table", True
  948:     ):
  949:         # dropna=False shouldn't synchronize row indexes
  950:         store.append_to_multiple(
  951:             {"df1a": ["A", "B"], "df2a": None}, df, selector="df1a", dropna=False
  952:         )
  953: 
  954:         msg = "all tables must have exactly the same nrows!"
  955:         with pytest.raises(ValueError, match=msg):
  956:             store.select_as_multiple(["df1a", "df2a"])
  957: 
  958:         assert not store.select("df1a").index.equals(store.select("df2a").index)
  959: 
  960: 
  961: def test_append_to_multiple_min_itemsize(setup_path):
  962:     # GH 11238
  963:     df = DataFrame(
  964:         {
  965:             "IX": np.arange(1, 21),
  966:             "Num": np.arange(1, 21),
  967:             "BigNum": np.arange(1, 21) * 88,
  968:             "Str": ["a" for _ in range(20)],
  969:             "LongStr": ["abcde" for _ in range(20)],
  970:         }
  971:     )
  972:     expected = df.iloc[[0]]
  973: 
  974:     with ensure_clean_store(setup_path) as store:
  975:         store.append_to_multiple(
  976:             {
  977:                 "index": ["IX"],
  978:                 "nums": ["Num", "BigNum"],
  979:                 "strs": ["Str", "LongStr"],
  980:             },
  981:             df.iloc[[0]],
  982:             "index",
  983:             min_itemsize={"Str": 10, "LongStr": 100, "Num": 2},
  984:         )
  985:         result = store.select_as_multiple(["index", "nums", "strs"])
  986:         tm.assert_frame_equal(result, expected, check_index_type=True)
