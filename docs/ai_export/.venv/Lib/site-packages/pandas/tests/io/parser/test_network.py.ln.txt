    1: """
    2: Tests parsers ability to read and parse non-local files
    3: and hence require a network connection to be read.
    4: """
    5: from io import BytesIO
    6: import logging
    7: import re
    8: 
    9: import numpy as np
   10: import pytest
   11: 
   12: import pandas.util._test_decorators as td
   13: 
   14: from pandas import DataFrame
   15: import pandas._testing as tm
   16: 
   17: from pandas.io.feather_format import read_feather
   18: from pandas.io.parsers import read_csv
   19: 
   20: pytestmark = pytest.mark.filterwarnings(
   21:     "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   22: )
   23: 
   24: 
   25: @pytest.mark.network
   26: @pytest.mark.single_cpu
   27: @pytest.mark.parametrize("mode", ["explicit", "infer"])
   28: @pytest.mark.parametrize("engine", ["python", "c"])
   29: def test_compressed_urls(
   30:     httpserver,
   31:     datapath,
   32:     salaries_table,
   33:     mode,
   34:     engine,
   35:     compression_only,
   36:     compression_to_extension,
   37: ):
   38:     # test reading compressed urls with various engines and
   39:     # extension inference
   40:     if compression_only == "tar":
   41:         pytest.skip("TODO: Add tar salaraies.csv to pandas/io/parsers/data")
   42: 
   43:     extension = compression_to_extension[compression_only]
   44:     with open(datapath("io", "parser", "data", "salaries.csv" + extension), "rb") as f:
   45:         httpserver.serve_content(content=f.read())
   46: 
   47:     url = httpserver.url + "/salaries.csv" + extension
   48: 
   49:     if mode != "explicit":
   50:         compression_only = mode
   51: 
   52:     url_table = read_csv(url, sep="\t", compression=compression_only, engine=engine)
   53:     tm.assert_frame_equal(url_table, salaries_table)
   54: 
   55: 
   56: @pytest.mark.network
   57: @pytest.mark.single_cpu
   58: def test_url_encoding_csv(httpserver, datapath):
   59:     """
   60:     read_csv should honor the requested encoding for URLs.
   61: 
   62:     GH 10424
   63:     """
   64:     with open(datapath("io", "parser", "data", "unicode_series.csv"), "rb") as f:
   65:         httpserver.serve_content(content=f.read())
   66:         df = read_csv(httpserver.url, encoding="latin-1", header=None)
   67:     assert df.loc[15, 1] == "ГЃ kГ¶ldum klaka (Cold Fever) (1994)"
   68: 
   69: 
   70: @pytest.fixture
   71: def tips_df(datapath):
   72:     """DataFrame with the tips dataset."""
   73:     return read_csv(datapath("io", "data", "csv", "tips.csv"))
   74: 
   75: 
   76: @pytest.mark.single_cpu
   77: @pytest.mark.usefixtures("s3_resource")
   78: @td.skip_if_not_us_locale()
   79: class TestS3:
   80:     def test_parse_public_s3_bucket(self, s3_public_bucket_with_data, tips_df, s3so):
   81:         # more of an integration test due to the not-public contents portion
   82:         # can probably mock this though.
   83:         pytest.importorskip("s3fs")
   84:         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
   85:             df = read_csv(
   86:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
   87:                 compression=comp,
   88:                 storage_options=s3so,
   89:             )
   90:             assert isinstance(df, DataFrame)
   91:             assert not df.empty
   92:             tm.assert_frame_equal(df, tips_df)
   93: 
   94:     def test_parse_private_s3_bucket(self, s3_private_bucket_with_data, tips_df, s3so):
   95:         # Read public file from bucket with not-public contents
   96:         pytest.importorskip("s3fs")
   97:         df = read_csv(
   98:             f"s3://{s3_private_bucket_with_data.name}/tips.csv", storage_options=s3so
   99:         )
  100:         assert isinstance(df, DataFrame)
  101:         assert not df.empty
  102:         tm.assert_frame_equal(df, tips_df)
  103: 
  104:     def test_parse_public_s3n_bucket(self, s3_public_bucket_with_data, tips_df, s3so):
  105:         # Read from AWS s3 as "s3n" URL
  106:         df = read_csv(
  107:             f"s3n://{s3_public_bucket_with_data.name}/tips.csv",
  108:             nrows=10,
  109:             storage_options=s3so,
  110:         )
  111:         assert isinstance(df, DataFrame)
  112:         assert not df.empty
  113:         tm.assert_frame_equal(tips_df.iloc[:10], df)
  114: 
  115:     def test_parse_public_s3a_bucket(self, s3_public_bucket_with_data, tips_df, s3so):
  116:         # Read from AWS s3 as "s3a" URL
  117:         df = read_csv(
  118:             f"s3a://{s3_public_bucket_with_data.name}/tips.csv",
  119:             nrows=10,
  120:             storage_options=s3so,
  121:         )
  122:         assert isinstance(df, DataFrame)
  123:         assert not df.empty
  124:         tm.assert_frame_equal(tips_df.iloc[:10], df)
  125: 
  126:     def test_parse_public_s3_bucket_nrows(
  127:         self, s3_public_bucket_with_data, tips_df, s3so
  128:     ):
  129:         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
  130:             df = read_csv(
  131:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
  132:                 nrows=10,
  133:                 compression=comp,
  134:                 storage_options=s3so,
  135:             )
  136:             assert isinstance(df, DataFrame)
  137:             assert not df.empty
  138:             tm.assert_frame_equal(tips_df.iloc[:10], df)
  139: 
  140:     def test_parse_public_s3_bucket_chunked(
  141:         self, s3_public_bucket_with_data, tips_df, s3so
  142:     ):
  143:         # Read with a chunksize
  144:         chunksize = 5
  145:         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
  146:             with read_csv(
  147:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
  148:                 chunksize=chunksize,
  149:                 compression=comp,
  150:                 storage_options=s3so,
  151:             ) as df_reader:
  152:                 assert df_reader.chunksize == chunksize
  153:                 for i_chunk in [0, 1, 2]:
  154:                     # Read a couple of chunks and make sure we see them
  155:                     # properly.
  156:                     df = df_reader.get_chunk()
  157:                     assert isinstance(df, DataFrame)
  158:                     assert not df.empty
  159:                     true_df = tips_df.iloc[
  160:                         chunksize * i_chunk : chunksize * (i_chunk + 1)
  161:                     ]
  162:                     tm.assert_frame_equal(true_df, df)
  163: 
  164:     def test_parse_public_s3_bucket_chunked_python(
  165:         self, s3_public_bucket_with_data, tips_df, s3so
  166:     ):
  167:         # Read with a chunksize using the Python parser
  168:         chunksize = 5
  169:         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
  170:             with read_csv(
  171:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
  172:                 chunksize=chunksize,
  173:                 compression=comp,
  174:                 engine="python",
  175:                 storage_options=s3so,
  176:             ) as df_reader:
  177:                 assert df_reader.chunksize == chunksize
  178:                 for i_chunk in [0, 1, 2]:
  179:                     # Read a couple of chunks and make sure we see them properly.
  180:                     df = df_reader.get_chunk()
  181:                     assert isinstance(df, DataFrame)
  182:                     assert not df.empty
  183:                     true_df = tips_df.iloc[
  184:                         chunksize * i_chunk : chunksize * (i_chunk + 1)
  185:                     ]
  186:                     tm.assert_frame_equal(true_df, df)
  187: 
  188:     def test_parse_public_s3_bucket_python(
  189:         self, s3_public_bucket_with_data, tips_df, s3so
  190:     ):
  191:         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
  192:             df = read_csv(
  193:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
  194:                 engine="python",
  195:                 compression=comp,
  196:                 storage_options=s3so,
  197:             )
  198:             assert isinstance(df, DataFrame)
  199:             assert not df.empty
  200:             tm.assert_frame_equal(df, tips_df)
  201: 
  202:     def test_infer_s3_compression(self, s3_public_bucket_with_data, tips_df, s3so):
  203:         for ext in ["", ".gz", ".bz2"]:
  204:             df = read_csv(
  205:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
  206:                 engine="python",
  207:                 compression="infer",
  208:                 storage_options=s3so,
  209:             )
  210:             assert isinstance(df, DataFrame)
  211:             assert not df.empty
  212:             tm.assert_frame_equal(df, tips_df)
  213: 
  214:     def test_parse_public_s3_bucket_nrows_python(
  215:         self, s3_public_bucket_with_data, tips_df, s3so
  216:     ):
  217:         for ext, comp in [("", None), (".gz", "gzip"), (".bz2", "bz2")]:
  218:             df = read_csv(
  219:                 f"s3://{s3_public_bucket_with_data.name}/tips.csv" + ext,
  220:                 engine="python",
  221:                 nrows=10,
  222:                 compression=comp,
  223:                 storage_options=s3so,
  224:             )
  225:             assert isinstance(df, DataFrame)
  226:             assert not df.empty
  227:             tm.assert_frame_equal(tips_df.iloc[:10], df)
  228: 
  229:     def test_read_s3_fails(self, s3so):
  230:         msg = "The specified bucket does not exist"
  231:         with pytest.raises(OSError, match=msg):
  232:             read_csv("s3://nyqpug/asdf.csv", storage_options=s3so)
  233: 
  234:     def test_read_s3_fails_private(self, s3_private_bucket, s3so):
  235:         msg = "The specified bucket does not exist"
  236:         # Receive a permission error when trying to read a private bucket.
  237:         # It's irrelevant here that this isn't actually a table.
  238:         with pytest.raises(OSError, match=msg):
  239:             read_csv(f"s3://{s3_private_bucket.name}/file.csv")
  240: 
  241:     @pytest.mark.xfail(reason="GH#39155 s3fs upgrade", strict=False)
  242:     def test_write_s3_csv_fails(self, tips_df, s3so):
  243:         # GH 32486
  244:         # Attempting to write to an invalid S3 path should raise
  245:         import botocore
  246: 
  247:         # GH 34087
  248:         # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html
  249:         # Catch a ClientError since AWS Service Errors are defined dynamically
  250:         error = (FileNotFoundError, botocore.exceptions.ClientError)
  251: 
  252:         with pytest.raises(error, match="The specified bucket does not exist"):
  253:             tips_df.to_csv(
  254:                 "s3://an_s3_bucket_data_doesnt_exit/not_real.csv", storage_options=s3so
  255:             )
  256: 
  257:     @pytest.mark.xfail(reason="GH#39155 s3fs upgrade", strict=False)
  258:     def test_write_s3_parquet_fails(self, tips_df, s3so):
  259:         # GH 27679
  260:         # Attempting to write to an invalid S3 path should raise
  261:         pytest.importorskip("pyarrow")
  262:         import botocore
  263: 
  264:         # GH 34087
  265:         # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/error-handling.html
  266:         # Catch a ClientError since AWS Service Errors are defined dynamically
  267:         error = (FileNotFoundError, botocore.exceptions.ClientError)
  268: 
  269:         with pytest.raises(error, match="The specified bucket does not exist"):
  270:             tips_df.to_parquet(
  271:                 "s3://an_s3_bucket_data_doesnt_exit/not_real.parquet",
  272:                 storage_options=s3so,
  273:             )
  274: 
  275:     @pytest.mark.single_cpu
  276:     def test_read_csv_handles_boto_s3_object(
  277:         self, s3_public_bucket_with_data, tips_file
  278:     ):
  279:         # see gh-16135
  280: 
  281:         s3_object = s3_public_bucket_with_data.Object("tips.csv")
  282: 
  283:         with BytesIO(s3_object.get()["Body"].read()) as buffer:
  284:             result = read_csv(buffer, encoding="utf8")
  285:         assert isinstance(result, DataFrame)
  286:         assert not result.empty
  287: 
  288:         expected = read_csv(tips_file)
  289:         tm.assert_frame_equal(result, expected)
  290: 
  291:     @pytest.mark.single_cpu
  292:     def test_read_csv_chunked_download(self, s3_public_bucket, caplog, s3so):
  293:         # 8 MB, S3FS uses 5MB chunks
  294:         df = DataFrame(np.zeros((100000, 4)), columns=list("abcd"))
  295:         with BytesIO(df.to_csv().encode("utf-8")) as buf:
  296:             s3_public_bucket.put_object(Key="large-file.csv", Body=buf)
  297:             uri = f"{s3_public_bucket.name}/large-file.csv"
  298:             match_re = re.compile(rf"^Fetch: {uri}, 0-(?P<stop>\d+)$")
  299:             with caplog.at_level(logging.DEBUG, logger="s3fs"):
  300:                 read_csv(
  301:                     f"s3://{uri}",
  302:                     nrows=5,
  303:                     storage_options=s3so,
  304:                 )
  305:                 for log in caplog.messages:
  306:                     if match := re.match(match_re, log):
  307:                         # Less than 8 MB
  308:                         assert int(match.group("stop")) < 8000000
  309: 
  310:     def test_read_s3_with_hash_in_key(self, s3_public_bucket_with_data, tips_df, s3so):
  311:         # GH 25945
  312:         result = read_csv(
  313:             f"s3://{s3_public_bucket_with_data.name}/tips#1.csv", storage_options=s3so
  314:         )
  315:         tm.assert_frame_equal(tips_df, result)
  316: 
  317:     def test_read_feather_s3_file_path(
  318:         self, s3_public_bucket_with_data, feather_file, s3so
  319:     ):
  320:         # GH 29055
  321:         pytest.importorskip("pyarrow")
  322:         expected = read_feather(feather_file)
  323:         res = read_feather(
  324:             f"s3://{s3_public_bucket_with_data.name}/simple_dataset.feather",
  325:             storage_options=s3so,
  326:         )
  327:         tm.assert_frame_equal(expected, res)
