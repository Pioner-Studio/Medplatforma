    1: import contextlib
    2: from datetime import datetime
    3: import io
    4: import os
    5: from pathlib import Path
    6: 
    7: import numpy as np
    8: import pytest
    9: 
   10: from pandas.compat import IS64
   11: from pandas.errors import EmptyDataError
   12: import pandas.util._test_decorators as td
   13: 
   14: import pandas as pd
   15: import pandas._testing as tm
   16: 
   17: from pandas.io.sas.sas7bdat import SAS7BDATReader
   18: 
   19: 
   20: @pytest.fixture
   21: def dirpath(datapath):
   22:     return datapath("io", "sas", "data")
   23: 
   24: 
   25: @pytest.fixture(params=[(1, range(1, 16)), (2, [16])])
   26: def data_test_ix(request, dirpath):
   27:     i, test_ix = request.param
   28:     fname = os.path.join(dirpath, f"test_sas7bdat_{i}.csv")
   29:     df = pd.read_csv(fname)
   30:     epoch = datetime(1960, 1, 1)
   31:     t1 = pd.to_timedelta(df["Column4"], unit="d")
   32:     df["Column4"] = (epoch + t1).astype("M8[s]")
   33:     t2 = pd.to_timedelta(df["Column12"], unit="d")
   34:     df["Column12"] = (epoch + t2).astype("M8[s]")
   35:     for k in range(df.shape[1]):
   36:         col = df.iloc[:, k]
   37:         if col.dtype == np.int64:
   38:             df.isetitem(k, df.iloc[:, k].astype(np.float64))
   39:     return df, test_ix
   40: 
   41: 
   42: # https://github.com/cython/cython/issues/1720
   43: class TestSAS7BDAT:
   44:     @pytest.mark.slow
   45:     def test_from_file(self, dirpath, data_test_ix):
   46:         expected, test_ix = data_test_ix
   47:         for k in test_ix:
   48:             fname = os.path.join(dirpath, f"test{k}.sas7bdat")
   49:             df = pd.read_sas(fname, encoding="utf-8")
   50:             tm.assert_frame_equal(df, expected)
   51: 
   52:     @pytest.mark.slow
   53:     def test_from_buffer(self, dirpath, data_test_ix):
   54:         expected, test_ix = data_test_ix
   55:         for k in test_ix:
   56:             fname = os.path.join(dirpath, f"test{k}.sas7bdat")
   57:             with open(fname, "rb") as f:
   58:                 byts = f.read()
   59:             buf = io.BytesIO(byts)
   60:             with pd.read_sas(
   61:                 buf, format="sas7bdat", iterator=True, encoding="utf-8"
   62:             ) as rdr:
   63:                 df = rdr.read()
   64:             tm.assert_frame_equal(df, expected)
   65: 
   66:     @pytest.mark.slow
   67:     def test_from_iterator(self, dirpath, data_test_ix):
   68:         expected, test_ix = data_test_ix
   69:         for k in test_ix:
   70:             fname = os.path.join(dirpath, f"test{k}.sas7bdat")
   71:             with pd.read_sas(fname, iterator=True, encoding="utf-8") as rdr:
   72:                 df = rdr.read(2)
   73:                 tm.assert_frame_equal(df, expected.iloc[0:2, :])
   74:                 df = rdr.read(3)
   75:                 tm.assert_frame_equal(df, expected.iloc[2:5, :])
   76: 
   77:     @pytest.mark.slow
   78:     def test_path_pathlib(self, dirpath, data_test_ix):
   79:         expected, test_ix = data_test_ix
   80:         for k in test_ix:
   81:             fname = Path(os.path.join(dirpath, f"test{k}.sas7bdat"))
   82:             df = pd.read_sas(fname, encoding="utf-8")
   83:             tm.assert_frame_equal(df, expected)
   84: 
   85:     @td.skip_if_no("py.path")
   86:     @pytest.mark.slow
   87:     def test_path_localpath(self, dirpath, data_test_ix):
   88:         from py.path import local as LocalPath
   89: 
   90:         expected, test_ix = data_test_ix
   91:         for k in test_ix:
   92:             fname = LocalPath(os.path.join(dirpath, f"test{k}.sas7bdat"))
   93:             df = pd.read_sas(fname, encoding="utf-8")
   94:             tm.assert_frame_equal(df, expected)
   95: 
   96:     @pytest.mark.slow
   97:     @pytest.mark.parametrize("chunksize", (3, 5, 10, 11))
   98:     @pytest.mark.parametrize("k", range(1, 17))
   99:     def test_iterator_loop(self, dirpath, k, chunksize):
  100:         # github #13654
  101:         fname = os.path.join(dirpath, f"test{k}.sas7bdat")
  102:         with pd.read_sas(fname, chunksize=chunksize, encoding="utf-8") as rdr:
  103:             y = 0
  104:             for x in rdr:
  105:                 y += x.shape[0]
  106:         assert y == rdr.row_count
  107: 
  108:     def test_iterator_read_too_much(self, dirpath):
  109:         # github #14734
  110:         fname = os.path.join(dirpath, "test1.sas7bdat")
  111:         with pd.read_sas(
  112:             fname, format="sas7bdat", iterator=True, encoding="utf-8"
  113:         ) as rdr:
  114:             d1 = rdr.read(rdr.row_count + 20)
  115: 
  116:         with pd.read_sas(fname, iterator=True, encoding="utf-8") as rdr:
  117:             d2 = rdr.read(rdr.row_count + 20)
  118:         tm.assert_frame_equal(d1, d2)
  119: 
  120: 
  121: def test_encoding_options(datapath):
  122:     fname = datapath("io", "sas", "data", "test1.sas7bdat")
  123:     df1 = pd.read_sas(fname)
  124:     df2 = pd.read_sas(fname, encoding="utf-8")
  125:     for col in df1.columns:
  126:         try:
  127:             df1[col] = df1[col].str.decode("utf-8")
  128:         except AttributeError:
  129:             pass
  130:     tm.assert_frame_equal(df1, df2)
  131: 
  132:     with contextlib.closing(SAS7BDATReader(fname, convert_header_text=False)) as rdr:
  133:         df3 = rdr.read()
  134:     for x, y in zip(df1.columns, df3.columns):
  135:         assert x == y.decode()
  136: 
  137: 
  138: def test_encoding_infer(datapath):
  139:     fname = datapath("io", "sas", "data", "test1.sas7bdat")
  140: 
  141:     with pd.read_sas(fname, encoding="infer", iterator=True) as df1_reader:
  142:         # check: is encoding inferred correctly from file
  143:         assert df1_reader.inferred_encoding == "cp1252"
  144:         df1 = df1_reader.read()
  145: 
  146:     with pd.read_sas(fname, encoding="cp1252", iterator=True) as df2_reader:
  147:         df2 = df2_reader.read()
  148: 
  149:     # check: reader reads correct information
  150:     tm.assert_frame_equal(df1, df2)
  151: 
  152: 
  153: def test_productsales(datapath):
  154:     fname = datapath("io", "sas", "data", "productsales.sas7bdat")
  155:     df = pd.read_sas(fname, encoding="utf-8")
  156:     fname = datapath("io", "sas", "data", "productsales.csv")
  157:     df0 = pd.read_csv(fname, parse_dates=["MONTH"])
  158:     vn = ["ACTUAL", "PREDICT", "QUARTER", "YEAR"]
  159:     df0[vn] = df0[vn].astype(np.float64)
  160: 
  161:     df0["MONTH"] = df0["MONTH"].astype("M8[s]")
  162:     tm.assert_frame_equal(df, df0)
  163: 
  164: 
  165: def test_12659(datapath):
  166:     fname = datapath("io", "sas", "data", "test_12659.sas7bdat")
  167:     df = pd.read_sas(fname)
  168:     fname = datapath("io", "sas", "data", "test_12659.csv")
  169:     df0 = pd.read_csv(fname)
  170:     df0 = df0.astype(np.float64)
  171:     tm.assert_frame_equal(df, df0)
  172: 
  173: 
  174: def test_airline(datapath):
  175:     fname = datapath("io", "sas", "data", "airline.sas7bdat")
  176:     df = pd.read_sas(fname)
  177:     fname = datapath("io", "sas", "data", "airline.csv")
  178:     df0 = pd.read_csv(fname)
  179:     df0 = df0.astype(np.float64)
  180:     tm.assert_frame_equal(df, df0)
  181: 
  182: 
  183: def test_date_time(datapath):
  184:     # Support of different SAS date/datetime formats (PR #15871)
  185:     fname = datapath("io", "sas", "data", "datetime.sas7bdat")
  186:     df = pd.read_sas(fname)
  187:     fname = datapath("io", "sas", "data", "datetime.csv")
  188:     df0 = pd.read_csv(
  189:         fname, parse_dates=["Date1", "Date2", "DateTime", "DateTimeHi", "Taiw"]
  190:     )
  191:     # GH 19732: Timestamps imported from sas will incur floating point errors
  192:     # See GH#56014 for discussion of the correct "expected" results
  193:     #  We are really just testing that we are "close". This only seems to be
  194:     #  an issue near the implementation bounds.
  195: 
  196:     df[df.columns[3]] = df.iloc[:, 3].dt.round("us")
  197:     df0["Date1"] = df0["Date1"].astype("M8[s]")
  198:     df0["Date2"] = df0["Date2"].astype("M8[s]")
  199:     df0["DateTime"] = df0["DateTime"].astype("M8[ms]")
  200:     df0["Taiw"] = df0["Taiw"].astype("M8[s]")
  201: 
  202:     res = df0["DateTimeHi"].astype("M8[us]").dt.round("ms")
  203:     df0["DateTimeHi"] = res.astype("M8[ms]")
  204: 
  205:     if not IS64:
  206:         # No good reason for this, just what we get on the CI
  207:         df0.loc[0, "DateTimeHi"] += np.timedelta64(1, "ms")
  208:         df0.loc[[2, 3], "DateTimeHi"] -= np.timedelta64(1, "ms")
  209:     tm.assert_frame_equal(df, df0)
  210: 
  211: 
  212: @pytest.mark.parametrize("column", ["WGT", "CYL"])
  213: def test_compact_numerical_values(datapath, column):
  214:     # Regression test for #21616
  215:     fname = datapath("io", "sas", "data", "cars.sas7bdat")
  216:     df = pd.read_sas(fname, encoding="latin-1")
  217:     # The two columns CYL and WGT in cars.sas7bdat have column
  218:     # width < 8 and only contain integral values.
  219:     # Test that pandas doesn't corrupt the numbers by adding
  220:     # decimals.
  221:     result = df[column]
  222:     expected = df[column].round()
  223:     tm.assert_series_equal(result, expected, check_exact=True)
  224: 
  225: 
  226: def test_many_columns(datapath):
  227:     # Test for looking for column information in more places (PR #22628)
  228:     fname = datapath("io", "sas", "data", "many_columns.sas7bdat")
  229: 
  230:     df = pd.read_sas(fname, encoding="latin-1")
  231: 
  232:     fname = datapath("io", "sas", "data", "many_columns.csv")
  233:     df0 = pd.read_csv(fname, encoding="latin-1")
  234:     tm.assert_frame_equal(df, df0)
  235: 
  236: 
  237: def test_inconsistent_number_of_rows(datapath):
  238:     # Regression test for issue #16615. (PR #22628)
  239:     fname = datapath("io", "sas", "data", "load_log.sas7bdat")
  240:     df = pd.read_sas(fname, encoding="latin-1")
  241:     assert len(df) == 2097
  242: 
  243: 
  244: def test_zero_variables(datapath):
  245:     # Check if the SAS file has zero variables (PR #18184)
  246:     fname = datapath("io", "sas", "data", "zero_variables.sas7bdat")
  247:     with pytest.raises(EmptyDataError, match="No columns to parse from file"):
  248:         pd.read_sas(fname)
  249: 
  250: 
  251: def test_zero_rows(datapath):
  252:     # GH 18198
  253:     fname = datapath("io", "sas", "data", "zero_rows.sas7bdat")
  254:     result = pd.read_sas(fname)
  255:     expected = pd.DataFrame([{"char_field": "a", "num_field": 1.0}]).iloc[:0]
  256:     tm.assert_frame_equal(result, expected)
  257: 
  258: 
  259: def test_corrupt_read(datapath):
  260:     # We don't really care about the exact failure, the important thing is
  261:     # that the resource should be cleaned up afterwards (BUG #35566)
  262:     fname = datapath("io", "sas", "data", "corrupt.sas7bdat")
  263:     msg = "'SAS7BDATReader' object has no attribute 'row_count'"
  264:     with pytest.raises(AttributeError, match=msg):
  265:         pd.read_sas(fname)
  266: 
  267: 
  268: def test_max_sas_date(datapath):
  269:     # GH 20927
  270:     # NB. max datetime in SAS dataset is 31DEC9999:23:59:59.999
  271:     #    but this is read as 29DEC9999:23:59:59.998993 by a buggy
  272:     #    sas7bdat module
  273:     # See also GH#56014 for discussion of the correct "expected" results.
  274:     fname = datapath("io", "sas", "data", "max_sas_date.sas7bdat")
  275:     df = pd.read_sas(fname, encoding="iso-8859-1")
  276: 
  277:     expected = pd.DataFrame(
  278:         {
  279:             "text": ["max", "normal"],
  280:             "dt_as_float": [253717747199.999, 1880323199.999],
  281:             "dt_as_dt": np.array(
  282:                 [
  283:                     datetime(9999, 12, 29, 23, 59, 59, 999000),
  284:                     datetime(2019, 8, 1, 23, 59, 59, 999000),
  285:                 ],
  286:                 dtype="M8[ms]",
  287:             ),
  288:             "date_as_float": [2936547.0, 21762.0],
  289:             "date_as_date": np.array(
  290:                 [
  291:                     datetime(9999, 12, 29),
  292:                     datetime(2019, 8, 1),
  293:                 ],
  294:                 dtype="M8[s]",
  295:             ),
  296:         },
  297:         columns=["text", "dt_as_float", "dt_as_dt", "date_as_float", "date_as_date"],
  298:     )
  299: 
  300:     if not IS64:
  301:         # No good reason for this, just what we get on the CI
  302:         expected.loc[:, "dt_as_dt"] -= np.timedelta64(1, "ms")
  303: 
  304:     tm.assert_frame_equal(df, expected)
  305: 
  306: 
  307: def test_max_sas_date_iterator(datapath):
  308:     # GH 20927
  309:     # when called as an iterator, only those chunks with a date > pd.Timestamp.max
  310:     # are returned as datetime.datetime, if this happens that whole chunk is returned
  311:     # as datetime.datetime
  312:     col_order = ["text", "dt_as_float", "dt_as_dt", "date_as_float", "date_as_date"]
  313:     fname = datapath("io", "sas", "data", "max_sas_date.sas7bdat")
  314:     results = []
  315:     for df in pd.read_sas(fname, encoding="iso-8859-1", chunksize=1):
  316:         # GH 19732: Timestamps imported from sas will incur floating point errors
  317:         df.reset_index(inplace=True, drop=True)
  318:         results.append(df)
  319:     expected = [
  320:         pd.DataFrame(
  321:             {
  322:                 "text": ["max"],
  323:                 "dt_as_float": [253717747199.999],
  324:                 "dt_as_dt": np.array(
  325:                     [datetime(9999, 12, 29, 23, 59, 59, 999000)], dtype="M8[ms]"
  326:                 ),
  327:                 "date_as_float": [2936547.0],
  328:                 "date_as_date": np.array([datetime(9999, 12, 29)], dtype="M8[s]"),
  329:             },
  330:             columns=col_order,
  331:         ),
  332:         pd.DataFrame(
  333:             {
  334:                 "text": ["normal"],
  335:                 "dt_as_float": [1880323199.999],
  336:                 "dt_as_dt": np.array(["2019-08-01 23:59:59.999"], dtype="M8[ms]"),
  337:                 "date_as_float": [21762.0],
  338:                 "date_as_date": np.array(["2019-08-01"], dtype="M8[s]"),
  339:             },
  340:             columns=col_order,
  341:         ),
  342:     ]
  343:     if not IS64:
  344:         # No good reason for this, just what we get on the CI
  345:         expected[0].loc[0, "dt_as_dt"] -= np.timedelta64(1, "ms")
  346:         expected[1].loc[0, "dt_as_dt"] -= np.timedelta64(1, "ms")
  347: 
  348:     tm.assert_frame_equal(results[0], expected[0])
  349:     tm.assert_frame_equal(results[1], expected[1])
  350: 
  351: 
  352: def test_null_date(datapath):
  353:     fname = datapath("io", "sas", "data", "dates_null.sas7bdat")
  354:     df = pd.read_sas(fname, encoding="utf-8")
  355: 
  356:     expected = pd.DataFrame(
  357:         {
  358:             "datecol": np.array(
  359:                 [
  360:                     datetime(9999, 12, 29),
  361:                     np.datetime64("NaT"),
  362:                 ],
  363:                 dtype="M8[s]",
  364:             ),
  365:             "datetimecol": np.array(
  366:                 [
  367:                     datetime(9999, 12, 29, 23, 59, 59, 999000),
  368:                     np.datetime64("NaT"),
  369:                 ],
  370:                 dtype="M8[ms]",
  371:             ),
  372:         },
  373:     )
  374:     if not IS64:
  375:         # No good reason for this, just what we get on the CI
  376:         expected.loc[0, "datetimecol"] -= np.timedelta64(1, "ms")
  377:     tm.assert_frame_equal(df, expected)
  378: 
  379: 
  380: def test_meta2_page(datapath):
  381:     # GH 35545
  382:     fname = datapath("io", "sas", "data", "test_meta2_page.sas7bdat")
  383:     df = pd.read_sas(fname)
  384:     assert len(df) == 1000
  385: 
  386: 
  387: @pytest.mark.parametrize(
  388:     "test_file, override_offset, override_value, expected_msg",
  389:     [
  390:         ("test2.sas7bdat", 0x10000 + 55229, 0x80 | 0x0F, "Out of bounds"),
  391:         ("test2.sas7bdat", 0x10000 + 55229, 0x10, "unknown control byte"),
  392:         ("test3.sas7bdat", 118170, 184, "Out of bounds"),
  393:     ],
  394: )
  395: def test_rle_rdc_exceptions(
  396:     datapath, test_file, override_offset, override_value, expected_msg
  397: ):
  398:     """Errors in RLE/RDC decompression should propagate."""
  399:     with open(datapath("io", "sas", "data", test_file), "rb") as fd:
  400:         data = bytearray(fd.read())
  401:     data[override_offset] = override_value
  402:     with pytest.raises(Exception, match=expected_msg):
  403:         pd.read_sas(io.BytesIO(data), format="sas7bdat")
  404: 
  405: 
  406: def test_0x40_control_byte(datapath):
  407:     # GH 31243
  408:     fname = datapath("io", "sas", "data", "0x40controlbyte.sas7bdat")
  409:     df = pd.read_sas(fname, encoding="ascii")
  410:     fname = datapath("io", "sas", "data", "0x40controlbyte.csv")
  411:     df0 = pd.read_csv(fname, dtype="object")
  412:     tm.assert_frame_equal(df, df0)
  413: 
  414: 
  415: def test_0x00_control_byte(datapath):
  416:     # GH 47099
  417:     fname = datapath("io", "sas", "data", "0x00controlbyte.sas7bdat.bz2")
  418:     df = next(pd.read_sas(fname, chunksize=11_000))
  419:     assert df.shape == (11_000, 20)
