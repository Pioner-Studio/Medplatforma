    1: """
    2: Tests encoding functionality during parsing
    3: for all of the parsers defined in parsers.py
    4: """
    5: from io import (
    6:     BytesIO,
    7:     TextIOWrapper,
    8: )
    9: import os
   10: import tempfile
   11: import uuid
   12: 
   13: import numpy as np
   14: import pytest
   15: 
   16: from pandas import (
   17:     DataFrame,
   18:     read_csv,
   19: )
   20: import pandas._testing as tm
   21: 
   22: pytestmark = pytest.mark.filterwarnings(
   23:     "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   24: )
   25: 
   26: skip_pyarrow = pytest.mark.usefixtures("pyarrow_skip")
   27: 
   28: 
   29: def test_bytes_io_input(all_parsers):
   30:     encoding = "cp1255"
   31:     parser = all_parsers
   32: 
   33:     data = BytesIO("Ч©ЧњЧ•Чќ:1234\n562:123".encode(encoding))
   34:     result = parser.read_csv(data, sep=":", encoding=encoding)
   35: 
   36:     expected = DataFrame([[562, 123]], columns=["Ч©ЧњЧ•Чќ", "1234"])
   37:     tm.assert_frame_equal(result, expected)
   38: 
   39: 
   40: @skip_pyarrow  # CSV parse error: Empty CSV file or block
   41: def test_read_csv_unicode(all_parsers):
   42:     parser = all_parsers
   43:     data = BytesIO("\u0141aski, Jan;1".encode())
   44: 
   45:     result = parser.read_csv(data, sep=";", encoding="utf-8", header=None)
   46:     expected = DataFrame([["\u0141aski, Jan", 1]])
   47:     tm.assert_frame_equal(result, expected)
   48: 
   49: 
   50: @skip_pyarrow
   51: @pytest.mark.parametrize("sep", [",", "\t"])
   52: @pytest.mark.parametrize("encoding", ["utf-16", "utf-16le", "utf-16be"])
   53: def test_utf16_bom_skiprows(all_parsers, sep, encoding):
   54:     # see gh-2298
   55:     parser = all_parsers
   56:     data = """skip this
   57: skip this too
   58: A,B,C
   59: 1,2,3
   60: 4,5,6""".replace(
   61:         ",", sep
   62:     )
   63:     path = f"__{uuid.uuid4()}__.csv"
   64:     kwargs = {"sep": sep, "skiprows": 2}
   65:     utf8 = "utf-8"
   66: 
   67:     with tm.ensure_clean(path) as path:
   68:         bytes_data = data.encode(encoding)
   69: 
   70:         with open(path, "wb") as f:
   71:             f.write(bytes_data)
   72: 
   73:         with TextIOWrapper(BytesIO(data.encode(utf8)), encoding=utf8) as bytes_buffer:
   74:             result = parser.read_csv(path, encoding=encoding, **kwargs)
   75:             expected = parser.read_csv(bytes_buffer, encoding=utf8, **kwargs)
   76:         tm.assert_frame_equal(result, expected)
   77: 
   78: 
   79: def test_utf16_example(all_parsers, csv_dir_path):
   80:     path = os.path.join(csv_dir_path, "utf16_ex.txt")
   81:     parser = all_parsers
   82:     result = parser.read_csv(path, encoding="utf-16", sep="\t")
   83:     assert len(result) == 50
   84: 
   85: 
   86: def test_unicode_encoding(all_parsers, csv_dir_path):
   87:     path = os.path.join(csv_dir_path, "unicode_series.csv")
   88:     parser = all_parsers
   89: 
   90:     result = parser.read_csv(path, header=None, encoding="latin-1")
   91:     result = result.set_index(0)
   92:     got = result[1][1632]
   93: 
   94:     expected = "\xc1 k\xf6ldum klaka (Cold Fever) (1994)"
   95:     assert got == expected
   96: 
   97: 
   98: @pytest.mark.parametrize(
   99:     "data,kwargs,expected",
  100:     [
  101:         # Basic test
  102:         ("a\n1", {}, DataFrame({"a": [1]})),
  103:         # "Regular" quoting
  104:         ('"a"\n1', {"quotechar": '"'}, DataFrame({"a": [1]})),
  105:         # Test in a data row instead of header
  106:         ("b\n1", {"names": ["a"]}, DataFrame({"a": ["b", "1"]})),
  107:         # Test in empty data row with skipping
  108:         ("\n1", {"names": ["a"], "skip_blank_lines": True}, DataFrame({"a": [1]})),
  109:         # Test in empty data row without skipping
  110:         (
  111:             "\n1",
  112:             {"names": ["a"], "skip_blank_lines": False},
  113:             DataFrame({"a": [np.nan, 1]}),
  114:         ),
  115:     ],
  116: )
  117: def test_utf8_bom(all_parsers, data, kwargs, expected, request):
  118:     # see gh-4793
  119:     parser = all_parsers
  120:     bom = "\ufeff"
  121:     utf8 = "utf-8"
  122: 
  123:     def _encode_data_with_bom(_data):
  124:         bom_data = (bom + _data).encode(utf8)
  125:         return BytesIO(bom_data)
  126: 
  127:     if (
  128:         parser.engine == "pyarrow"
  129:         and data == "\n1"
  130:         and kwargs.get("skip_blank_lines", True)
  131:     ):
  132:         # CSV parse error: Empty CSV file or block: cannot infer number of columns
  133:         pytest.skip(reason="https://github.com/apache/arrow/issues/38676")
  134: 
  135:     result = parser.read_csv(_encode_data_with_bom(data), encoding=utf8, **kwargs)
  136:     tm.assert_frame_equal(result, expected)
  137: 
  138: 
  139: def test_read_csv_utf_aliases(all_parsers, utf_value, encoding_fmt):
  140:     # see gh-13549
  141:     expected = DataFrame({"mb_num": [4.8], "multibyte": ["test"]})
  142:     parser = all_parsers
  143: 
  144:     encoding = encoding_fmt.format(utf_value)
  145:     data = "mb_num,multibyte\n4.8,test".encode(encoding)
  146: 
  147:     result = parser.read_csv(BytesIO(data), encoding=encoding)
  148:     tm.assert_frame_equal(result, expected)
  149: 
  150: 
  151: @pytest.mark.parametrize(
  152:     "file_path,encoding",
  153:     [
  154:         (("io", "data", "csv", "test1.csv"), "utf-8"),
  155:         (("io", "parser", "data", "unicode_series.csv"), "latin-1"),
  156:         (("io", "parser", "data", "sauron.SHIFT_JIS.csv"), "shiftjis"),
  157:     ],
  158: )
  159: def test_binary_mode_file_buffers(all_parsers, file_path, encoding, datapath):
  160:     # gh-23779: Python csv engine shouldn't error on files opened in binary.
  161:     # gh-31575: Python csv engine shouldn't error on files opened in raw binary.
  162:     parser = all_parsers
  163: 
  164:     fpath = datapath(*file_path)
  165:     expected = parser.read_csv(fpath, encoding=encoding)
  166: 
  167:     with open(fpath, encoding=encoding) as fa:
  168:         result = parser.read_csv(fa)
  169:         assert not fa.closed
  170:     tm.assert_frame_equal(expected, result)
  171: 
  172:     with open(fpath, mode="rb") as fb:
  173:         result = parser.read_csv(fb, encoding=encoding)
  174:         assert not fb.closed
  175:     tm.assert_frame_equal(expected, result)
  176: 
  177:     with open(fpath, mode="rb", buffering=0) as fb:
  178:         result = parser.read_csv(fb, encoding=encoding)
  179:         assert not fb.closed
  180:     tm.assert_frame_equal(expected, result)
  181: 
  182: 
  183: @pytest.mark.parametrize("pass_encoding", [True, False])
  184: def test_encoding_temp_file(all_parsers, utf_value, encoding_fmt, pass_encoding):
  185:     # see gh-24130
  186:     parser = all_parsers
  187:     encoding = encoding_fmt.format(utf_value)
  188: 
  189:     if parser.engine == "pyarrow" and pass_encoding is True and utf_value in [16, 32]:
  190:         # FIXME: this is bad!
  191:         pytest.skip("These cases freeze")
  192: 
  193:     expected = DataFrame({"foo": ["bar"]})
  194: 
  195:     with tm.ensure_clean(mode="w+", encoding=encoding, return_filelike=True) as f:
  196:         f.write("foo\nbar")
  197:         f.seek(0)
  198: 
  199:         result = parser.read_csv(f, encoding=encoding if pass_encoding else None)
  200:         tm.assert_frame_equal(result, expected)
  201: 
  202: 
  203: def test_encoding_named_temp_file(all_parsers):
  204:     # see gh-31819
  205:     parser = all_parsers
  206:     encoding = "shift-jis"
  207: 
  208:     title = "гЃ¦гЃ™гЃЁ"
  209:     data = "гЃ“г‚Ђ"
  210: 
  211:     expected = DataFrame({title: [data]})
  212: 
  213:     with tempfile.NamedTemporaryFile() as f:
  214:         f.write(f"{title}\n{data}".encode(encoding))
  215: 
  216:         f.seek(0)
  217: 
  218:         result = parser.read_csv(f, encoding=encoding)
  219:         tm.assert_frame_equal(result, expected)
  220:         assert not f.closed
  221: 
  222: 
  223: @pytest.mark.parametrize(
  224:     "encoding", ["utf-8", "utf-16", "utf-16-be", "utf-16-le", "utf-32"]
  225: )
  226: def test_parse_encoded_special_characters(encoding):
  227:     # GH16218 Verify parsing of data with encoded special characters
  228:     # Data contains a Unicode 'FULLWIDTH COLON' (U+FF1A) at position (0,"a")
  229:     data = "a\tb\nпјљfoo\t0\nbar\t1\nbaz\t2"  # noqa: RUF001
  230:     encoded_data = BytesIO(data.encode(encoding))
  231:     result = read_csv(encoded_data, delimiter="\t", encoding=encoding)
  232: 
  233:     expected = DataFrame(
  234:         data=[["пјљfoo", 0], ["bar", 1], ["baz", 2]],  # noqa: RUF001
  235:         columns=["a", "b"],
  236:     )
  237:     tm.assert_frame_equal(result, expected)
  238: 
  239: 
  240: @pytest.mark.parametrize("encoding", ["utf-8", None, "utf-16", "cp1255", "latin-1"])
  241: def test_encoding_memory_map(all_parsers, encoding):
  242:     # GH40986
  243:     parser = all_parsers
  244:     expected = DataFrame(
  245:         {
  246:             "name": ["Raphael", "Donatello", "Miguel Angel", "Leonardo"],
  247:             "mask": ["red", "purple", "orange", "blue"],
  248:             "weapon": ["sai", "bo staff", "nunchunk", "katana"],
  249:         }
  250:     )
  251:     with tm.ensure_clean() as file:
  252:         expected.to_csv(file, index=False, encoding=encoding)
  253: 
  254:         if parser.engine == "pyarrow":
  255:             msg = "The 'memory_map' option is not supported with the 'pyarrow' engine"
  256:             with pytest.raises(ValueError, match=msg):
  257:                 parser.read_csv(file, encoding=encoding, memory_map=True)
  258:             return
  259: 
  260:         df = parser.read_csv(file, encoding=encoding, memory_map=True)
  261:     tm.assert_frame_equal(df, expected)
  262: 
  263: 
  264: def test_chunk_splits_multibyte_char(all_parsers):
  265:     """
  266:     Chunk splits a multibyte character with memory_map=True
  267: 
  268:     GH 43540
  269:     """
  270:     parser = all_parsers
  271:     # DEFAULT_CHUNKSIZE = 262144, defined in parsers.pyx
  272:     df = DataFrame(data=["a" * 127] * 2048)
  273: 
  274:     # Put two-bytes utf-8 encoded character "Д…" at the end of chunk
  275:     # utf-8 encoding of "Д…" is b'\xc4\x85'
  276:     df.iloc[2047] = "a" * 127 + "Д…"
  277:     with tm.ensure_clean("bug-gh43540.csv") as fname:
  278:         df.to_csv(fname, index=False, header=False, encoding="utf-8")
  279: 
  280:         if parser.engine == "pyarrow":
  281:             msg = "The 'memory_map' option is not supported with the 'pyarrow' engine"
  282:             with pytest.raises(ValueError, match=msg):
  283:                 parser.read_csv(fname, header=None, memory_map=True)
  284:             return
  285: 
  286:         dfr = parser.read_csv(fname, header=None, memory_map=True)
  287:     tm.assert_frame_equal(dfr, df)
  288: 
  289: 
  290: def test_readcsv_memmap_utf8(all_parsers):
  291:     """
  292:     GH 43787
  293: 
  294:     Test correct handling of UTF-8 chars when memory_map=True and encoding is UTF-8
  295:     """
  296:     lines = []
  297:     line_length = 128
  298:     start_char = " "
  299:     end_char = "\U00010080"
  300:     # This for loop creates a list of 128-char strings
  301:     # consisting of consecutive Unicode chars
  302:     for lnum in range(ord(start_char), ord(end_char), line_length):
  303:         line = "".join([chr(c) for c in range(lnum, lnum + 0x80)]) + "\n"
  304:         try:
  305:             line.encode("utf-8")
  306:         except UnicodeEncodeError:
  307:             continue
  308:         lines.append(line)
  309:     parser = all_parsers
  310:     df = DataFrame(lines)
  311:     with tm.ensure_clean("utf8test.csv") as fname:
  312:         df.to_csv(fname, index=False, header=False, encoding="utf-8")
  313: 
  314:         if parser.engine == "pyarrow":
  315:             msg = "The 'memory_map' option is not supported with the 'pyarrow' engine"
  316:             with pytest.raises(ValueError, match=msg):
  317:                 parser.read_csv(fname, header=None, memory_map=True, encoding="utf-8")
  318:             return
  319: 
  320:         dfr = parser.read_csv(fname, header=None, memory_map=True, encoding="utf-8")
  321:     tm.assert_frame_equal(df, dfr)
  322: 
  323: 
  324: @pytest.mark.usefixtures("pyarrow_xfail")
  325: @pytest.mark.parametrize("mode", ["w+b", "w+t"])
  326: def test_not_readable(all_parsers, mode):
  327:     # GH43439
  328:     parser = all_parsers
  329:     content = b"abcd"
  330:     if "t" in mode:
  331:         content = "abcd"
  332:     with tempfile.SpooledTemporaryFile(mode=mode, encoding="utf-8") as handle:
  333:         handle.write(content)
  334:         handle.seek(0)
  335:         df = parser.read_csv(handle)
  336:     expected = DataFrame([], columns=["abcd"])
  337:     tm.assert_frame_equal(df, expected)
