    1: """
    2: Tests that work on both the Python and C engines but do not have a
    3: specific classification into the other test modules.
    4: """
    5: from io import StringIO
    6: 
    7: import numpy as np
    8: import pytest
    9: 
   10: from pandas._libs import parsers as libparsers
   11: from pandas.errors import DtypeWarning
   12: 
   13: from pandas import (
   14:     DataFrame,
   15:     concat,
   16: )
   17: import pandas._testing as tm
   18: 
   19: pytestmark = pytest.mark.filterwarnings(
   20:     "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   21: )
   22: 
   23: 
   24: @pytest.mark.parametrize("index_col", [0, "index"])
   25: def test_read_chunksize_with_index(all_parsers, index_col):
   26:     parser = all_parsers
   27:     data = """index,A,B,C,D
   28: foo,2,3,4,5
   29: bar,7,8,9,10
   30: baz,12,13,14,15
   31: qux,12,13,14,15
   32: foo2,12,13,14,15
   33: bar2,12,13,14,15
   34: """
   35: 
   36:     expected = DataFrame(
   37:         [
   38:             ["foo", 2, 3, 4, 5],
   39:             ["bar", 7, 8, 9, 10],
   40:             ["baz", 12, 13, 14, 15],
   41:             ["qux", 12, 13, 14, 15],
   42:             ["foo2", 12, 13, 14, 15],
   43:             ["bar2", 12, 13, 14, 15],
   44:         ],
   45:         columns=["index", "A", "B", "C", "D"],
   46:     )
   47:     expected = expected.set_index("index")
   48: 
   49:     if parser.engine == "pyarrow":
   50:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
   51:         with pytest.raises(ValueError, match=msg):
   52:             with parser.read_csv(StringIO(data), index_col=0, chunksize=2) as reader:
   53:                 list(reader)
   54:         return
   55: 
   56:     with parser.read_csv(StringIO(data), index_col=0, chunksize=2) as reader:
   57:         chunks = list(reader)
   58:     tm.assert_frame_equal(chunks[0], expected[:2])
   59:     tm.assert_frame_equal(chunks[1], expected[2:4])
   60:     tm.assert_frame_equal(chunks[2], expected[4:])
   61: 
   62: 
   63: @pytest.mark.parametrize("chunksize", [1.3, "foo", 0])
   64: def test_read_chunksize_bad(all_parsers, chunksize):
   65:     data = """index,A,B,C,D
   66: foo,2,3,4,5
   67: bar,7,8,9,10
   68: baz,12,13,14,15
   69: qux,12,13,14,15
   70: foo2,12,13,14,15
   71: bar2,12,13,14,15
   72: """
   73:     parser = all_parsers
   74:     msg = r"'chunksize' must be an integer >=1"
   75:     if parser.engine == "pyarrow":
   76:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
   77: 
   78:     with pytest.raises(ValueError, match=msg):
   79:         with parser.read_csv(StringIO(data), chunksize=chunksize) as _:
   80:             pass
   81: 
   82: 
   83: @pytest.mark.parametrize("chunksize", [2, 8])
   84: def test_read_chunksize_and_nrows(all_parsers, chunksize):
   85:     # see gh-15755
   86:     data = """index,A,B,C,D
   87: foo,2,3,4,5
   88: bar,7,8,9,10
   89: baz,12,13,14,15
   90: qux,12,13,14,15
   91: foo2,12,13,14,15
   92: bar2,12,13,14,15
   93: """
   94:     parser = all_parsers
   95:     kwargs = {"index_col": 0, "nrows": 5}
   96: 
   97:     if parser.engine == "pyarrow":
   98:         msg = "The 'nrows' option is not supported with the 'pyarrow' engine"
   99:         with pytest.raises(ValueError, match=msg):
  100:             parser.read_csv(StringIO(data), **kwargs)
  101:         return
  102: 
  103:     expected = parser.read_csv(StringIO(data), **kwargs)
  104:     with parser.read_csv(StringIO(data), chunksize=chunksize, **kwargs) as reader:
  105:         tm.assert_frame_equal(concat(reader), expected)
  106: 
  107: 
  108: def test_read_chunksize_and_nrows_changing_size(all_parsers):
  109:     data = """index,A,B,C,D
  110: foo,2,3,4,5
  111: bar,7,8,9,10
  112: baz,12,13,14,15
  113: qux,12,13,14,15
  114: foo2,12,13,14,15
  115: bar2,12,13,14,15
  116: """
  117:     parser = all_parsers
  118:     kwargs = {"index_col": 0, "nrows": 5}
  119: 
  120:     if parser.engine == "pyarrow":
  121:         msg = "The 'nrows' option is not supported with the 'pyarrow' engine"
  122:         with pytest.raises(ValueError, match=msg):
  123:             parser.read_csv(StringIO(data), **kwargs)
  124:         return
  125: 
  126:     expected = parser.read_csv(StringIO(data), **kwargs)
  127:     with parser.read_csv(StringIO(data), chunksize=8, **kwargs) as reader:
  128:         tm.assert_frame_equal(reader.get_chunk(size=2), expected.iloc[:2])
  129:         tm.assert_frame_equal(reader.get_chunk(size=4), expected.iloc[2:5])
  130: 
  131:         with pytest.raises(StopIteration, match=""):
  132:             reader.get_chunk(size=3)
  133: 
  134: 
  135: def test_get_chunk_passed_chunksize(all_parsers):
  136:     parser = all_parsers
  137:     data = """A,B,C
  138: 1,2,3
  139: 4,5,6
  140: 7,8,9
  141: 1,2,3"""
  142: 
  143:     if parser.engine == "pyarrow":
  144:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
  145:         with pytest.raises(ValueError, match=msg):
  146:             with parser.read_csv(StringIO(data), chunksize=2) as reader:
  147:                 reader.get_chunk()
  148:         return
  149: 
  150:     with parser.read_csv(StringIO(data), chunksize=2) as reader:
  151:         result = reader.get_chunk()
  152: 
  153:     expected = DataFrame([[1, 2, 3], [4, 5, 6]], columns=["A", "B", "C"])
  154:     tm.assert_frame_equal(result, expected)
  155: 
  156: 
  157: @pytest.mark.parametrize("kwargs", [{}, {"index_col": 0}])
  158: def test_read_chunksize_compat(all_parsers, kwargs):
  159:     # see gh-12185
  160:     data = """index,A,B,C,D
  161: foo,2,3,4,5
  162: bar,7,8,9,10
  163: baz,12,13,14,15
  164: qux,12,13,14,15
  165: foo2,12,13,14,15
  166: bar2,12,13,14,15
  167: """
  168:     parser = all_parsers
  169:     result = parser.read_csv(StringIO(data), **kwargs)
  170: 
  171:     if parser.engine == "pyarrow":
  172:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
  173:         with pytest.raises(ValueError, match=msg):
  174:             with parser.read_csv(StringIO(data), chunksize=2, **kwargs) as reader:
  175:                 concat(reader)
  176:         return
  177: 
  178:     with parser.read_csv(StringIO(data), chunksize=2, **kwargs) as reader:
  179:         via_reader = concat(reader)
  180:     tm.assert_frame_equal(via_reader, result)
  181: 
  182: 
  183: def test_read_chunksize_jagged_names(all_parsers):
  184:     # see gh-23509
  185:     parser = all_parsers
  186:     data = "\n".join(["0"] * 7 + [",".join(["0"] * 10)])
  187: 
  188:     expected = DataFrame([[0] + [np.nan] * 9] * 7 + [[0] * 10])
  189: 
  190:     if parser.engine == "pyarrow":
  191:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
  192:         with pytest.raises(ValueError, match=msg):
  193:             with parser.read_csv(
  194:                 StringIO(data), names=range(10), chunksize=4
  195:             ) as reader:
  196:                 concat(reader)
  197:         return
  198: 
  199:     with parser.read_csv(StringIO(data), names=range(10), chunksize=4) as reader:
  200:         result = concat(reader)
  201:     tm.assert_frame_equal(result, expected)
  202: 
  203: 
  204: def test_chunk_begins_with_newline_whitespace(all_parsers):
  205:     # see gh-10022
  206:     parser = all_parsers
  207:     data = "\n hello\nworld\n"
  208: 
  209:     result = parser.read_csv(StringIO(data), header=None)
  210:     expected = DataFrame([" hello", "world"])
  211:     tm.assert_frame_equal(result, expected)
  212: 
  213: 
  214: @pytest.mark.slow
  215: def test_chunks_have_consistent_numerical_type(all_parsers, monkeypatch):
  216:     # mainly an issue with the C parser
  217:     heuristic = 2**3
  218:     parser = all_parsers
  219:     integers = [str(i) for i in range(heuristic - 1)]
  220:     data = "a\n" + "\n".join(integers + ["1.0", "2.0"] + integers)
  221: 
  222:     # Coercions should work without warnings.
  223:     with monkeypatch.context() as m:
  224:         m.setattr(libparsers, "DEFAULT_BUFFER_HEURISTIC", heuristic)
  225:         result = parser.read_csv(StringIO(data))
  226: 
  227:     assert type(result.a[0]) is np.float64
  228:     assert result.a.dtype == float
  229: 
  230: 
  231: def test_warn_if_chunks_have_mismatched_type(all_parsers):
  232:     warning_type = None
  233:     parser = all_parsers
  234:     size = 10000
  235: 
  236:     # see gh-3866: if chunks are different types and can't
  237:     # be coerced using numerical types, then issue warning.
  238:     if parser.engine == "c" and parser.low_memory:
  239:         warning_type = DtypeWarning
  240:         # Use larger size to hit warning path
  241:         size = 499999
  242: 
  243:     integers = [str(i) for i in range(size)]
  244:     data = "a\n" + "\n".join(integers + ["a", "b"] + integers)
  245: 
  246:     buf = StringIO(data)
  247: 
  248:     if parser.engine == "pyarrow":
  249:         df = parser.read_csv(
  250:             buf,
  251:         )
  252:     else:
  253:         df = parser.read_csv_check_warnings(
  254:             warning_type,
  255:             r"Columns \(0\) have mixed types. "
  256:             "Specify dtype option on import or set low_memory=False.",
  257:             buf,
  258:         )
  259: 
  260:     assert df.a.dtype == object
  261: 
  262: 
  263: @pytest.mark.parametrize("iterator", [True, False])
  264: def test_empty_with_nrows_chunksize(all_parsers, iterator):
  265:     # see gh-9535
  266:     parser = all_parsers
  267:     expected = DataFrame(columns=["foo", "bar"])
  268: 
  269:     nrows = 10
  270:     data = StringIO("foo,bar\n")
  271: 
  272:     if parser.engine == "pyarrow":
  273:         msg = (
  274:             "The '(nrows|chunksize)' option is not supported with the 'pyarrow' engine"
  275:         )
  276:         with pytest.raises(ValueError, match=msg):
  277:             if iterator:
  278:                 with parser.read_csv(data, chunksize=nrows) as reader:
  279:                     next(iter(reader))
  280:             else:
  281:                 parser.read_csv(data, nrows=nrows)
  282:         return
  283: 
  284:     if iterator:
  285:         with parser.read_csv(data, chunksize=nrows) as reader:
  286:             result = next(iter(reader))
  287:     else:
  288:         result = parser.read_csv(data, nrows=nrows)
  289: 
  290:     tm.assert_frame_equal(result, expected)
  291: 
  292: 
  293: def test_read_csv_memory_growth_chunksize(all_parsers):
  294:     # see gh-24805
  295:     #
  296:     # Let's just make sure that we don't crash
  297:     # as we iteratively process all chunks.
  298:     parser = all_parsers
  299: 
  300:     with tm.ensure_clean() as path:
  301:         with open(path, "w", encoding="utf-8") as f:
  302:             for i in range(1000):
  303:                 f.write(str(i) + "\n")
  304: 
  305:         if parser.engine == "pyarrow":
  306:             msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
  307:             with pytest.raises(ValueError, match=msg):
  308:                 with parser.read_csv(path, chunksize=20) as result:
  309:                     for _ in result:
  310:                         pass
  311:             return
  312: 
  313:         with parser.read_csv(path, chunksize=20) as result:
  314:             for _ in result:
  315:                 pass
  316: 
  317: 
  318: def test_chunksize_with_usecols_second_block_shorter(all_parsers):
  319:     # GH#21211
  320:     parser = all_parsers
  321:     data = """1,2,3,4
  322: 5,6,7,8
  323: 9,10,11
  324: """
  325: 
  326:     if parser.engine == "pyarrow":
  327:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
  328:         with pytest.raises(ValueError, match=msg):
  329:             parser.read_csv(
  330:                 StringIO(data),
  331:                 names=["a", "b"],
  332:                 chunksize=2,
  333:                 usecols=[0, 1],
  334:                 header=None,
  335:             )
  336:         return
  337: 
  338:     result_chunks = parser.read_csv(
  339:         StringIO(data),
  340:         names=["a", "b"],
  341:         chunksize=2,
  342:         usecols=[0, 1],
  343:         header=None,
  344:     )
  345: 
  346:     expected_frames = [
  347:         DataFrame({"a": [1, 5], "b": [2, 6]}),
  348:         DataFrame({"a": [9], "b": [10]}, index=[2]),
  349:     ]
  350: 
  351:     for i, result in enumerate(result_chunks):
  352:         tm.assert_frame_equal(result, expected_frames[i])
  353: 
  354: 
  355: def test_chunksize_second_block_shorter(all_parsers):
  356:     # GH#21211
  357:     parser = all_parsers
  358:     data = """a,b,c,d
  359: 1,2,3,4
  360: 5,6,7,8
  361: 9,10,11
  362: """
  363: 
  364:     if parser.engine == "pyarrow":
  365:         msg = "The 'chunksize' option is not supported with the 'pyarrow' engine"
  366:         with pytest.raises(ValueError, match=msg):
  367:             parser.read_csv(StringIO(data), chunksize=2)
  368:         return
  369: 
  370:     result_chunks = parser.read_csv(StringIO(data), chunksize=2)
  371: 
  372:     expected_frames = [
  373:         DataFrame({"a": [1, 5], "b": [2, 6], "c": [3, 7], "d": [4, 8]}),
  374:         DataFrame({"a": [9], "b": [10], "c": [11], "d": [np.nan]}, index=[2]),
  375:     ]
  376: 
  377:     for i, result in enumerate(result_chunks):
  378:         tm.assert_frame_equal(result, expected_frames[i])
