    1: import datetime
    2: from datetime import timedelta
    3: from decimal import Decimal
    4: from io import (
    5:     BytesIO,
    6:     StringIO,
    7: )
    8: import json
    9: import os
   10: import sys
   11: import time
   12: 
   13: import numpy as np
   14: import pytest
   15: 
   16: from pandas._config import using_pyarrow_string_dtype
   17: 
   18: from pandas.compat import IS64
   19: import pandas.util._test_decorators as td
   20: 
   21: import pandas as pd
   22: from pandas import (
   23:     NA,
   24:     DataFrame,
   25:     DatetimeIndex,
   26:     Index,
   27:     RangeIndex,
   28:     Series,
   29:     Timestamp,
   30:     date_range,
   31:     read_json,
   32: )
   33: import pandas._testing as tm
   34: from pandas.core.arrays import (
   35:     ArrowStringArray,
   36:     StringArray,
   37: )
   38: from pandas.core.arrays.string_arrow import ArrowStringArrayNumpySemantics
   39: 
   40: from pandas.io.json import ujson_dumps
   41: 
   42: 
   43: def test_literal_json_deprecation():
   44:     # PR 53409
   45:     expected = DataFrame([[1, 2], [1, 2]], columns=["a", "b"])
   46: 
   47:     jsonl = """{"a": 1, "b": 2}
   48:         {"a": 3, "b": 4}
   49:         {"a": 5, "b": 6}
   50:         {"a": 7, "b": 8}"""
   51: 
   52:     msg = (
   53:         "Passing literal json to 'read_json' is deprecated and "
   54:         "will be removed in a future version. To read from a "
   55:         "literal string, wrap it in a 'StringIO' object."
   56:     )
   57: 
   58:     with tm.assert_produces_warning(FutureWarning, match=msg):
   59:         try:
   60:             read_json(jsonl, lines=False)
   61:         except ValueError:
   62:             pass
   63: 
   64:     with tm.assert_produces_warning(FutureWarning, match=msg):
   65:         read_json(expected.to_json(), lines=False)
   66: 
   67:     with tm.assert_produces_warning(FutureWarning, match=msg):
   68:         result = read_json('{"a": 1, "b": 2}\n{"b":2, "a" :1}\n', lines=True)
   69:         tm.assert_frame_equal(result, expected)
   70: 
   71:     with tm.assert_produces_warning(FutureWarning, match=msg):
   72:         try:
   73:             result = read_json(
   74:                 '{"a\\\\":"foo\\\\","b":"bar"}\n{"a\\\\":"foo\\"","b":"bar"}\n',
   75:                 lines=False,
   76:             )
   77:         except ValueError:
   78:             pass
   79: 
   80:     with tm.assert_produces_warning(FutureWarning, match=msg):
   81:         try:
   82:             result = read_json('{"a": 1, "b": 2}\n{"b":2, "a" :1}\n', lines=False)
   83:         except ValueError:
   84:             pass
   85:         tm.assert_frame_equal(result, expected)
   86: 
   87: 
   88: def assert_json_roundtrip_equal(result, expected, orient):
   89:     if orient in ("records", "values"):
   90:         expected = expected.reset_index(drop=True)
   91:     if orient == "values":
   92:         expected.columns = range(len(expected.columns))
   93:     tm.assert_frame_equal(result, expected)
   94: 
   95: 
   96: class TestPandasContainer:
   97:     @pytest.fixture
   98:     def categorical_frame(self):
   99:         data = {
  100:             c: np.random.default_rng(i).standard_normal(30)
  101:             for i, c in enumerate(list("ABCD"))
  102:         }
  103:         cat = ["bah"] * 5 + ["bar"] * 5 + ["baz"] * 5 + ["foo"] * 15
  104:         data["E"] = list(reversed(cat))
  105:         data["sort"] = np.arange(30, dtype="int64")
  106:         return DataFrame(data, index=pd.CategoricalIndex(cat, name="E"))
  107: 
  108:     @pytest.fixture
  109:     def datetime_series(self):
  110:         # Same as usual datetime_series, but with index freq set to None,
  111:         #  since that doesn't round-trip, see GH#33711
  112:         ser = Series(
  113:             1.1 * np.arange(10, dtype=np.float64),
  114:             index=date_range("2020-01-01", periods=10),
  115:             name="ts",
  116:         )
  117:         ser.index = ser.index._with_freq(None)
  118:         return ser
  119: 
  120:     @pytest.fixture
  121:     def datetime_frame(self):
  122:         # Same as usual datetime_frame, but with index freq set to None,
  123:         #  since that doesn't round-trip, see GH#33711
  124:         df = DataFrame(
  125:             np.random.default_rng(2).standard_normal((30, 4)),
  126:             columns=Index(list("ABCD"), dtype=object),
  127:             index=date_range("2000-01-01", periods=30, freq="B"),
  128:         )
  129:         df.index = df.index._with_freq(None)
  130:         return df
  131: 
  132:     def test_frame_double_encoded_labels(self, orient):
  133:         df = DataFrame(
  134:             [["a", "b"], ["c", "d"]],
  135:             index=['index " 1', "index / 2"],
  136:             columns=["a \\ b", "y / z"],
  137:         )
  138: 
  139:         data = StringIO(df.to_json(orient=orient))
  140:         result = read_json(data, orient=orient)
  141:         expected = df.copy()
  142:         assert_json_roundtrip_equal(result, expected, orient)
  143: 
  144:     @pytest.mark.parametrize("orient", ["split", "records", "values"])
  145:     def test_frame_non_unique_index(self, orient):
  146:         df = DataFrame([["a", "b"], ["c", "d"]], index=[1, 1], columns=["x", "y"])
  147:         data = StringIO(df.to_json(orient=orient))
  148:         result = read_json(data, orient=orient)
  149:         expected = df.copy()
  150: 
  151:         assert_json_roundtrip_equal(result, expected, orient)
  152: 
  153:     @pytest.mark.parametrize("orient", ["index", "columns"])
  154:     def test_frame_non_unique_index_raises(self, orient):
  155:         df = DataFrame([["a", "b"], ["c", "d"]], index=[1, 1], columns=["x", "y"])
  156:         msg = f"DataFrame index must be unique for orient='{orient}'"
  157:         with pytest.raises(ValueError, match=msg):
  158:             df.to_json(orient=orient)
  159: 
  160:     @pytest.mark.parametrize("orient", ["split", "values"])
  161:     @pytest.mark.parametrize(
  162:         "data",
  163:         [
  164:             [["a", "b"], ["c", "d"]],
  165:             [[1.5, 2.5], [3.5, 4.5]],
  166:             [[1, 2.5], [3, 4.5]],
  167:             [[Timestamp("20130101"), 3.5], [Timestamp("20130102"), 4.5]],
  168:         ],
  169:     )
  170:     def test_frame_non_unique_columns(self, orient, data):
  171:         df = DataFrame(data, index=[1, 2], columns=["x", "x"])
  172: 
  173:         result = read_json(
  174:             StringIO(df.to_json(orient=orient)), orient=orient, convert_dates=["x"]
  175:         )
  176:         if orient == "values":
  177:             expected = DataFrame(data)
  178:             if expected.iloc[:, 0].dtype == "datetime64[ns]":
  179:                 # orient == "values" by default will write Timestamp objects out
  180:                 # in milliseconds; these are internally stored in nanosecond,
  181:                 # so divide to get where we need
  182:                 # TODO: a to_epoch method would also solve; see GH 14772
  183:                 expected.isetitem(0, expected.iloc[:, 0].astype(np.int64) // 1000000)
  184:         elif orient == "split":
  185:             expected = df
  186:             expected.columns = ["x", "x.1"]
  187: 
  188:         tm.assert_frame_equal(result, expected)
  189: 
  190:     @pytest.mark.parametrize("orient", ["index", "columns", "records"])
  191:     def test_frame_non_unique_columns_raises(self, orient):
  192:         df = DataFrame([["a", "b"], ["c", "d"]], index=[1, 2], columns=["x", "x"])
  193: 
  194:         msg = f"DataFrame columns must be unique for orient='{orient}'"
  195:         with pytest.raises(ValueError, match=msg):
  196:             df.to_json(orient=orient)
  197: 
  198:     def test_frame_default_orient(self, float_frame):
  199:         assert float_frame.to_json() == float_frame.to_json(orient="columns")
  200: 
  201:     @pytest.mark.parametrize("dtype", [False, float])
  202:     @pytest.mark.parametrize("convert_axes", [True, False])
  203:     def test_roundtrip_simple(self, orient, convert_axes, dtype, float_frame):
  204:         data = StringIO(float_frame.to_json(orient=orient))
  205:         result = read_json(data, orient=orient, convert_axes=convert_axes, dtype=dtype)
  206: 
  207:         expected = float_frame
  208: 
  209:         assert_json_roundtrip_equal(result, expected, orient)
  210: 
  211:     @pytest.mark.parametrize("dtype", [False, np.int64])
  212:     @pytest.mark.parametrize("convert_axes", [True, False])
  213:     def test_roundtrip_intframe(self, orient, convert_axes, dtype, int_frame):
  214:         data = StringIO(int_frame.to_json(orient=orient))
  215:         result = read_json(data, orient=orient, convert_axes=convert_axes, dtype=dtype)
  216:         expected = int_frame
  217:         assert_json_roundtrip_equal(result, expected, orient)
  218: 
  219:     @pytest.mark.parametrize("dtype", [None, np.float64, int, "U3"])
  220:     @pytest.mark.parametrize("convert_axes", [True, False])
  221:     def test_roundtrip_str_axes(self, orient, convert_axes, dtype):
  222:         df = DataFrame(
  223:             np.zeros((200, 4)),
  224:             columns=[str(i) for i in range(4)],
  225:             index=[str(i) for i in range(200)],
  226:             dtype=dtype,
  227:         )
  228: 
  229:         data = StringIO(df.to_json(orient=orient))
  230:         result = read_json(data, orient=orient, convert_axes=convert_axes, dtype=dtype)
  231: 
  232:         expected = df.copy()
  233:         if not dtype:
  234:             expected = expected.astype(np.int64)
  235: 
  236:         # index columns, and records orients cannot fully preserve the string
  237:         # dtype for axes as the index and column labels are used as keys in
  238:         # JSON objects. JSON keys are by definition strings, so there's no way
  239:         # to disambiguate whether those keys actually were strings or numeric
  240:         # beforehand and numeric wins out.
  241:         if convert_axes and (orient in ("index", "columns")):
  242:             expected.columns = expected.columns.astype(np.int64)
  243:             expected.index = expected.index.astype(np.int64)
  244:         elif orient == "records" and convert_axes:
  245:             expected.columns = expected.columns.astype(np.int64)
  246:         elif convert_axes and orient == "split":
  247:             expected.columns = expected.columns.astype(np.int64)
  248: 
  249:         assert_json_roundtrip_equal(result, expected, orient)
  250: 
  251:     @pytest.mark.parametrize("convert_axes", [True, False])
  252:     def test_roundtrip_categorical(
  253:         self, request, orient, categorical_frame, convert_axes, using_infer_string
  254:     ):
  255:         # TODO: create a better frame to test with and improve coverage
  256:         if orient in ("index", "columns"):
  257:             request.applymarker(
  258:                 pytest.mark.xfail(
  259:                     reason=f"Can't have duplicate index values for orient '{orient}')"
  260:                 )
  261:             )
  262: 
  263:         data = StringIO(categorical_frame.to_json(orient=orient))
  264:         result = read_json(data, orient=orient, convert_axes=convert_axes)
  265: 
  266:         expected = categorical_frame.copy()
  267:         expected.index = expected.index.astype(
  268:             str if not using_infer_string else "string[pyarrow_numpy]"
  269:         )  # Categorical not preserved
  270:         expected.index.name = None  # index names aren't preserved in JSON
  271:         assert_json_roundtrip_equal(result, expected, orient)
  272: 
  273:     @pytest.mark.parametrize("convert_axes", [True, False])
  274:     def test_roundtrip_empty(self, orient, convert_axes):
  275:         empty_frame = DataFrame()
  276:         data = StringIO(empty_frame.to_json(orient=orient))
  277:         result = read_json(data, orient=orient, convert_axes=convert_axes)
  278:         if orient == "split":
  279:             idx = Index([], dtype=(float if convert_axes else object))
  280:             expected = DataFrame(index=idx, columns=idx)
  281:         elif orient in ["index", "columns"]:
  282:             expected = DataFrame()
  283:         else:
  284:             expected = empty_frame.copy()
  285: 
  286:         tm.assert_frame_equal(result, expected)
  287: 
  288:     @pytest.mark.parametrize("convert_axes", [True, False])
  289:     def test_roundtrip_timestamp(self, orient, convert_axes, datetime_frame):
  290:         # TODO: improve coverage with date_format parameter
  291:         data = StringIO(datetime_frame.to_json(orient=orient))
  292:         result = read_json(data, orient=orient, convert_axes=convert_axes)
  293:         expected = datetime_frame.copy()
  294: 
  295:         if not convert_axes:  # one off for ts handling
  296:             # DTI gets converted to epoch values
  297:             idx = expected.index.view(np.int64) // 1000000
  298:             if orient != "split":  # TODO: handle consistently across orients
  299:                 idx = idx.astype(str)
  300: 
  301:             expected.index = idx
  302: 
  303:         assert_json_roundtrip_equal(result, expected, orient)
  304: 
  305:     @pytest.mark.parametrize("convert_axes", [True, False])
  306:     def test_roundtrip_mixed(self, orient, convert_axes):
  307:         index = Index(["a", "b", "c", "d", "e"])
  308:         values = {
  309:             "A": [0.0, 1.0, 2.0, 3.0, 4.0],
  310:             "B": [0.0, 1.0, 0.0, 1.0, 0.0],
  311:             "C": ["foo1", "foo2", "foo3", "foo4", "foo5"],
  312:             "D": [True, False, True, False, True],
  313:         }
  314: 
  315:         df = DataFrame(data=values, index=index)
  316: 
  317:         data = StringIO(df.to_json(orient=orient))
  318:         result = read_json(data, orient=orient, convert_axes=convert_axes)
  319: 
  320:         expected = df.copy()
  321:         expected = expected.assign(**expected.select_dtypes("number").astype(np.int64))
  322: 
  323:         assert_json_roundtrip_equal(result, expected, orient)
  324: 
  325:     @pytest.mark.xfail(
  326:         reason="#50456 Column multiindex is stored and loaded differently",
  327:         raises=AssertionError,
  328:     )
  329:     @pytest.mark.parametrize(
  330:         "columns",
  331:         [
  332:             [["2022", "2022"], ["JAN", "FEB"]],
  333:             [["2022", "2023"], ["JAN", "JAN"]],
  334:             [["2022", "2022"], ["JAN", "JAN"]],
  335:         ],
  336:     )
  337:     def test_roundtrip_multiindex(self, columns):
  338:         df = DataFrame(
  339:             [[1, 2], [3, 4]],
  340:             columns=pd.MultiIndex.from_arrays(columns),
  341:         )
  342:         data = StringIO(df.to_json(orient="split"))
  343:         result = read_json(data, orient="split")
  344:         tm.assert_frame_equal(result, df)
  345: 
  346:     @pytest.mark.parametrize(
  347:         "data,msg,orient",
  348:         [
  349:             ('{"key":b:a:d}', "Expected object or value", "columns"),
  350:             # too few indices
  351:             (
  352:                 '{"columns":["A","B"],'
  353:                 '"index":["2","3"],'
  354:                 '"data":[[1.0,"1"],[2.0,"2"],[null,"3"]]}',
  355:                 "|".join(
  356:                     [
  357:                         r"Length of values \(3\) does not match length of index \(2\)",
  358:                     ]
  359:                 ),
  360:                 "split",
  361:             ),
  362:             # too many columns
  363:             (
  364:                 '{"columns":["A","B","C"],'
  365:                 '"index":["1","2","3"],'
  366:                 '"data":[[1.0,"1"],[2.0,"2"],[null,"3"]]}',
  367:                 "3 columns passed, passed data had 2 columns",
  368:                 "split",
  369:             ),
  370:             # bad key
  371:             (
  372:                 '{"badkey":["A","B"],'
  373:                 '"index":["2","3"],'
  374:                 '"data":[[1.0,"1"],[2.0,"2"],[null,"3"]]}',
  375:                 r"unexpected key\(s\): badkey",
  376:                 "split",
  377:             ),
  378:         ],
  379:     )
  380:     def test_frame_from_json_bad_data_raises(self, data, msg, orient):
  381:         with pytest.raises(ValueError, match=msg):
  382:             read_json(StringIO(data), orient=orient)
  383: 
  384:     @pytest.mark.parametrize("dtype", [True, False])
  385:     @pytest.mark.parametrize("convert_axes", [True, False])
  386:     def test_frame_from_json_missing_data(self, orient, convert_axes, dtype):
  387:         num_df = DataFrame([[1, 2], [4, 5, 6]])
  388: 
  389:         result = read_json(
  390:             StringIO(num_df.to_json(orient=orient)),
  391:             orient=orient,
  392:             convert_axes=convert_axes,
  393:             dtype=dtype,
  394:         )
  395:         assert np.isnan(result.iloc[0, 2])
  396: 
  397:         obj_df = DataFrame([["1", "2"], ["4", "5", "6"]])
  398:         result = read_json(
  399:             StringIO(obj_df.to_json(orient=orient)),
  400:             orient=orient,
  401:             convert_axes=convert_axes,
  402:             dtype=dtype,
  403:         )
  404:         assert np.isnan(result.iloc[0, 2])
  405: 
  406:     @pytest.mark.parametrize("dtype", [True, False])
  407:     def test_frame_read_json_dtype_missing_value(self, dtype):
  408:         # GH28501 Parse missing values using read_json with dtype=False
  409:         # to NaN instead of None
  410:         result = read_json(StringIO("[null]"), dtype=dtype)
  411:         expected = DataFrame([np.nan])
  412: 
  413:         tm.assert_frame_equal(result, expected)
  414: 
  415:     @pytest.mark.parametrize("inf", [np.inf, -np.inf])
  416:     @pytest.mark.parametrize("dtype", [True, False])
  417:     def test_frame_infinity(self, inf, dtype):
  418:         # infinities get mapped to nulls which get mapped to NaNs during
  419:         # deserialisation
  420:         df = DataFrame([[1, 2], [4, 5, 6]])
  421:         df.loc[0, 2] = inf
  422: 
  423:         data = StringIO(df.to_json())
  424:         result = read_json(data, dtype=dtype)
  425:         assert np.isnan(result.iloc[0, 2])
  426: 
  427:     @pytest.mark.skipif(not IS64, reason="not compliant on 32-bit, xref #15865")
  428:     @pytest.mark.parametrize(
  429:         "value,precision,expected_val",
  430:         [
  431:             (0.95, 1, 1.0),
  432:             (1.95, 1, 2.0),
  433:             (-1.95, 1, -2.0),
  434:             (0.995, 2, 1.0),
  435:             (0.9995, 3, 1.0),
  436:             (0.99999999999999944, 15, 1.0),
  437:         ],
  438:     )
  439:     def test_frame_to_json_float_precision(self, value, precision, expected_val):
  440:         df = DataFrame([{"a_float": value}])
  441:         encoded = df.to_json(double_precision=precision)
  442:         assert encoded == f'{{"a_float":{{"0":{expected_val}}}}}'
  443: 
  444:     def test_frame_to_json_except(self):
  445:         df = DataFrame([1, 2, 3])
  446:         msg = "Invalid value 'garbage' for option 'orient'"
  447:         with pytest.raises(ValueError, match=msg):
  448:             df.to_json(orient="garbage")
  449: 
  450:     def test_frame_empty(self):
  451:         df = DataFrame(columns=["jim", "joe"])
  452:         assert not df._is_mixed_type
  453: 
  454:         data = StringIO(df.to_json())
  455:         result = read_json(data, dtype=dict(df.dtypes))
  456:         tm.assert_frame_equal(result, df, check_index_type=False)
  457: 
  458:     def test_frame_empty_to_json(self):
  459:         # GH 7445
  460:         df = DataFrame({"test": []}, index=[])
  461:         result = df.to_json(orient="columns")
  462:         expected = '{"test":{}}'
  463:         assert result == expected
  464: 
  465:     def test_frame_empty_mixedtype(self):
  466:         # mixed type
  467:         df = DataFrame(columns=["jim", "joe"])
  468:         df["joe"] = df["joe"].astype("i8")
  469:         assert df._is_mixed_type
  470:         data = df.to_json()
  471:         tm.assert_frame_equal(
  472:             read_json(StringIO(data), dtype=dict(df.dtypes)),
  473:             df,
  474:             check_index_type=False,
  475:         )
  476: 
  477:     def test_frame_mixedtype_orient(self):  # GH10289
  478:         vals = [
  479:             [10, 1, "foo", 0.1, 0.01],
  480:             [20, 2, "bar", 0.2, 0.02],
  481:             [30, 3, "baz", 0.3, 0.03],
  482:             [40, 4, "qux", 0.4, 0.04],
  483:         ]
  484: 
  485:         df = DataFrame(
  486:             vals, index=list("abcd"), columns=["1st", "2nd", "3rd", "4th", "5th"]
  487:         )
  488: 
  489:         assert df._is_mixed_type
  490:         right = df.copy()
  491: 
  492:         for orient in ["split", "index", "columns"]:
  493:             inp = StringIO(df.to_json(orient=orient))
  494:             left = read_json(inp, orient=orient, convert_axes=False)
  495:             tm.assert_frame_equal(left, right)
  496: 
  497:         right.index = RangeIndex(len(df))
  498:         inp = StringIO(df.to_json(orient="records"))
  499:         left = read_json(inp, orient="records", convert_axes=False)
  500:         tm.assert_frame_equal(left, right)
  501: 
  502:         right.columns = RangeIndex(df.shape[1])
  503:         inp = StringIO(df.to_json(orient="values"))
  504:         left = read_json(inp, orient="values", convert_axes=False)
  505:         tm.assert_frame_equal(left, right)
  506: 
  507:     def test_v12_compat(self, datapath):
  508:         dti = date_range("2000-01-03", "2000-01-07")
  509:         # freq doesn't roundtrip
  510:         dti = DatetimeIndex(np.asarray(dti), freq=None)
  511:         df = DataFrame(
  512:             [
  513:                 [1.56808523, 0.65727391, 1.81021139, -0.17251653],
  514:                 [-0.2550111, -0.08072427, -0.03202878, -0.17581665],
  515:                 [1.51493992, 0.11805825, 1.629455, -1.31506612],
  516:                 [-0.02765498, 0.44679743, 0.33192641, -0.27885413],
  517:                 [0.05951614, -2.69652057, 1.28163262, 0.34703478],
  518:             ],
  519:             columns=["A", "B", "C", "D"],
  520:             index=dti,
  521:         )
  522:         df["date"] = Timestamp("19920106 18:21:32.12").as_unit("ns")
  523:         df.iloc[3, df.columns.get_loc("date")] = Timestamp("20130101")
  524:         df["modified"] = df["date"]
  525:         df.iloc[1, df.columns.get_loc("modified")] = pd.NaT
  526: 
  527:         dirpath = datapath("io", "json", "data")
  528:         v12_json = os.path.join(dirpath, "tsframe_v012.json")
  529:         df_unser = read_json(v12_json)
  530:         tm.assert_frame_equal(df, df_unser)
  531: 
  532:         df_iso = df.drop(["modified"], axis=1)
  533:         v12_iso_json = os.path.join(dirpath, "tsframe_iso_v012.json")
  534:         df_unser_iso = read_json(v12_iso_json)
  535:         tm.assert_frame_equal(df_iso, df_unser_iso, check_column_type=False)
  536: 
  537:     def test_blocks_compat_GH9037(self, using_infer_string):
  538:         index = date_range("20000101", periods=10, freq="h")
  539:         # freq doesn't round-trip
  540:         index = DatetimeIndex(list(index), freq=None)
  541: 
  542:         df_mixed = DataFrame(
  543:             {
  544:                 "float_1": [
  545:                     -0.92077639,
  546:                     0.77434435,
  547:                     1.25234727,
  548:                     0.61485564,
  549:                     -0.60316077,
  550:                     0.24653374,
  551:                     0.28668979,
  552:                     -2.51969012,
  553:                     0.95748401,
  554:                     -1.02970536,
  555:                 ],
  556:                 "int_1": [
  557:                     19680418,
  558:                     75337055,
  559:                     99973684,
  560:                     65103179,
  561:                     79373900,
  562:                     40314334,
  563:                     21290235,
  564:                     4991321,
  565:                     41903419,
  566:                     16008365,
  567:                 ],
  568:                 "str_1": [
  569:                     "78c608f1",
  570:                     "64a99743",
  571:                     "13d2ff52",
  572:                     "ca7f4af2",
  573:                     "97236474",
  574:                     "bde7e214",
  575:                     "1a6bde47",
  576:                     "b1190be5",
  577:                     "7a669144",
  578:                     "8d64d068",
  579:                 ],
  580:                 "float_2": [
  581:                     -0.0428278,
  582:                     -1.80872357,
  583:                     3.36042349,
  584:                     -0.7573685,
  585:                     -0.48217572,
  586:                     0.86229683,
  587:                     1.08935819,
  588:                     0.93898739,
  589:                     -0.03030452,
  590:                     1.43366348,
  591:                 ],
  592:                 "str_2": [
  593:                     "14f04af9",
  594:                     "d085da90",
  595:                     "4bcfac83",
  596:                     "81504caf",
  597:                     "2ffef4a9",
  598:                     "08e2f5c4",
  599:                     "07e1af03",
  600:                     "addbd4a7",
  601:                     "1f6a09ba",
  602:                     "4bfc4d87",
  603:                 ],
  604:                 "int_2": [
  605:                     86967717,
  606:                     98098830,
  607:                     51927505,
  608:                     20372254,
  609:                     12601730,
  610:                     20884027,
  611:                     34193846,
  612:                     10561746,
  613:                     24867120,
  614:                     76131025,
  615:                 ],
  616:             },
  617:             index=index,
  618:         )
  619: 
  620:         # JSON deserialisation always creates unicode strings
  621:         df_mixed.columns = df_mixed.columns.astype(
  622:             np.str_ if not using_infer_string else "string[pyarrow_numpy]"
  623:         )
  624:         data = StringIO(df_mixed.to_json(orient="split"))
  625:         df_roundtrip = read_json(data, orient="split")
  626:         tm.assert_frame_equal(
  627:             df_mixed,
  628:             df_roundtrip,
  629:             check_index_type=True,
  630:             check_column_type=True,
  631:             by_blocks=True,
  632:             check_exact=True,
  633:         )
  634: 
  635:     def test_frame_nonprintable_bytes(self):
  636:         # GH14256: failing column caused segfaults, if it is not the last one
  637: 
  638:         class BinaryThing:
  639:             def __init__(self, hexed) -> None:
  640:                 self.hexed = hexed
  641:                 self.binary = bytes.fromhex(hexed)
  642: 
  643:             def __str__(self) -> str:
  644:                 return self.hexed
  645: 
  646:         hexed = "574b4454ba8c5eb4f98a8f45"
  647:         binthing = BinaryThing(hexed)
  648: 
  649:         # verify the proper conversion of printable content
  650:         df_printable = DataFrame({"A": [binthing.hexed]})
  651:         assert df_printable.to_json() == f'{{"A":{{"0":"{hexed}"}}}}'
  652: 
  653:         # check if non-printable content throws appropriate Exception
  654:         df_nonprintable = DataFrame({"A": [binthing]})
  655:         msg = "Unsupported UTF-8 sequence length when encoding string"
  656:         with pytest.raises(OverflowError, match=msg):
  657:             df_nonprintable.to_json()
  658: 
  659:         # the same with multiple columns threw segfaults
  660:         df_mixed = DataFrame({"A": [binthing], "B": [1]}, columns=["A", "B"])
  661:         with pytest.raises(OverflowError, match=msg):
  662:             df_mixed.to_json()
  663: 
  664:         # default_handler should resolve exceptions for non-string types
  665:         result = df_nonprintable.to_json(default_handler=str)
  666:         expected = f'{{"A":{{"0":"{hexed}"}}}}'
  667:         assert result == expected
  668:         assert (
  669:             df_mixed.to_json(default_handler=str)
  670:             == f'{{"A":{{"0":"{hexed}"}},"B":{{"0":1}}}}'
  671:         )
  672: 
  673:     def test_label_overflow(self):
  674:         # GH14256: buffer length not checked when writing label
  675:         result = DataFrame({"bar" * 100000: [1], "foo": [1337]}).to_json()
  676:         expected = f'{{"{"bar" * 100000}":{{"0":1}},"foo":{{"0":1337}}}}'
  677:         assert result == expected
  678: 
  679:     def test_series_non_unique_index(self):
  680:         s = Series(["a", "b"], index=[1, 1])
  681: 
  682:         msg = "Series index must be unique for orient='index'"
  683:         with pytest.raises(ValueError, match=msg):
  684:             s.to_json(orient="index")
  685: 
  686:         tm.assert_series_equal(
  687:             s,
  688:             read_json(
  689:                 StringIO(s.to_json(orient="split")), orient="split", typ="series"
  690:             ),
  691:         )
  692:         unserialized = read_json(
  693:             StringIO(s.to_json(orient="records")), orient="records", typ="series"
  694:         )
  695:         tm.assert_equal(s.values, unserialized.values)
  696: 
  697:     def test_series_default_orient(self, string_series):
  698:         assert string_series.to_json() == string_series.to_json(orient="index")
  699: 
  700:     def test_series_roundtrip_simple(self, orient, string_series, using_infer_string):
  701:         data = StringIO(string_series.to_json(orient=orient))
  702:         result = read_json(data, typ="series", orient=orient)
  703: 
  704:         expected = string_series
  705:         if using_infer_string and orient in ("split", "index", "columns"):
  706:             # These schemas don't contain dtypes, so we infer string
  707:             expected.index = expected.index.astype("string[pyarrow_numpy]")
  708:         if orient in ("values", "records"):
  709:             expected = expected.reset_index(drop=True)
  710:         if orient != "split":
  711:             expected.name = None
  712: 
  713:         tm.assert_series_equal(result, expected)
  714: 
  715:     @pytest.mark.parametrize("dtype", [False, None])
  716:     def test_series_roundtrip_object(self, orient, dtype, object_series):
  717:         data = StringIO(object_series.to_json(orient=orient))
  718:         result = read_json(data, typ="series", orient=orient, dtype=dtype)
  719: 
  720:         expected = object_series
  721:         if orient in ("values", "records"):
  722:             expected = expected.reset_index(drop=True)
  723:         if orient != "split":
  724:             expected.name = None
  725: 
  726:         tm.assert_series_equal(result, expected)
  727: 
  728:     def test_series_roundtrip_empty(self, orient):
  729:         empty_series = Series([], index=[], dtype=np.float64)
  730:         data = StringIO(empty_series.to_json(orient=orient))
  731:         result = read_json(data, typ="series", orient=orient)
  732: 
  733:         expected = empty_series.reset_index(drop=True)
  734:         if orient in ("split"):
  735:             expected.index = expected.index.astype(np.float64)
  736: 
  737:         tm.assert_series_equal(result, expected)
  738: 
  739:     def test_series_roundtrip_timeseries(self, orient, datetime_series):
  740:         data = StringIO(datetime_series.to_json(orient=orient))
  741:         result = read_json(data, typ="series", orient=orient)
  742: 
  743:         expected = datetime_series
  744:         if orient in ("values", "records"):
  745:             expected = expected.reset_index(drop=True)
  746:         if orient != "split":
  747:             expected.name = None
  748: 
  749:         tm.assert_series_equal(result, expected)
  750: 
  751:     @pytest.mark.parametrize("dtype", [np.float64, int])
  752:     def test_series_roundtrip_numeric(self, orient, dtype):
  753:         s = Series(range(6), index=["a", "b", "c", "d", "e", "f"])
  754:         data = StringIO(s.to_json(orient=orient))
  755:         result = read_json(data, typ="series", orient=orient)
  756: 
  757:         expected = s.copy()
  758:         if orient in ("values", "records"):
  759:             expected = expected.reset_index(drop=True)
  760: 
  761:         tm.assert_series_equal(result, expected)
  762: 
  763:     def test_series_to_json_except(self):
  764:         s = Series([1, 2, 3])
  765:         msg = "Invalid value 'garbage' for option 'orient'"
  766:         with pytest.raises(ValueError, match=msg):
  767:             s.to_json(orient="garbage")
  768: 
  769:     def test_series_from_json_precise_float(self):
  770:         s = Series([4.56, 4.56, 4.56])
  771:         result = read_json(StringIO(s.to_json()), typ="series", precise_float=True)
  772:         tm.assert_series_equal(result, s, check_index_type=False)
  773: 
  774:     def test_series_with_dtype(self):
  775:         # GH 21986
  776:         s = Series([4.56, 4.56, 4.56])
  777:         result = read_json(StringIO(s.to_json()), typ="series", dtype=np.int64)
  778:         expected = Series([4] * 3)
  779:         tm.assert_series_equal(result, expected)
  780: 
  781:     @pytest.mark.parametrize(
  782:         "dtype,expected",
  783:         [
  784:             (True, Series(["2000-01-01"], dtype="datetime64[ns]")),
  785:             (False, Series([946684800000])),
  786:         ],
  787:     )
  788:     def test_series_with_dtype_datetime(self, dtype, expected):
  789:         s = Series(["2000-01-01"], dtype="datetime64[ns]")
  790:         data = StringIO(s.to_json())
  791:         result = read_json(data, typ="series", dtype=dtype)
  792:         tm.assert_series_equal(result, expected)
  793: 
  794:     def test_frame_from_json_precise_float(self):
  795:         df = DataFrame([[4.56, 4.56, 4.56], [4.56, 4.56, 4.56]])
  796:         result = read_json(StringIO(df.to_json()), precise_float=True)
  797:         tm.assert_frame_equal(result, df)
  798: 
  799:     def test_typ(self):
  800:         s = Series(range(6), index=["a", "b", "c", "d", "e", "f"], dtype="int64")
  801:         result = read_json(StringIO(s.to_json()), typ=None)
  802:         tm.assert_series_equal(result, s)
  803: 
  804:     def test_reconstruction_index(self):
  805:         df = DataFrame([[1, 2, 3], [4, 5, 6]])
  806:         result = read_json(StringIO(df.to_json()))
  807:         tm.assert_frame_equal(result, df)
  808: 
  809:         df = DataFrame({"a": [1, 2, 3], "b": [4, 5, 6]}, index=["A", "B", "C"])
  810:         result = read_json(StringIO(df.to_json()))
  811:         tm.assert_frame_equal(result, df)
  812: 
  813:     def test_path(self, float_frame, int_frame, datetime_frame):
  814:         with tm.ensure_clean("test.json") as path:
  815:             for df in [float_frame, int_frame, datetime_frame]:
  816:                 df.to_json(path)
  817:                 read_json(path)
  818: 
  819:     def test_axis_dates(self, datetime_series, datetime_frame):
  820:         # frame
  821:         json = StringIO(datetime_frame.to_json())
  822:         result = read_json(json)
  823:         tm.assert_frame_equal(result, datetime_frame)
  824: 
  825:         # series
  826:         json = StringIO(datetime_series.to_json())
  827:         result = read_json(json, typ="series")
  828:         tm.assert_series_equal(result, datetime_series, check_names=False)
  829:         assert result.name is None
  830: 
  831:     def test_convert_dates(self, datetime_series, datetime_frame):
  832:         # frame
  833:         df = datetime_frame
  834:         df["date"] = Timestamp("20130101").as_unit("ns")
  835: 
  836:         json = StringIO(df.to_json())
  837:         result = read_json(json)
  838:         tm.assert_frame_equal(result, df)
  839: 
  840:         df["foo"] = 1.0
  841:         json = StringIO(df.to_json(date_unit="ns"))
  842: 
  843:         result = read_json(json, convert_dates=False)
  844:         expected = df.copy()
  845:         expected["date"] = expected["date"].values.view("i8")
  846:         expected["foo"] = expected["foo"].astype("int64")
  847:         tm.assert_frame_equal(result, expected)
  848: 
  849:         # series
  850:         ts = Series(Timestamp("20130101").as_unit("ns"), index=datetime_series.index)
  851:         json = StringIO(ts.to_json())
  852:         result = read_json(json, typ="series")
  853:         tm.assert_series_equal(result, ts)
  854: 
  855:     @pytest.mark.parametrize("date_format", ["epoch", "iso"])
  856:     @pytest.mark.parametrize("as_object", [True, False])
  857:     @pytest.mark.parametrize("date_typ", [datetime.date, datetime.datetime, Timestamp])
  858:     def test_date_index_and_values(self, date_format, as_object, date_typ):
  859:         data = [date_typ(year=2020, month=1, day=1), pd.NaT]
  860:         if as_object:
  861:             data.append("a")
  862: 
  863:         ser = Series(data, index=data)
  864:         result = ser.to_json(date_format=date_format)
  865: 
  866:         if date_format == "epoch":
  867:             expected = '{"1577836800000":1577836800000,"null":null}'
  868:         else:
  869:             expected = (
  870:                 '{"2020-01-01T00:00:00.000":"2020-01-01T00:00:00.000","null":null}'
  871:             )
  872: 
  873:         if as_object:
  874:             expected = expected.replace("}", ',"a":"a"}')
  875: 
  876:         assert result == expected
  877: 
  878:     @pytest.mark.parametrize(
  879:         "infer_word",
  880:         [
  881:             "trade_time",
  882:             "date",
  883:             "datetime",
  884:             "sold_at",
  885:             "modified",
  886:             "timestamp",
  887:             "timestamps",
  888:         ],
  889:     )
  890:     def test_convert_dates_infer(self, infer_word):
  891:         # GH10747
  892: 
  893:         data = [{"id": 1, infer_word: 1036713600000}, {"id": 2}]
  894:         expected = DataFrame(
  895:             [[1, Timestamp("2002-11-08")], [2, pd.NaT]], columns=["id", infer_word]
  896:         )
  897: 
  898:         result = read_json(StringIO(ujson_dumps(data)))[["id", infer_word]]
  899:         tm.assert_frame_equal(result, expected)
  900: 
  901:     @pytest.mark.parametrize(
  902:         "date,date_unit",
  903:         [
  904:             ("20130101 20:43:42.123", None),
  905:             ("20130101 20:43:42", "s"),
  906:             ("20130101 20:43:42.123", "ms"),
  907:             ("20130101 20:43:42.123456", "us"),
  908:             ("20130101 20:43:42.123456789", "ns"),
  909:         ],
  910:     )
  911:     def test_date_format_frame(self, date, date_unit, datetime_frame):
  912:         df = datetime_frame
  913: 
  914:         df["date"] = Timestamp(date).as_unit("ns")
  915:         df.iloc[1, df.columns.get_loc("date")] = pd.NaT
  916:         df.iloc[5, df.columns.get_loc("date")] = pd.NaT
  917:         if date_unit:
  918:             json = df.to_json(date_format="iso", date_unit=date_unit)
  919:         else:
  920:             json = df.to_json(date_format="iso")
  921: 
  922:         result = read_json(StringIO(json))
  923:         expected = df.copy()
  924:         tm.assert_frame_equal(result, expected)
  925: 
  926:     def test_date_format_frame_raises(self, datetime_frame):
  927:         df = datetime_frame
  928:         msg = "Invalid value 'foo' for option 'date_unit'"
  929:         with pytest.raises(ValueError, match=msg):
  930:             df.to_json(date_format="iso", date_unit="foo")
  931: 
  932:     @pytest.mark.parametrize(
  933:         "date,date_unit",
  934:         [
  935:             ("20130101 20:43:42.123", None),
  936:             ("20130101 20:43:42", "s"),
  937:             ("20130101 20:43:42.123", "ms"),
  938:             ("20130101 20:43:42.123456", "us"),
  939:             ("20130101 20:43:42.123456789", "ns"),
  940:         ],
  941:     )
  942:     def test_date_format_series(self, date, date_unit, datetime_series):
  943:         ts = Series(Timestamp(date).as_unit("ns"), index=datetime_series.index)
  944:         ts.iloc[1] = pd.NaT
  945:         ts.iloc[5] = pd.NaT
  946:         if date_unit:
  947:             json = ts.to_json(date_format="iso", date_unit=date_unit)
  948:         else:
  949:             json = ts.to_json(date_format="iso")
  950: 
  951:         result = read_json(StringIO(json), typ="series")
  952:         expected = ts.copy()
  953:         tm.assert_series_equal(result, expected)
  954: 
  955:     def test_date_format_series_raises(self, datetime_series):
  956:         ts = Series(Timestamp("20130101 20:43:42.123"), index=datetime_series.index)
  957:         msg = "Invalid value 'foo' for option 'date_unit'"
  958:         with pytest.raises(ValueError, match=msg):
  959:             ts.to_json(date_format="iso", date_unit="foo")
  960: 
  961:     @pytest.mark.parametrize("unit", ["s", "ms", "us", "ns"])
  962:     def test_date_unit(self, unit, datetime_frame):
  963:         df = datetime_frame
  964:         df["date"] = Timestamp("20130101 20:43:42").as_unit("ns")
  965:         dl = df.columns.get_loc("date")
  966:         df.iloc[1, dl] = Timestamp("19710101 20:43:42")
  967:         df.iloc[2, dl] = Timestamp("21460101 20:43:42")
  968:         df.iloc[4, dl] = pd.NaT
  969: 
  970:         json = df.to_json(date_format="epoch", date_unit=unit)
  971: 
  972:         # force date unit
  973:         result = read_json(StringIO(json), date_unit=unit)
  974:         tm.assert_frame_equal(result, df)
  975: 
  976:         # detect date unit
  977:         result = read_json(StringIO(json), date_unit=None)
  978:         tm.assert_frame_equal(result, df)
  979: 
  980:     @pytest.mark.parametrize("unit", ["s", "ms", "us"])
  981:     def test_iso_non_nano_datetimes(self, unit):
  982:         # Test that numpy datetimes
  983:         # in an Index or a column with non-nano resolution can be serialized
  984:         # correctly
  985:         # GH53686
  986:         index = DatetimeIndex(
  987:             [np.datetime64("2023-01-01T11:22:33.123456", unit)],
  988:             dtype=f"datetime64[{unit}]",
  989:         )
  990:         df = DataFrame(
  991:             {
  992:                 "date": Series(
  993:                     [np.datetime64("2022-01-01T11:22:33.123456", unit)],
  994:                     dtype=f"datetime64[{unit}]",
  995:                     index=index,
  996:                 ),
  997:                 "date_obj": Series(
  998:                     [np.datetime64("2023-01-01T11:22:33.123456", unit)],
  999:                     dtype=object,
 1000:                     index=index,
 1001:                 ),
 1002:             },
 1003:         )
 1004: 
 1005:         buf = StringIO()
 1006:         df.to_json(buf, date_format="iso", date_unit=unit)
 1007:         buf.seek(0)
 1008: 
 1009:         # read_json always reads datetimes in nanosecond resolution
 1010:         # TODO: check_dtype/check_index_type should be removable
 1011:         # once read_json gets non-nano support
 1012:         tm.assert_frame_equal(
 1013:             read_json(buf, convert_dates=["date", "date_obj"]),
 1014:             df,
 1015:             check_index_type=False,
 1016:             check_dtype=False,
 1017:         )
 1018: 
 1019:     def test_weird_nested_json(self):
 1020:         # this used to core dump the parser
 1021:         s = r"""{
 1022:         "status": "success",
 1023:         "data": {
 1024:         "posts": [
 1025:             {
 1026:             "id": 1,
 1027:             "title": "A blog post",
 1028:             "body": "Some useful content"
 1029:             },
 1030:             {
 1031:             "id": 2,
 1032:             "title": "Another blog post",
 1033:             "body": "More content"
 1034:             }
 1035:            ]
 1036:           }
 1037:         }"""
 1038:         read_json(StringIO(s))
 1039: 
 1040:     def test_doc_example(self):
 1041:         dfj2 = DataFrame(
 1042:             np.random.default_rng(2).standard_normal((5, 2)), columns=list("AB")
 1043:         )
 1044:         dfj2["date"] = Timestamp("20130101")
 1045:         dfj2["ints"] = range(5)
 1046:         dfj2["bools"] = True
 1047:         dfj2.index = date_range("20130101", periods=5)
 1048: 
 1049:         json = StringIO(dfj2.to_json())
 1050:         result = read_json(json, dtype={"ints": np.int64, "bools": np.bool_})
 1051:         tm.assert_frame_equal(result, result)
 1052: 
 1053:     def test_round_trip_exception(self, datapath):
 1054:         # GH 3867
 1055:         path = datapath("io", "json", "data", "teams.csv")
 1056:         df = pd.read_csv(path)
 1057:         s = df.to_json()
 1058: 
 1059:         result = read_json(StringIO(s))
 1060:         res = result.reindex(index=df.index, columns=df.columns)
 1061:         msg = "The 'downcast' keyword in fillna is deprecated"
 1062:         with tm.assert_produces_warning(FutureWarning, match=msg):
 1063:             res = res.fillna(np.nan, downcast=False)
 1064:         tm.assert_frame_equal(res, df)
 1065: 
 1066:     @pytest.mark.network
 1067:     @pytest.mark.single_cpu
 1068:     @pytest.mark.parametrize(
 1069:         "field,dtype",
 1070:         [
 1071:             ["created_at", pd.DatetimeTZDtype(tz="UTC")],
 1072:             ["closed_at", "datetime64[ns]"],
 1073:             ["updated_at", pd.DatetimeTZDtype(tz="UTC")],
 1074:         ],
 1075:     )
 1076:     def test_url(self, field, dtype, httpserver):
 1077:         data = '{"created_at": ["2023-06-23T18:21:36Z"], "closed_at": ["2023-06-23T18:21:36"], "updated_at": ["2023-06-23T18:21:36Z"]}\n'  # noqa: E501
 1078:         httpserver.serve_content(content=data)
 1079:         result = read_json(httpserver.url, convert_dates=True)
 1080:         assert result[field].dtype == dtype
 1081: 
 1082:     def test_timedelta(self):
 1083:         converter = lambda x: pd.to_timedelta(x, unit="ms")
 1084: 
 1085:         ser = Series([timedelta(23), timedelta(seconds=5)])
 1086:         assert ser.dtype == "timedelta64[ns]"
 1087: 
 1088:         result = read_json(StringIO(ser.to_json()), typ="series").apply(converter)
 1089:         tm.assert_series_equal(result, ser)
 1090: 
 1091:         ser = Series([timedelta(23), timedelta(seconds=5)], index=Index([0, 1]))
 1092:         assert ser.dtype == "timedelta64[ns]"
 1093:         result = read_json(StringIO(ser.to_json()), typ="series").apply(converter)
 1094:         tm.assert_series_equal(result, ser)
 1095: 
 1096:         frame = DataFrame([timedelta(23), timedelta(seconds=5)])
 1097:         assert frame[0].dtype == "timedelta64[ns]"
 1098:         tm.assert_frame_equal(
 1099:             frame, read_json(StringIO(frame.to_json())).apply(converter)
 1100:         )
 1101: 
 1102:     def test_timedelta2(self):
 1103:         frame = DataFrame(
 1104:             {
 1105:                 "a": [timedelta(days=23), timedelta(seconds=5)],
 1106:                 "b": [1, 2],
 1107:                 "c": date_range(start="20130101", periods=2),
 1108:             }
 1109:         )
 1110:         data = StringIO(frame.to_json(date_unit="ns"))
 1111:         result = read_json(data)
 1112:         result["a"] = pd.to_timedelta(result.a, unit="ns")
 1113:         result["c"] = pd.to_datetime(result.c)
 1114:         tm.assert_frame_equal(frame, result)
 1115: 
 1116:     def test_mixed_timedelta_datetime(self):
 1117:         td = timedelta(23)
 1118:         ts = Timestamp("20130101")
 1119:         frame = DataFrame({"a": [td, ts]}, dtype=object)
 1120: 
 1121:         expected = DataFrame(
 1122:             {"a": [pd.Timedelta(td).as_unit("ns")._value, ts.as_unit("ns")._value]}
 1123:         )
 1124:         data = StringIO(frame.to_json(date_unit="ns"))
 1125:         result = read_json(data, dtype={"a": "int64"})
 1126:         tm.assert_frame_equal(result, expected, check_index_type=False)
 1127: 
 1128:     @pytest.mark.parametrize("as_object", [True, False])
 1129:     @pytest.mark.parametrize("date_format", ["iso", "epoch"])
 1130:     @pytest.mark.parametrize("timedelta_typ", [pd.Timedelta, timedelta])
 1131:     def test_timedelta_to_json(self, as_object, date_format, timedelta_typ):
 1132:         # GH28156: to_json not correctly formatting Timedelta
 1133:         data = [timedelta_typ(days=1), timedelta_typ(days=2), pd.NaT]
 1134:         if as_object:
 1135:             data.append("a")
 1136: 
 1137:         ser = Series(data, index=data)
 1138:         if date_format == "iso":
 1139:             expected = (
 1140:                 '{"P1DT0H0M0S":"P1DT0H0M0S","P2DT0H0M0S":"P2DT0H0M0S","null":null}'
 1141:             )
 1142:         else:
 1143:             expected = '{"86400000":86400000,"172800000":172800000,"null":null}'
 1144: 
 1145:         if as_object:
 1146:             expected = expected.replace("}", ',"a":"a"}')
 1147: 
 1148:         result = ser.to_json(date_format=date_format)
 1149:         assert result == expected
 1150: 
 1151:     @pytest.mark.parametrize("as_object", [True, False])
 1152:     @pytest.mark.parametrize("timedelta_typ", [pd.Timedelta, timedelta])
 1153:     def test_timedelta_to_json_fractional_precision(self, as_object, timedelta_typ):
 1154:         data = [timedelta_typ(milliseconds=42)]
 1155:         ser = Series(data, index=data)
 1156:         if as_object:
 1157:             ser = ser.astype(object)
 1158: 
 1159:         result = ser.to_json()
 1160:         expected = '{"42":42}'
 1161:         assert result == expected
 1162: 
 1163:     def test_default_handler(self):
 1164:         value = object()
 1165:         frame = DataFrame({"a": [7, value]})
 1166:         expected = DataFrame({"a": [7, str(value)]})
 1167:         result = read_json(StringIO(frame.to_json(default_handler=str)))
 1168:         tm.assert_frame_equal(expected, result, check_index_type=False)
 1169: 
 1170:     def test_default_handler_indirect(self):
 1171:         def default(obj):
 1172:             if isinstance(obj, complex):
 1173:                 return [("mathjs", "Complex"), ("re", obj.real), ("im", obj.imag)]
 1174:             return str(obj)
 1175: 
 1176:         df_list = [
 1177:             9,
 1178:             DataFrame(
 1179:                 {"a": [1, "STR", complex(4, -5)], "b": [float("nan"), None, "N/A"]},
 1180:                 columns=["a", "b"],
 1181:             ),
 1182:         ]
 1183:         expected = (
 1184:             '[9,[[1,null],["STR",null],[[["mathjs","Complex"],'
 1185:             '["re",4.0],["im",-5.0]],"N\\/A"]]]'
 1186:         )
 1187:         assert (
 1188:             ujson_dumps(df_list, default_handler=default, orient="values") == expected
 1189:         )
 1190: 
 1191:     def test_default_handler_numpy_unsupported_dtype(self):
 1192:         # GH12554 to_json raises 'Unhandled numpy dtype 15'
 1193:         df = DataFrame(
 1194:             {"a": [1, 2.3, complex(4, -5)], "b": [float("nan"), None, complex(1.2, 0)]},
 1195:             columns=["a", "b"],
 1196:         )
 1197:         expected = (
 1198:             '[["(1+0j)","(nan+0j)"],'
 1199:             '["(2.3+0j)","(nan+0j)"],'
 1200:             '["(4-5j)","(1.2+0j)"]]'
 1201:         )
 1202:         assert df.to_json(default_handler=str, orient="values") == expected
 1203: 
 1204:     def test_default_handler_raises(self):
 1205:         msg = "raisin"
 1206: 
 1207:         def my_handler_raises(obj):
 1208:             raise TypeError(msg)
 1209: 
 1210:         with pytest.raises(TypeError, match=msg):
 1211:             DataFrame({"a": [1, 2, object()]}).to_json(
 1212:                 default_handler=my_handler_raises
 1213:             )
 1214:         with pytest.raises(TypeError, match=msg):
 1215:             DataFrame({"a": [1, 2, complex(4, -5)]}).to_json(
 1216:                 default_handler=my_handler_raises
 1217:             )
 1218: 
 1219:     def test_categorical(self):
 1220:         # GH4377 df.to_json segfaults with non-ndarray blocks
 1221:         df = DataFrame({"A": ["a", "b", "c", "a", "b", "b", "a"]})
 1222:         df["B"] = df["A"]
 1223:         expected = df.to_json()
 1224: 
 1225:         df["B"] = df["A"].astype("category")
 1226:         assert expected == df.to_json()
 1227: 
 1228:         s = df["A"]
 1229:         sc = df["B"]
 1230:         assert s.to_json() == sc.to_json()
 1231: 
 1232:     def test_datetime_tz(self):
 1233:         # GH4377 df.to_json segfaults with non-ndarray blocks
 1234:         tz_range = date_range("20130101", periods=3, tz="US/Eastern")
 1235:         tz_naive = tz_range.tz_convert("utc").tz_localize(None)
 1236: 
 1237:         df = DataFrame({"A": tz_range, "B": date_range("20130101", periods=3)})
 1238: 
 1239:         df_naive = df.copy()
 1240:         df_naive["A"] = tz_naive
 1241:         expected = df_naive.to_json()
 1242:         assert expected == df.to_json()
 1243: 
 1244:         stz = Series(tz_range)
 1245:         s_naive = Series(tz_naive)
 1246:         assert stz.to_json() == s_naive.to_json()
 1247: 
 1248:     def test_sparse(self):
 1249:         # GH4377 df.to_json segfaults with non-ndarray blocks
 1250:         df = DataFrame(np.random.default_rng(2).standard_normal((10, 4)))
 1251:         df.loc[:8] = np.nan
 1252: 
 1253:         sdf = df.astype("Sparse")
 1254:         expected = df.to_json()
 1255:         assert expected == sdf.to_json()
 1256: 
 1257:         s = Series(np.random.default_rng(2).standard_normal(10))
 1258:         s.loc[:8] = np.nan
 1259:         ss = s.astype("Sparse")
 1260: 
 1261:         expected = s.to_json()
 1262:         assert expected == ss.to_json()
 1263: 
 1264:     @pytest.mark.parametrize(
 1265:         "ts",
 1266:         [
 1267:             Timestamp("2013-01-10 05:00:00Z"),
 1268:             Timestamp("2013-01-10 00:00:00", tz="US/Eastern"),
 1269:             Timestamp("2013-01-10 00:00:00-0500"),
 1270:         ],
 1271:     )
 1272:     def test_tz_is_utc(self, ts):
 1273:         exp = '"2013-01-10T05:00:00.000Z"'
 1274: 
 1275:         assert ujson_dumps(ts, iso_dates=True) == exp
 1276:         dt = ts.to_pydatetime()
 1277:         assert ujson_dumps(dt, iso_dates=True) == exp
 1278: 
 1279:     def test_tz_is_naive(self):
 1280:         ts = Timestamp("2013-01-10 05:00:00")
 1281:         exp = '"2013-01-10T05:00:00.000"'
 1282: 
 1283:         assert ujson_dumps(ts, iso_dates=True) == exp
 1284:         dt = ts.to_pydatetime()
 1285:         assert ujson_dumps(dt, iso_dates=True) == exp
 1286: 
 1287:     @pytest.mark.parametrize(
 1288:         "tz_range",
 1289:         [
 1290:             date_range("2013-01-01 05:00:00Z", periods=2),
 1291:             date_range("2013-01-01 00:00:00", periods=2, tz="US/Eastern"),
 1292:             date_range("2013-01-01 00:00:00-0500", periods=2),
 1293:         ],
 1294:     )
 1295:     def test_tz_range_is_utc(self, tz_range):
 1296:         exp = '["2013-01-01T05:00:00.000Z","2013-01-02T05:00:00.000Z"]'
 1297:         dfexp = (
 1298:             '{"DT":{'
 1299:             '"0":"2013-01-01T05:00:00.000Z",'
 1300:             '"1":"2013-01-02T05:00:00.000Z"}}'
 1301:         )
 1302: 
 1303:         assert ujson_dumps(tz_range, iso_dates=True) == exp
 1304:         dti = DatetimeIndex(tz_range)
 1305:         # Ensure datetimes in object array are serialized correctly
 1306:         # in addition to the normal DTI case
 1307:         assert ujson_dumps(dti, iso_dates=True) == exp
 1308:         assert ujson_dumps(dti.astype(object), iso_dates=True) == exp
 1309:         df = DataFrame({"DT": dti})
 1310:         result = ujson_dumps(df, iso_dates=True)
 1311:         assert result == dfexp
 1312:         assert ujson_dumps(df.astype({"DT": object}), iso_dates=True)
 1313: 
 1314:     def test_tz_range_is_naive(self):
 1315:         dti = date_range("2013-01-01 05:00:00", periods=2)
 1316: 
 1317:         exp = '["2013-01-01T05:00:00.000","2013-01-02T05:00:00.000"]'
 1318:         dfexp = '{"DT":{"0":"2013-01-01T05:00:00.000","1":"2013-01-02T05:00:00.000"}}'
 1319: 
 1320:         # Ensure datetimes in object array are serialized correctly
 1321:         # in addition to the normal DTI case
 1322:         assert ujson_dumps(dti, iso_dates=True) == exp
 1323:         assert ujson_dumps(dti.astype(object), iso_dates=True) == exp
 1324:         df = DataFrame({"DT": dti})
 1325:         result = ujson_dumps(df, iso_dates=True)
 1326:         assert result == dfexp
 1327:         assert ujson_dumps(df.astype({"DT": object}), iso_dates=True)
 1328: 
 1329:     def test_read_inline_jsonl(self):
 1330:         # GH9180
 1331: 
 1332:         result = read_json(StringIO('{"a": 1, "b": 2}\n{"b":2, "a" :1}\n'), lines=True)
 1333:         expected = DataFrame([[1, 2], [1, 2]], columns=["a", "b"])
 1334:         tm.assert_frame_equal(result, expected)
 1335: 
 1336:     @pytest.mark.single_cpu
 1337:     @td.skip_if_not_us_locale
 1338:     def test_read_s3_jsonl(self, s3_public_bucket_with_data, s3so):
 1339:         # GH17200
 1340: 
 1341:         result = read_json(
 1342:             f"s3n://{s3_public_bucket_with_data.name}/items.jsonl",
 1343:             lines=True,
 1344:             storage_options=s3so,
 1345:         )
 1346:         expected = DataFrame([[1, 2], [1, 2]], columns=["a", "b"])
 1347:         tm.assert_frame_equal(result, expected)
 1348: 
 1349:     def test_read_local_jsonl(self):
 1350:         # GH17200
 1351:         with tm.ensure_clean("tmp_items.json") as path:
 1352:             with open(path, "w", encoding="utf-8") as infile:
 1353:                 infile.write('{"a": 1, "b": 2}\n{"b":2, "a" :1}\n')
 1354:             result = read_json(path, lines=True)
 1355:             expected = DataFrame([[1, 2], [1, 2]], columns=["a", "b"])
 1356:             tm.assert_frame_equal(result, expected)
 1357: 
 1358:     def test_read_jsonl_unicode_chars(self):
 1359:         # GH15132: non-ascii unicode characters
 1360:         # \u201d == RIGHT DOUBLE QUOTATION MARK
 1361: 
 1362:         # simulate file handle
 1363:         json = '{"a": "fooвЂќ", "b": "bar"}\n{"a": "foo", "b": "bar"}\n'
 1364:         json = StringIO(json)
 1365:         result = read_json(json, lines=True)
 1366:         expected = DataFrame([["foo\u201d", "bar"], ["foo", "bar"]], columns=["a", "b"])
 1367:         tm.assert_frame_equal(result, expected)
 1368: 
 1369:         # simulate string
 1370:         json = StringIO('{"a": "fooвЂќ", "b": "bar"}\n{"a": "foo", "b": "bar"}\n')
 1371:         result = read_json(json, lines=True)
 1372:         expected = DataFrame([["foo\u201d", "bar"], ["foo", "bar"]], columns=["a", "b"])
 1373:         tm.assert_frame_equal(result, expected)
 1374: 
 1375:     @pytest.mark.parametrize("bigNum", [sys.maxsize + 1, -(sys.maxsize + 2)])
 1376:     def test_to_json_large_numbers(self, bigNum):
 1377:         # GH34473
 1378:         series = Series(bigNum, dtype=object, index=["articleId"])
 1379:         json = series.to_json()
 1380:         expected = '{"articleId":' + str(bigNum) + "}"
 1381:         assert json == expected
 1382: 
 1383:         df = DataFrame(bigNum, dtype=object, index=["articleId"], columns=[0])
 1384:         json = df.to_json()
 1385:         expected = '{"0":{"articleId":' + str(bigNum) + "}}"
 1386:         assert json == expected
 1387: 
 1388:     @pytest.mark.parametrize("bigNum", [-(2**63) - 1, 2**64])
 1389:     def test_read_json_large_numbers(self, bigNum):
 1390:         # GH20599, 26068
 1391:         json = StringIO('{"articleId":' + str(bigNum) + "}")
 1392:         msg = r"Value is too small|Value is too big"
 1393:         with pytest.raises(ValueError, match=msg):
 1394:             read_json(json)
 1395: 
 1396:         json = StringIO('{"0":{"articleId":' + str(bigNum) + "}}")
 1397:         with pytest.raises(ValueError, match=msg):
 1398:             read_json(json)
 1399: 
 1400:     def test_read_json_large_numbers2(self):
 1401:         # GH18842
 1402:         json = '{"articleId": "1404366058080022500245"}'
 1403:         json = StringIO(json)
 1404:         result = read_json(json, typ="series")
 1405:         expected = Series(1.404366e21, index=["articleId"])
 1406:         tm.assert_series_equal(result, expected)
 1407: 
 1408:         json = '{"0": {"articleId": "1404366058080022500245"}}'
 1409:         json = StringIO(json)
 1410:         result = read_json(json)
 1411:         expected = DataFrame(1.404366e21, index=["articleId"], columns=[0])
 1412:         tm.assert_frame_equal(result, expected)
 1413: 
 1414:     def test_to_jsonl(self):
 1415:         # GH9180
 1416:         df = DataFrame([[1, 2], [1, 2]], columns=["a", "b"])
 1417:         result = df.to_json(orient="records", lines=True)
 1418:         expected = '{"a":1,"b":2}\n{"a":1,"b":2}\n'
 1419:         assert result == expected
 1420: 
 1421:         df = DataFrame([["foo}", "bar"], ['foo"', "bar"]], columns=["a", "b"])
 1422:         result = df.to_json(orient="records", lines=True)
 1423:         expected = '{"a":"foo}","b":"bar"}\n{"a":"foo\\"","b":"bar"}\n'
 1424:         assert result == expected
 1425:         tm.assert_frame_equal(read_json(StringIO(result), lines=True), df)
 1426: 
 1427:         # GH15096: escaped characters in columns and data
 1428:         df = DataFrame([["foo\\", "bar"], ['foo"', "bar"]], columns=["a\\", "b"])
 1429:         result = df.to_json(orient="records", lines=True)
 1430:         expected = '{"a\\\\":"foo\\\\","b":"bar"}\n{"a\\\\":"foo\\"","b":"bar"}\n'
 1431:         assert result == expected
 1432: 
 1433:         tm.assert_frame_equal(read_json(StringIO(result), lines=True), df)
 1434: 
 1435:     # TODO: there is a near-identical test for pytables; can we share?
 1436:     @pytest.mark.xfail(reason="GH#13774 encoding kwarg not supported", raises=TypeError)
 1437:     @pytest.mark.parametrize(
 1438:         "val",
 1439:         [
 1440:             [b"E\xc9, 17", b"", b"a", b"b", b"c"],
 1441:             [b"E\xc9, 17", b"a", b"b", b"c"],
 1442:             [b"EE, 17", b"", b"a", b"b", b"c"],
 1443:             [b"E\xc9, 17", b"\xf8\xfc", b"a", b"b", b"c"],
 1444:             [b"", b"a", b"b", b"c"],
 1445:             [b"\xf8\xfc", b"a", b"b", b"c"],
 1446:             [b"A\xf8\xfc", b"", b"a", b"b", b"c"],
 1447:             [np.nan, b"", b"b", b"c"],
 1448:             [b"A\xf8\xfc", np.nan, b"", b"b", b"c"],
 1449:         ],
 1450:     )
 1451:     @pytest.mark.parametrize("dtype", ["category", object])
 1452:     def test_latin_encoding(self, dtype, val):
 1453:         # GH 13774
 1454:         ser = Series(
 1455:             [x.decode("latin-1") if isinstance(x, bytes) else x for x in val],
 1456:             dtype=dtype,
 1457:         )
 1458:         encoding = "latin-1"
 1459:         with tm.ensure_clean("test.json") as path:
 1460:             ser.to_json(path, encoding=encoding)
 1461:             retr = read_json(StringIO(path), encoding=encoding)
 1462:             tm.assert_series_equal(ser, retr, check_categorical=False)
 1463: 
 1464:     def test_data_frame_size_after_to_json(self):
 1465:         # GH15344
 1466:         df = DataFrame({"a": [str(1)]})
 1467: 
 1468:         size_before = df.memory_usage(index=True, deep=True).sum()
 1469:         df.to_json()
 1470:         size_after = df.memory_usage(index=True, deep=True).sum()
 1471: 
 1472:         assert size_before == size_after
 1473: 
 1474:     @pytest.mark.parametrize(
 1475:         "index", [None, [1, 2], [1.0, 2.0], ["a", "b"], ["1", "2"], ["1.", "2."]]
 1476:     )
 1477:     @pytest.mark.parametrize("columns", [["a", "b"], ["1", "2"], ["1.", "2."]])
 1478:     def test_from_json_to_json_table_index_and_columns(self, index, columns):
 1479:         # GH25433 GH25435
 1480:         expected = DataFrame([[1, 2], [3, 4]], index=index, columns=columns)
 1481:         dfjson = expected.to_json(orient="table")
 1482: 
 1483:         result = read_json(StringIO(dfjson), orient="table")
 1484:         tm.assert_frame_equal(result, expected)
 1485: 
 1486:     def test_from_json_to_json_table_dtypes(self):
 1487:         # GH21345
 1488:         expected = DataFrame({"a": [1, 2], "b": [3.0, 4.0], "c": ["5", "6"]})
 1489:         dfjson = expected.to_json(orient="table")
 1490:         result = read_json(StringIO(dfjson), orient="table")
 1491:         tm.assert_frame_equal(result, expected)
 1492: 
 1493:     # TODO: We are casting to string which coerces None to NaN before casting back
 1494:     # to object, ending up with incorrect na values
 1495:     @pytest.mark.xfail(using_pyarrow_string_dtype(), reason="incorrect na conversion")
 1496:     @pytest.mark.parametrize("orient", ["split", "records", "index", "columns"])
 1497:     def test_to_json_from_json_columns_dtypes(self, orient):
 1498:         # GH21892 GH33205
 1499:         expected = DataFrame.from_dict(
 1500:             {
 1501:                 "Integer": Series([1, 2, 3], dtype="int64"),
 1502:                 "Float": Series([None, 2.0, 3.0], dtype="float64"),
 1503:                 "Object": Series([None, "", "c"], dtype="object"),
 1504:                 "Bool": Series([True, False, True], dtype="bool"),
 1505:                 "Category": Series(["a", "b", None], dtype="category"),
 1506:                 "Datetime": Series(
 1507:                     ["2020-01-01", None, "2020-01-03"], dtype="datetime64[ns]"
 1508:                 ),
 1509:             }
 1510:         )
 1511:         dfjson = expected.to_json(orient=orient)
 1512: 
 1513:         result = read_json(
 1514:             StringIO(dfjson),
 1515:             orient=orient,
 1516:             dtype={
 1517:                 "Integer": "int64",
 1518:                 "Float": "float64",
 1519:                 "Object": "object",
 1520:                 "Bool": "bool",
 1521:                 "Category": "category",
 1522:                 "Datetime": "datetime64[ns]",
 1523:             },
 1524:         )
 1525:         tm.assert_frame_equal(result, expected)
 1526: 
 1527:     @pytest.mark.parametrize("dtype", [True, {"b": int, "c": int}])
 1528:     def test_read_json_table_dtype_raises(self, dtype):
 1529:         # GH21345
 1530:         df = DataFrame({"a": [1, 2], "b": [3.0, 4.0], "c": ["5", "6"]})
 1531:         dfjson = df.to_json(orient="table")
 1532:         msg = "cannot pass both dtype and orient='table'"
 1533:         with pytest.raises(ValueError, match=msg):
 1534:             read_json(dfjson, orient="table", dtype=dtype)
 1535: 
 1536:     @pytest.mark.parametrize("orient", ["index", "columns", "records", "values"])
 1537:     def test_read_json_table_empty_axes_dtype(self, orient):
 1538:         # GH28558
 1539: 
 1540:         expected = DataFrame()
 1541:         result = read_json(StringIO("{}"), orient=orient, convert_axes=True)
 1542:         tm.assert_index_equal(result.index, expected.index)
 1543:         tm.assert_index_equal(result.columns, expected.columns)
 1544: 
 1545:     def test_read_json_table_convert_axes_raises(self):
 1546:         # GH25433 GH25435
 1547:         df = DataFrame([[1, 2], [3, 4]], index=[1.0, 2.0], columns=["1.", "2."])
 1548:         dfjson = df.to_json(orient="table")
 1549:         msg = "cannot pass both convert_axes and orient='table'"
 1550:         with pytest.raises(ValueError, match=msg):
 1551:             read_json(dfjson, orient="table", convert_axes=True)
 1552: 
 1553:     @pytest.mark.parametrize(
 1554:         "data, expected",
 1555:         [
 1556:             (
 1557:                 DataFrame([[1, 2], [4, 5]], columns=["a", "b"]),
 1558:                 {"columns": ["a", "b"], "data": [[1, 2], [4, 5]]},
 1559:             ),
 1560:             (
 1561:                 DataFrame([[1, 2], [4, 5]], columns=["a", "b"]).rename_axis("foo"),
 1562:                 {"columns": ["a", "b"], "data": [[1, 2], [4, 5]]},
 1563:             ),
 1564:             (
 1565:                 DataFrame(
 1566:                     [[1, 2], [4, 5]], columns=["a", "b"], index=[["a", "b"], ["c", "d"]]
 1567:                 ),
 1568:                 {"columns": ["a", "b"], "data": [[1, 2], [4, 5]]},
 1569:             ),
 1570:             (Series([1, 2, 3], name="A"), {"name": "A", "data": [1, 2, 3]}),
 1571:             (
 1572:                 Series([1, 2, 3], name="A").rename_axis("foo"),
 1573:                 {"name": "A", "data": [1, 2, 3]},
 1574:             ),
 1575:             (
 1576:                 Series([1, 2], name="A", index=[["a", "b"], ["c", "d"]]),
 1577:                 {"name": "A", "data": [1, 2]},
 1578:             ),
 1579:         ],
 1580:     )
 1581:     def test_index_false_to_json_split(self, data, expected):
 1582:         # GH 17394
 1583:         # Testing index=False in to_json with orient='split'
 1584: 
 1585:         result = data.to_json(orient="split", index=False)
 1586:         result = json.loads(result)
 1587: 
 1588:         assert result == expected
 1589: 
 1590:     @pytest.mark.parametrize(
 1591:         "data",
 1592:         [
 1593:             (DataFrame([[1, 2], [4, 5]], columns=["a", "b"])),
 1594:             (DataFrame([[1, 2], [4, 5]], columns=["a", "b"]).rename_axis("foo")),
 1595:             (
 1596:                 DataFrame(
 1597:                     [[1, 2], [4, 5]], columns=["a", "b"], index=[["a", "b"], ["c", "d"]]
 1598:                 )
 1599:             ),
 1600:             (Series([1, 2, 3], name="A")),
 1601:             (Series([1, 2, 3], name="A").rename_axis("foo")),
 1602:             (Series([1, 2], name="A", index=[["a", "b"], ["c", "d"]])),
 1603:         ],
 1604:     )
 1605:     def test_index_false_to_json_table(self, data):
 1606:         # GH 17394
 1607:         # Testing index=False in to_json with orient='table'
 1608: 
 1609:         result = data.to_json(orient="table", index=False)
 1610:         result = json.loads(result)
 1611: 
 1612:         expected = {
 1613:             "schema": pd.io.json.build_table_schema(data, index=False),
 1614:             "data": DataFrame(data).to_dict(orient="records"),
 1615:         }
 1616: 
 1617:         assert result == expected
 1618: 
 1619:     @pytest.mark.parametrize("orient", ["index", "columns"])
 1620:     def test_index_false_error_to_json(self, orient):
 1621:         # GH 17394, 25513
 1622:         # Testing error message from to_json with index=False
 1623: 
 1624:         df = DataFrame([[1, 2], [4, 5]], columns=["a", "b"])
 1625: 
 1626:         msg = (
 1627:             "'index=False' is only valid when 'orient' is 'split', "
 1628:             "'table', 'records', or 'values'"
 1629:         )
 1630:         with pytest.raises(ValueError, match=msg):
 1631:             df.to_json(orient=orient, index=False)
 1632: 
 1633:     @pytest.mark.parametrize("orient", ["records", "values"])
 1634:     def test_index_true_error_to_json(self, orient):
 1635:         # GH 25513
 1636:         # Testing error message from to_json with index=True
 1637: 
 1638:         df = DataFrame([[1, 2], [4, 5]], columns=["a", "b"])
 1639: 
 1640:         msg = (
 1641:             "'index=True' is only valid when 'orient' is 'split', "
 1642:             "'table', 'index', or 'columns'"
 1643:         )
 1644:         with pytest.raises(ValueError, match=msg):
 1645:             df.to_json(orient=orient, index=True)
 1646: 
 1647:     @pytest.mark.parametrize("orient", ["split", "table"])
 1648:     @pytest.mark.parametrize("index", [True, False])
 1649:     def test_index_false_from_json_to_json(self, orient, index):
 1650:         # GH25170
 1651:         # Test index=False in from_json to_json
 1652:         expected = DataFrame({"a": [1, 2], "b": [3, 4]})
 1653:         dfjson = expected.to_json(orient=orient, index=index)
 1654:         result = read_json(StringIO(dfjson), orient=orient)
 1655:         tm.assert_frame_equal(result, expected)
 1656: 
 1657:     def test_read_timezone_information(self):
 1658:         # GH 25546
 1659:         result = read_json(
 1660:             StringIO('{"2019-01-01T11:00:00.000Z":88}'), typ="series", orient="index"
 1661:         )
 1662:         exp_dti = DatetimeIndex(["2019-01-01 11:00:00"], dtype="M8[ns, UTC]")
 1663:         expected = Series([88], index=exp_dti)
 1664:         tm.assert_series_equal(result, expected)
 1665: 
 1666:     @pytest.mark.parametrize(
 1667:         "url",
 1668:         [
 1669:             "s3://example-fsspec/",
 1670:             "gcs://another-fsspec/file.json",
 1671:             "https://example-site.com/data",
 1672:             "some-protocol://data.txt",
 1673:         ],
 1674:     )
 1675:     def test_read_json_with_url_value(self, url):
 1676:         # GH 36271
 1677:         result = read_json(StringIO(f'{{"url":{{"0":"{url}"}}}}'))
 1678:         expected = DataFrame({"url": [url]})
 1679:         tm.assert_frame_equal(result, expected)
 1680: 
 1681:     @pytest.mark.parametrize(
 1682:         "compression",
 1683:         ["", ".gz", ".bz2", ".tar"],
 1684:     )
 1685:     def test_read_json_with_very_long_file_path(self, compression):
 1686:         # GH 46718
 1687:         long_json_path = f'{"a" * 1000}.json{compression}'
 1688:         with pytest.raises(
 1689:             FileNotFoundError, match=f"File {long_json_path} does not exist"
 1690:         ):
 1691:             # path too long for Windows is handled in file_exists() but raises in
 1692:             # _get_data_from_filepath()
 1693:             read_json(long_json_path)
 1694: 
 1695:     @pytest.mark.parametrize(
 1696:         "date_format,key", [("epoch", 86400000), ("iso", "P1DT0H0M0S")]
 1697:     )
 1698:     def test_timedelta_as_label(self, date_format, key):
 1699:         df = DataFrame([[1]], columns=[pd.Timedelta("1D")])
 1700:         expected = f'{{"{key}":{{"0":1}}}}'
 1701:         result = df.to_json(date_format=date_format)
 1702: 
 1703:         assert result == expected
 1704: 
 1705:     @pytest.mark.parametrize(
 1706:         "orient,expected",
 1707:         [
 1708:             ("index", "{\"('a', 'b')\":{\"('c', 'd')\":1}}"),
 1709:             ("columns", "{\"('c', 'd')\":{\"('a', 'b')\":1}}"),
 1710:             # TODO: the below have separate encoding procedures
 1711:             pytest.param(
 1712:                 "split",
 1713:                 "",
 1714:                 marks=pytest.mark.xfail(
 1715:                     reason="Produces JSON but not in a consistent manner"
 1716:                 ),
 1717:             ),
 1718:             pytest.param(
 1719:                 "table",
 1720:                 "",
 1721:                 marks=pytest.mark.xfail(
 1722:                     reason="Produces JSON but not in a consistent manner"
 1723:                 ),
 1724:             ),
 1725:         ],
 1726:     )
 1727:     def test_tuple_labels(self, orient, expected):
 1728:         # GH 20500
 1729:         df = DataFrame([[1]], index=[("a", "b")], columns=[("c", "d")])
 1730:         result = df.to_json(orient=orient)
 1731:         assert result == expected
 1732: 
 1733:     @pytest.mark.parametrize("indent", [1, 2, 4])
 1734:     def test_to_json_indent(self, indent):
 1735:         # GH 12004
 1736:         df = DataFrame([["foo", "bar"], ["baz", "qux"]], columns=["a", "b"])
 1737: 
 1738:         result = df.to_json(indent=indent)
 1739:         spaces = " " * indent
 1740:         expected = f"""{{
 1741: {spaces}"a":{{
 1742: {spaces}{spaces}"0":"foo",
 1743: {spaces}{spaces}"1":"baz"
 1744: {spaces}}},
 1745: {spaces}"b":{{
 1746: {spaces}{spaces}"0":"bar",
 1747: {spaces}{spaces}"1":"qux"
 1748: {spaces}}}
 1749: }}"""
 1750: 
 1751:         assert result == expected
 1752: 
 1753:     @pytest.mark.skipif(
 1754:         using_pyarrow_string_dtype(),
 1755:         reason="Adjust expected when infer_string is default, no bug here, "
 1756:         "just a complicated parametrization",
 1757:     )
 1758:     @pytest.mark.parametrize(
 1759:         "orient,expected",
 1760:         [
 1761:             (
 1762:                 "split",
 1763:                 """{
 1764:     "columns":[
 1765:         "a",
 1766:         "b"
 1767:     ],
 1768:     "index":[
 1769:         0,
 1770:         1
 1771:     ],
 1772:     "data":[
 1773:         [
 1774:             "foo",
 1775:             "bar"
 1776:         ],
 1777:         [
 1778:             "baz",
 1779:             "qux"
 1780:         ]
 1781:     ]
 1782: }""",
 1783:             ),
 1784:             (
 1785:                 "records",
 1786:                 """[
 1787:     {
 1788:         "a":"foo",
 1789:         "b":"bar"
 1790:     },
 1791:     {
 1792:         "a":"baz",
 1793:         "b":"qux"
 1794:     }
 1795: ]""",
 1796:             ),
 1797:             (
 1798:                 "index",
 1799:                 """{
 1800:     "0":{
 1801:         "a":"foo",
 1802:         "b":"bar"
 1803:     },
 1804:     "1":{
 1805:         "a":"baz",
 1806:         "b":"qux"
 1807:     }
 1808: }""",
 1809:             ),
 1810:             (
 1811:                 "columns",
 1812:                 """{
 1813:     "a":{
 1814:         "0":"foo",
 1815:         "1":"baz"
 1816:     },
 1817:     "b":{
 1818:         "0":"bar",
 1819:         "1":"qux"
 1820:     }
 1821: }""",
 1822:             ),
 1823:             (
 1824:                 "values",
 1825:                 """[
 1826:     [
 1827:         "foo",
 1828:         "bar"
 1829:     ],
 1830:     [
 1831:         "baz",
 1832:         "qux"
 1833:     ]
 1834: ]""",
 1835:             ),
 1836:             (
 1837:                 "table",
 1838:                 """{
 1839:     "schema":{
 1840:         "fields":[
 1841:             {
 1842:                 "name":"index",
 1843:                 "type":"integer"
 1844:             },
 1845:             {
 1846:                 "name":"a",
 1847:                 "type":"string"
 1848:             },
 1849:             {
 1850:                 "name":"b",
 1851:                 "type":"string"
 1852:             }
 1853:         ],
 1854:         "primaryKey":[
 1855:             "index"
 1856:         ],
 1857:         "pandas_version":"1.4.0"
 1858:     },
 1859:     "data":[
 1860:         {
 1861:             "index":0,
 1862:             "a":"foo",
 1863:             "b":"bar"
 1864:         },
 1865:         {
 1866:             "index":1,
 1867:             "a":"baz",
 1868:             "b":"qux"
 1869:         }
 1870:     ]
 1871: }""",
 1872:             ),
 1873:         ],
 1874:     )
 1875:     def test_json_indent_all_orients(self, orient, expected):
 1876:         # GH 12004
 1877:         df = DataFrame([["foo", "bar"], ["baz", "qux"]], columns=["a", "b"])
 1878:         result = df.to_json(orient=orient, indent=4)
 1879:         assert result == expected
 1880: 
 1881:     def test_json_negative_indent_raises(self):
 1882:         with pytest.raises(ValueError, match="must be a nonnegative integer"):
 1883:             DataFrame().to_json(indent=-1)
 1884: 
 1885:     def test_emca_262_nan_inf_support(self):
 1886:         # GH 12213
 1887:         data = StringIO(
 1888:             '["a", NaN, "NaN", Infinity, "Infinity", -Infinity, "-Infinity"]'
 1889:         )
 1890:         result = read_json(data)
 1891:         expected = DataFrame(
 1892:             ["a", None, "NaN", np.inf, "Infinity", -np.inf, "-Infinity"]
 1893:         )
 1894:         tm.assert_frame_equal(result, expected)
 1895: 
 1896:     def test_frame_int_overflow(self):
 1897:         # GH 30320
 1898:         encoded_json = json.dumps([{"col": "31900441201190696999"}, {"col": "Text"}])
 1899:         expected = DataFrame({"col": ["31900441201190696999", "Text"]})
 1900:         result = read_json(StringIO(encoded_json))
 1901:         tm.assert_frame_equal(result, expected)
 1902: 
 1903:     @pytest.mark.parametrize(
 1904:         "dataframe,expected",
 1905:         [
 1906:             (
 1907:                 DataFrame({"x": [1, 2, 3], "y": ["a", "b", "c"]}),
 1908:                 '{"(0, \'x\')":1,"(0, \'y\')":"a","(1, \'x\')":2,'
 1909:                 '"(1, \'y\')":"b","(2, \'x\')":3,"(2, \'y\')":"c"}',
 1910:             )
 1911:         ],
 1912:     )
 1913:     def test_json_multiindex(self, dataframe, expected):
 1914:         series = dataframe.stack(future_stack=True)
 1915:         result = series.to_json(orient="index")
 1916:         assert result == expected
 1917: 
 1918:     @pytest.mark.single_cpu
 1919:     def test_to_s3(self, s3_public_bucket, s3so):
 1920:         # GH 28375
 1921:         mock_bucket_name, target_file = s3_public_bucket.name, "test.json"
 1922:         df = DataFrame({"x": [1, 2, 3], "y": [2, 4, 6]})
 1923:         df.to_json(f"s3://{mock_bucket_name}/{target_file}", storage_options=s3so)
 1924:         timeout = 5
 1925:         while True:
 1926:             if target_file in (obj.key for obj in s3_public_bucket.objects.all()):
 1927:                 break
 1928:             time.sleep(0.1)
 1929:             timeout -= 0.1
 1930:             assert timeout > 0, "Timed out waiting for file to appear on moto"
 1931: 
 1932:     def test_json_pandas_nulls(self, nulls_fixture, request):
 1933:         # GH 31615
 1934:         if isinstance(nulls_fixture, Decimal):
 1935:             mark = pytest.mark.xfail(reason="not implemented")
 1936:             request.applymarker(mark)
 1937: 
 1938:         result = DataFrame([[nulls_fixture]]).to_json()
 1939:         assert result == '{"0":{"0":null}}'
 1940: 
 1941:     def test_readjson_bool_series(self):
 1942:         # GH31464
 1943:         result = read_json(StringIO("[true, true, false]"), typ="series")
 1944:         expected = Series([True, True, False])
 1945:         tm.assert_series_equal(result, expected)
 1946: 
 1947:     def test_to_json_multiindex_escape(self):
 1948:         # GH 15273
 1949:         df = DataFrame(
 1950:             True,
 1951:             index=date_range("2017-01-20", "2017-01-23"),
 1952:             columns=["foo", "bar"],
 1953:         ).stack(future_stack=True)
 1954:         result = df.to_json()
 1955:         expected = (
 1956:             "{\"(Timestamp('2017-01-20 00:00:00'), 'foo')\":true,"
 1957:             "\"(Timestamp('2017-01-20 00:00:00'), 'bar')\":true,"
 1958:             "\"(Timestamp('2017-01-21 00:00:00'), 'foo')\":true,"
 1959:             "\"(Timestamp('2017-01-21 00:00:00'), 'bar')\":true,"
 1960:             "\"(Timestamp('2017-01-22 00:00:00'), 'foo')\":true,"
 1961:             "\"(Timestamp('2017-01-22 00:00:00'), 'bar')\":true,"
 1962:             "\"(Timestamp('2017-01-23 00:00:00'), 'foo')\":true,"
 1963:             "\"(Timestamp('2017-01-23 00:00:00'), 'bar')\":true}"
 1964:         )
 1965:         assert result == expected
 1966: 
 1967:     def test_to_json_series_of_objects(self):
 1968:         class _TestObject:
 1969:             def __init__(self, a, b, _c, d) -> None:
 1970:                 self.a = a
 1971:                 self.b = b
 1972:                 self._c = _c
 1973:                 self.d = d
 1974: 
 1975:             def e(self):
 1976:                 return 5
 1977: 
 1978:         # JSON keys should be all non-callable non-underscore attributes, see GH-42768
 1979:         series = Series([_TestObject(a=1, b=2, _c=3, d=4)])
 1980:         assert json.loads(series.to_json()) == {"0": {"a": 1, "b": 2, "d": 4}}
 1981: 
 1982:     @pytest.mark.parametrize(
 1983:         "data,expected",
 1984:         [
 1985:             (
 1986:                 Series({0: -6 + 8j, 1: 0 + 1j, 2: 9 - 5j}),
 1987:                 '{"0":{"imag":8.0,"real":-6.0},'
 1988:                 '"1":{"imag":1.0,"real":0.0},'
 1989:                 '"2":{"imag":-5.0,"real":9.0}}',
 1990:             ),
 1991:             (
 1992:                 Series({0: -9.39 + 0.66j, 1: 3.95 + 9.32j, 2: 4.03 - 0.17j}),
 1993:                 '{"0":{"imag":0.66,"real":-9.39},'
 1994:                 '"1":{"imag":9.32,"real":3.95},'
 1995:                 '"2":{"imag":-0.17,"real":4.03}}',
 1996:             ),
 1997:             (
 1998:                 DataFrame([[-2 + 3j, -1 - 0j], [4 - 3j, -0 - 10j]]),
 1999:                 '{"0":{"0":{"imag":3.0,"real":-2.0},'
 2000:                 '"1":{"imag":-3.0,"real":4.0}},'
 2001:                 '"1":{"0":{"imag":0.0,"real":-1.0},'
 2002:                 '"1":{"imag":-10.0,"real":0.0}}}',
 2003:             ),
 2004:             (
 2005:                 DataFrame(
 2006:                     [[-0.28 + 0.34j, -1.08 - 0.39j], [0.41 - 0.34j, -0.78 - 1.35j]]
 2007:                 ),
 2008:                 '{"0":{"0":{"imag":0.34,"real":-0.28},'
 2009:                 '"1":{"imag":-0.34,"real":0.41}},'
 2010:                 '"1":{"0":{"imag":-0.39,"real":-1.08},'
 2011:                 '"1":{"imag":-1.35,"real":-0.78}}}',
 2012:             ),
 2013:         ],
 2014:     )
 2015:     def test_complex_data_tojson(self, data, expected):
 2016:         # GH41174
 2017:         result = data.to_json()
 2018:         assert result == expected
 2019: 
 2020:     def test_json_uint64(self):
 2021:         # GH21073
 2022:         expected = (
 2023:             '{"columns":["col1"],"index":[0,1],'
 2024:             '"data":[[13342205958987758245],[12388075603347835679]]}'
 2025:         )
 2026:         df = DataFrame(data={"col1": [13342205958987758245, 12388075603347835679]})
 2027:         result = df.to_json(orient="split")
 2028:         assert result == expected
 2029: 
 2030:     @pytest.mark.parametrize(
 2031:         "orient", ["split", "records", "values", "index", "columns"]
 2032:     )
 2033:     def test_read_json_dtype_backend(
 2034:         self, string_storage, dtype_backend, orient, using_infer_string
 2035:     ):
 2036:         # GH#50750
 2037:         pa = pytest.importorskip("pyarrow")
 2038:         df = DataFrame(
 2039:             {
 2040:                 "a": Series([1, np.nan, 3], dtype="Int64"),
 2041:                 "b": Series([1, 2, 3], dtype="Int64"),
 2042:                 "c": Series([1.5, np.nan, 2.5], dtype="Float64"),
 2043:                 "d": Series([1.5, 2.0, 2.5], dtype="Float64"),
 2044:                 "e": [True, False, None],
 2045:                 "f": [True, False, True],
 2046:                 "g": ["a", "b", "c"],
 2047:                 "h": ["a", "b", None],
 2048:             }
 2049:         )
 2050: 
 2051:         if using_infer_string:
 2052:             string_array = ArrowStringArrayNumpySemantics(pa.array(["a", "b", "c"]))
 2053:             string_array_na = ArrowStringArrayNumpySemantics(pa.array(["a", "b", None]))
 2054:         elif string_storage == "python":
 2055:             string_array = StringArray(np.array(["a", "b", "c"], dtype=np.object_))
 2056:             string_array_na = StringArray(np.array(["a", "b", NA], dtype=np.object_))
 2057: 
 2058:         elif dtype_backend == "pyarrow":
 2059:             pa = pytest.importorskip("pyarrow")
 2060:             from pandas.arrays import ArrowExtensionArray
 2061: 
 2062:             string_array = ArrowExtensionArray(pa.array(["a", "b", "c"]))
 2063:             string_array_na = ArrowExtensionArray(pa.array(["a", "b", None]))
 2064: 
 2065:         else:
 2066:             string_array = ArrowStringArray(pa.array(["a", "b", "c"]))
 2067:             string_array_na = ArrowStringArray(pa.array(["a", "b", None]))
 2068: 
 2069:         out = df.to_json(orient=orient)
 2070:         with pd.option_context("mode.string_storage", string_storage):
 2071:             result = read_json(
 2072:                 StringIO(out), dtype_backend=dtype_backend, orient=orient
 2073:             )
 2074: 
 2075:         expected = DataFrame(
 2076:             {
 2077:                 "a": Series([1, np.nan, 3], dtype="Int64"),
 2078:                 "b": Series([1, 2, 3], dtype="Int64"),
 2079:                 "c": Series([1.5, np.nan, 2.5], dtype="Float64"),
 2080:                 "d": Series([1.5, 2.0, 2.5], dtype="Float64"),
 2081:                 "e": Series([True, False, NA], dtype="boolean"),
 2082:                 "f": Series([True, False, True], dtype="boolean"),
 2083:                 "g": string_array,
 2084:                 "h": string_array_na,
 2085:             }
 2086:         )
 2087: 
 2088:         if dtype_backend == "pyarrow":
 2089:             from pandas.arrays import ArrowExtensionArray
 2090: 
 2091:             expected = DataFrame(
 2092:                 {
 2093:                     col: ArrowExtensionArray(pa.array(expected[col], from_pandas=True))
 2094:                     for col in expected.columns
 2095:                 }
 2096:             )
 2097: 
 2098:         if orient == "values":
 2099:             expected.columns = list(range(8))
 2100: 
 2101:         tm.assert_frame_equal(result, expected)
 2102: 
 2103:     @pytest.mark.parametrize("orient", ["split", "records", "index"])
 2104:     def test_read_json_nullable_series(self, string_storage, dtype_backend, orient):
 2105:         # GH#50750
 2106:         pa = pytest.importorskip("pyarrow")
 2107:         ser = Series([1, np.nan, 3], dtype="Int64")
 2108: 
 2109:         out = ser.to_json(orient=orient)
 2110:         with pd.option_context("mode.string_storage", string_storage):
 2111:             result = read_json(
 2112:                 StringIO(out), dtype_backend=dtype_backend, orient=orient, typ="series"
 2113:             )
 2114: 
 2115:         expected = Series([1, np.nan, 3], dtype="Int64")
 2116: 
 2117:         if dtype_backend == "pyarrow":
 2118:             from pandas.arrays import ArrowExtensionArray
 2119: 
 2120:             expected = Series(ArrowExtensionArray(pa.array(expected, from_pandas=True)))
 2121: 
 2122:         tm.assert_series_equal(result, expected)
 2123: 
 2124:     def test_invalid_dtype_backend(self):
 2125:         msg = (
 2126:             "dtype_backend numpy is invalid, only 'numpy_nullable' and "
 2127:             "'pyarrow' are allowed."
 2128:         )
 2129:         with pytest.raises(ValueError, match=msg):
 2130:             read_json("test", dtype_backend="numpy")
 2131: 
 2132: 
 2133: def test_invalid_engine():
 2134:     # GH 48893
 2135:     ser = Series(range(1))
 2136:     out = ser.to_json()
 2137:     with pytest.raises(ValueError, match="The engine type foo"):
 2138:         read_json(out, engine="foo")
 2139: 
 2140: 
 2141: def test_pyarrow_engine_lines_false():
 2142:     # GH 48893
 2143:     ser = Series(range(1))
 2144:     out = ser.to_json()
 2145:     with pytest.raises(ValueError, match="currently pyarrow engine only supports"):
 2146:         read_json(out, engine="pyarrow", lines=False)
 2147: 
 2148: 
 2149: def test_json_roundtrip_string_inference(orient):
 2150:     pytest.importorskip("pyarrow")
 2151:     df = DataFrame(
 2152:         [["a", "b"], ["c", "d"]], index=["row 1", "row 2"], columns=["col 1", "col 2"]
 2153:     )
 2154:     out = df.to_json()
 2155:     with pd.option_context("future.infer_string", True):
 2156:         result = read_json(StringIO(out))
 2157:     expected = DataFrame(
 2158:         [["a", "b"], ["c", "d"]],
 2159:         dtype="string[pyarrow_numpy]",
 2160:         index=Index(["row 1", "row 2"], dtype="string[pyarrow_numpy]"),
 2161:         columns=Index(["col 1", "col 2"], dtype="string[pyarrow_numpy]"),
 2162:     )
 2163:     tm.assert_frame_equal(result, expected)
 2164: 
 2165: 
 2166: def test_json_pos_args_deprecation():
 2167:     # GH-54229
 2168:     df = DataFrame({"a": [1, 2, 3]})
 2169:     msg = (
 2170:         r"Starting with pandas version 3.0 all arguments of to_json except for the "
 2171:         r"argument 'path_or_buf' will be keyword-only."
 2172:     )
 2173:     with tm.assert_produces_warning(FutureWarning, match=msg):
 2174:         buf = BytesIO()
 2175:         df.to_json(buf, "split")
 2176: 
 2177: 
 2178: @td.skip_if_no("pyarrow")
 2179: def test_to_json_ea_null():
 2180:     # GH#57224
 2181:     df = DataFrame(
 2182:         {
 2183:             "a": Series([1, NA], dtype="int64[pyarrow]"),
 2184:             "b": Series([2, NA], dtype="Int64"),
 2185:         }
 2186:     )
 2187:     result = df.to_json(orient="records", lines=True)
 2188:     expected = """{"a":1,"b":2}
 2189: {"a":null,"b":null}
 2190: """
 2191:     assert result == expected
 2192: 
 2193: 
 2194: def test_read_json_lines_rangeindex():
 2195:     # GH 57429
 2196:     data = """
 2197: {"a": 1, "b": 2}
 2198: {"a": 3, "b": 4}
 2199: """
 2200:     result = read_json(StringIO(data), lines=True).index
 2201:     expected = RangeIndex(2)
 2202:     tm.assert_index_equal(result, expected, exact=True)
