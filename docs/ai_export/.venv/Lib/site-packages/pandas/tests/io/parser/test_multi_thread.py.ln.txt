    1: """
    2: Tests multithreading behaviour for reading and
    3: parsing files for each parser defined in parsers.py
    4: """
    5: from contextlib import ExitStack
    6: from io import BytesIO
    7: from multiprocessing.pool import ThreadPool
    8: 
    9: import numpy as np
   10: import pytest
   11: 
   12: import pandas as pd
   13: from pandas import DataFrame
   14: import pandas._testing as tm
   15: 
   16: xfail_pyarrow = pytest.mark.usefixtures("pyarrow_xfail")
   17: 
   18: # We'll probably always skip these for pyarrow
   19: # Maybe we'll add our own tests for pyarrow too
   20: pytestmark = [
   21:     pytest.mark.single_cpu,
   22:     pytest.mark.slow,
   23: ]
   24: 
   25: 
   26: @xfail_pyarrow  # ValueError: Found non-unique column index
   27: def test_multi_thread_string_io_read_csv(all_parsers):
   28:     # see gh-11786
   29:     parser = all_parsers
   30:     max_row_range = 100
   31:     num_files = 10
   32: 
   33:     bytes_to_df = (
   34:         "\n".join([f"{i:d},{i:d},{i:d}" for i in range(max_row_range)]).encode()
   35:         for _ in range(num_files)
   36:     )
   37: 
   38:     # Read all files in many threads.
   39:     with ExitStack() as stack:
   40:         files = [stack.enter_context(BytesIO(b)) for b in bytes_to_df]
   41: 
   42:         pool = stack.enter_context(ThreadPool(8))
   43: 
   44:         results = pool.map(parser.read_csv, files)
   45:         first_result = results[0]
   46: 
   47:         for result in results:
   48:             tm.assert_frame_equal(first_result, result)
   49: 
   50: 
   51: def _generate_multi_thread_dataframe(parser, path, num_rows, num_tasks):
   52:     """
   53:     Generate a DataFrame via multi-thread.
   54: 
   55:     Parameters
   56:     ----------
   57:     parser : BaseParser
   58:         The parser object to use for reading the data.
   59:     path : str
   60:         The location of the CSV file to read.
   61:     num_rows : int
   62:         The number of rows to read per task.
   63:     num_tasks : int
   64:         The number of tasks to use for reading this DataFrame.
   65: 
   66:     Returns
   67:     -------
   68:     df : DataFrame
   69:     """
   70: 
   71:     def reader(arg):
   72:         """
   73:         Create a reader for part of the CSV.
   74: 
   75:         Parameters
   76:         ----------
   77:         arg : tuple
   78:             A tuple of the following:
   79: 
   80:             * start : int
   81:                 The starting row to start for parsing CSV
   82:             * nrows : int
   83:                 The number of rows to read.
   84: 
   85:         Returns
   86:         -------
   87:         df : DataFrame
   88:         """
   89:         start, nrows = arg
   90: 
   91:         if not start:
   92:             return parser.read_csv(
   93:                 path, index_col=0, header=0, nrows=nrows, parse_dates=["date"]
   94:             )
   95: 
   96:         return parser.read_csv(
   97:             path,
   98:             index_col=0,
   99:             header=None,
  100:             skiprows=int(start) + 1,
  101:             nrows=nrows,
  102:             parse_dates=[9],
  103:         )
  104: 
  105:     tasks = [
  106:         (num_rows * i // num_tasks, num_rows // num_tasks) for i in range(num_tasks)
  107:     ]
  108: 
  109:     with ThreadPool(processes=num_tasks) as pool:
  110:         results = pool.map(reader, tasks)
  111: 
  112:     header = results[0].columns
  113: 
  114:     for r in results[1:]:
  115:         r.columns = header
  116: 
  117:     final_dataframe = pd.concat(results)
  118:     return final_dataframe
  119: 
  120: 
  121: @xfail_pyarrow  # ValueError: The 'nrows' option is not supported
  122: def test_multi_thread_path_multipart_read_csv(all_parsers):
  123:     # see gh-11786
  124:     num_tasks = 4
  125:     num_rows = 48
  126: 
  127:     parser = all_parsers
  128:     file_name = "__thread_pool_reader__.csv"
  129:     df = DataFrame(
  130:         {
  131:             "a": np.random.default_rng(2).random(num_rows),
  132:             "b": np.random.default_rng(2).random(num_rows),
  133:             "c": np.random.default_rng(2).random(num_rows),
  134:             "d": np.random.default_rng(2).random(num_rows),
  135:             "e": np.random.default_rng(2).random(num_rows),
  136:             "foo": ["foo"] * num_rows,
  137:             "bar": ["bar"] * num_rows,
  138:             "baz": ["baz"] * num_rows,
  139:             "date": pd.date_range("20000101 09:00:00", periods=num_rows, freq="s"),
  140:             "int": np.arange(num_rows, dtype="int64"),
  141:         }
  142:     )
  143: 
  144:     with tm.ensure_clean(file_name) as path:
  145:         df.to_csv(path)
  146: 
  147:         final_dataframe = _generate_multi_thread_dataframe(
  148:             parser, path, num_rows, num_tasks
  149:         )
  150:         tm.assert_frame_equal(df, final_dataframe)
