    1: """ test parquet compat """
    2: import datetime
    3: from decimal import Decimal
    4: from io import BytesIO
    5: import os
    6: import pathlib
    7: 
    8: import numpy as np
    9: import pytest
   10: 
   11: from pandas._config import using_copy_on_write
   12: from pandas._config.config import _get_option
   13: 
   14: from pandas.compat import is_platform_windows
   15: from pandas.compat.pyarrow import (
   16:     pa_version_under11p0,
   17:     pa_version_under13p0,
   18:     pa_version_under15p0,
   19: )
   20: 
   21: import pandas as pd
   22: import pandas._testing as tm
   23: from pandas.util.version import Version
   24: 
   25: from pandas.io.parquet import (
   26:     FastParquetImpl,
   27:     PyArrowImpl,
   28:     get_engine,
   29:     read_parquet,
   30:     to_parquet,
   31: )
   32: 
   33: try:
   34:     import pyarrow
   35: 
   36:     _HAVE_PYARROW = True
   37: except ImportError:
   38:     _HAVE_PYARROW = False
   39: 
   40: try:
   41:     import fastparquet
   42: 
   43:     _HAVE_FASTPARQUET = True
   44: except ImportError:
   45:     _HAVE_FASTPARQUET = False
   46: 
   47: 
   48: # TODO(ArrayManager) fastparquet relies on BlockManager internals
   49: 
   50: pytestmark = [
   51:     pytest.mark.filterwarnings("ignore:DataFrame._data is deprecated:FutureWarning"),
   52:     pytest.mark.filterwarnings(
   53:         "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   54:     ),
   55: ]
   56: 
   57: 
   58: # setup engines & skips
   59: @pytest.fixture(
   60:     params=[
   61:         pytest.param(
   62:             "fastparquet",
   63:             marks=pytest.mark.skipif(
   64:                 not _HAVE_FASTPARQUET
   65:                 or _get_option("mode.data_manager", silent=True) == "array",
   66:                 reason="fastparquet is not installed or ArrayManager is used",
   67:             ),
   68:         ),
   69:         pytest.param(
   70:             "pyarrow",
   71:             marks=pytest.mark.skipif(
   72:                 not _HAVE_PYARROW, reason="pyarrow is not installed"
   73:             ),
   74:         ),
   75:     ]
   76: )
   77: def engine(request):
   78:     return request.param
   79: 
   80: 
   81: @pytest.fixture
   82: def pa():
   83:     if not _HAVE_PYARROW:
   84:         pytest.skip("pyarrow is not installed")
   85:     return "pyarrow"
   86: 
   87: 
   88: @pytest.fixture
   89: def fp():
   90:     if not _HAVE_FASTPARQUET:
   91:         pytest.skip("fastparquet is not installed")
   92:     elif _get_option("mode.data_manager", silent=True) == "array":
   93:         pytest.skip("ArrayManager is not supported with fastparquet")
   94:     return "fastparquet"
   95: 
   96: 
   97: @pytest.fixture
   98: def df_compat():
   99:     return pd.DataFrame({"A": [1, 2, 3], "B": "foo"})
  100: 
  101: 
  102: @pytest.fixture
  103: def df_cross_compat():
  104:     df = pd.DataFrame(
  105:         {
  106:             "a": list("abc"),
  107:             "b": list(range(1, 4)),
  108:             # 'c': np.arange(3, 6).astype('u1'),
  109:             "d": np.arange(4.0, 7.0, dtype="float64"),
  110:             "e": [True, False, True],
  111:             "f": pd.date_range("20130101", periods=3),
  112:             # 'g': pd.date_range('20130101', periods=3,
  113:             #                    tz='US/Eastern'),
  114:             # 'h': pd.date_range('20130101', periods=3, freq='ns')
  115:         }
  116:     )
  117:     return df
  118: 
  119: 
  120: @pytest.fixture
  121: def df_full():
  122:     return pd.DataFrame(
  123:         {
  124:             "string": list("abc"),
  125:             "string_with_nan": ["a", np.nan, "c"],
  126:             "string_with_none": ["a", None, "c"],
  127:             "bytes": [b"foo", b"bar", b"baz"],
  128:             "unicode": ["foo", "bar", "baz"],
  129:             "int": list(range(1, 4)),
  130:             "uint": np.arange(3, 6).astype("u1"),
  131:             "float": np.arange(4.0, 7.0, dtype="float64"),
  132:             "float_with_nan": [2.0, np.nan, 3.0],
  133:             "bool": [True, False, True],
  134:             "datetime": pd.date_range("20130101", periods=3),
  135:             "datetime_with_nat": [
  136:                 pd.Timestamp("20130101"),
  137:                 pd.NaT,
  138:                 pd.Timestamp("20130103"),
  139:             ],
  140:         }
  141:     )
  142: 
  143: 
  144: @pytest.fixture(
  145:     params=[
  146:         datetime.datetime.now(datetime.timezone.utc),
  147:         datetime.datetime.now(datetime.timezone.min),
  148:         datetime.datetime.now(datetime.timezone.max),
  149:         datetime.datetime.strptime("2019-01-04T16:41:24+0200", "%Y-%m-%dT%H:%M:%S%z"),
  150:         datetime.datetime.strptime("2019-01-04T16:41:24+0215", "%Y-%m-%dT%H:%M:%S%z"),
  151:         datetime.datetime.strptime("2019-01-04T16:41:24-0200", "%Y-%m-%dT%H:%M:%S%z"),
  152:         datetime.datetime.strptime("2019-01-04T16:41:24-0215", "%Y-%m-%dT%H:%M:%S%z"),
  153:     ]
  154: )
  155: def timezone_aware_date_list(request):
  156:     return request.param
  157: 
  158: 
  159: def check_round_trip(
  160:     df,
  161:     engine=None,
  162:     path=None,
  163:     write_kwargs=None,
  164:     read_kwargs=None,
  165:     expected=None,
  166:     check_names=True,
  167:     check_like=False,
  168:     check_dtype=True,
  169:     repeat=2,
  170: ):
  171:     """Verify parquet serializer and deserializer produce the same results.
  172: 
  173:     Performs a pandas to disk and disk to pandas round trip,
  174:     then compares the 2 resulting DataFrames to verify equality.
  175: 
  176:     Parameters
  177:     ----------
  178:     df: Dataframe
  179:     engine: str, optional
  180:         'pyarrow' or 'fastparquet'
  181:     path: str, optional
  182:     write_kwargs: dict of str:str, optional
  183:     read_kwargs: dict of str:str, optional
  184:     expected: DataFrame, optional
  185:         Expected deserialization result, otherwise will be equal to `df`
  186:     check_names: list of str, optional
  187:         Closed set of column names to be compared
  188:     check_like: bool, optional
  189:         If True, ignore the order of index & columns.
  190:     repeat: int, optional
  191:         How many times to repeat the test
  192:     """
  193:     write_kwargs = write_kwargs or {"compression": None}
  194:     read_kwargs = read_kwargs or {}
  195: 
  196:     if expected is None:
  197:         expected = df
  198: 
  199:     if engine:
  200:         write_kwargs["engine"] = engine
  201:         read_kwargs["engine"] = engine
  202: 
  203:     def compare(repeat):
  204:         for _ in range(repeat):
  205:             df.to_parquet(path, **write_kwargs)
  206:             actual = read_parquet(path, **read_kwargs)
  207: 
  208:             if "string_with_nan" in expected:
  209:                 expected.loc[1, "string_with_nan"] = None
  210:             tm.assert_frame_equal(
  211:                 expected,
  212:                 actual,
  213:                 check_names=check_names,
  214:                 check_like=check_like,
  215:                 check_dtype=check_dtype,
  216:             )
  217: 
  218:     if path is None:
  219:         with tm.ensure_clean() as path:
  220:             compare(repeat)
  221:     else:
  222:         compare(repeat)
  223: 
  224: 
  225: def check_partition_names(path, expected):
  226:     """Check partitions of a parquet file are as expected.
  227: 
  228:     Parameters
  229:     ----------
  230:     path: str
  231:         Path of the dataset.
  232:     expected: iterable of str
  233:         Expected partition names.
  234:     """
  235:     import pyarrow.dataset as ds
  236: 
  237:     dataset = ds.dataset(path, partitioning="hive")
  238:     assert dataset.partitioning.schema.names == expected
  239: 
  240: 
  241: def test_invalid_engine(df_compat):
  242:     msg = "engine must be one of 'pyarrow', 'fastparquet'"
  243:     with pytest.raises(ValueError, match=msg):
  244:         check_round_trip(df_compat, "foo", "bar")
  245: 
  246: 
  247: def test_options_py(df_compat, pa):
  248:     # use the set option
  249: 
  250:     with pd.option_context("io.parquet.engine", "pyarrow"):
  251:         check_round_trip(df_compat)
  252: 
  253: 
  254: def test_options_fp(df_compat, fp):
  255:     # use the set option
  256: 
  257:     with pd.option_context("io.parquet.engine", "fastparquet"):
  258:         check_round_trip(df_compat)
  259: 
  260: 
  261: def test_options_auto(df_compat, fp, pa):
  262:     # use the set option
  263: 
  264:     with pd.option_context("io.parquet.engine", "auto"):
  265:         check_round_trip(df_compat)
  266: 
  267: 
  268: def test_options_get_engine(fp, pa):
  269:     assert isinstance(get_engine("pyarrow"), PyArrowImpl)
  270:     assert isinstance(get_engine("fastparquet"), FastParquetImpl)
  271: 
  272:     with pd.option_context("io.parquet.engine", "pyarrow"):
  273:         assert isinstance(get_engine("auto"), PyArrowImpl)
  274:         assert isinstance(get_engine("pyarrow"), PyArrowImpl)
  275:         assert isinstance(get_engine("fastparquet"), FastParquetImpl)
  276: 
  277:     with pd.option_context("io.parquet.engine", "fastparquet"):
  278:         assert isinstance(get_engine("auto"), FastParquetImpl)
  279:         assert isinstance(get_engine("pyarrow"), PyArrowImpl)
  280:         assert isinstance(get_engine("fastparquet"), FastParquetImpl)
  281: 
  282:     with pd.option_context("io.parquet.engine", "auto"):
  283:         assert isinstance(get_engine("auto"), PyArrowImpl)
  284:         assert isinstance(get_engine("pyarrow"), PyArrowImpl)
  285:         assert isinstance(get_engine("fastparquet"), FastParquetImpl)
  286: 
  287: 
  288: def test_get_engine_auto_error_message():
  289:     # Expect different error messages from get_engine(engine="auto")
  290:     # if engines aren't installed vs. are installed but bad version
  291:     from pandas.compat._optional import VERSIONS
  292: 
  293:     # Do we have engines installed, but a bad version of them?
  294:     pa_min_ver = VERSIONS.get("pyarrow")
  295:     fp_min_ver = VERSIONS.get("fastparquet")
  296:     have_pa_bad_version = (
  297:         False
  298:         if not _HAVE_PYARROW
  299:         else Version(pyarrow.__version__) < Version(pa_min_ver)
  300:     )
  301:     have_fp_bad_version = (
  302:         False
  303:         if not _HAVE_FASTPARQUET
  304:         else Version(fastparquet.__version__) < Version(fp_min_ver)
  305:     )
  306:     # Do we have usable engines installed?
  307:     have_usable_pa = _HAVE_PYARROW and not have_pa_bad_version
  308:     have_usable_fp = _HAVE_FASTPARQUET and not have_fp_bad_version
  309: 
  310:     if not have_usable_pa and not have_usable_fp:
  311:         # No usable engines found.
  312:         if have_pa_bad_version:
  313:             match = f"Pandas requires version .{pa_min_ver}. or newer of .pyarrow."
  314:             with pytest.raises(ImportError, match=match):
  315:                 get_engine("auto")
  316:         else:
  317:             match = "Missing optional dependency .pyarrow."
  318:             with pytest.raises(ImportError, match=match):
  319:                 get_engine("auto")
  320: 
  321:         if have_fp_bad_version:
  322:             match = f"Pandas requires version .{fp_min_ver}. or newer of .fastparquet."
  323:             with pytest.raises(ImportError, match=match):
  324:                 get_engine("auto")
  325:         else:
  326:             match = "Missing optional dependency .fastparquet."
  327:             with pytest.raises(ImportError, match=match):
  328:                 get_engine("auto")
  329: 
  330: 
  331: def test_cross_engine_pa_fp(df_cross_compat, pa, fp):
  332:     # cross-compat with differing reading/writing engines
  333: 
  334:     df = df_cross_compat
  335:     with tm.ensure_clean() as path:
  336:         df.to_parquet(path, engine=pa, compression=None)
  337: 
  338:         result = read_parquet(path, engine=fp)
  339:         tm.assert_frame_equal(result, df)
  340: 
  341:         result = read_parquet(path, engine=fp, columns=["a", "d"])
  342:         tm.assert_frame_equal(result, df[["a", "d"]])
  343: 
  344: 
  345: def test_cross_engine_fp_pa(df_cross_compat, pa, fp):
  346:     # cross-compat with differing reading/writing engines
  347:     df = df_cross_compat
  348:     with tm.ensure_clean() as path:
  349:         df.to_parquet(path, engine=fp, compression=None)
  350: 
  351:         result = read_parquet(path, engine=pa)
  352:         tm.assert_frame_equal(result, df)
  353: 
  354:         result = read_parquet(path, engine=pa, columns=["a", "d"])
  355:         tm.assert_frame_equal(result, df[["a", "d"]])
  356: 
  357: 
  358: def test_parquet_pos_args_deprecation(engine):
  359:     # GH-54229
  360:     df = pd.DataFrame({"a": [1, 2, 3]})
  361:     msg = (
  362:         r"Starting with pandas version 3.0 all arguments of to_parquet except for the "
  363:         r"argument 'path' will be keyword-only."
  364:     )
  365:     with tm.ensure_clean() as path:
  366:         with tm.assert_produces_warning(
  367:             FutureWarning,
  368:             match=msg,
  369:             check_stacklevel=False,
  370:             raise_on_extra_warnings=False,
  371:         ):
  372:             df.to_parquet(path, engine)
  373: 
  374: 
  375: class Base:
  376:     def check_error_on_write(self, df, engine, exc, err_msg):
  377:         # check that we are raising the exception on writing
  378:         with tm.ensure_clean() as path:
  379:             with pytest.raises(exc, match=err_msg):
  380:                 to_parquet(df, path, engine, compression=None)
  381: 
  382:     def check_external_error_on_write(self, df, engine, exc):
  383:         # check that an external library is raising the exception on writing
  384:         with tm.ensure_clean() as path:
  385:             with tm.external_error_raised(exc):
  386:                 to_parquet(df, path, engine, compression=None)
  387: 
  388:     @pytest.mark.network
  389:     @pytest.mark.single_cpu
  390:     def test_parquet_read_from_url(self, httpserver, datapath, df_compat, engine):
  391:         if engine != "auto":
  392:             pytest.importorskip(engine)
  393:         with open(datapath("io", "data", "parquet", "simple.parquet"), mode="rb") as f:
  394:             httpserver.serve_content(content=f.read())
  395:             df = read_parquet(httpserver.url)
  396:         tm.assert_frame_equal(df, df_compat)
  397: 
  398: 
  399: class TestBasic(Base):
  400:     def test_error(self, engine):
  401:         for obj in [
  402:             pd.Series([1, 2, 3]),
  403:             1,
  404:             "foo",
  405:             pd.Timestamp("20130101"),
  406:             np.array([1, 2, 3]),
  407:         ]:
  408:             msg = "to_parquet only supports IO with DataFrames"
  409:             self.check_error_on_write(obj, engine, ValueError, msg)
  410: 
  411:     def test_columns_dtypes(self, engine):
  412:         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
  413: 
  414:         # unicode
  415:         df.columns = ["foo", "bar"]
  416:         check_round_trip(df, engine)
  417: 
  418:     @pytest.mark.parametrize("compression", [None, "gzip", "snappy", "brotli"])
  419:     def test_compression(self, engine, compression):
  420:         df = pd.DataFrame({"A": [1, 2, 3]})
  421:         check_round_trip(df, engine, write_kwargs={"compression": compression})
  422: 
  423:     def test_read_columns(self, engine):
  424:         # GH18154
  425:         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
  426: 
  427:         expected = pd.DataFrame({"string": list("abc")})
  428:         check_round_trip(
  429:             df, engine, expected=expected, read_kwargs={"columns": ["string"]}
  430:         )
  431: 
  432:     def test_read_filters(self, engine, tmp_path):
  433:         df = pd.DataFrame(
  434:             {
  435:                 "int": list(range(4)),
  436:                 "part": list("aabb"),
  437:             }
  438:         )
  439: 
  440:         expected = pd.DataFrame({"int": [0, 1]})
  441:         check_round_trip(
  442:             df,
  443:             engine,
  444:             path=tmp_path,
  445:             expected=expected,
  446:             write_kwargs={"partition_cols": ["part"]},
  447:             read_kwargs={"filters": [("part", "==", "a")], "columns": ["int"]},
  448:             repeat=1,
  449:         )
  450: 
  451:     def test_write_index(self, engine, using_copy_on_write, request):
  452:         check_names = engine != "fastparquet"
  453:         if using_copy_on_write and engine == "fastparquet":
  454:             request.applymarker(
  455:                 pytest.mark.xfail(reason="fastparquet write into index")
  456:             )
  457: 
  458:         df = pd.DataFrame({"A": [1, 2, 3]})
  459:         check_round_trip(df, engine)
  460: 
  461:         indexes = [
  462:             [2, 3, 4],
  463:             pd.date_range("20130101", periods=3),
  464:             list("abc"),
  465:             [1, 3, 4],
  466:         ]
  467:         # non-default index
  468:         for index in indexes:
  469:             df.index = index
  470:             if isinstance(index, pd.DatetimeIndex):
  471:                 df.index = df.index._with_freq(None)  # freq doesn't round-trip
  472:             check_round_trip(df, engine, check_names=check_names)
  473: 
  474:         # index with meta-data
  475:         df.index = [0, 1, 2]
  476:         df.index.name = "foo"
  477:         check_round_trip(df, engine)
  478: 
  479:     def test_write_multiindex(self, pa):
  480:         # Not supported in fastparquet as of 0.1.3 or older pyarrow version
  481:         engine = pa
  482: 
  483:         df = pd.DataFrame({"A": [1, 2, 3]})
  484:         index = pd.MultiIndex.from_tuples([("a", 1), ("a", 2), ("b", 1)])
  485:         df.index = index
  486:         check_round_trip(df, engine)
  487: 
  488:     def test_multiindex_with_columns(self, pa):
  489:         engine = pa
  490:         dates = pd.date_range("01-Jan-2018", "01-Dec-2018", freq="MS")
  491:         df = pd.DataFrame(
  492:             np.random.default_rng(2).standard_normal((2 * len(dates), 3)),
  493:             columns=list("ABC"),
  494:         )
  495:         index1 = pd.MultiIndex.from_product(
  496:             [["Level1", "Level2"], dates], names=["level", "date"]
  497:         )
  498:         index2 = index1.copy(names=None)
  499:         for index in [index1, index2]:
  500:             df.index = index
  501: 
  502:             check_round_trip(df, engine)
  503:             check_round_trip(
  504:                 df, engine, read_kwargs={"columns": ["A", "B"]}, expected=df[["A", "B"]]
  505:             )
  506: 
  507:     def test_write_ignoring_index(self, engine):
  508:         # ENH 20768
  509:         # Ensure index=False omits the index from the written Parquet file.
  510:         df = pd.DataFrame({"a": [1, 2, 3], "b": ["q", "r", "s"]})
  511: 
  512:         write_kwargs = {"compression": None, "index": False}
  513: 
  514:         # Because we're dropping the index, we expect the loaded dataframe to
  515:         # have the default integer index.
  516:         expected = df.reset_index(drop=True)
  517: 
  518:         check_round_trip(df, engine, write_kwargs=write_kwargs, expected=expected)
  519: 
  520:         # Ignore custom index
  521:         df = pd.DataFrame(
  522:             {"a": [1, 2, 3], "b": ["q", "r", "s"]}, index=["zyx", "wvu", "tsr"]
  523:         )
  524: 
  525:         check_round_trip(df, engine, write_kwargs=write_kwargs, expected=expected)
  526: 
  527:         # Ignore multi-indexes as well.
  528:         arrays = [
  529:             ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
  530:             ["one", "two", "one", "two", "one", "two", "one", "two"],
  531:         ]
  532:         df = pd.DataFrame(
  533:             {"one": list(range(8)), "two": [-i for i in range(8)]}, index=arrays
  534:         )
  535: 
  536:         expected = df.reset_index(drop=True)
  537:         check_round_trip(df, engine, write_kwargs=write_kwargs, expected=expected)
  538: 
  539:     def test_write_column_multiindex(self, engine):
  540:         # Not able to write column multi-indexes with non-string column names.
  541:         mi_columns = pd.MultiIndex.from_tuples([("a", 1), ("a", 2), ("b", 1)])
  542:         df = pd.DataFrame(
  543:             np.random.default_rng(2).standard_normal((4, 3)), columns=mi_columns
  544:         )
  545: 
  546:         if engine == "fastparquet":
  547:             self.check_error_on_write(
  548:                 df, engine, TypeError, "Column name must be a string"
  549:             )
  550:         elif engine == "pyarrow":
  551:             check_round_trip(df, engine)
  552: 
  553:     def test_write_column_multiindex_nonstring(self, engine):
  554:         # GH #34777
  555: 
  556:         # Not able to write column multi-indexes with non-string column names
  557:         arrays = [
  558:             ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
  559:             [1, 2, 1, 2, 1, 2, 1, 2],
  560:         ]
  561:         df = pd.DataFrame(
  562:             np.random.default_rng(2).standard_normal((8, 8)), columns=arrays
  563:         )
  564:         df.columns.names = ["Level1", "Level2"]
  565:         if engine == "fastparquet":
  566:             self.check_error_on_write(df, engine, ValueError, "Column name")
  567:         elif engine == "pyarrow":
  568:             check_round_trip(df, engine)
  569: 
  570:     def test_write_column_multiindex_string(self, pa):
  571:         # GH #34777
  572:         # Not supported in fastparquet as of 0.1.3
  573:         engine = pa
  574: 
  575:         # Write column multi-indexes with string column names
  576:         arrays = [
  577:             ["bar", "bar", "baz", "baz", "foo", "foo", "qux", "qux"],
  578:             ["one", "two", "one", "two", "one", "two", "one", "two"],
  579:         ]
  580:         df = pd.DataFrame(
  581:             np.random.default_rng(2).standard_normal((8, 8)), columns=arrays
  582:         )
  583:         df.columns.names = ["ColLevel1", "ColLevel2"]
  584: 
  585:         check_round_trip(df, engine)
  586: 
  587:     def test_write_column_index_string(self, pa):
  588:         # GH #34777
  589:         # Not supported in fastparquet as of 0.1.3
  590:         engine = pa
  591: 
  592:         # Write column indexes with string column names
  593:         arrays = ["bar", "baz", "foo", "qux"]
  594:         df = pd.DataFrame(
  595:             np.random.default_rng(2).standard_normal((8, 4)), columns=arrays
  596:         )
  597:         df.columns.name = "StringCol"
  598: 
  599:         check_round_trip(df, engine)
  600: 
  601:     def test_write_column_index_nonstring(self, engine):
  602:         # GH #34777
  603: 
  604:         # Write column indexes with string column names
  605:         arrays = [1, 2, 3, 4]
  606:         df = pd.DataFrame(
  607:             np.random.default_rng(2).standard_normal((8, 4)), columns=arrays
  608:         )
  609:         df.columns.name = "NonStringCol"
  610:         if engine == "fastparquet":
  611:             self.check_error_on_write(
  612:                 df, engine, TypeError, "Column name must be a string"
  613:             )
  614:         else:
  615:             check_round_trip(df, engine)
  616: 
  617:     def test_dtype_backend(self, engine, request):
  618:         pq = pytest.importorskip("pyarrow.parquet")
  619: 
  620:         if engine == "fastparquet":
  621:             # We are manually disabling fastparquet's
  622:             # nullable dtype support pending discussion
  623:             mark = pytest.mark.xfail(
  624:                 reason="Fastparquet nullable dtype support is disabled"
  625:             )
  626:             request.applymarker(mark)
  627: 
  628:         table = pyarrow.table(
  629:             {
  630:                 "a": pyarrow.array([1, 2, 3, None], "int64"),
  631:                 "b": pyarrow.array([1, 2, 3, None], "uint8"),
  632:                 "c": pyarrow.array(["a", "b", "c", None]),
  633:                 "d": pyarrow.array([True, False, True, None]),
  634:                 # Test that nullable dtypes used even in absence of nulls
  635:                 "e": pyarrow.array([1, 2, 3, 4], "int64"),
  636:                 # GH 45694
  637:                 "f": pyarrow.array([1.0, 2.0, 3.0, None], "float32"),
  638:                 "g": pyarrow.array([1.0, 2.0, 3.0, None], "float64"),
  639:             }
  640:         )
  641:         with tm.ensure_clean() as path:
  642:             # write manually with pyarrow to write integers
  643:             pq.write_table(table, path)
  644:             result1 = read_parquet(path, engine=engine)
  645:             result2 = read_parquet(path, engine=engine, dtype_backend="numpy_nullable")
  646: 
  647:         assert result1["a"].dtype == np.dtype("float64")
  648:         expected = pd.DataFrame(
  649:             {
  650:                 "a": pd.array([1, 2, 3, None], dtype="Int64"),
  651:                 "b": pd.array([1, 2, 3, None], dtype="UInt8"),
  652:                 "c": pd.array(["a", "b", "c", None], dtype="string"),
  653:                 "d": pd.array([True, False, True, None], dtype="boolean"),
  654:                 "e": pd.array([1, 2, 3, 4], dtype="Int64"),
  655:                 "f": pd.array([1.0, 2.0, 3.0, None], dtype="Float32"),
  656:                 "g": pd.array([1.0, 2.0, 3.0, None], dtype="Float64"),
  657:             }
  658:         )
  659:         if engine == "fastparquet":
  660:             # Fastparquet doesn't support string columns yet
  661:             # Only int and boolean
  662:             result2 = result2.drop("c", axis=1)
  663:             expected = expected.drop("c", axis=1)
  664:         tm.assert_frame_equal(result2, expected)
  665: 
  666:     @pytest.mark.parametrize(
  667:         "dtype",
  668:         [
  669:             "Int64",
  670:             "UInt8",
  671:             "boolean",
  672:             "object",
  673:             "datetime64[ns, UTC]",
  674:             "float",
  675:             "period[D]",
  676:             "Float64",
  677:             "string",
  678:         ],
  679:     )
  680:     def test_read_empty_array(self, pa, dtype):
  681:         # GH #41241
  682:         df = pd.DataFrame(
  683:             {
  684:                 "value": pd.array([], dtype=dtype),
  685:             }
  686:         )
  687:         # GH 45694
  688:         expected = None
  689:         if dtype == "float":
  690:             expected = pd.DataFrame(
  691:                 {
  692:                     "value": pd.array([], dtype="Float64"),
  693:                 }
  694:             )
  695:         check_round_trip(
  696:             df, pa, read_kwargs={"dtype_backend": "numpy_nullable"}, expected=expected
  697:         )
  698: 
  699: 
  700: class TestParquetPyArrow(Base):
  701:     def test_basic(self, pa, df_full):
  702:         df = df_full
  703: 
  704:         # additional supported types for pyarrow
  705:         dti = pd.date_range("20130101", periods=3, tz="Europe/Brussels")
  706:         dti = dti._with_freq(None)  # freq doesn't round-trip
  707:         df["datetime_tz"] = dti
  708:         df["bool_with_none"] = [True, None, True]
  709: 
  710:         check_round_trip(df, pa)
  711: 
  712:     def test_basic_subset_columns(self, pa, df_full):
  713:         # GH18628
  714: 
  715:         df = df_full
  716:         # additional supported types for pyarrow
  717:         df["datetime_tz"] = pd.date_range("20130101", periods=3, tz="Europe/Brussels")
  718: 
  719:         check_round_trip(
  720:             df,
  721:             pa,
  722:             expected=df[["string", "int"]],
  723:             read_kwargs={"columns": ["string", "int"]},
  724:         )
  725: 
  726:     def test_to_bytes_without_path_or_buf_provided(self, pa, df_full):
  727:         # GH 37105
  728:         buf_bytes = df_full.to_parquet(engine=pa)
  729:         assert isinstance(buf_bytes, bytes)
  730: 
  731:         buf_stream = BytesIO(buf_bytes)
  732:         res = read_parquet(buf_stream)
  733: 
  734:         expected = df_full.copy()
  735:         expected.loc[1, "string_with_nan"] = None
  736:         tm.assert_frame_equal(res, expected)
  737: 
  738:     def test_duplicate_columns(self, pa):
  739:         # not currently able to handle duplicate columns
  740:         df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=list("aaa")).copy()
  741:         self.check_error_on_write(df, pa, ValueError, "Duplicate column names found")
  742: 
  743:     def test_timedelta(self, pa):
  744:         df = pd.DataFrame({"a": pd.timedelta_range("1 day", periods=3)})
  745:         check_round_trip(df, pa)
  746: 
  747:     def test_unsupported(self, pa):
  748:         # mixed python objects
  749:         df = pd.DataFrame({"a": ["a", 1, 2.0]})
  750:         # pyarrow 0.11 raises ArrowTypeError
  751:         # older pyarrows raise ArrowInvalid
  752:         self.check_external_error_on_write(df, pa, pyarrow.ArrowException)
  753: 
  754:     def test_unsupported_float16(self, pa):
  755:         # #44847, #44914
  756:         # Not able to write float 16 column using pyarrow.
  757:         data = np.arange(2, 10, dtype=np.float16)
  758:         df = pd.DataFrame(data=data, columns=["fp16"])
  759:         if pa_version_under15p0:
  760:             self.check_external_error_on_write(df, pa, pyarrow.ArrowException)
  761:         else:
  762:             check_round_trip(df, pa)
  763: 
  764:     @pytest.mark.xfail(
  765:         is_platform_windows(),
  766:         reason=(
  767:             "PyArrow does not cleanup of partial files dumps when unsupported "
  768:             "dtypes are passed to_parquet function in windows"
  769:         ),
  770:     )
  771:     @pytest.mark.skipif(not pa_version_under15p0, reason="float16 works on 15")
  772:     @pytest.mark.parametrize("path_type", [str, pathlib.Path])
  773:     def test_unsupported_float16_cleanup(self, pa, path_type):
  774:         # #44847, #44914
  775:         # Not able to write float 16 column using pyarrow.
  776:         # Tests cleanup by pyarrow in case of an error
  777:         data = np.arange(2, 10, dtype=np.float16)
  778:         df = pd.DataFrame(data=data, columns=["fp16"])
  779: 
  780:         with tm.ensure_clean() as path_str:
  781:             path = path_type(path_str)
  782:             with tm.external_error_raised(pyarrow.ArrowException):
  783:                 df.to_parquet(path=path, engine=pa)
  784:             assert not os.path.isfile(path)
  785: 
  786:     def test_categorical(self, pa):
  787:         # supported in >= 0.7.0
  788:         df = pd.DataFrame()
  789:         df["a"] = pd.Categorical(list("abcdef"))
  790: 
  791:         # test for null, out-of-order values, and unobserved category
  792:         df["b"] = pd.Categorical(
  793:             ["bar", "foo", "foo", "bar", None, "bar"],
  794:             dtype=pd.CategoricalDtype(["foo", "bar", "baz"]),
  795:         )
  796: 
  797:         # test for ordered flag
  798:         df["c"] = pd.Categorical(
  799:             ["a", "b", "c", "a", "c", "b"], categories=["b", "c", "d"], ordered=True
  800:         )
  801: 
  802:         check_round_trip(df, pa)
  803: 
  804:     @pytest.mark.single_cpu
  805:     def test_s3_roundtrip_explicit_fs(self, df_compat, s3_public_bucket, pa, s3so):
  806:         s3fs = pytest.importorskip("s3fs")
  807:         s3 = s3fs.S3FileSystem(**s3so)
  808:         kw = {"filesystem": s3}
  809:         check_round_trip(
  810:             df_compat,
  811:             pa,
  812:             path=f"{s3_public_bucket.name}/pyarrow.parquet",
  813:             read_kwargs=kw,
  814:             write_kwargs=kw,
  815:         )
  816: 
  817:     @pytest.mark.single_cpu
  818:     def test_s3_roundtrip(self, df_compat, s3_public_bucket, pa, s3so):
  819:         # GH #19134
  820:         s3so = {"storage_options": s3so}
  821:         check_round_trip(
  822:             df_compat,
  823:             pa,
  824:             path=f"s3://{s3_public_bucket.name}/pyarrow.parquet",
  825:             read_kwargs=s3so,
  826:             write_kwargs=s3so,
  827:         )
  828: 
  829:     @pytest.mark.single_cpu
  830:     @pytest.mark.parametrize(
  831:         "partition_col",
  832:         [
  833:             ["A"],
  834:             [],
  835:         ],
  836:     )
  837:     def test_s3_roundtrip_for_dir(
  838:         self, df_compat, s3_public_bucket, pa, partition_col, s3so
  839:     ):
  840:         pytest.importorskip("s3fs")
  841:         # GH #26388
  842:         expected_df = df_compat.copy()
  843: 
  844:         # GH #35791
  845:         if partition_col:
  846:             expected_df = expected_df.astype(dict.fromkeys(partition_col, np.int32))
  847:             partition_col_type = "category"
  848: 
  849:             expected_df[partition_col] = expected_df[partition_col].astype(
  850:                 partition_col_type
  851:             )
  852: 
  853:         check_round_trip(
  854:             df_compat,
  855:             pa,
  856:             expected=expected_df,
  857:             path=f"s3://{s3_public_bucket.name}/parquet_dir",
  858:             read_kwargs={"storage_options": s3so},
  859:             write_kwargs={
  860:                 "partition_cols": partition_col,
  861:                 "compression": None,
  862:                 "storage_options": s3so,
  863:             },
  864:             check_like=True,
  865:             repeat=1,
  866:         )
  867: 
  868:     def test_read_file_like_obj_support(self, df_compat):
  869:         pytest.importorskip("pyarrow")
  870:         buffer = BytesIO()
  871:         df_compat.to_parquet(buffer)
  872:         df_from_buf = read_parquet(buffer)
  873:         tm.assert_frame_equal(df_compat, df_from_buf)
  874: 
  875:     def test_expand_user(self, df_compat, monkeypatch):
  876:         pytest.importorskip("pyarrow")
  877:         monkeypatch.setenv("HOME", "TestingUser")
  878:         monkeypatch.setenv("USERPROFILE", "TestingUser")
  879:         with pytest.raises(OSError, match=r".*TestingUser.*"):
  880:             read_parquet("~/file.parquet")
  881:         with pytest.raises(OSError, match=r".*TestingUser.*"):
  882:             df_compat.to_parquet("~/file.parquet")
  883: 
  884:     def test_partition_cols_supported(self, tmp_path, pa, df_full):
  885:         # GH #23283
  886:         partition_cols = ["bool", "int"]
  887:         df = df_full
  888:         df.to_parquet(tmp_path, partition_cols=partition_cols, compression=None)
  889:         check_partition_names(tmp_path, partition_cols)
  890:         assert read_parquet(tmp_path).shape == df.shape
  891: 
  892:     def test_partition_cols_string(self, tmp_path, pa, df_full):
  893:         # GH #27117
  894:         partition_cols = "bool"
  895:         partition_cols_list = [partition_cols]
  896:         df = df_full
  897:         df.to_parquet(tmp_path, partition_cols=partition_cols, compression=None)
  898:         check_partition_names(tmp_path, partition_cols_list)
  899:         assert read_parquet(tmp_path).shape == df.shape
  900: 
  901:     @pytest.mark.parametrize(
  902:         "path_type", [str, lambda x: x], ids=["string", "pathlib.Path"]
  903:     )
  904:     def test_partition_cols_pathlib(self, tmp_path, pa, df_compat, path_type):
  905:         # GH 35902
  906: 
  907:         partition_cols = "B"
  908:         partition_cols_list = [partition_cols]
  909:         df = df_compat
  910: 
  911:         path = path_type(tmp_path)
  912:         df.to_parquet(path, partition_cols=partition_cols_list)
  913:         assert read_parquet(path).shape == df.shape
  914: 
  915:     def test_empty_dataframe(self, pa):
  916:         # GH #27339
  917:         df = pd.DataFrame(index=[], columns=[])
  918:         check_round_trip(df, pa)
  919: 
  920:     def test_write_with_schema(self, pa):
  921:         import pyarrow
  922: 
  923:         df = pd.DataFrame({"x": [0, 1]})
  924:         schema = pyarrow.schema([pyarrow.field("x", type=pyarrow.bool_())])
  925:         out_df = df.astype(bool)
  926:         check_round_trip(df, pa, write_kwargs={"schema": schema}, expected=out_df)
  927: 
  928:     def test_additional_extension_arrays(self, pa):
  929:         # test additional ExtensionArrays that are supported through the
  930:         # __arrow_array__ protocol
  931:         pytest.importorskip("pyarrow")
  932:         df = pd.DataFrame(
  933:             {
  934:                 "a": pd.Series([1, 2, 3], dtype="Int64"),
  935:                 "b": pd.Series([1, 2, 3], dtype="UInt32"),
  936:                 "c": pd.Series(["a", None, "c"], dtype="string"),
  937:             }
  938:         )
  939:         check_round_trip(df, pa)
  940: 
  941:         df = pd.DataFrame({"a": pd.Series([1, 2, 3, None], dtype="Int64")})
  942:         check_round_trip(df, pa)
  943: 
  944:     def test_pyarrow_backed_string_array(self, pa, string_storage):
  945:         # test ArrowStringArray supported through the __arrow_array__ protocol
  946:         pytest.importorskip("pyarrow")
  947:         df = pd.DataFrame({"a": pd.Series(["a", None, "c"], dtype="string[pyarrow]")})
  948:         with pd.option_context("string_storage", string_storage):
  949:             check_round_trip(df, pa, expected=df.astype(f"string[{string_storage}]"))
  950: 
  951:     def test_additional_extension_types(self, pa):
  952:         # test additional ExtensionArrays that are supported through the
  953:         # __arrow_array__ protocol + by defining a custom ExtensionType
  954:         pytest.importorskip("pyarrow")
  955:         df = pd.DataFrame(
  956:             {
  957:                 "c": pd.IntervalIndex.from_tuples([(0, 1), (1, 2), (3, 4)]),
  958:                 "d": pd.period_range("2012-01-01", periods=3, freq="D"),
  959:                 # GH-45881 issue with interval with datetime64[ns] subtype
  960:                 "e": pd.IntervalIndex.from_breaks(
  961:                     pd.date_range("2012-01-01", periods=4, freq="D")
  962:                 ),
  963:             }
  964:         )
  965:         check_round_trip(df, pa)
  966: 
  967:     def test_timestamp_nanoseconds(self, pa):
  968:         # with version 2.6, pyarrow defaults to writing the nanoseconds, so
  969:         # this should work without error
  970:         # Note in previous pyarrows(<7.0.0), only the pseudo-version 2.0 was available
  971:         ver = "2.6"
  972:         df = pd.DataFrame({"a": pd.date_range("2017-01-01", freq="1ns", periods=10)})
  973:         check_round_trip(df, pa, write_kwargs={"version": ver})
  974: 
  975:     def test_timezone_aware_index(self, request, pa, timezone_aware_date_list):
  976:         if timezone_aware_date_list.tzinfo != datetime.timezone.utc:
  977:             request.applymarker(
  978:                 pytest.mark.xfail(
  979:                     reason="temporary skip this test until it is properly resolved: "
  980:                     "https://github.com/pandas-dev/pandas/issues/37286"
  981:                 )
  982:             )
  983:         idx = 5 * [timezone_aware_date_list]
  984:         df = pd.DataFrame(index=idx, data={"index_as_col": idx})
  985: 
  986:         # see gh-36004
  987:         # compare time(zone) values only, skip their class:
  988:         # pyarrow always creates fixed offset timezones using pytz.FixedOffset()
  989:         # even if it was datetime.timezone() originally
  990:         #
  991:         # technically they are the same:
  992:         # they both implement datetime.tzinfo
  993:         # they both wrap datetime.timedelta()
  994:         # this use-case sets the resolution to 1 minute
  995:         check_round_trip(df, pa, check_dtype=False)
  996: 
  997:     def test_filter_row_groups(self, pa):
  998:         # https://github.com/pandas-dev/pandas/issues/26551
  999:         pytest.importorskip("pyarrow")
 1000:         df = pd.DataFrame({"a": list(range(3))})
 1001:         with tm.ensure_clean() as path:
 1002:             df.to_parquet(path, engine=pa)
 1003:             result = read_parquet(path, pa, filters=[("a", "==", 0)])
 1004:         assert len(result) == 1
 1005: 
 1006:     def test_read_parquet_manager(self, pa, using_array_manager):
 1007:         # ensure that read_parquet honors the pandas.options.mode.data_manager option
 1008:         df = pd.DataFrame(
 1009:             np.random.default_rng(2).standard_normal((10, 3)), columns=["A", "B", "C"]
 1010:         )
 1011: 
 1012:         with tm.ensure_clean() as path:
 1013:             df.to_parquet(path, engine=pa)
 1014:             result = read_parquet(path, pa)
 1015:         if using_array_manager:
 1016:             assert isinstance(result._mgr, pd.core.internals.ArrayManager)
 1017:         else:
 1018:             assert isinstance(result._mgr, pd.core.internals.BlockManager)
 1019: 
 1020:     def test_read_dtype_backend_pyarrow_config(self, pa, df_full):
 1021:         import pyarrow
 1022: 
 1023:         df = df_full
 1024: 
 1025:         # additional supported types for pyarrow
 1026:         dti = pd.date_range("20130101", periods=3, tz="Europe/Brussels")
 1027:         dti = dti._with_freq(None)  # freq doesn't round-trip
 1028:         df["datetime_tz"] = dti
 1029:         df["bool_with_none"] = [True, None, True]
 1030: 
 1031:         pa_table = pyarrow.Table.from_pandas(df)
 1032:         expected = pa_table.to_pandas(types_mapper=pd.ArrowDtype)
 1033:         if pa_version_under13p0:
 1034:             # pyarrow infers datetimes as us instead of ns
 1035:             expected["datetime"] = expected["datetime"].astype("timestamp[us][pyarrow]")
 1036:             expected["datetime_with_nat"] = expected["datetime_with_nat"].astype(
 1037:                 "timestamp[us][pyarrow]"
 1038:             )
 1039:             expected["datetime_tz"] = expected["datetime_tz"].astype(
 1040:                 pd.ArrowDtype(pyarrow.timestamp(unit="us", tz="Europe/Brussels"))
 1041:             )
 1042: 
 1043:         check_round_trip(
 1044:             df,
 1045:             engine=pa,
 1046:             read_kwargs={"dtype_backend": "pyarrow"},
 1047:             expected=expected,
 1048:         )
 1049: 
 1050:     def test_read_dtype_backend_pyarrow_config_index(self, pa):
 1051:         df = pd.DataFrame(
 1052:             {"a": [1, 2]}, index=pd.Index([3, 4], name="test"), dtype="int64[pyarrow]"
 1053:         )
 1054:         expected = df.copy()
 1055:         import pyarrow
 1056: 
 1057:         if Version(pyarrow.__version__) > Version("11.0.0"):
 1058:             expected.index = expected.index.astype("int64[pyarrow]")
 1059:         check_round_trip(
 1060:             df,
 1061:             engine=pa,
 1062:             read_kwargs={"dtype_backend": "pyarrow"},
 1063:             expected=expected,
 1064:         )
 1065: 
 1066:     def test_columns_dtypes_not_invalid(self, pa):
 1067:         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
 1068: 
 1069:         # numeric
 1070:         df.columns = [0, 1]
 1071:         check_round_trip(df, pa)
 1072: 
 1073:         # bytes
 1074:         df.columns = [b"foo", b"bar"]
 1075:         with pytest.raises(NotImplementedError, match="|S3"):
 1076:             # Bytes fails on read_parquet
 1077:             check_round_trip(df, pa)
 1078: 
 1079:         # python object
 1080:         df.columns = [
 1081:             datetime.datetime(2011, 1, 1, 0, 0),
 1082:             datetime.datetime(2011, 1, 1, 1, 1),
 1083:         ]
 1084:         check_round_trip(df, pa)
 1085: 
 1086:     def test_empty_columns(self, pa):
 1087:         # GH 52034
 1088:         df = pd.DataFrame(index=pd.Index(["a", "b", "c"], name="custom name"))
 1089:         check_round_trip(df, pa)
 1090: 
 1091:     def test_df_attrs_persistence(self, tmp_path, pa):
 1092:         path = tmp_path / "test_df_metadata.p"
 1093:         df = pd.DataFrame(data={1: [1]})
 1094:         df.attrs = {"test_attribute": 1}
 1095:         df.to_parquet(path, engine=pa)
 1096:         new_df = read_parquet(path, engine=pa)
 1097:         assert new_df.attrs == df.attrs
 1098: 
 1099:     def test_string_inference(self, tmp_path, pa):
 1100:         # GH#54431
 1101:         path = tmp_path / "test_string_inference.p"
 1102:         df = pd.DataFrame(data={"a": ["x", "y"]}, index=["a", "b"])
 1103:         df.to_parquet(path, engine="pyarrow")
 1104:         with pd.option_context("future.infer_string", True):
 1105:             result = read_parquet(path, engine="pyarrow")
 1106:         expected = pd.DataFrame(
 1107:             data={"a": ["x", "y"]},
 1108:             dtype="string[pyarrow_numpy]",
 1109:             index=pd.Index(["a", "b"], dtype="string[pyarrow_numpy]"),
 1110:         )
 1111:         tm.assert_frame_equal(result, expected)
 1112: 
 1113:     @pytest.mark.skipif(pa_version_under11p0, reason="not supported before 11.0")
 1114:     def test_roundtrip_decimal(self, tmp_path, pa):
 1115:         # GH#54768
 1116:         import pyarrow as pa
 1117: 
 1118:         path = tmp_path / "decimal.p"
 1119:         df = pd.DataFrame({"a": [Decimal("123.00")]}, dtype="string[pyarrow]")
 1120:         df.to_parquet(path, schema=pa.schema([("a", pa.decimal128(5))]))
 1121:         result = read_parquet(path)
 1122:         expected = pd.DataFrame({"a": ["123"]}, dtype="string[python]")
 1123:         tm.assert_frame_equal(result, expected)
 1124: 
 1125:     def test_infer_string_large_string_type(self, tmp_path, pa):
 1126:         # GH#54798
 1127:         import pyarrow as pa
 1128:         import pyarrow.parquet as pq
 1129: 
 1130:         path = tmp_path / "large_string.p"
 1131: 
 1132:         table = pa.table({"a": pa.array([None, "b", "c"], pa.large_string())})
 1133:         pq.write_table(table, path)
 1134: 
 1135:         with pd.option_context("future.infer_string", True):
 1136:             result = read_parquet(path)
 1137:         expected = pd.DataFrame(
 1138:             data={"a": [None, "b", "c"]},
 1139:             dtype="string[pyarrow_numpy]",
 1140:             columns=pd.Index(["a"], dtype="string[pyarrow_numpy]"),
 1141:         )
 1142:         tm.assert_frame_equal(result, expected)
 1143: 
 1144:     # NOTE: this test is not run by default, because it requires a lot of memory (>5GB)
 1145:     # @pytest.mark.slow
 1146:     # def test_string_column_above_2GB(self, tmp_path, pa):
 1147:     #     # https://github.com/pandas-dev/pandas/issues/55606
 1148:     #     # above 2GB of string data
 1149:     #     v1 = b"x" * 100000000
 1150:     #     v2 = b"x" * 147483646
 1151:     #     df = pd.DataFrame({"strings": [v1] * 20 + [v2] + ["x"] * 20}, dtype="string")
 1152:     #     df.to_parquet(tmp_path / "test.parquet")
 1153:     #     result = read_parquet(tmp_path / "test.parquet")
 1154:     #     assert result["strings"].dtype == "string"
 1155: 
 1156: 
 1157: class TestParquetFastParquet(Base):
 1158:     def test_basic(self, fp, df_full):
 1159:         df = df_full
 1160: 
 1161:         dti = pd.date_range("20130101", periods=3, tz="US/Eastern")
 1162:         dti = dti._with_freq(None)  # freq doesn't round-trip
 1163:         df["datetime_tz"] = dti
 1164:         df["timedelta"] = pd.timedelta_range("1 day", periods=3)
 1165:         check_round_trip(df, fp)
 1166: 
 1167:     def test_columns_dtypes_invalid(self, fp):
 1168:         df = pd.DataFrame({"string": list("abc"), "int": list(range(1, 4))})
 1169: 
 1170:         err = TypeError
 1171:         msg = "Column name must be a string"
 1172: 
 1173:         # numeric
 1174:         df.columns = [0, 1]
 1175:         self.check_error_on_write(df, fp, err, msg)
 1176: 
 1177:         # bytes
 1178:         df.columns = [b"foo", b"bar"]
 1179:         self.check_error_on_write(df, fp, err, msg)
 1180: 
 1181:         # python object
 1182:         df.columns = [
 1183:             datetime.datetime(2011, 1, 1, 0, 0),
 1184:             datetime.datetime(2011, 1, 1, 1, 1),
 1185:         ]
 1186:         self.check_error_on_write(df, fp, err, msg)
 1187: 
 1188:     def test_duplicate_columns(self, fp):
 1189:         # not currently able to handle duplicate columns
 1190:         df = pd.DataFrame(np.arange(12).reshape(4, 3), columns=list("aaa")).copy()
 1191:         msg = "Cannot create parquet dataset with duplicate column names"
 1192:         self.check_error_on_write(df, fp, ValueError, msg)
 1193: 
 1194:     def test_bool_with_none(self, fp):
 1195:         df = pd.DataFrame({"a": [True, None, False]})
 1196:         expected = pd.DataFrame({"a": [1.0, np.nan, 0.0]}, dtype="float16")
 1197:         # Fastparquet bug in 0.7.1 makes it so that this dtype becomes
 1198:         # float64
 1199:         check_round_trip(df, fp, expected=expected, check_dtype=False)
 1200: 
 1201:     def test_unsupported(self, fp):
 1202:         # period
 1203:         df = pd.DataFrame({"a": pd.period_range("2013", freq="M", periods=3)})
 1204:         # error from fastparquet -> don't check exact error message
 1205:         self.check_error_on_write(df, fp, ValueError, None)
 1206: 
 1207:         # mixed
 1208:         df = pd.DataFrame({"a": ["a", 1, 2.0]})
 1209:         msg = "Can't infer object conversion type"
 1210:         self.check_error_on_write(df, fp, ValueError, msg)
 1211: 
 1212:     def test_categorical(self, fp):
 1213:         df = pd.DataFrame({"a": pd.Categorical(list("abc"))})
 1214:         check_round_trip(df, fp)
 1215: 
 1216:     def test_filter_row_groups(self, fp):
 1217:         d = {"a": list(range(3))}
 1218:         df = pd.DataFrame(d)
 1219:         with tm.ensure_clean() as path:
 1220:             df.to_parquet(path, engine=fp, compression=None, row_group_offsets=1)
 1221:             result = read_parquet(path, fp, filters=[("a", "==", 0)])
 1222:         assert len(result) == 1
 1223: 
 1224:     @pytest.mark.single_cpu
 1225:     def test_s3_roundtrip(self, df_compat, s3_public_bucket, fp, s3so):
 1226:         # GH #19134
 1227:         check_round_trip(
 1228:             df_compat,
 1229:             fp,
 1230:             path=f"s3://{s3_public_bucket.name}/fastparquet.parquet",
 1231:             read_kwargs={"storage_options": s3so},
 1232:             write_kwargs={"compression": None, "storage_options": s3so},
 1233:         )
 1234: 
 1235:     def test_partition_cols_supported(self, tmp_path, fp, df_full):
 1236:         # GH #23283
 1237:         partition_cols = ["bool", "int"]
 1238:         df = df_full
 1239:         df.to_parquet(
 1240:             tmp_path,
 1241:             engine="fastparquet",
 1242:             partition_cols=partition_cols,
 1243:             compression=None,
 1244:         )
 1245:         assert os.path.exists(tmp_path)
 1246:         import fastparquet
 1247: 
 1248:         actual_partition_cols = fastparquet.ParquetFile(str(tmp_path), False).cats
 1249:         assert len(actual_partition_cols) == 2
 1250: 
 1251:     def test_partition_cols_string(self, tmp_path, fp, df_full):
 1252:         # GH #27117
 1253:         partition_cols = "bool"
 1254:         df = df_full
 1255:         df.to_parquet(
 1256:             tmp_path,
 1257:             engine="fastparquet",
 1258:             partition_cols=partition_cols,
 1259:             compression=None,
 1260:         )
 1261:         assert os.path.exists(tmp_path)
 1262:         import fastparquet
 1263: 
 1264:         actual_partition_cols = fastparquet.ParquetFile(str(tmp_path), False).cats
 1265:         assert len(actual_partition_cols) == 1
 1266: 
 1267:     def test_partition_on_supported(self, tmp_path, fp, df_full):
 1268:         # GH #23283
 1269:         partition_cols = ["bool", "int"]
 1270:         df = df_full
 1271:         df.to_parquet(
 1272:             tmp_path,
 1273:             engine="fastparquet",
 1274:             compression=None,
 1275:             partition_on=partition_cols,
 1276:         )
 1277:         assert os.path.exists(tmp_path)
 1278:         import fastparquet
 1279: 
 1280:         actual_partition_cols = fastparquet.ParquetFile(str(tmp_path), False).cats
 1281:         assert len(actual_partition_cols) == 2
 1282: 
 1283:     def test_error_on_using_partition_cols_and_partition_on(
 1284:         self, tmp_path, fp, df_full
 1285:     ):
 1286:         # GH #23283
 1287:         partition_cols = ["bool", "int"]
 1288:         df = df_full
 1289:         msg = (
 1290:             "Cannot use both partition_on and partition_cols. Use partition_cols for "
 1291:             "partitioning data"
 1292:         )
 1293:         with pytest.raises(ValueError, match=msg):
 1294:             df.to_parquet(
 1295:                 tmp_path,
 1296:                 engine="fastparquet",
 1297:                 compression=None,
 1298:                 partition_on=partition_cols,
 1299:                 partition_cols=partition_cols,
 1300:             )
 1301: 
 1302:     @pytest.mark.skipif(using_copy_on_write(), reason="fastparquet writes into Index")
 1303:     def test_empty_dataframe(self, fp):
 1304:         # GH #27339
 1305:         df = pd.DataFrame()
 1306:         expected = df.copy()
 1307:         check_round_trip(df, fp, expected=expected)
 1308: 
 1309:     @pytest.mark.skipif(using_copy_on_write(), reason="fastparquet writes into Index")
 1310:     def test_timezone_aware_index(self, fp, timezone_aware_date_list):
 1311:         idx = 5 * [timezone_aware_date_list]
 1312: 
 1313:         df = pd.DataFrame(index=idx, data={"index_as_col": idx})
 1314: 
 1315:         expected = df.copy()
 1316:         expected.index.name = "index"
 1317:         check_round_trip(df, fp, expected=expected)
 1318: 
 1319:     def test_use_nullable_dtypes_not_supported(self, fp):
 1320:         df = pd.DataFrame({"a": [1, 2]})
 1321: 
 1322:         with tm.ensure_clean() as path:
 1323:             df.to_parquet(path)
 1324:             with pytest.raises(ValueError, match="not supported for the fastparquet"):
 1325:                 with tm.assert_produces_warning(FutureWarning):
 1326:                     read_parquet(path, engine="fastparquet", use_nullable_dtypes=True)
 1327:             with pytest.raises(ValueError, match="not supported for the fastparquet"):
 1328:                 read_parquet(path, engine="fastparquet", dtype_backend="pyarrow")
 1329: 
 1330:     def test_close_file_handle_on_read_error(self):
 1331:         with tm.ensure_clean("test.parquet") as path:
 1332:             pathlib.Path(path).write_bytes(b"breakit")
 1333:             with pytest.raises(Exception, match=""):  # Not important which exception
 1334:                 read_parquet(path, engine="fastparquet")
 1335:             # The next line raises an error on Windows if the file is still open
 1336:             pathlib.Path(path).unlink(missing_ok=False)
 1337: 
 1338:     def test_bytes_file_name(self, engine):
 1339:         # GH#48944
 1340:         df = pd.DataFrame(data={"A": [0, 1], "B": [1, 0]})
 1341:         with tm.ensure_clean("test.parquet") as path:
 1342:             with open(path.encode(), "wb") as f:
 1343:                 df.to_parquet(f)
 1344: 
 1345:             result = read_parquet(path, engine=engine)
 1346:         tm.assert_frame_equal(result, df)
 1347: 
 1348:     def test_filesystem_notimplemented(self):
 1349:         pytest.importorskip("fastparquet")
 1350:         df = pd.DataFrame(data={"A": [0, 1], "B": [1, 0]})
 1351:         with tm.ensure_clean() as path:
 1352:             with pytest.raises(
 1353:                 NotImplementedError, match="filesystem is not implemented"
 1354:             ):
 1355:                 df.to_parquet(path, engine="fastparquet", filesystem="foo")
 1356: 
 1357:         with tm.ensure_clean() as path:
 1358:             pathlib.Path(path).write_bytes(b"foo")
 1359:             with pytest.raises(
 1360:                 NotImplementedError, match="filesystem is not implemented"
 1361:             ):
 1362:                 read_parquet(path, engine="fastparquet", filesystem="foo")
 1363: 
 1364:     def test_invalid_filesystem(self):
 1365:         pytest.importorskip("pyarrow")
 1366:         df = pd.DataFrame(data={"A": [0, 1], "B": [1, 0]})
 1367:         with tm.ensure_clean() as path:
 1368:             with pytest.raises(
 1369:                 ValueError, match="filesystem must be a pyarrow or fsspec FileSystem"
 1370:             ):
 1371:                 df.to_parquet(path, engine="pyarrow", filesystem="foo")
 1372: 
 1373:         with tm.ensure_clean() as path:
 1374:             pathlib.Path(path).write_bytes(b"foo")
 1375:             with pytest.raises(
 1376:                 ValueError, match="filesystem must be a pyarrow or fsspec FileSystem"
 1377:             ):
 1378:                 read_parquet(path, engine="pyarrow", filesystem="foo")
 1379: 
 1380:     def test_unsupported_pa_filesystem_storage_options(self):
 1381:         pa_fs = pytest.importorskip("pyarrow.fs")
 1382:         df = pd.DataFrame(data={"A": [0, 1], "B": [1, 0]})
 1383:         with tm.ensure_clean() as path:
 1384:             with pytest.raises(
 1385:                 NotImplementedError,
 1386:                 match="storage_options not supported with a pyarrow FileSystem.",
 1387:             ):
 1388:                 df.to_parquet(
 1389:                     path,
 1390:                     engine="pyarrow",
 1391:                     filesystem=pa_fs.LocalFileSystem(),
 1392:                     storage_options={"foo": "bar"},
 1393:                 )
 1394: 
 1395:         with tm.ensure_clean() as path:
 1396:             pathlib.Path(path).write_bytes(b"foo")
 1397:             with pytest.raises(
 1398:                 NotImplementedError,
 1399:                 match="storage_options not supported with a pyarrow FileSystem.",
 1400:             ):
 1401:                 read_parquet(
 1402:                     path,
 1403:                     engine="pyarrow",
 1404:                     filesystem=pa_fs.LocalFileSystem(),
 1405:                     storage_options={"foo": "bar"},
 1406:                 )
 1407: 
 1408:     def test_invalid_dtype_backend(self, engine):
 1409:         msg = (
 1410:             "dtype_backend numpy is invalid, only 'numpy_nullable' and "
 1411:             "'pyarrow' are allowed."
 1412:         )
 1413:         df = pd.DataFrame({"int": list(range(1, 4))})
 1414:         with tm.ensure_clean("tmp.parquet") as path:
 1415:             df.to_parquet(path)
 1416:             with pytest.raises(ValueError, match=msg):
 1417:                 read_parquet(path, dtype_backend="numpy")
 1418: 
 1419:     @pytest.mark.skipif(using_copy_on_write(), reason="fastparquet writes into Index")
 1420:     def test_empty_columns(self, fp):
 1421:         # GH 52034
 1422:         df = pd.DataFrame(index=pd.Index(["a", "b", "c"], name="custom name"))
 1423:         expected = pd.DataFrame(index=pd.Index(["a", "b", "c"], name="custom name"))
 1424:         check_round_trip(df, fp, expected=expected)
