    1: """
    2: Tests that work on both the Python and C engines but do not have a
    3: specific classification into the other test modules.
    4: """
    5: from datetime import datetime
    6: from inspect import signature
    7: from io import StringIO
    8: import os
    9: from pathlib import Path
   10: import sys
   11: 
   12: import numpy as np
   13: import pytest
   14: 
   15: from pandas.errors import (
   16:     EmptyDataError,
   17:     ParserError,
   18:     ParserWarning,
   19: )
   20: 
   21: from pandas import (
   22:     DataFrame,
   23:     Index,
   24:     Timestamp,
   25:     compat,
   26: )
   27: import pandas._testing as tm
   28: 
   29: from pandas.io.parsers import TextFileReader
   30: from pandas.io.parsers.c_parser_wrapper import CParserWrapper
   31: 
   32: pytestmark = pytest.mark.filterwarnings(
   33:     "ignore:Passing a BlockManager to DataFrame:DeprecationWarning"
   34: )
   35: 
   36: xfail_pyarrow = pytest.mark.usefixtures("pyarrow_xfail")
   37: skip_pyarrow = pytest.mark.usefixtures("pyarrow_skip")
   38: 
   39: 
   40: def test_override_set_noconvert_columns():
   41:     # see gh-17351
   42:     #
   43:     # Usecols needs to be sorted in _set_noconvert_columns based
   44:     # on the test_usecols_with_parse_dates test from test_usecols.py
   45:     class MyTextFileReader(TextFileReader):
   46:         def __init__(self) -> None:
   47:             self._currow = 0
   48:             self.squeeze = False
   49: 
   50:     class MyCParserWrapper(CParserWrapper):
   51:         def _set_noconvert_columns(self):
   52:             if self.usecols_dtype == "integer":
   53:                 # self.usecols is a set, which is documented as unordered
   54:                 # but in practice, a CPython set of integers is sorted.
   55:                 # In other implementations this assumption does not hold.
   56:                 # The following code simulates a different order, which
   57:                 # before GH 17351 would cause the wrong columns to be
   58:                 # converted via the parse_dates parameter
   59:                 self.usecols = list(self.usecols)
   60:                 self.usecols.reverse()
   61:             return CParserWrapper._set_noconvert_columns(self)
   62: 
   63:     data = """a,b,c,d,e
   64: 0,1,2014-01-01,09:00,4
   65: 0,1,2014-01-02,10:00,4"""
   66: 
   67:     parse_dates = [[1, 2]]
   68:     cols = {
   69:         "a": [0, 0],
   70:         "c_d": [Timestamp("2014-01-01 09:00:00"), Timestamp("2014-01-02 10:00:00")],
   71:     }
   72:     expected = DataFrame(cols, columns=["c_d", "a"])
   73: 
   74:     parser = MyTextFileReader()
   75:     parser.options = {
   76:         "usecols": [0, 2, 3],
   77:         "parse_dates": parse_dates,
   78:         "delimiter": ",",
   79:     }
   80:     parser.engine = "c"
   81:     parser._engine = MyCParserWrapper(StringIO(data), **parser.options)
   82: 
   83:     result = parser.read()
   84:     tm.assert_frame_equal(result, expected)
   85: 
   86: 
   87: def test_read_csv_local(all_parsers, csv1):
   88:     prefix = "file:///" if compat.is_platform_windows() else "file://"
   89:     parser = all_parsers
   90: 
   91:     fname = prefix + str(os.path.abspath(csv1))
   92:     result = parser.read_csv(fname, index_col=0, parse_dates=True)
   93:     # TODO: make unit check more specific
   94:     if parser.engine == "pyarrow":
   95:         result.index = result.index.as_unit("ns")
   96:     expected = DataFrame(
   97:         [
   98:             [0.980269, 3.685731, -0.364216805298, -1.159738],
   99:             [1.047916, -0.041232, -0.16181208307, 0.212549],
  100:             [0.498581, 0.731168, -0.537677223318, 1.346270],
  101:             [1.120202, 1.567621, 0.00364077397681, 0.675253],
  102:             [-0.487094, 0.571455, -1.6116394093, 0.103469],
  103:             [0.836649, 0.246462, 0.588542635376, 1.062782],
  104:             [-0.157161, 1.340307, 1.1957779562, -1.097007],
  105:         ],
  106:         columns=["A", "B", "C", "D"],
  107:         index=Index(
  108:             [
  109:                 datetime(2000, 1, 3),
  110:                 datetime(2000, 1, 4),
  111:                 datetime(2000, 1, 5),
  112:                 datetime(2000, 1, 6),
  113:                 datetime(2000, 1, 7),
  114:                 datetime(2000, 1, 10),
  115:                 datetime(2000, 1, 11),
  116:             ],
  117:             name="index",
  118:         ),
  119:     )
  120:     tm.assert_frame_equal(result, expected)
  121: 
  122: 
  123: def test_1000_sep(all_parsers):
  124:     parser = all_parsers
  125:     data = """A|B|C
  126: 1|2,334|5
  127: 10|13|10.
  128: """
  129:     expected = DataFrame({"A": [1, 10], "B": [2334, 13], "C": [5, 10.0]})
  130: 
  131:     if parser.engine == "pyarrow":
  132:         msg = "The 'thousands' option is not supported with the 'pyarrow' engine"
  133:         with pytest.raises(ValueError, match=msg):
  134:             parser.read_csv(StringIO(data), sep="|", thousands=",")
  135:         return
  136: 
  137:     result = parser.read_csv(StringIO(data), sep="|", thousands=",")
  138:     tm.assert_frame_equal(result, expected)
  139: 
  140: 
  141: @xfail_pyarrow  # ValueError: Found non-unique column index
  142: def test_unnamed_columns(all_parsers):
  143:     data = """A,B,C,,
  144: 1,2,3,4,5
  145: 6,7,8,9,10
  146: 11,12,13,14,15
  147: """
  148:     parser = all_parsers
  149:     expected = DataFrame(
  150:         [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15]],
  151:         dtype=np.int64,
  152:         columns=["A", "B", "C", "Unnamed: 3", "Unnamed: 4"],
  153:     )
  154:     result = parser.read_csv(StringIO(data))
  155:     tm.assert_frame_equal(result, expected)
  156: 
  157: 
  158: def test_csv_mixed_type(all_parsers):
  159:     data = """A,B,C
  160: a,1,2
  161: b,3,4
  162: c,4,5
  163: """
  164:     parser = all_parsers
  165:     expected = DataFrame({"A": ["a", "b", "c"], "B": [1, 3, 4], "C": [2, 4, 5]})
  166:     result = parser.read_csv(StringIO(data))
  167:     tm.assert_frame_equal(result, expected)
  168: 
  169: 
  170: def test_read_csv_low_memory_no_rows_with_index(all_parsers):
  171:     # see gh-21141
  172:     parser = all_parsers
  173: 
  174:     if not parser.low_memory:
  175:         pytest.skip("This is a low-memory specific test")
  176: 
  177:     data = """A,B,C
  178: 1,1,1,2
  179: 2,2,3,4
  180: 3,3,4,5
  181: """
  182: 
  183:     if parser.engine == "pyarrow":
  184:         msg = "The 'nrows' option is not supported with the 'pyarrow' engine"
  185:         with pytest.raises(ValueError, match=msg):
  186:             parser.read_csv(StringIO(data), low_memory=True, index_col=0, nrows=0)
  187:         return
  188: 
  189:     result = parser.read_csv(StringIO(data), low_memory=True, index_col=0, nrows=0)
  190:     expected = DataFrame(columns=["A", "B", "C"])
  191:     tm.assert_frame_equal(result, expected)
  192: 
  193: 
  194: def test_read_csv_dataframe(all_parsers, csv1):
  195:     parser = all_parsers
  196:     result = parser.read_csv(csv1, index_col=0, parse_dates=True)
  197:     # TODO: make unit check more specific
  198:     if parser.engine == "pyarrow":
  199:         result.index = result.index.as_unit("ns")
  200:     expected = DataFrame(
  201:         [
  202:             [0.980269, 3.685731, -0.364216805298, -1.159738],
  203:             [1.047916, -0.041232, -0.16181208307, 0.212549],
  204:             [0.498581, 0.731168, -0.537677223318, 1.346270],
  205:             [1.120202, 1.567621, 0.00364077397681, 0.675253],
  206:             [-0.487094, 0.571455, -1.6116394093, 0.103469],
  207:             [0.836649, 0.246462, 0.588542635376, 1.062782],
  208:             [-0.157161, 1.340307, 1.1957779562, -1.097007],
  209:         ],
  210:         columns=["A", "B", "C", "D"],
  211:         index=Index(
  212:             [
  213:                 datetime(2000, 1, 3),
  214:                 datetime(2000, 1, 4),
  215:                 datetime(2000, 1, 5),
  216:                 datetime(2000, 1, 6),
  217:                 datetime(2000, 1, 7),
  218:                 datetime(2000, 1, 10),
  219:                 datetime(2000, 1, 11),
  220:             ],
  221:             name="index",
  222:         ),
  223:     )
  224:     tm.assert_frame_equal(result, expected)
  225: 
  226: 
  227: @pytest.mark.parametrize("nrows", [3, 3.0])
  228: def test_read_nrows(all_parsers, nrows):
  229:     # see gh-10476
  230:     data = """index,A,B,C,D
  231: foo,2,3,4,5
  232: bar,7,8,9,10
  233: baz,12,13,14,15
  234: qux,12,13,14,15
  235: foo2,12,13,14,15
  236: bar2,12,13,14,15
  237: """
  238:     expected = DataFrame(
  239:         [["foo", 2, 3, 4, 5], ["bar", 7, 8, 9, 10], ["baz", 12, 13, 14, 15]],
  240:         columns=["index", "A", "B", "C", "D"],
  241:     )
  242:     parser = all_parsers
  243: 
  244:     if parser.engine == "pyarrow":
  245:         msg = "The 'nrows' option is not supported with the 'pyarrow' engine"
  246:         with pytest.raises(ValueError, match=msg):
  247:             parser.read_csv(StringIO(data), nrows=nrows)
  248:         return
  249: 
  250:     result = parser.read_csv(StringIO(data), nrows=nrows)
  251:     tm.assert_frame_equal(result, expected)
  252: 
  253: 
  254: @pytest.mark.parametrize("nrows", [1.2, "foo", -1])
  255: def test_read_nrows_bad(all_parsers, nrows):
  256:     data = """index,A,B,C,D
  257: foo,2,3,4,5
  258: bar,7,8,9,10
  259: baz,12,13,14,15
  260: qux,12,13,14,15
  261: foo2,12,13,14,15
  262: bar2,12,13,14,15
  263: """
  264:     msg = r"'nrows' must be an integer >=0"
  265:     parser = all_parsers
  266:     if parser.engine == "pyarrow":
  267:         msg = "The 'nrows' option is not supported with the 'pyarrow' engine"
  268: 
  269:     with pytest.raises(ValueError, match=msg):
  270:         parser.read_csv(StringIO(data), nrows=nrows)
  271: 
  272: 
  273: def test_nrows_skipfooter_errors(all_parsers):
  274:     msg = "'skipfooter' not supported with 'nrows'"
  275:     data = "a\n1\n2\n3\n4\n5\n6"
  276:     parser = all_parsers
  277: 
  278:     with pytest.raises(ValueError, match=msg):
  279:         parser.read_csv(StringIO(data), skipfooter=1, nrows=5)
  280: 
  281: 
  282: @skip_pyarrow
  283: def test_missing_trailing_delimiters(all_parsers):
  284:     parser = all_parsers
  285:     data = """A,B,C,D
  286: 1,2,3,4
  287: 1,3,3,
  288: 1,4,5"""
  289: 
  290:     result = parser.read_csv(StringIO(data))
  291:     expected = DataFrame(
  292:         [[1, 2, 3, 4], [1, 3, 3, np.nan], [1, 4, 5, np.nan]],
  293:         columns=["A", "B", "C", "D"],
  294:     )
  295:     tm.assert_frame_equal(result, expected)
  296: 
  297: 
  298: def test_skip_initial_space(all_parsers):
  299:     data = (
  300:         '"09-Apr-2012", "01:10:18.300", 2456026.548822908, 12849, '
  301:         "1.00361,  1.12551, 330.65659, 0355626618.16711,  73.48821, "
  302:         "314.11625,  1917.09447,   179.71425,  80.000, 240.000, -350,  "
  303:         "70.06056, 344.98370, 1,   1, -0.689265, -0.692787,  "
  304:         "0.212036,    14.7674,   41.605,   -9999.0,   -9999.0,   "
  305:         "-9999.0,   -9999.0,   -9999.0,  -9999.0, 000, 012, 128"
  306:     )
  307:     parser = all_parsers
  308: 
  309:     if parser.engine == "pyarrow":
  310:         msg = "The 'skipinitialspace' option is not supported with the 'pyarrow' engine"
  311:         with pytest.raises(ValueError, match=msg):
  312:             parser.read_csv(
  313:                 StringIO(data),
  314:                 names=list(range(33)),
  315:                 header=None,
  316:                 na_values=["-9999.0"],
  317:                 skipinitialspace=True,
  318:             )
  319:         return
  320: 
  321:     result = parser.read_csv(
  322:         StringIO(data),
  323:         names=list(range(33)),
  324:         header=None,
  325:         na_values=["-9999.0"],
  326:         skipinitialspace=True,
  327:     )
  328:     expected = DataFrame(
  329:         [
  330:             [
  331:                 "09-Apr-2012",
  332:                 "01:10:18.300",
  333:                 2456026.548822908,
  334:                 12849,
  335:                 1.00361,
  336:                 1.12551,
  337:                 330.65659,
  338:                 355626618.16711,
  339:                 73.48821,
  340:                 314.11625,
  341:                 1917.09447,
  342:                 179.71425,
  343:                 80.0,
  344:                 240.0,
  345:                 -350,
  346:                 70.06056,
  347:                 344.9837,
  348:                 1,
  349:                 1,
  350:                 -0.689265,
  351:                 -0.692787,
  352:                 0.212036,
  353:                 14.7674,
  354:                 41.605,
  355:                 np.nan,
  356:                 np.nan,
  357:                 np.nan,
  358:                 np.nan,
  359:                 np.nan,
  360:                 np.nan,
  361:                 0,
  362:                 12,
  363:                 128,
  364:             ]
  365:         ]
  366:     )
  367:     tm.assert_frame_equal(result, expected)
  368: 
  369: 
  370: @skip_pyarrow
  371: def test_trailing_delimiters(all_parsers):
  372:     # see gh-2442
  373:     data = """A,B,C
  374: 1,2,3,
  375: 4,5,6,
  376: 7,8,9,"""
  377:     parser = all_parsers
  378:     result = parser.read_csv(StringIO(data), index_col=False)
  379: 
  380:     expected = DataFrame({"A": [1, 4, 7], "B": [2, 5, 8], "C": [3, 6, 9]})
  381:     tm.assert_frame_equal(result, expected)
  382: 
  383: 
  384: def test_escapechar(all_parsers):
  385:     # https://stackoverflow.com/questions/13824840/feature-request-for-
  386:     # pandas-read-csv
  387:     data = '''SEARCH_TERM,ACTUAL_URL
  388: "bra tv board","http://www.ikea.com/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord"
  389: "tv p\xc3\xa5 hjul","http://www.ikea.com/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord"
  390: "SLAGBORD, \\"Bergslagen\\", IKEA:s 1700-tals series","http://www.ikea.com/se/sv/catalog/categories/departments/living_room/10475/?se%7cps%7cnonbranded%7cvardagsrum%7cgoogle%7ctv_bord"'''
  391: 
  392:     parser = all_parsers
  393:     result = parser.read_csv(
  394:         StringIO(data), escapechar="\\", quotechar='"', encoding="utf-8"
  395:     )
  396: 
  397:     assert result["SEARCH_TERM"][2] == 'SLAGBORD, "Bergslagen", IKEA:s 1700-tals series'
  398: 
  399:     tm.assert_index_equal(result.columns, Index(["SEARCH_TERM", "ACTUAL_URL"]))
  400: 
  401: 
  402: def test_ignore_leading_whitespace(all_parsers):
  403:     # see gh-3374, gh-6607
  404:     parser = all_parsers
  405:     data = " a b c\n 1 2 3\n 4 5 6\n 7 8 9"
  406: 
  407:     if parser.engine == "pyarrow":
  408:         msg = "the 'pyarrow' engine does not support regex separators"
  409:         with pytest.raises(ValueError, match=msg):
  410:             parser.read_csv(StringIO(data), sep=r"\s+")
  411:         return
  412:     result = parser.read_csv(StringIO(data), sep=r"\s+")
  413: 
  414:     expected = DataFrame({"a": [1, 4, 7], "b": [2, 5, 8], "c": [3, 6, 9]})
  415:     tm.assert_frame_equal(result, expected)
  416: 
  417: 
  418: @skip_pyarrow
  419: @pytest.mark.parametrize("usecols", [None, [0, 1], ["a", "b"]])
  420: def test_uneven_lines_with_usecols(all_parsers, usecols):
  421:     # see gh-12203
  422:     parser = all_parsers
  423:     data = r"""a,b,c
  424: 0,1,2
  425: 3,4,5,6,7
  426: 8,9,10"""
  427: 
  428:     if usecols is None:
  429:         # Make sure that an error is still raised
  430:         # when the "usecols" parameter is not provided.
  431:         msg = r"Expected \d+ fields in line \d+, saw \d+"
  432:         with pytest.raises(ParserError, match=msg):
  433:             parser.read_csv(StringIO(data))
  434:     else:
  435:         expected = DataFrame({"a": [0, 3, 8], "b": [1, 4, 9]})
  436: 
  437:         result = parser.read_csv(StringIO(data), usecols=usecols)
  438:         tm.assert_frame_equal(result, expected)
  439: 
  440: 
  441: @skip_pyarrow
  442: @pytest.mark.parametrize(
  443:     "data,kwargs,expected",
  444:     [
  445:         # First, check to see that the response of parser when faced with no
  446:         # provided columns raises the correct error, with or without usecols.
  447:         ("", {}, None),
  448:         ("", {"usecols": ["X"]}, None),
  449:         (
  450:             ",,",
  451:             {"names": ["Dummy", "X", "Dummy_2"], "usecols": ["X"]},
  452:             DataFrame(columns=["X"], index=[0], dtype=np.float64),
  453:         ),
  454:         (
  455:             "",
  456:             {"names": ["Dummy", "X", "Dummy_2"], "usecols": ["X"]},
  457:             DataFrame(columns=["X"]),
  458:         ),
  459:     ],
  460: )
  461: def test_read_empty_with_usecols(all_parsers, data, kwargs, expected):
  462:     # see gh-12493
  463:     parser = all_parsers
  464: 
  465:     if expected is None:
  466:         msg = "No columns to parse from file"
  467:         with pytest.raises(EmptyDataError, match=msg):
  468:             parser.read_csv(StringIO(data), **kwargs)
  469:     else:
  470:         result = parser.read_csv(StringIO(data), **kwargs)
  471:         tm.assert_frame_equal(result, expected)
  472: 
  473: 
  474: @pytest.mark.parametrize(
  475:     "kwargs,expected",
  476:     [
  477:         # gh-8661, gh-8679: this should ignore six lines, including
  478:         # lines with trailing whitespace and blank lines.
  479:         (
  480:             {
  481:                 "header": None,
  482:                 "delim_whitespace": True,
  483:                 "skiprows": [0, 1, 2, 3, 5, 6],
  484:                 "skip_blank_lines": True,
  485:             },
  486:             DataFrame([[1.0, 2.0, 4.0], [5.1, np.nan, 10.0]]),
  487:         ),
  488:         # gh-8983: test skipping set of rows after a row with trailing spaces.
  489:         (
  490:             {
  491:                 "delim_whitespace": True,
  492:                 "skiprows": [1, 2, 3, 5, 6],
  493:                 "skip_blank_lines": True,
  494:             },
  495:             DataFrame({"A": [1.0, 5.1], "B": [2.0, np.nan], "C": [4.0, 10]}),
  496:         ),
  497:     ],
  498: )
  499: def test_trailing_spaces(all_parsers, kwargs, expected):
  500:     data = "A B C  \nrandom line with trailing spaces    \nskip\n1,2,3\n1,2.,4.\nrandom line with trailing tabs\t\t\t\n   \n5.1,NaN,10.0\n"  # noqa: E501
  501:     parser = all_parsers
  502: 
  503:     depr_msg = "The 'delim_whitespace' keyword in pd.read_csv is deprecated"
  504: 
  505:     if parser.engine == "pyarrow":
  506:         msg = "The 'delim_whitespace' option is not supported with the 'pyarrow' engine"
  507:         with pytest.raises(ValueError, match=msg):
  508:             with tm.assert_produces_warning(
  509:                 FutureWarning, match=depr_msg, check_stacklevel=False
  510:             ):
  511:                 parser.read_csv(StringIO(data.replace(",", "  ")), **kwargs)
  512:         return
  513: 
  514:     with tm.assert_produces_warning(
  515:         FutureWarning, match=depr_msg, check_stacklevel=False
  516:     ):
  517:         result = parser.read_csv(StringIO(data.replace(",", "  ")), **kwargs)
  518:     tm.assert_frame_equal(result, expected)
  519: 
  520: 
  521: def test_raise_on_sep_with_delim_whitespace(all_parsers):
  522:     # see gh-6607
  523:     data = "a b c\n1 2 3"
  524:     parser = all_parsers
  525: 
  526:     depr_msg = "The 'delim_whitespace' keyword in pd.read_csv is deprecated"
  527:     with pytest.raises(ValueError, match="you can only specify one"):
  528:         with tm.assert_produces_warning(
  529:             FutureWarning, match=depr_msg, check_stacklevel=False
  530:         ):
  531:             parser.read_csv(StringIO(data), sep=r"\s", delim_whitespace=True)
  532: 
  533: 
  534: def test_read_filepath_or_buffer(all_parsers):
  535:     # see gh-43366
  536:     parser = all_parsers
  537: 
  538:     with pytest.raises(TypeError, match="Expected file path name or file-like"):
  539:         parser.read_csv(filepath_or_buffer=b"input")
  540: 
  541: 
  542: @pytest.mark.parametrize("delim_whitespace", [True, False])
  543: def test_single_char_leading_whitespace(all_parsers, delim_whitespace):
  544:     # see gh-9710
  545:     parser = all_parsers
  546:     data = """\
  547: MyColumn
  548: a
  549: b
  550: a
  551: b\n"""
  552: 
  553:     expected = DataFrame({"MyColumn": list("abab")})
  554:     depr_msg = "The 'delim_whitespace' keyword in pd.read_csv is deprecated"
  555: 
  556:     if parser.engine == "pyarrow":
  557:         msg = "The 'skipinitialspace' option is not supported with the 'pyarrow' engine"
  558:         with pytest.raises(ValueError, match=msg):
  559:             with tm.assert_produces_warning(
  560:                 FutureWarning, match=depr_msg, check_stacklevel=False
  561:             ):
  562:                 parser.read_csv(
  563:                     StringIO(data),
  564:                     skipinitialspace=True,
  565:                     delim_whitespace=delim_whitespace,
  566:                 )
  567:         return
  568: 
  569:     with tm.assert_produces_warning(
  570:         FutureWarning, match=depr_msg, check_stacklevel=False
  571:     ):
  572:         result = parser.read_csv(
  573:             StringIO(data), skipinitialspace=True, delim_whitespace=delim_whitespace
  574:         )
  575:     tm.assert_frame_equal(result, expected)
  576: 
  577: 
  578: @pytest.mark.parametrize(
  579:     "sep,skip_blank_lines,exp_data",
  580:     [
  581:         (",", True, [[1.0, 2.0, 4.0], [5.0, np.nan, 10.0], [-70.0, 0.4, 1.0]]),
  582:         (r"\s+", True, [[1.0, 2.0, 4.0], [5.0, np.nan, 10.0], [-70.0, 0.4, 1.0]]),
  583:         (
  584:             ",",
  585:             False,
  586:             [
  587:                 [1.0, 2.0, 4.0],
  588:                 [np.nan, np.nan, np.nan],
  589:                 [np.nan, np.nan, np.nan],
  590:                 [5.0, np.nan, 10.0],
  591:                 [np.nan, np.nan, np.nan],
  592:                 [-70.0, 0.4, 1.0],
  593:             ],
  594:         ),
  595:     ],
  596: )
  597: def test_empty_lines(all_parsers, sep, skip_blank_lines, exp_data, request):
  598:     parser = all_parsers
  599:     data = """\
  600: A,B,C
  601: 1,2.,4.
  602: 
  603: 
  604: 5.,NaN,10.0
  605: 
  606: -70,.4,1
  607: """
  608: 
  609:     if sep == r"\s+":
  610:         data = data.replace(",", "  ")
  611: 
  612:         if parser.engine == "pyarrow":
  613:             msg = "the 'pyarrow' engine does not support regex separators"
  614:             with pytest.raises(ValueError, match=msg):
  615:                 parser.read_csv(
  616:                     StringIO(data), sep=sep, skip_blank_lines=skip_blank_lines
  617:                 )
  618:             return
  619: 
  620:     result = parser.read_csv(StringIO(data), sep=sep, skip_blank_lines=skip_blank_lines)
  621:     expected = DataFrame(exp_data, columns=["A", "B", "C"])
  622:     tm.assert_frame_equal(result, expected)
  623: 
  624: 
  625: @skip_pyarrow
  626: def test_whitespace_lines(all_parsers):
  627:     parser = all_parsers
  628:     data = """
  629: 
  630: \t  \t\t
  631: \t
  632: A,B,C
  633: \t    1,2.,4.
  634: 5.,NaN,10.0
  635: """
  636:     expected = DataFrame([[1, 2.0, 4.0], [5.0, np.nan, 10.0]], columns=["A", "B", "C"])
  637:     result = parser.read_csv(StringIO(data))
  638:     tm.assert_frame_equal(result, expected)
  639: 
  640: 
  641: @pytest.mark.parametrize(
  642:     "data,expected",
  643:     [
  644:         (
  645:             """   A   B   C   D
  646: a   1   2   3   4
  647: b   1   2   3   4
  648: c   1   2   3   4
  649: """,
  650:             DataFrame(
  651:                 [[1, 2, 3, 4], [1, 2, 3, 4], [1, 2, 3, 4]],
  652:                 columns=["A", "B", "C", "D"],
  653:                 index=["a", "b", "c"],
  654:             ),
  655:         ),
  656:         (
  657:             "    a b c\n1 2 3 \n4 5  6\n 7 8 9",
  658:             DataFrame([[1, 2, 3], [4, 5, 6], [7, 8, 9]], columns=["a", "b", "c"]),
  659:         ),
  660:     ],
  661: )
  662: def test_whitespace_regex_separator(all_parsers, data, expected):
  663:     # see gh-6607
  664:     parser = all_parsers
  665:     if parser.engine == "pyarrow":
  666:         msg = "the 'pyarrow' engine does not support regex separators"
  667:         with pytest.raises(ValueError, match=msg):
  668:             parser.read_csv(StringIO(data), sep=r"\s+")
  669:         return
  670: 
  671:     result = parser.read_csv(StringIO(data), sep=r"\s+")
  672:     tm.assert_frame_equal(result, expected)
  673: 
  674: 
  675: def test_sub_character(all_parsers, csv_dir_path):
  676:     # see gh-16893
  677:     filename = os.path.join(csv_dir_path, "sub_char.csv")
  678:     expected = DataFrame([[1, 2, 3]], columns=["a", "\x1ab", "c"])
  679: 
  680:     parser = all_parsers
  681:     result = parser.read_csv(filename)
  682:     tm.assert_frame_equal(result, expected)
  683: 
  684: 
  685: @pytest.mark.parametrize("filename", ["sГ©-es-vГ©.csv", "ru-sР№.csv", "дё­ж–‡ж–‡д»¶еђЌ.csv"])
  686: def test_filename_with_special_chars(all_parsers, filename):
  687:     # see gh-15086.
  688:     parser = all_parsers
  689:     df = DataFrame({"a": [1, 2, 3]})
  690: 
  691:     with tm.ensure_clean(filename) as path:
  692:         df.to_csv(path, index=False)
  693: 
  694:         result = parser.read_csv(path)
  695:         tm.assert_frame_equal(result, df)
  696: 
  697: 
  698: def test_read_table_same_signature_as_read_csv(all_parsers):
  699:     # GH-34976
  700:     parser = all_parsers
  701: 
  702:     table_sign = signature(parser.read_table)
  703:     csv_sign = signature(parser.read_csv)
  704: 
  705:     assert table_sign.parameters.keys() == csv_sign.parameters.keys()
  706:     assert table_sign.return_annotation == csv_sign.return_annotation
  707: 
  708:     for key, csv_param in csv_sign.parameters.items():
  709:         table_param = table_sign.parameters[key]
  710:         if key == "sep":
  711:             assert csv_param.default == ","
  712:             assert table_param.default == "\t"
  713:             assert table_param.annotation == csv_param.annotation
  714:             assert table_param.kind == csv_param.kind
  715:             continue
  716: 
  717:         assert table_param == csv_param
  718: 
  719: 
  720: def test_read_table_equivalency_to_read_csv(all_parsers):
  721:     # see gh-21948
  722:     # As of 0.25.0, read_table is undeprecated
  723:     parser = all_parsers
  724:     data = "a\tb\n1\t2\n3\t4"
  725:     expected = parser.read_csv(StringIO(data), sep="\t")
  726:     result = parser.read_table(StringIO(data))
  727:     tm.assert_frame_equal(result, expected)
  728: 
  729: 
  730: @pytest.mark.parametrize("read_func", ["read_csv", "read_table"])
  731: def test_read_csv_and_table_sys_setprofile(all_parsers, read_func):
  732:     # GH#41069
  733:     parser = all_parsers
  734:     data = "a b\n0 1"
  735: 
  736:     sys.setprofile(lambda *a, **k: None)
  737:     result = getattr(parser, read_func)(StringIO(data))
  738:     sys.setprofile(None)
  739: 
  740:     expected = DataFrame({"a b": ["0 1"]})
  741:     tm.assert_frame_equal(result, expected)
  742: 
  743: 
  744: @skip_pyarrow
  745: def test_first_row_bom(all_parsers):
  746:     # see gh-26545
  747:     parser = all_parsers
  748:     data = '''\ufeff"Head1"\t"Head2"\t"Head3"'''
  749: 
  750:     result = parser.read_csv(StringIO(data), delimiter="\t")
  751:     expected = DataFrame(columns=["Head1", "Head2", "Head3"])
  752:     tm.assert_frame_equal(result, expected)
  753: 
  754: 
  755: @skip_pyarrow
  756: def test_first_row_bom_unquoted(all_parsers):
  757:     # see gh-36343
  758:     parser = all_parsers
  759:     data = """\ufeffHead1\tHead2\tHead3"""
  760: 
  761:     result = parser.read_csv(StringIO(data), delimiter="\t")
  762:     expected = DataFrame(columns=["Head1", "Head2", "Head3"])
  763:     tm.assert_frame_equal(result, expected)
  764: 
  765: 
  766: @pytest.mark.parametrize("nrows", range(1, 6))
  767: def test_blank_lines_between_header_and_data_rows(all_parsers, nrows):
  768:     # GH 28071
  769:     ref = DataFrame(
  770:         [[np.nan, np.nan], [np.nan, np.nan], [1, 2], [np.nan, np.nan], [3, 4]],
  771:         columns=list("ab"),
  772:     )
  773:     csv = "\nheader\n\na,b\n\n\n1,2\n\n3,4"
  774:     parser = all_parsers
  775: 
  776:     if parser.engine == "pyarrow":
  777:         msg = "The 'nrows' option is not supported with the 'pyarrow' engine"
  778:         with pytest.raises(ValueError, match=msg):
  779:             parser.read_csv(
  780:                 StringIO(csv), header=3, nrows=nrows, skip_blank_lines=False
  781:             )
  782:         return
  783: 
  784:     df = parser.read_csv(StringIO(csv), header=3, nrows=nrows, skip_blank_lines=False)
  785:     tm.assert_frame_equal(df, ref[:nrows])
  786: 
  787: 
  788: @skip_pyarrow
  789: def test_no_header_two_extra_columns(all_parsers):
  790:     # GH 26218
  791:     column_names = ["one", "two", "three"]
  792:     ref = DataFrame([["foo", "bar", "baz"]], columns=column_names)
  793:     stream = StringIO("foo,bar,baz,bam,blah")
  794:     parser = all_parsers
  795:     df = parser.read_csv_check_warnings(
  796:         ParserWarning,
  797:         "Length of header or names does not match length of data. "
  798:         "This leads to a loss of data with index_col=False.",
  799:         stream,
  800:         header=None,
  801:         names=column_names,
  802:         index_col=False,
  803:     )
  804:     tm.assert_frame_equal(df, ref)
  805: 
  806: 
  807: def test_read_csv_names_not_accepting_sets(all_parsers):
  808:     # GH 34946
  809:     data = """\
  810:     1,2,3
  811:     4,5,6\n"""
  812:     parser = all_parsers
  813:     with pytest.raises(ValueError, match="Names should be an ordered collection."):
  814:         parser.read_csv(StringIO(data), names=set("QAZ"))
  815: 
  816: 
  817: def test_read_table_delim_whitespace_default_sep(all_parsers):
  818:     # GH: 35958
  819:     f = StringIO("a  b  c\n1 -2 -3\n4  5   6")
  820:     parser = all_parsers
  821: 
  822:     depr_msg = "The 'delim_whitespace' keyword in pd.read_table is deprecated"
  823: 
  824:     if parser.engine == "pyarrow":
  825:         msg = "The 'delim_whitespace' option is not supported with the 'pyarrow' engine"
  826:         with pytest.raises(ValueError, match=msg):
  827:             with tm.assert_produces_warning(
  828:                 FutureWarning, match=depr_msg, check_stacklevel=False
  829:             ):
  830:                 parser.read_table(f, delim_whitespace=True)
  831:         return
  832:     with tm.assert_produces_warning(
  833:         FutureWarning, match=depr_msg, check_stacklevel=False
  834:     ):
  835:         result = parser.read_table(f, delim_whitespace=True)
  836:     expected = DataFrame({"a": [1, 4], "b": [-2, 5], "c": [-3, 6]})
  837:     tm.assert_frame_equal(result, expected)
  838: 
  839: 
  840: @pytest.mark.parametrize("delimiter", [",", "\t"])
  841: def test_read_csv_delim_whitespace_non_default_sep(all_parsers, delimiter):
  842:     # GH: 35958
  843:     f = StringIO("a  b  c\n1 -2 -3\n4  5   6")
  844:     parser = all_parsers
  845:     msg = (
  846:         "Specified a delimiter with both sep and "
  847:         "delim_whitespace=True; you can only specify one."
  848:     )
  849:     depr_msg = "The 'delim_whitespace' keyword in pd.read_csv is deprecated"
  850:     with tm.assert_produces_warning(
  851:         FutureWarning, match=depr_msg, check_stacklevel=False
  852:     ):
  853:         with pytest.raises(ValueError, match=msg):
  854:             parser.read_csv(f, delim_whitespace=True, sep=delimiter)
  855: 
  856:         with pytest.raises(ValueError, match=msg):
  857:             parser.read_csv(f, delim_whitespace=True, delimiter=delimiter)
  858: 
  859: 
  860: def test_read_csv_delimiter_and_sep_no_default(all_parsers):
  861:     # GH#39823
  862:     f = StringIO("a,b\n1,2")
  863:     parser = all_parsers
  864:     msg = "Specified a sep and a delimiter; you can only specify one."
  865:     with pytest.raises(ValueError, match=msg):
  866:         parser.read_csv(f, sep=" ", delimiter=".")
  867: 
  868: 
  869: @pytest.mark.parametrize("kwargs", [{"delimiter": "\n"}, {"sep": "\n"}])
  870: def test_read_csv_line_break_as_separator(kwargs, all_parsers):
  871:     # GH#43528
  872:     parser = all_parsers
  873:     data = """a,b,c
  874: 1,2,3
  875:     """
  876:     msg = (
  877:         r"Specified \\n as separator or delimiter. This forces the python engine "
  878:         r"which does not accept a line terminator. Hence it is not allowed to use "
  879:         r"the line terminator as separator."
  880:     )
  881:     with pytest.raises(ValueError, match=msg):
  882:         parser.read_csv(StringIO(data), **kwargs)
  883: 
  884: 
  885: @pytest.mark.parametrize("delimiter", [",", "\t"])
  886: def test_read_table_delim_whitespace_non_default_sep(all_parsers, delimiter):
  887:     # GH: 35958
  888:     f = StringIO("a  b  c\n1 -2 -3\n4  5   6")
  889:     parser = all_parsers
  890:     msg = (
  891:         "Specified a delimiter with both sep and "
  892:         "delim_whitespace=True; you can only specify one."
  893:     )
  894:     depr_msg = "The 'delim_whitespace' keyword in pd.read_table is deprecated"
  895:     with tm.assert_produces_warning(
  896:         FutureWarning, match=depr_msg, check_stacklevel=False
  897:     ):
  898:         with pytest.raises(ValueError, match=msg):
  899:             parser.read_table(f, delim_whitespace=True, sep=delimiter)
  900: 
  901:         with pytest.raises(ValueError, match=msg):
  902:             parser.read_table(f, delim_whitespace=True, delimiter=delimiter)
  903: 
  904: 
  905: @skip_pyarrow
  906: def test_dict_keys_as_names(all_parsers):
  907:     # GH: 36928
  908:     data = "1,2"
  909: 
  910:     keys = {"a": int, "b": int}.keys()
  911:     parser = all_parsers
  912: 
  913:     result = parser.read_csv(StringIO(data), names=keys)
  914:     expected = DataFrame({"a": [1], "b": [2]})
  915:     tm.assert_frame_equal(result, expected)
  916: 
  917: 
  918: @xfail_pyarrow  # UnicodeDecodeError: 'utf-8' codec can't decode byte 0xed in position 0
  919: def test_encoding_surrogatepass(all_parsers):
  920:     # GH39017
  921:     parser = all_parsers
  922:     content = b"\xed\xbd\xbf"
  923:     decoded = content.decode("utf-8", errors="surrogatepass")
  924:     expected = DataFrame({decoded: [decoded]}, index=[decoded * 2])
  925:     expected.index.name = decoded * 2
  926: 
  927:     with tm.ensure_clean() as path:
  928:         Path(path).write_bytes(
  929:             content * 2 + b"," + content + b"\n" + content * 2 + b"," + content
  930:         )
  931:         df = parser.read_csv(path, encoding_errors="surrogatepass", index_col=0)
  932:         tm.assert_frame_equal(df, expected)
  933:         with pytest.raises(UnicodeDecodeError, match="'utf-8' codec can't decode byte"):
  934:             parser.read_csv(path)
  935: 
  936: 
  937: def test_malformed_second_line(all_parsers):
  938:     # see GH14782
  939:     parser = all_parsers
  940:     data = "\na\nb\n"
  941:     result = parser.read_csv(StringIO(data), skip_blank_lines=False, header=1)
  942:     expected = DataFrame({"a": ["b"]})
  943:     tm.assert_frame_equal(result, expected)
  944: 
  945: 
  946: @skip_pyarrow
  947: def test_short_single_line(all_parsers):
  948:     # GH 47566
  949:     parser = all_parsers
  950:     columns = ["a", "b", "c"]
  951:     data = "1,2"
  952:     result = parser.read_csv(StringIO(data), header=None, names=columns)
  953:     expected = DataFrame({"a": [1], "b": [2], "c": [np.nan]})
  954:     tm.assert_frame_equal(result, expected)
  955: 
  956: 
  957: @xfail_pyarrow  # ValueError: Length mismatch: Expected axis has 2 elements
  958: def test_short_multi_line(all_parsers):
  959:     # GH 47566
  960:     parser = all_parsers
  961:     columns = ["a", "b", "c"]
  962:     data = "1,2\n1,2"
  963:     result = parser.read_csv(StringIO(data), header=None, names=columns)
  964:     expected = DataFrame({"a": [1, 1], "b": [2, 2], "c": [np.nan, np.nan]})
  965:     tm.assert_frame_equal(result, expected)
  966: 
  967: 
  968: def test_read_seek(all_parsers):
  969:     # GH48646
  970:     parser = all_parsers
  971:     prefix = "### DATA\n"
  972:     content = "nkey,value\ntables,rectangular\n"
  973:     with tm.ensure_clean() as path:
  974:         Path(path).write_text(prefix + content, encoding="utf-8")
  975:         with open(path, encoding="utf-8") as file:
  976:             file.readline()
  977:             actual = parser.read_csv(file)
  978:         expected = parser.read_csv(StringIO(content))
  979:     tm.assert_frame_equal(actual, expected)
