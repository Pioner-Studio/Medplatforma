    1: import numpy as np
    2: import pytest
    3: 
    4: from pandas.compat.pyarrow import pa_version_under10p1
    5: 
    6: from pandas.core.dtypes.missing import na_value_for_dtype
    7: 
    8: import pandas as pd
    9: import pandas._testing as tm
   10: from pandas.tests.groupby import get_groupby_method_args
   11: 
   12: 
   13: @pytest.mark.parametrize(
   14:     "dropna, tuples, outputs",
   15:     [
   16:         (
   17:             True,
   18:             [["A", "B"], ["B", "A"]],
   19:             {"c": [13.0, 123.23], "d": [13.0, 123.0], "e": [13.0, 1.0]},
   20:         ),
   21:         (
   22:             False,
   23:             [["A", "B"], ["A", np.nan], ["B", "A"]],
   24:             {
   25:                 "c": [13.0, 12.3, 123.23],
   26:                 "d": [13.0, 233.0, 123.0],
   27:                 "e": [13.0, 12.0, 1.0],
   28:             },
   29:         ),
   30:     ],
   31: )
   32: def test_groupby_dropna_multi_index_dataframe_nan_in_one_group(
   33:     dropna, tuples, outputs, nulls_fixture
   34: ):
   35:     # GH 3729 this is to test that NA is in one group
   36:     df_list = [
   37:         ["A", "B", 12, 12, 12],
   38:         ["A", nulls_fixture, 12.3, 233.0, 12],
   39:         ["B", "A", 123.23, 123, 1],
   40:         ["A", "B", 1, 1, 1.0],
   41:     ]
   42:     df = pd.DataFrame(df_list, columns=["a", "b", "c", "d", "e"])
   43:     grouped = df.groupby(["a", "b"], dropna=dropna).sum()
   44: 
   45:     mi = pd.MultiIndex.from_tuples(tuples, names=list("ab"))
   46: 
   47:     # Since right now, by default MI will drop NA from levels when we create MI
   48:     # via `from_*`, so we need to add NA for level manually afterwards.
   49:     if not dropna:
   50:         mi = mi.set_levels(["A", "B", np.nan], level="b")
   51:     expected = pd.DataFrame(outputs, index=mi)
   52: 
   53:     tm.assert_frame_equal(grouped, expected)
   54: 
   55: 
   56: @pytest.mark.parametrize(
   57:     "dropna, tuples, outputs",
   58:     [
   59:         (
   60:             True,
   61:             [["A", "B"], ["B", "A"]],
   62:             {"c": [12.0, 123.23], "d": [12.0, 123.0], "e": [12.0, 1.0]},
   63:         ),
   64:         (
   65:             False,
   66:             [["A", "B"], ["A", np.nan], ["B", "A"], [np.nan, "B"]],
   67:             {
   68:                 "c": [12.0, 13.3, 123.23, 1.0],
   69:                 "d": [12.0, 234.0, 123.0, 1.0],
   70:                 "e": [12.0, 13.0, 1.0, 1.0],
   71:             },
   72:         ),
   73:     ],
   74: )
   75: def test_groupby_dropna_multi_index_dataframe_nan_in_two_groups(
   76:     dropna, tuples, outputs, nulls_fixture, nulls_fixture2
   77: ):
   78:     # GH 3729 this is to test that NA in different groups with different representations
   79:     df_list = [
   80:         ["A", "B", 12, 12, 12],
   81:         ["A", nulls_fixture, 12.3, 233.0, 12],
   82:         ["B", "A", 123.23, 123, 1],
   83:         [nulls_fixture2, "B", 1, 1, 1.0],
   84:         ["A", nulls_fixture2, 1, 1, 1.0],
   85:     ]
   86:     df = pd.DataFrame(df_list, columns=["a", "b", "c", "d", "e"])
   87:     grouped = df.groupby(["a", "b"], dropna=dropna).sum()
   88: 
   89:     mi = pd.MultiIndex.from_tuples(tuples, names=list("ab"))
   90: 
   91:     # Since right now, by default MI will drop NA from levels when we create MI
   92:     # via `from_*`, so we need to add NA for level manually afterwards.
   93:     if not dropna:
   94:         mi = mi.set_levels([["A", "B", np.nan], ["A", "B", np.nan]])
   95:     expected = pd.DataFrame(outputs, index=mi)
   96: 
   97:     tm.assert_frame_equal(grouped, expected)
   98: 
   99: 
  100: @pytest.mark.parametrize(
  101:     "dropna, idx, outputs",
  102:     [
  103:         (True, ["A", "B"], {"b": [123.23, 13.0], "c": [123.0, 13.0], "d": [1.0, 13.0]}),
  104:         (
  105:             False,
  106:             ["A", "B", np.nan],
  107:             {
  108:                 "b": [123.23, 13.0, 12.3],
  109:                 "c": [123.0, 13.0, 233.0],
  110:                 "d": [1.0, 13.0, 12.0],
  111:             },
  112:         ),
  113:     ],
  114: )
  115: def test_groupby_dropna_normal_index_dataframe(dropna, idx, outputs):
  116:     # GH 3729
  117:     df_list = [
  118:         ["B", 12, 12, 12],
  119:         [None, 12.3, 233.0, 12],
  120:         ["A", 123.23, 123, 1],
  121:         ["B", 1, 1, 1.0],
  122:     ]
  123:     df = pd.DataFrame(df_list, columns=["a", "b", "c", "d"])
  124:     grouped = df.groupby("a", dropna=dropna).sum()
  125: 
  126:     expected = pd.DataFrame(outputs, index=pd.Index(idx, dtype="object", name="a"))
  127: 
  128:     tm.assert_frame_equal(grouped, expected)
  129: 
  130: 
  131: @pytest.mark.parametrize(
  132:     "dropna, idx, expected",
  133:     [
  134:         (True, ["a", "a", "b", np.nan], pd.Series([3, 3], index=["a", "b"])),
  135:         (
  136:             False,
  137:             ["a", "a", "b", np.nan],
  138:             pd.Series([3, 3, 3], index=["a", "b", np.nan]),
  139:         ),
  140:     ],
  141: )
  142: def test_groupby_dropna_series_level(dropna, idx, expected):
  143:     ser = pd.Series([1, 2, 3, 3], index=idx)
  144: 
  145:     result = ser.groupby(level=0, dropna=dropna).sum()
  146:     tm.assert_series_equal(result, expected)
  147: 
  148: 
  149: @pytest.mark.parametrize(
  150:     "dropna, expected",
  151:     [
  152:         (True, pd.Series([210.0, 350.0], index=["a", "b"], name="Max Speed")),
  153:         (
  154:             False,
  155:             pd.Series([210.0, 350.0, 20.0], index=["a", "b", np.nan], name="Max Speed"),
  156:         ),
  157:     ],
  158: )
  159: def test_groupby_dropna_series_by(dropna, expected):
  160:     ser = pd.Series(
  161:         [390.0, 350.0, 30.0, 20.0],
  162:         index=["Falcon", "Falcon", "Parrot", "Parrot"],
  163:         name="Max Speed",
  164:     )
  165: 
  166:     result = ser.groupby(["a", "b", "a", np.nan], dropna=dropna).mean()
  167:     tm.assert_series_equal(result, expected)
  168: 
  169: 
  170: @pytest.mark.parametrize("dropna", (False, True))
  171: def test_grouper_dropna_propagation(dropna):
  172:     # GH 36604
  173:     df = pd.DataFrame({"A": [0, 0, 1, None], "B": [1, 2, 3, None]})
  174:     gb = df.groupby("A", dropna=dropna)
  175:     assert gb._grouper.dropna == dropna
  176: 
  177: 
  178: @pytest.mark.parametrize(
  179:     "index",
  180:     [
  181:         pd.RangeIndex(0, 4),
  182:         list("abcd"),
  183:         pd.MultiIndex.from_product([(1, 2), ("R", "B")], names=["num", "col"]),
  184:     ],
  185: )
  186: def test_groupby_dataframe_slice_then_transform(dropna, index):
  187:     # GH35014 & GH35612
  188:     expected_data = {"B": [2, 2, 1, np.nan if dropna else 1]}
  189: 
  190:     df = pd.DataFrame({"A": [0, 0, 1, None], "B": [1, 2, 3, None]}, index=index)
  191:     gb = df.groupby("A", dropna=dropna)
  192: 
  193:     result = gb.transform(len)
  194:     expected = pd.DataFrame(expected_data, index=index)
  195:     tm.assert_frame_equal(result, expected)
  196: 
  197:     result = gb[["B"]].transform(len)
  198:     expected = pd.DataFrame(expected_data, index=index)
  199:     tm.assert_frame_equal(result, expected)
  200: 
  201:     result = gb["B"].transform(len)
  202:     expected = pd.Series(expected_data["B"], index=index, name="B")
  203:     tm.assert_series_equal(result, expected)
  204: 
  205: 
  206: @pytest.mark.parametrize(
  207:     "dropna, tuples, outputs",
  208:     [
  209:         (
  210:             True,
  211:             [["A", "B"], ["B", "A"]],
  212:             {"c": [13.0, 123.23], "d": [12.0, 123.0], "e": [1.0, 1.0]},
  213:         ),
  214:         (
  215:             False,
  216:             [["A", "B"], ["A", np.nan], ["B", "A"]],
  217:             {
  218:                 "c": [13.0, 12.3, 123.23],
  219:                 "d": [12.0, 233.0, 123.0],
  220:                 "e": [1.0, 12.0, 1.0],
  221:             },
  222:         ),
  223:     ],
  224: )
  225: def test_groupby_dropna_multi_index_dataframe_agg(dropna, tuples, outputs):
  226:     # GH 3729
  227:     df_list = [
  228:         ["A", "B", 12, 12, 12],
  229:         ["A", None, 12.3, 233.0, 12],
  230:         ["B", "A", 123.23, 123, 1],
  231:         ["A", "B", 1, 1, 1.0],
  232:     ]
  233:     df = pd.DataFrame(df_list, columns=["a", "b", "c", "d", "e"])
  234:     agg_dict = {"c": "sum", "d": "max", "e": "min"}
  235:     grouped = df.groupby(["a", "b"], dropna=dropna).agg(agg_dict)
  236: 
  237:     mi = pd.MultiIndex.from_tuples(tuples, names=list("ab"))
  238: 
  239:     # Since right now, by default MI will drop NA from levels when we create MI
  240:     # via `from_*`, so we need to add NA for level manually afterwards.
  241:     if not dropna:
  242:         mi = mi.set_levels(["A", "B", np.nan], level="b")
  243:     expected = pd.DataFrame(outputs, index=mi)
  244: 
  245:     tm.assert_frame_equal(grouped, expected)
  246: 
  247: 
  248: @pytest.mark.arm_slow
  249: @pytest.mark.parametrize(
  250:     "datetime1, datetime2",
  251:     [
  252:         (pd.Timestamp("2020-01-01"), pd.Timestamp("2020-02-01")),
  253:         (pd.Timedelta("-2 days"), pd.Timedelta("-1 days")),
  254:         (pd.Period("2020-01-01"), pd.Period("2020-02-01")),
  255:     ],
  256: )
  257: @pytest.mark.parametrize("dropna, values", [(True, [12, 3]), (False, [12, 3, 6])])
  258: def test_groupby_dropna_datetime_like_data(
  259:     dropna, values, datetime1, datetime2, unique_nulls_fixture, unique_nulls_fixture2
  260: ):
  261:     # 3729
  262:     df = pd.DataFrame(
  263:         {
  264:             "values": [1, 2, 3, 4, 5, 6],
  265:             "dt": [
  266:                 datetime1,
  267:                 unique_nulls_fixture,
  268:                 datetime2,
  269:                 unique_nulls_fixture2,
  270:                 datetime1,
  271:                 datetime1,
  272:             ],
  273:         }
  274:     )
  275: 
  276:     if dropna:
  277:         indexes = [datetime1, datetime2]
  278:     else:
  279:         indexes = [datetime1, datetime2, np.nan]
  280: 
  281:     grouped = df.groupby("dt", dropna=dropna).agg({"values": "sum"})
  282:     expected = pd.DataFrame({"values": values}, index=pd.Index(indexes, name="dt"))
  283: 
  284:     tm.assert_frame_equal(grouped, expected)
  285: 
  286: 
  287: @pytest.mark.parametrize(
  288:     "dropna, data, selected_data, levels",
  289:     [
  290:         pytest.param(
  291:             False,
  292:             {"groups": ["a", "a", "b", np.nan], "values": [10, 10, 20, 30]},
  293:             {"values": [0, 1, 0, 0]},
  294:             ["a", "b", np.nan],
  295:             id="dropna_false_has_nan",
  296:         ),
  297:         pytest.param(
  298:             True,
  299:             {"groups": ["a", "a", "b", np.nan], "values": [10, 10, 20, 30]},
  300:             {"values": [0, 1, 0]},
  301:             None,
  302:             id="dropna_true_has_nan",
  303:         ),
  304:         pytest.param(
  305:             # no nan in "groups"; dropna=True|False should be same.
  306:             False,
  307:             {"groups": ["a", "a", "b", "c"], "values": [10, 10, 20, 30]},
  308:             {"values": [0, 1, 0, 0]},
  309:             None,
  310:             id="dropna_false_no_nan",
  311:         ),
  312:         pytest.param(
  313:             # no nan in "groups"; dropna=True|False should be same.
  314:             True,
  315:             {"groups": ["a", "a", "b", "c"], "values": [10, 10, 20, 30]},
  316:             {"values": [0, 1, 0, 0]},
  317:             None,
  318:             id="dropna_true_no_nan",
  319:         ),
  320:     ],
  321: )
  322: def test_groupby_apply_with_dropna_for_multi_index(dropna, data, selected_data, levels):
  323:     # GH 35889
  324: 
  325:     df = pd.DataFrame(data)
  326:     gb = df.groupby("groups", dropna=dropna)
  327:     msg = "DataFrameGroupBy.apply operated on the grouping columns"
  328:     with tm.assert_produces_warning(DeprecationWarning, match=msg):
  329:         result = gb.apply(lambda grp: pd.DataFrame({"values": range(len(grp))}))
  330: 
  331:     mi_tuples = tuple(zip(data["groups"], selected_data["values"]))
  332:     mi = pd.MultiIndex.from_tuples(mi_tuples, names=["groups", None])
  333:     # Since right now, by default MI will drop NA from levels when we create MI
  334:     # via `from_*`, so we need to add NA for level manually afterwards.
  335:     if not dropna and levels:
  336:         mi = mi.set_levels(levels, level="groups")
  337: 
  338:     expected = pd.DataFrame(selected_data, index=mi)
  339:     tm.assert_frame_equal(result, expected)
  340: 
  341: 
  342: @pytest.mark.parametrize("input_index", [None, ["a"], ["a", "b"]])
  343: @pytest.mark.parametrize("keys", [["a"], ["a", "b"]])
  344: @pytest.mark.parametrize("series", [True, False])
  345: def test_groupby_dropna_with_multiindex_input(input_index, keys, series):
  346:     # GH#46783
  347:     obj = pd.DataFrame(
  348:         {
  349:             "a": [1, np.nan],
  350:             "b": [1, 1],
  351:             "c": [2, 3],
  352:         }
  353:     )
  354: 
  355:     expected = obj.set_index(keys)
  356:     if series:
  357:         expected = expected["c"]
  358:     elif input_index == ["a", "b"] and keys == ["a"]:
  359:         # Column b should not be aggregated
  360:         expected = expected[["c"]]
  361: 
  362:     if input_index is not None:
  363:         obj = obj.set_index(input_index)
  364:     gb = obj.groupby(keys, dropna=False)
  365:     if series:
  366:         gb = gb["c"]
  367:     result = gb.sum()
  368: 
  369:     tm.assert_equal(result, expected)
  370: 
  371: 
  372: def test_groupby_nan_included():
  373:     # GH 35646
  374:     data = {"group": ["g1", np.nan, "g1", "g2", np.nan], "B": [0, 1, 2, 3, 4]}
  375:     df = pd.DataFrame(data)
  376:     grouped = df.groupby("group", dropna=False)
  377:     result = grouped.indices
  378:     dtype = np.intp
  379:     expected = {
  380:         "g1": np.array([0, 2], dtype=dtype),
  381:         "g2": np.array([3], dtype=dtype),
  382:         np.nan: np.array([1, 4], dtype=dtype),
  383:     }
  384:     for result_values, expected_values in zip(result.values(), expected.values()):
  385:         tm.assert_numpy_array_equal(result_values, expected_values)
  386:     assert np.isnan(list(result.keys())[2])
  387:     assert list(result.keys())[0:2] == ["g1", "g2"]
  388: 
  389: 
  390: def test_groupby_drop_nan_with_multi_index():
  391:     # GH 39895
  392:     df = pd.DataFrame([[np.nan, 0, 1]], columns=["a", "b", "c"])
  393:     df = df.set_index(["a", "b"])
  394:     result = df.groupby(["a", "b"], dropna=False).first()
  395:     expected = df
  396:     tm.assert_frame_equal(result, expected)
  397: 
  398: 
  399: # sequence_index enumerates all strings made up of x, y, z of length 4
  400: @pytest.mark.parametrize("sequence_index", range(3**4))
  401: @pytest.mark.parametrize(
  402:     "dtype",
  403:     [
  404:         None,
  405:         "UInt8",
  406:         "Int8",
  407:         "UInt16",
  408:         "Int16",
  409:         "UInt32",
  410:         "Int32",
  411:         "UInt64",
  412:         "Int64",
  413:         "Float32",
  414:         "Int64",
  415:         "Float64",
  416:         "category",
  417:         "string",
  418:         pytest.param(
  419:             "string[pyarrow]",
  420:             marks=pytest.mark.skipif(
  421:                 pa_version_under10p1, reason="pyarrow is not installed"
  422:             ),
  423:         ),
  424:         "datetime64[ns]",
  425:         "period[d]",
  426:         "Sparse[float]",
  427:     ],
  428: )
  429: @pytest.mark.parametrize("test_series", [True, False])
  430: def test_no_sort_keep_na(sequence_index, dtype, test_series, as_index):
  431:     # GH#46584, GH#48794
  432: 
  433:     # Convert sequence_index into a string sequence, e.g. 5 becomes "xxyz"
  434:     # This sequence is used for the grouper.
  435:     sequence = "".join(
  436:         [{0: "x", 1: "y", 2: "z"}[sequence_index // (3**k) % 3] for k in range(4)]
  437:     )
  438: 
  439:     # Unique values to use for grouper, depends on dtype
  440:     if dtype in ("string", "string[pyarrow]"):
  441:         uniques = {"x": "x", "y": "y", "z": pd.NA}
  442:     elif dtype in ("datetime64[ns]", "period[d]"):
  443:         uniques = {"x": "2016-01-01", "y": "2017-01-01", "z": pd.NA}
  444:     else:
  445:         uniques = {"x": 1, "y": 2, "z": np.nan}
  446: 
  447:     df = pd.DataFrame(
  448:         {
  449:             "key": pd.Series([uniques[label] for label in sequence], dtype=dtype),
  450:             "a": [0, 1, 2, 3],
  451:         }
  452:     )
  453:     gb = df.groupby("key", dropna=False, sort=False, as_index=as_index, observed=False)
  454:     if test_series:
  455:         gb = gb["a"]
  456:     result = gb.sum()
  457: 
  458:     # Manually compute the groupby sum, use the labels "x", "y", and "z" to avoid
  459:     # issues with hashing np.nan
  460:     summed = {}
  461:     for idx, label in enumerate(sequence):
  462:         summed[label] = summed.get(label, 0) + idx
  463:     if dtype == "category":
  464:         index = pd.CategoricalIndex(
  465:             [uniques[e] for e in summed],
  466:             df["key"].cat.categories,
  467:             name="key",
  468:         )
  469:     elif isinstance(dtype, str) and dtype.startswith("Sparse"):
  470:         index = pd.Index(
  471:             pd.array([uniques[label] for label in summed], dtype=dtype), name="key"
  472:         )
  473:     else:
  474:         index = pd.Index([uniques[label] for label in summed], dtype=dtype, name="key")
  475:     expected = pd.Series(summed.values(), index=index, name="a", dtype=None)
  476:     if not test_series:
  477:         expected = expected.to_frame()
  478:     if not as_index:
  479:         expected = expected.reset_index()
  480:         if dtype is not None and dtype.startswith("Sparse"):
  481:             expected["key"] = expected["key"].astype(dtype)
  482: 
  483:     tm.assert_equal(result, expected)
  484: 
  485: 
  486: @pytest.mark.parametrize("test_series", [True, False])
  487: @pytest.mark.parametrize("dtype", [object, None])
  488: def test_null_is_null_for_dtype(
  489:     sort, dtype, nulls_fixture, nulls_fixture2, test_series
  490: ):
  491:     # GH#48506 - groups should always result in using the null for the dtype
  492:     df = pd.DataFrame({"a": [1, 2]})
  493:     groups = pd.Series([nulls_fixture, nulls_fixture2], dtype=dtype)
  494:     obj = df["a"] if test_series else df
  495:     gb = obj.groupby(groups, dropna=False, sort=sort)
  496:     result = gb.sum()
  497:     index = pd.Index([na_value_for_dtype(groups.dtype)])
  498:     expected = pd.DataFrame({"a": [3]}, index=index)
  499:     if test_series:
  500:         tm.assert_series_equal(result, expected["a"])
  501:     else:
  502:         tm.assert_frame_equal(result, expected)
  503: 
  504: 
  505: @pytest.mark.parametrize("index_kind", ["range", "single", "multi"])
  506: def test_categorical_reducers(reduction_func, observed, sort, as_index, index_kind):
  507:     # Ensure there is at least one null value by appending to the end
  508:     values = np.append(np.random.default_rng(2).choice([1, 2, None], size=19), None)
  509:     df = pd.DataFrame(
  510:         {"x": pd.Categorical(values, categories=[1, 2, 3]), "y": range(20)}
  511:     )
  512: 
  513:     # Strategy: Compare to dropna=True by filling null values with a new code
  514:     df_filled = df.copy()
  515:     df_filled["x"] = pd.Categorical(values, categories=[1, 2, 3, 4]).fillna(4)
  516: 
  517:     if index_kind == "range":
  518:         keys = ["x"]
  519:     elif index_kind == "single":
  520:         keys = ["x"]
  521:         df = df.set_index("x")
  522:         df_filled = df_filled.set_index("x")
  523:     else:
  524:         keys = ["x", "x2"]
  525:         df["x2"] = df["x"]
  526:         df = df.set_index(["x", "x2"])
  527:         df_filled["x2"] = df_filled["x"]
  528:         df_filled = df_filled.set_index(["x", "x2"])
  529:     args = get_groupby_method_args(reduction_func, df)
  530:     args_filled = get_groupby_method_args(reduction_func, df_filled)
  531:     if reduction_func == "corrwith" and index_kind == "range":
  532:         # Don't include the grouping columns so we can call reset_index
  533:         args = (args[0].drop(columns=keys),)
  534:         args_filled = (args_filled[0].drop(columns=keys),)
  535: 
  536:     gb_keepna = df.groupby(
  537:         keys, dropna=False, observed=observed, sort=sort, as_index=as_index
  538:     )
  539: 
  540:     if not observed and reduction_func in ["idxmin", "idxmax"]:
  541:         with pytest.raises(
  542:             ValueError, match="empty group due to unobserved categories"
  543:         ):
  544:             getattr(gb_keepna, reduction_func)(*args)
  545:         return
  546: 
  547:     gb_filled = df_filled.groupby(keys, observed=observed, sort=sort, as_index=True)
  548:     expected = getattr(gb_filled, reduction_func)(*args_filled).reset_index()
  549:     expected["x"] = expected["x"].cat.remove_categories([4])
  550:     if index_kind == "multi":
  551:         expected["x2"] = expected["x2"].cat.remove_categories([4])
  552:     if as_index:
  553:         if index_kind == "multi":
  554:             expected = expected.set_index(["x", "x2"])
  555:         else:
  556:             expected = expected.set_index("x")
  557:     elif index_kind != "range" and reduction_func != "size":
  558:         # size, unlike other methods, has the desired behavior in GH#49519
  559:         expected = expected.drop(columns="x")
  560:         if index_kind == "multi":
  561:             expected = expected.drop(columns="x2")
  562:     if reduction_func in ("idxmax", "idxmin") and index_kind != "range":
  563:         # expected was computed with a RangeIndex; need to translate to index values
  564:         values = expected["y"].values.tolist()
  565:         if index_kind == "single":
  566:             values = [np.nan if e == 4 else e for e in values]
  567:             expected["y"] = pd.Categorical(values, categories=[1, 2, 3])
  568:         else:
  569:             values = [(np.nan, np.nan) if e == (4, 4) else e for e in values]
  570:             expected["y"] = values
  571:     if reduction_func == "size":
  572:         # size, unlike other methods, has the desired behavior in GH#49519
  573:         expected = expected.rename(columns={0: "size"})
  574:         if as_index:
  575:             expected = expected["size"].rename(None)
  576: 
  577:     if as_index or index_kind == "range" or reduction_func == "size":
  578:         warn = None
  579:     else:
  580:         warn = FutureWarning
  581:     msg = "A grouping .* was excluded from the result"
  582:     with tm.assert_produces_warning(warn, match=msg):
  583:         result = getattr(gb_keepna, reduction_func)(*args)
  584: 
  585:     # size will return a Series, others are DataFrame
  586:     tm.assert_equal(result, expected)
  587: 
  588: 
  589: def test_categorical_transformers(
  590:     request, transformation_func, observed, sort, as_index
  591: ):
  592:     # GH#36327
  593:     if transformation_func == "fillna":
  594:         msg = "GH#49651 fillna may incorrectly reorders results when dropna=False"
  595:         request.applymarker(pytest.mark.xfail(reason=msg, strict=False))
  596: 
  597:     values = np.append(np.random.default_rng(2).choice([1, 2, None], size=19), None)
  598:     df = pd.DataFrame(
  599:         {"x": pd.Categorical(values, categories=[1, 2, 3]), "y": range(20)}
  600:     )
  601:     args = get_groupby_method_args(transformation_func, df)
  602: 
  603:     # Compute result for null group
  604:     null_group_values = df[df["x"].isnull()]["y"]
  605:     if transformation_func == "cumcount":
  606:         null_group_data = list(range(len(null_group_values)))
  607:     elif transformation_func == "ngroup":
  608:         if sort:
  609:             if observed:
  610:                 na_group = df["x"].nunique(dropna=False) - 1
  611:             else:
  612:                 # TODO: Should this be 3?
  613:                 na_group = df["x"].nunique(dropna=False) - 1
  614:         else:
  615:             na_group = df.iloc[: null_group_values.index[0]]["x"].nunique()
  616:         null_group_data = len(null_group_values) * [na_group]
  617:     else:
  618:         null_group_data = getattr(null_group_values, transformation_func)(*args)
  619:     null_group_result = pd.DataFrame({"y": null_group_data})
  620: 
  621:     gb_keepna = df.groupby(
  622:         "x", dropna=False, observed=observed, sort=sort, as_index=as_index
  623:     )
  624:     gb_dropna = df.groupby("x", dropna=True, observed=observed, sort=sort)
  625: 
  626:     msg = "The default fill_method='ffill' in DataFrameGroupBy.pct_change is deprecated"
  627:     if transformation_func == "pct_change":
  628:         with tm.assert_produces_warning(FutureWarning, match=msg):
  629:             result = getattr(gb_keepna, "pct_change")(*args)
  630:     else:
  631:         result = getattr(gb_keepna, transformation_func)(*args)
  632:     expected = getattr(gb_dropna, transformation_func)(*args)
  633: 
  634:     for iloc, value in zip(
  635:         df[df["x"].isnull()].index.tolist(), null_group_result.values.ravel()
  636:     ):
  637:         if expected.ndim == 1:
  638:             expected.iloc[iloc] = value
  639:         else:
  640:             expected.iloc[iloc, 0] = value
  641:     if transformation_func == "ngroup":
  642:         expected[df["x"].notnull() & expected.ge(na_group)] += 1
  643:     if transformation_func not in ("rank", "diff", "pct_change", "shift"):
  644:         expected = expected.astype("int64")
  645: 
  646:     tm.assert_equal(result, expected)
  647: 
  648: 
  649: @pytest.mark.parametrize("method", ["head", "tail"])
  650: def test_categorical_head_tail(method, observed, sort, as_index):
  651:     # GH#36327
  652:     values = np.random.default_rng(2).choice([1, 2, None], 30)
  653:     df = pd.DataFrame(
  654:         {"x": pd.Categorical(values, categories=[1, 2, 3]), "y": range(len(values))}
  655:     )
  656:     gb = df.groupby("x", dropna=False, observed=observed, sort=sort, as_index=as_index)
  657:     result = getattr(gb, method)()
  658: 
  659:     if method == "tail":
  660:         values = values[::-1]
  661:     # Take the top 5 values from each group
  662:     mask = (
  663:         ((values == 1) & ((values == 1).cumsum() <= 5))
  664:         | ((values == 2) & ((values == 2).cumsum() <= 5))
  665:         # flake8 doesn't like the vectorized check for None, thinks we should use `is`
  666:         | ((values == None) & ((values == None).cumsum() <= 5))  # noqa: E711
  667:     )
  668:     if method == "tail":
  669:         mask = mask[::-1]
  670:     expected = df[mask]
  671: 
  672:     tm.assert_frame_equal(result, expected)
  673: 
  674: 
  675: def test_categorical_agg():
  676:     # GH#36327
  677:     values = np.random.default_rng(2).choice([1, 2, None], 30)
  678:     df = pd.DataFrame(
  679:         {"x": pd.Categorical(values, categories=[1, 2, 3]), "y": range(len(values))}
  680:     )
  681:     gb = df.groupby("x", dropna=False, observed=False)
  682:     result = gb.agg(lambda x: x.sum())
  683:     expected = gb.sum()
  684:     tm.assert_frame_equal(result, expected)
  685: 
  686: 
  687: def test_categorical_transform():
  688:     # GH#36327
  689:     values = np.random.default_rng(2).choice([1, 2, None], 30)
  690:     df = pd.DataFrame(
  691:         {"x": pd.Categorical(values, categories=[1, 2, 3]), "y": range(len(values))}
  692:     )
  693:     gb = df.groupby("x", dropna=False, observed=False)
  694:     result = gb.transform(lambda x: x.sum())
  695:     expected = gb.transform("sum")
  696:     tm.assert_frame_equal(result, expected)
