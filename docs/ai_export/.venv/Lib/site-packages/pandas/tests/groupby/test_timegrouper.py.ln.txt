    1: """
    2: test with the TimeGrouper / grouping with datetimes
    3: """
    4: from datetime import (
    5:     datetime,
    6:     timedelta,
    7: )
    8: 
    9: import numpy as np
   10: import pytest
   11: import pytz
   12: 
   13: import pandas as pd
   14: from pandas import (
   15:     DataFrame,
   16:     DatetimeIndex,
   17:     Index,
   18:     MultiIndex,
   19:     Series,
   20:     Timestamp,
   21:     date_range,
   22:     offsets,
   23: )
   24: import pandas._testing as tm
   25: from pandas.core.groupby.grouper import Grouper
   26: from pandas.core.groupby.ops import BinGrouper
   27: 
   28: 
   29: @pytest.fixture
   30: def frame_for_truncated_bingrouper():
   31:     """
   32:     DataFrame used by groupby_with_truncated_bingrouper, made into
   33:     a separate fixture for easier reuse in
   34:     test_groupby_apply_timegrouper_with_nat_apply_squeeze
   35:     """
   36:     df = DataFrame(
   37:         {
   38:             "Quantity": [18, 3, 5, 1, 9, 3],
   39:             "Date": [
   40:                 Timestamp(2013, 9, 1, 13, 0),
   41:                 Timestamp(2013, 9, 1, 13, 5),
   42:                 Timestamp(2013, 10, 1, 20, 0),
   43:                 Timestamp(2013, 10, 3, 10, 0),
   44:                 pd.NaT,
   45:                 Timestamp(2013, 9, 2, 14, 0),
   46:             ],
   47:         }
   48:     )
   49:     return df
   50: 
   51: 
   52: @pytest.fixture
   53: def groupby_with_truncated_bingrouper(frame_for_truncated_bingrouper):
   54:     """
   55:     GroupBy object such that gb._grouper is a BinGrouper and
   56:     len(gb._grouper.result_index) < len(gb._grouper.group_keys_seq)
   57: 
   58:     Aggregations on this groupby should have
   59: 
   60:         dti = date_range("2013-09-01", "2013-10-01", freq="5D", name="Date")
   61: 
   62:     As either the index or an index level.
   63:     """
   64:     df = frame_for_truncated_bingrouper
   65: 
   66:     tdg = Grouper(key="Date", freq="5D")
   67:     gb = df.groupby(tdg)
   68: 
   69:     # check we're testing the case we're interested in
   70:     assert len(gb._grouper.result_index) != len(gb._grouper.group_keys_seq)
   71: 
   72:     return gb
   73: 
   74: 
   75: class TestGroupBy:
   76:     def test_groupby_with_timegrouper(self):
   77:         # GH 4161
   78:         # TimeGrouper requires a sorted index
   79:         # also verifies that the resultant index has the correct name
   80:         df_original = DataFrame(
   81:             {
   82:                 "Buyer": "Carl Carl Carl Carl Joe Carl".split(),
   83:                 "Quantity": [18, 3, 5, 1, 9, 3],
   84:                 "Date": [
   85:                     datetime(2013, 9, 1, 13, 0),
   86:                     datetime(2013, 9, 1, 13, 5),
   87:                     datetime(2013, 10, 1, 20, 0),
   88:                     datetime(2013, 10, 3, 10, 0),
   89:                     datetime(2013, 12, 2, 12, 0),
   90:                     datetime(2013, 9, 2, 14, 0),
   91:                 ],
   92:             }
   93:         )
   94: 
   95:         # GH 6908 change target column's order
   96:         df_reordered = df_original.sort_values(by="Quantity")
   97: 
   98:         for df in [df_original, df_reordered]:
   99:             df = df.set_index(["Date"])
  100: 
  101:             exp_dti = date_range(
  102:                 "20130901",
  103:                 "20131205",
  104:                 freq="5D",
  105:                 name="Date",
  106:                 inclusive="left",
  107:                 unit=df.index.unit,
  108:             )
  109:             expected = DataFrame(
  110:                 {"Buyer": 0, "Quantity": 0},
  111:                 index=exp_dti,
  112:             )
  113:             # Cast to object to avoid implicit cast when setting entry to "CarlCarlCarl"
  114:             expected = expected.astype({"Buyer": object})
  115:             expected.iloc[0, 0] = "CarlCarlCarl"
  116:             expected.iloc[6, 0] = "CarlCarl"
  117:             expected.iloc[18, 0] = "Joe"
  118:             expected.iloc[[0, 6, 18], 1] = np.array([24, 6, 9], dtype="int64")
  119: 
  120:             result1 = df.resample("5D").sum()
  121:             tm.assert_frame_equal(result1, expected)
  122: 
  123:             df_sorted = df.sort_index()
  124:             result2 = df_sorted.groupby(Grouper(freq="5D")).sum()
  125:             tm.assert_frame_equal(result2, expected)
  126: 
  127:             result3 = df.groupby(Grouper(freq="5D")).sum()
  128:             tm.assert_frame_equal(result3, expected)
  129: 
  130:     @pytest.mark.parametrize("should_sort", [True, False])
  131:     def test_groupby_with_timegrouper_methods(self, should_sort):
  132:         # GH 3881
  133:         # make sure API of timegrouper conforms
  134: 
  135:         df = DataFrame(
  136:             {
  137:                 "Branch": "A A A A A B".split(),
  138:                 "Buyer": "Carl Mark Carl Joe Joe Carl".split(),
  139:                 "Quantity": [1, 3, 5, 8, 9, 3],
  140:                 "Date": [
  141:                     datetime(2013, 1, 1, 13, 0),
  142:                     datetime(2013, 1, 1, 13, 5),
  143:                     datetime(2013, 10, 1, 20, 0),
  144:                     datetime(2013, 10, 2, 10, 0),
  145:                     datetime(2013, 12, 2, 12, 0),
  146:                     datetime(2013, 12, 2, 14, 0),
  147:                 ],
  148:             }
  149:         )
  150: 
  151:         if should_sort:
  152:             df = df.sort_values(by="Quantity", ascending=False)
  153: 
  154:         df = df.set_index("Date", drop=False)
  155:         g = df.groupby(Grouper(freq="6ME"))
  156:         assert g.group_keys
  157: 
  158:         assert isinstance(g._grouper, BinGrouper)
  159:         groups = g.groups
  160:         assert isinstance(groups, dict)
  161:         assert len(groups) == 3
  162: 
  163:     def test_timegrouper_with_reg_groups(self):
  164:         # GH 3794
  165:         # allow combination of timegrouper/reg groups
  166: 
  167:         df_original = DataFrame(
  168:             {
  169:                 "Branch": "A A A A A A A B".split(),
  170:                 "Buyer": "Carl Mark Carl Carl Joe Joe Joe Carl".split(),
  171:                 "Quantity": [1, 3, 5, 1, 8, 1, 9, 3],
  172:                 "Date": [
  173:                     datetime(2013, 1, 1, 13, 0),
  174:                     datetime(2013, 1, 1, 13, 5),
  175:                     datetime(2013, 10, 1, 20, 0),
  176:                     datetime(2013, 10, 2, 10, 0),
  177:                     datetime(2013, 10, 1, 20, 0),
  178:                     datetime(2013, 10, 2, 10, 0),
  179:                     datetime(2013, 12, 2, 12, 0),
  180:                     datetime(2013, 12, 2, 14, 0),
  181:                 ],
  182:             }
  183:         ).set_index("Date")
  184: 
  185:         df_sorted = df_original.sort_values(by="Quantity", ascending=False)
  186: 
  187:         for df in [df_original, df_sorted]:
  188:             expected = DataFrame(
  189:                 {
  190:                     "Buyer": "Carl Joe Mark".split(),
  191:                     "Quantity": [10, 18, 3],
  192:                     "Date": [
  193:                         datetime(2013, 12, 31, 0, 0),
  194:                         datetime(2013, 12, 31, 0, 0),
  195:                         datetime(2013, 12, 31, 0, 0),
  196:                     ],
  197:                 }
  198:             ).set_index(["Date", "Buyer"])
  199: 
  200:             msg = "The default value of numeric_only"
  201:             result = df.groupby([Grouper(freq="YE"), "Buyer"]).sum(numeric_only=True)
  202:             tm.assert_frame_equal(result, expected)
  203: 
  204:             expected = DataFrame(
  205:                 {
  206:                     "Buyer": "Carl Mark Carl Joe".split(),
  207:                     "Quantity": [1, 3, 9, 18],
  208:                     "Date": [
  209:                         datetime(2013, 1, 1, 0, 0),
  210:                         datetime(2013, 1, 1, 0, 0),
  211:                         datetime(2013, 7, 1, 0, 0),
  212:                         datetime(2013, 7, 1, 0, 0),
  213:                     ],
  214:                 }
  215:             ).set_index(["Date", "Buyer"])
  216:             result = df.groupby([Grouper(freq="6MS"), "Buyer"]).sum(numeric_only=True)
  217:             tm.assert_frame_equal(result, expected)
  218: 
  219:         df_original = DataFrame(
  220:             {
  221:                 "Branch": "A A A A A A A B".split(),
  222:                 "Buyer": "Carl Mark Carl Carl Joe Joe Joe Carl".split(),
  223:                 "Quantity": [1, 3, 5, 1, 8, 1, 9, 3],
  224:                 "Date": [
  225:                     datetime(2013, 10, 1, 13, 0),
  226:                     datetime(2013, 10, 1, 13, 5),
  227:                     datetime(2013, 10, 1, 20, 0),
  228:                     datetime(2013, 10, 2, 10, 0),
  229:                     datetime(2013, 10, 1, 20, 0),
  230:                     datetime(2013, 10, 2, 10, 0),
  231:                     datetime(2013, 10, 2, 12, 0),
  232:                     datetime(2013, 10, 2, 14, 0),
  233:                 ],
  234:             }
  235:         ).set_index("Date")
  236: 
  237:         df_sorted = df_original.sort_values(by="Quantity", ascending=False)
  238:         for df in [df_original, df_sorted]:
  239:             expected = DataFrame(
  240:                 {
  241:                     "Buyer": "Carl Joe Mark Carl Joe".split(),
  242:                     "Quantity": [6, 8, 3, 4, 10],
  243:                     "Date": [
  244:                         datetime(2013, 10, 1, 0, 0),
  245:                         datetime(2013, 10, 1, 0, 0),
  246:                         datetime(2013, 10, 1, 0, 0),
  247:                         datetime(2013, 10, 2, 0, 0),
  248:                         datetime(2013, 10, 2, 0, 0),
  249:                     ],
  250:                 }
  251:             ).set_index(["Date", "Buyer"])
  252: 
  253:             result = df.groupby([Grouper(freq="1D"), "Buyer"]).sum(numeric_only=True)
  254:             tm.assert_frame_equal(result, expected)
  255: 
  256:             result = df.groupby([Grouper(freq="1ME"), "Buyer"]).sum(numeric_only=True)
  257:             expected = DataFrame(
  258:                 {
  259:                     "Buyer": "Carl Joe Mark".split(),
  260:                     "Quantity": [10, 18, 3],
  261:                     "Date": [
  262:                         datetime(2013, 10, 31, 0, 0),
  263:                         datetime(2013, 10, 31, 0, 0),
  264:                         datetime(2013, 10, 31, 0, 0),
  265:                     ],
  266:                 }
  267:             ).set_index(["Date", "Buyer"])
  268:             tm.assert_frame_equal(result, expected)
  269: 
  270:             # passing the name
  271:             df = df.reset_index()
  272:             result = df.groupby([Grouper(freq="1ME", key="Date"), "Buyer"]).sum(
  273:                 numeric_only=True
  274:             )
  275:             tm.assert_frame_equal(result, expected)
  276: 
  277:             with pytest.raises(KeyError, match="'The grouper name foo is not found'"):
  278:                 df.groupby([Grouper(freq="1ME", key="foo"), "Buyer"]).sum()
  279: 
  280:             # passing the level
  281:             df = df.set_index("Date")
  282:             result = df.groupby([Grouper(freq="1ME", level="Date"), "Buyer"]).sum(
  283:                 numeric_only=True
  284:             )
  285:             tm.assert_frame_equal(result, expected)
  286:             result = df.groupby([Grouper(freq="1ME", level=0), "Buyer"]).sum(
  287:                 numeric_only=True
  288:             )
  289:             tm.assert_frame_equal(result, expected)
  290: 
  291:             with pytest.raises(ValueError, match="The level foo is not valid"):
  292:                 df.groupby([Grouper(freq="1ME", level="foo"), "Buyer"]).sum()
  293: 
  294:             # multi names
  295:             df = df.copy()
  296:             df["Date"] = df.index + offsets.MonthEnd(2)
  297:             result = df.groupby([Grouper(freq="1ME", key="Date"), "Buyer"]).sum(
  298:                 numeric_only=True
  299:             )
  300:             expected = DataFrame(
  301:                 {
  302:                     "Buyer": "Carl Joe Mark".split(),
  303:                     "Quantity": [10, 18, 3],
  304:                     "Date": [
  305:                         datetime(2013, 11, 30, 0, 0),
  306:                         datetime(2013, 11, 30, 0, 0),
  307:                         datetime(2013, 11, 30, 0, 0),
  308:                     ],
  309:                 }
  310:             ).set_index(["Date", "Buyer"])
  311:             tm.assert_frame_equal(result, expected)
  312: 
  313:             # error as we have both a level and a name!
  314:             msg = "The Grouper cannot specify both a key and a level!"
  315:             with pytest.raises(ValueError, match=msg):
  316:                 df.groupby(
  317:                     [Grouper(freq="1ME", key="Date", level="Date"), "Buyer"]
  318:                 ).sum()
  319: 
  320:             # single groupers
  321:             expected = DataFrame(
  322:                 [[31]],
  323:                 columns=["Quantity"],
  324:                 index=DatetimeIndex(
  325:                     [datetime(2013, 10, 31, 0, 0)], freq=offsets.MonthEnd(), name="Date"
  326:                 ),
  327:             )
  328:             result = df.groupby(Grouper(freq="1ME")).sum(numeric_only=True)
  329:             tm.assert_frame_equal(result, expected)
  330: 
  331:             result = df.groupby([Grouper(freq="1ME")]).sum(numeric_only=True)
  332:             tm.assert_frame_equal(result, expected)
  333: 
  334:             expected.index = expected.index.shift(1)
  335:             assert expected.index.freq == offsets.MonthEnd()
  336:             result = df.groupby(Grouper(freq="1ME", key="Date")).sum(numeric_only=True)
  337:             tm.assert_frame_equal(result, expected)
  338: 
  339:             result = df.groupby([Grouper(freq="1ME", key="Date")]).sum(
  340:                 numeric_only=True
  341:             )
  342:             tm.assert_frame_equal(result, expected)
  343: 
  344:     @pytest.mark.parametrize("freq", ["D", "ME", "YE", "QE-APR"])
  345:     def test_timegrouper_with_reg_groups_freq(self, freq):
  346:         # GH 6764 multiple grouping with/without sort
  347:         df = DataFrame(
  348:             {
  349:                 "date": pd.to_datetime(
  350:                     [
  351:                         "20121002",
  352:                         "20121007",
  353:                         "20130130",
  354:                         "20130202",
  355:                         "20130305",
  356:                         "20121002",
  357:                         "20121207",
  358:                         "20130130",
  359:                         "20130202",
  360:                         "20130305",
  361:                         "20130202",
  362:                         "20130305",
  363:                     ]
  364:                 ),
  365:                 "user_id": [1, 1, 1, 1, 1, 3, 3, 3, 5, 5, 5, 5],
  366:                 "whole_cost": [
  367:                     1790,
  368:                     364,
  369:                     280,
  370:                     259,
  371:                     201,
  372:                     623,
  373:                     90,
  374:                     312,
  375:                     359,
  376:                     301,
  377:                     359,
  378:                     801,
  379:                 ],
  380:                 "cost1": [12, 15, 10, 24, 39, 1, 0, 90, 45, 34, 1, 12],
  381:             }
  382:         ).set_index("date")
  383: 
  384:         expected = (
  385:             df.groupby("user_id")["whole_cost"]
  386:             .resample(freq)
  387:             .sum(min_count=1)  # XXX
  388:             .dropna()
  389:             .reorder_levels(["date", "user_id"])
  390:             .sort_index()
  391:             .astype("int64")
  392:         )
  393:         expected.name = "whole_cost"
  394: 
  395:         result1 = (
  396:             df.sort_index().groupby([Grouper(freq=freq), "user_id"])["whole_cost"].sum()
  397:         )
  398:         tm.assert_series_equal(result1, expected)
  399: 
  400:         result2 = df.groupby([Grouper(freq=freq), "user_id"])["whole_cost"].sum()
  401:         tm.assert_series_equal(result2, expected)
  402: 
  403:     def test_timegrouper_get_group(self):
  404:         # GH 6914
  405: 
  406:         df_original = DataFrame(
  407:             {
  408:                 "Buyer": "Carl Joe Joe Carl Joe Carl".split(),
  409:                 "Quantity": [18, 3, 5, 1, 9, 3],
  410:                 "Date": [
  411:                     datetime(2013, 9, 1, 13, 0),
  412:                     datetime(2013, 9, 1, 13, 5),
  413:                     datetime(2013, 10, 1, 20, 0),
  414:                     datetime(2013, 10, 3, 10, 0),
  415:                     datetime(2013, 12, 2, 12, 0),
  416:                     datetime(2013, 9, 2, 14, 0),
  417:                 ],
  418:             }
  419:         )
  420:         df_reordered = df_original.sort_values(by="Quantity")
  421: 
  422:         # single grouping
  423:         expected_list = [
  424:             df_original.iloc[[0, 1, 5]],
  425:             df_original.iloc[[2, 3]],
  426:             df_original.iloc[[4]],
  427:         ]
  428:         dt_list = ["2013-09-30", "2013-10-31", "2013-12-31"]
  429: 
  430:         for df in [df_original, df_reordered]:
  431:             grouped = df.groupby(Grouper(freq="ME", key="Date"))
  432:             for t, expected in zip(dt_list, expected_list):
  433:                 dt = Timestamp(t)
  434:                 result = grouped.get_group(dt)
  435:                 tm.assert_frame_equal(result, expected)
  436: 
  437:         # multiple grouping
  438:         expected_list = [
  439:             df_original.iloc[[1]],
  440:             df_original.iloc[[3]],
  441:             df_original.iloc[[4]],
  442:         ]
  443:         g_list = [("Joe", "2013-09-30"), ("Carl", "2013-10-31"), ("Joe", "2013-12-31")]
  444: 
  445:         for df in [df_original, df_reordered]:
  446:             grouped = df.groupby(["Buyer", Grouper(freq="ME", key="Date")])
  447:             for (b, t), expected in zip(g_list, expected_list):
  448:                 dt = Timestamp(t)
  449:                 result = grouped.get_group((b, dt))
  450:                 tm.assert_frame_equal(result, expected)
  451: 
  452:         # with index
  453:         df_original = df_original.set_index("Date")
  454:         df_reordered = df_original.sort_values(by="Quantity")
  455: 
  456:         expected_list = [
  457:             df_original.iloc[[0, 1, 5]],
  458:             df_original.iloc[[2, 3]],
  459:             df_original.iloc[[4]],
  460:         ]
  461: 
  462:         for df in [df_original, df_reordered]:
  463:             grouped = df.groupby(Grouper(freq="ME"))
  464:             for t, expected in zip(dt_list, expected_list):
  465:                 dt = Timestamp(t)
  466:                 result = grouped.get_group(dt)
  467:                 tm.assert_frame_equal(result, expected)
  468: 
  469:     def test_timegrouper_apply_return_type_series(self):
  470:         # Using `apply` with the `TimeGrouper` should give the
  471:         # same return type as an `apply` with a `Grouper`.
  472:         # Issue #11742
  473:         df = DataFrame({"date": ["10/10/2000", "11/10/2000"], "value": [10, 13]})
  474:         df_dt = df.copy()
  475:         df_dt["date"] = pd.to_datetime(df_dt["date"])
  476: 
  477:         def sumfunc_series(x):
  478:             return Series([x["value"].sum()], ("sum",))
  479: 
  480:         msg = "DataFrameGroupBy.apply operated on the grouping columns"
  481:         with tm.assert_produces_warning(DeprecationWarning, match=msg):
  482:             expected = df.groupby(Grouper(key="date")).apply(sumfunc_series)
  483:         msg = "DataFrameGroupBy.apply operated on the grouping columns"
  484:         with tm.assert_produces_warning(DeprecationWarning, match=msg):
  485:             result = df_dt.groupby(Grouper(freq="ME", key="date")).apply(sumfunc_series)
  486:         tm.assert_frame_equal(
  487:             result.reset_index(drop=True), expected.reset_index(drop=True)
  488:         )
  489: 
  490:     def test_timegrouper_apply_return_type_value(self):
  491:         # Using `apply` with the `TimeGrouper` should give the
  492:         # same return type as an `apply` with a `Grouper`.
  493:         # Issue #11742
  494:         df = DataFrame({"date": ["10/10/2000", "11/10/2000"], "value": [10, 13]})
  495:         df_dt = df.copy()
  496:         df_dt["date"] = pd.to_datetime(df_dt["date"])
  497: 
  498:         def sumfunc_value(x):
  499:             return x.value.sum()
  500: 
  501:         msg = "DataFrameGroupBy.apply operated on the grouping columns"
  502:         with tm.assert_produces_warning(DeprecationWarning, match=msg):
  503:             expected = df.groupby(Grouper(key="date")).apply(sumfunc_value)
  504:         with tm.assert_produces_warning(DeprecationWarning, match=msg):
  505:             result = df_dt.groupby(Grouper(freq="ME", key="date")).apply(sumfunc_value)
  506:         tm.assert_series_equal(
  507:             result.reset_index(drop=True), expected.reset_index(drop=True)
  508:         )
  509: 
  510:     def test_groupby_groups_datetimeindex(self):
  511:         # GH#1430
  512:         periods = 1000
  513:         ind = date_range(start="2012/1/1", freq="5min", periods=periods)
  514:         df = DataFrame(
  515:             {"high": np.arange(periods), "low": np.arange(periods)}, index=ind
  516:         )
  517:         grouped = df.groupby(lambda x: datetime(x.year, x.month, x.day))
  518: 
  519:         # it works!
  520:         groups = grouped.groups
  521:         assert isinstance(next(iter(groups.keys())), datetime)
  522: 
  523:     def test_groupby_groups_datetimeindex2(self):
  524:         # GH#11442
  525:         index = date_range("2015/01/01", periods=5, name="date")
  526:         df = DataFrame({"A": [5, 6, 7, 8, 9], "B": [1, 2, 3, 4, 5]}, index=index)
  527:         result = df.groupby(level="date").groups
  528:         dates = ["2015-01-05", "2015-01-04", "2015-01-03", "2015-01-02", "2015-01-01"]
  529:         expected = {
  530:             Timestamp(date): DatetimeIndex([date], name="date") for date in dates
  531:         }
  532:         tm.assert_dict_equal(result, expected)
  533: 
  534:         grouped = df.groupby(level="date")
  535:         for date in dates:
  536:             result = grouped.get_group(date)
  537:             data = [[df.loc[date, "A"], df.loc[date, "B"]]]
  538:             expected_index = DatetimeIndex(
  539:                 [date], name="date", freq="D", dtype=index.dtype
  540:             )
  541:             expected = DataFrame(data, columns=list("AB"), index=expected_index)
  542:             tm.assert_frame_equal(result, expected)
  543: 
  544:     def test_groupby_groups_datetimeindex_tz(self):
  545:         # GH 3950
  546:         dates = [
  547:             "2011-07-19 07:00:00",
  548:             "2011-07-19 08:00:00",
  549:             "2011-07-19 09:00:00",
  550:             "2011-07-19 07:00:00",
  551:             "2011-07-19 08:00:00",
  552:             "2011-07-19 09:00:00",
  553:         ]
  554:         df = DataFrame(
  555:             {
  556:                 "label": ["a", "a", "a", "b", "b", "b"],
  557:                 "datetime": dates,
  558:                 "value1": np.arange(6, dtype="int64"),
  559:                 "value2": [1, 2] * 3,
  560:             }
  561:         )
  562:         df["datetime"] = df["datetime"].apply(lambda d: Timestamp(d, tz="US/Pacific"))
  563: 
  564:         exp_idx1 = DatetimeIndex(
  565:             [
  566:                 "2011-07-19 07:00:00",
  567:                 "2011-07-19 07:00:00",
  568:                 "2011-07-19 08:00:00",
  569:                 "2011-07-19 08:00:00",
  570:                 "2011-07-19 09:00:00",
  571:                 "2011-07-19 09:00:00",
  572:             ],
  573:             tz="US/Pacific",
  574:             name="datetime",
  575:         )
  576:         exp_idx2 = Index(["a", "b"] * 3, name="label")
  577:         exp_idx = MultiIndex.from_arrays([exp_idx1, exp_idx2])
  578:         expected = DataFrame(
  579:             {"value1": [0, 3, 1, 4, 2, 5], "value2": [1, 2, 2, 1, 1, 2]},
  580:             index=exp_idx,
  581:             columns=["value1", "value2"],
  582:         )
  583: 
  584:         result = df.groupby(["datetime", "label"]).sum()
  585:         tm.assert_frame_equal(result, expected)
  586: 
  587:         # by level
  588:         didx = DatetimeIndex(dates, tz="Asia/Tokyo")
  589:         df = DataFrame(
  590:             {"value1": np.arange(6, dtype="int64"), "value2": [1, 2, 3, 1, 2, 3]},
  591:             index=didx,
  592:         )
  593: 
  594:         exp_idx = DatetimeIndex(
  595:             ["2011-07-19 07:00:00", "2011-07-19 08:00:00", "2011-07-19 09:00:00"],
  596:             tz="Asia/Tokyo",
  597:         )
  598:         expected = DataFrame(
  599:             {"value1": [3, 5, 7], "value2": [2, 4, 6]},
  600:             index=exp_idx,
  601:             columns=["value1", "value2"],
  602:         )
  603: 
  604:         result = df.groupby(level=0).sum()
  605:         tm.assert_frame_equal(result, expected)
  606: 
  607:     def test_frame_datetime64_handling_groupby(self):
  608:         # it works!
  609:         df = DataFrame(
  610:             [(3, np.datetime64("2012-07-03")), (3, np.datetime64("2012-07-04"))],
  611:             columns=["a", "date"],
  612:         )
  613:         result = df.groupby("a").first()
  614:         assert result["date"][3] == Timestamp("2012-07-03")
  615: 
  616:     def test_groupby_multi_timezone(self):
  617:         # combining multiple / different timezones yields UTC
  618:         df = DataFrame(
  619:             {
  620:                 "value": range(5),
  621:                 "date": [
  622:                     "2000-01-28 16:47:00",
  623:                     "2000-01-29 16:48:00",
  624:                     "2000-01-30 16:49:00",
  625:                     "2000-01-31 16:50:00",
  626:                     "2000-01-01 16:50:00",
  627:                 ],
  628:                 "tz": [
  629:                     "America/Chicago",
  630:                     "America/Chicago",
  631:                     "America/Los_Angeles",
  632:                     "America/Chicago",
  633:                     "America/New_York",
  634:                 ],
  635:             }
  636:         )
  637: 
  638:         result = df.groupby("tz", group_keys=False).date.apply(
  639:             lambda x: pd.to_datetime(x).dt.tz_localize(x.name)
  640:         )
  641: 
  642:         expected = Series(
  643:             [
  644:                 Timestamp("2000-01-28 16:47:00-0600", tz="America/Chicago"),
  645:                 Timestamp("2000-01-29 16:48:00-0600", tz="America/Chicago"),
  646:                 Timestamp("2000-01-30 16:49:00-0800", tz="America/Los_Angeles"),
  647:                 Timestamp("2000-01-31 16:50:00-0600", tz="America/Chicago"),
  648:                 Timestamp("2000-01-01 16:50:00-0500", tz="America/New_York"),
  649:             ],
  650:             name="date",
  651:             dtype=object,
  652:         )
  653:         tm.assert_series_equal(result, expected)
  654: 
  655:         tz = "America/Chicago"
  656:         res_values = df.groupby("tz").date.get_group(tz)
  657:         result = pd.to_datetime(res_values).dt.tz_localize(tz)
  658:         exp_values = Series(
  659:             ["2000-01-28 16:47:00", "2000-01-29 16:48:00", "2000-01-31 16:50:00"],
  660:             index=[0, 1, 3],
  661:             name="date",
  662:         )
  663:         expected = pd.to_datetime(exp_values).dt.tz_localize(tz)
  664:         tm.assert_series_equal(result, expected)
  665: 
  666:     def test_groupby_groups_periods(self):
  667:         dates = [
  668:             "2011-07-19 07:00:00",
  669:             "2011-07-19 08:00:00",
  670:             "2011-07-19 09:00:00",
  671:             "2011-07-19 07:00:00",
  672:             "2011-07-19 08:00:00",
  673:             "2011-07-19 09:00:00",
  674:         ]
  675:         df = DataFrame(
  676:             {
  677:                 "label": ["a", "a", "a", "b", "b", "b"],
  678:                 "period": [pd.Period(d, freq="h") for d in dates],
  679:                 "value1": np.arange(6, dtype="int64"),
  680:                 "value2": [1, 2] * 3,
  681:             }
  682:         )
  683: 
  684:         exp_idx1 = pd.PeriodIndex(
  685:             [
  686:                 "2011-07-19 07:00:00",
  687:                 "2011-07-19 07:00:00",
  688:                 "2011-07-19 08:00:00",
  689:                 "2011-07-19 08:00:00",
  690:                 "2011-07-19 09:00:00",
  691:                 "2011-07-19 09:00:00",
  692:             ],
  693:             freq="h",
  694:             name="period",
  695:         )
  696:         exp_idx2 = Index(["a", "b"] * 3, name="label")
  697:         exp_idx = MultiIndex.from_arrays([exp_idx1, exp_idx2])
  698:         expected = DataFrame(
  699:             {"value1": [0, 3, 1, 4, 2, 5], "value2": [1, 2, 2, 1, 1, 2]},
  700:             index=exp_idx,
  701:             columns=["value1", "value2"],
  702:         )
  703: 
  704:         result = df.groupby(["period", "label"]).sum()
  705:         tm.assert_frame_equal(result, expected)
  706: 
  707:         # by level
  708:         didx = pd.PeriodIndex(dates, freq="h")
  709:         df = DataFrame(
  710:             {"value1": np.arange(6, dtype="int64"), "value2": [1, 2, 3, 1, 2, 3]},
  711:             index=didx,
  712:         )
  713: 
  714:         exp_idx = pd.PeriodIndex(
  715:             ["2011-07-19 07:00:00", "2011-07-19 08:00:00", "2011-07-19 09:00:00"],
  716:             freq="h",
  717:         )
  718:         expected = DataFrame(
  719:             {"value1": [3, 5, 7], "value2": [2, 4, 6]},
  720:             index=exp_idx,
  721:             columns=["value1", "value2"],
  722:         )
  723: 
  724:         result = df.groupby(level=0).sum()
  725:         tm.assert_frame_equal(result, expected)
  726: 
  727:     def test_groupby_first_datetime64(self):
  728:         df = DataFrame([(1, 1351036800000000000), (2, 1351036800000000000)])
  729:         df[1] = df[1].astype("M8[ns]")
  730: 
  731:         assert issubclass(df[1].dtype.type, np.datetime64)
  732: 
  733:         result = df.groupby(level=0).first()
  734:         got_dt = result[1].dtype
  735:         assert issubclass(got_dt.type, np.datetime64)
  736: 
  737:         result = df[1].groupby(level=0).first()
  738:         got_dt = result.dtype
  739:         assert issubclass(got_dt.type, np.datetime64)
  740: 
  741:     def test_groupby_max_datetime64(self):
  742:         # GH 5869
  743:         # datetimelike dtype conversion from int
  744:         df = DataFrame({"A": Timestamp("20130101"), "B": np.arange(5)})
  745:         # TODO: can we retain second reso in .apply here?
  746:         expected = df.groupby("A")["A"].apply(lambda x: x.max()).astype("M8[s]")
  747:         result = df.groupby("A")["A"].max()
  748:         tm.assert_series_equal(result, expected)
  749: 
  750:     def test_groupby_datetime64_32_bit(self):
  751:         # GH 6410 / numpy 4328
  752:         # 32-bit under 1.9-dev indexing issue
  753: 
  754:         df = DataFrame({"A": range(2), "B": [Timestamp("2000-01-1")] * 2})
  755:         result = df.groupby("A")["B"].transform("min")
  756:         expected = Series([Timestamp("2000-01-1")] * 2, name="B")
  757:         tm.assert_series_equal(result, expected)
  758: 
  759:     def test_groupby_with_timezone_selection(self):
  760:         # GH 11616
  761:         # Test that column selection returns output in correct timezone.
  762: 
  763:         df = DataFrame(
  764:             {
  765:                 "factor": np.random.default_rng(2).integers(0, 3, size=60),
  766:                 "time": date_range("01/01/2000 00:00", periods=60, freq="s", tz="UTC"),
  767:             }
  768:         )
  769:         df1 = df.groupby("factor").max()["time"]
  770:         df2 = df.groupby("factor")["time"].max()
  771:         tm.assert_series_equal(df1, df2)
  772: 
  773:     def test_timezone_info(self):
  774:         # see gh-11682: Timezone info lost when broadcasting
  775:         # scalar datetime to DataFrame
  776: 
  777:         df = DataFrame({"a": [1], "b": [datetime.now(pytz.utc)]})
  778:         assert df["b"][0].tzinfo == pytz.utc
  779:         df = DataFrame({"a": [1, 2, 3]})
  780:         df["b"] = datetime.now(pytz.utc)
  781:         assert df["b"][0].tzinfo == pytz.utc
  782: 
  783:     def test_datetime_count(self):
  784:         df = DataFrame(
  785:             {"a": [1, 2, 3] * 2, "dates": date_range("now", periods=6, freq="min")}
  786:         )
  787:         result = df.groupby("a").dates.count()
  788:         expected = Series([2, 2, 2], index=Index([1, 2, 3], name="a"), name="dates")
  789:         tm.assert_series_equal(result, expected)
  790: 
  791:     def test_first_last_max_min_on_time_data(self):
  792:         # GH 10295
  793:         # Verify that NaT is not in the result of max, min, first and last on
  794:         # Dataframe with datetime or timedelta values.
  795:         df_test = DataFrame(
  796:             {
  797:                 "dt": [
  798:                     np.nan,
  799:                     "2015-07-24 10:10",
  800:                     "2015-07-25 11:11",
  801:                     "2015-07-23 12:12",
  802:                     np.nan,
  803:                 ],
  804:                 "td": [
  805:                     np.nan,
  806:                     timedelta(days=1),
  807:                     timedelta(days=2),
  808:                     timedelta(days=3),
  809:                     np.nan,
  810:                 ],
  811:             }
  812:         )
  813:         df_test.dt = pd.to_datetime(df_test.dt)
  814:         df_test["group"] = "A"
  815:         df_ref = df_test[df_test.dt.notna()]
  816: 
  817:         grouped_test = df_test.groupby("group")
  818:         grouped_ref = df_ref.groupby("group")
  819: 
  820:         tm.assert_frame_equal(grouped_ref.max(), grouped_test.max())
  821:         tm.assert_frame_equal(grouped_ref.min(), grouped_test.min())
  822:         tm.assert_frame_equal(grouped_ref.first(), grouped_test.first())
  823:         tm.assert_frame_equal(grouped_ref.last(), grouped_test.last())
  824: 
  825:     def test_nunique_with_timegrouper_and_nat(self):
  826:         # GH 17575
  827:         test = DataFrame(
  828:             {
  829:                 "time": [
  830:                     Timestamp("2016-06-28 09:35:35"),
  831:                     pd.NaT,
  832:                     Timestamp("2016-06-28 16:46:28"),
  833:                 ],
  834:                 "data": ["1", "2", "3"],
  835:             }
  836:         )
  837: 
  838:         grouper = Grouper(key="time", freq="h")
  839:         result = test.groupby(grouper)["data"].nunique()
  840:         expected = test[test.time.notnull()].groupby(grouper)["data"].nunique()
  841:         expected.index = expected.index._with_freq(None)
  842:         tm.assert_series_equal(result, expected)
  843: 
  844:     def test_scalar_call_versus_list_call(self):
  845:         # Issue: 17530
  846:         data_frame = {
  847:             "location": ["shanghai", "beijing", "shanghai"],
  848:             "time": Series(
  849:                 ["2017-08-09 13:32:23", "2017-08-11 23:23:15", "2017-08-11 22:23:15"],
  850:                 dtype="datetime64[ns]",
  851:             ),
  852:             "value": [1, 2, 3],
  853:         }
  854:         data_frame = DataFrame(data_frame).set_index("time")
  855:         grouper = Grouper(freq="D")
  856: 
  857:         grouped = data_frame.groupby(grouper)
  858:         result = grouped.count()
  859:         grouped = data_frame.groupby([grouper])
  860:         expected = grouped.count()
  861: 
  862:         tm.assert_frame_equal(result, expected)
  863: 
  864:     def test_grouper_period_index(self):
  865:         # GH 32108
  866:         periods = 2
  867:         index = pd.period_range(
  868:             start="2018-01", periods=periods, freq="M", name="Month"
  869:         )
  870:         period_series = Series(range(periods), index=index)
  871:         result = period_series.groupby(period_series.index.month).sum()
  872: 
  873:         expected = Series(
  874:             range(periods), index=Index(range(1, periods + 1), name=index.name)
  875:         )
  876:         tm.assert_series_equal(result, expected)
  877: 
  878:     def test_groupby_apply_timegrouper_with_nat_dict_returns(
  879:         self, groupby_with_truncated_bingrouper
  880:     ):
  881:         # GH#43500 case where gb._grouper.result_index and gb._grouper.group_keys_seq
  882:         #  have different lengths that goes through the `isinstance(values[0], dict)`
  883:         #  path
  884:         gb = groupby_with_truncated_bingrouper
  885: 
  886:         res = gb["Quantity"].apply(lambda x: {"foo": len(x)})
  887: 
  888:         df = gb.obj
  889:         unit = df["Date"]._values.unit
  890:         dti = date_range("2013-09-01", "2013-10-01", freq="5D", name="Date", unit=unit)
  891:         mi = MultiIndex.from_arrays([dti, ["foo"] * len(dti)])
  892:         expected = Series([3, 0, 0, 0, 0, 0, 2], index=mi, name="Quantity")
  893:         tm.assert_series_equal(res, expected)
  894: 
  895:     def test_groupby_apply_timegrouper_with_nat_scalar_returns(
  896:         self, groupby_with_truncated_bingrouper
  897:     ):
  898:         # GH#43500 Previously raised ValueError bc used index with incorrect
  899:         #  length in wrap_applied_result
  900:         gb = groupby_with_truncated_bingrouper
  901: 
  902:         res = gb["Quantity"].apply(lambda x: x.iloc[0] if len(x) else np.nan)
  903: 
  904:         df = gb.obj
  905:         unit = df["Date"]._values.unit
  906:         dti = date_range("2013-09-01", "2013-10-01", freq="5D", name="Date", unit=unit)
  907:         expected = Series(
  908:             [18, np.nan, np.nan, np.nan, np.nan, np.nan, 5],
  909:             index=dti._with_freq(None),
  910:             name="Quantity",
  911:         )
  912: 
  913:         tm.assert_series_equal(res, expected)
  914: 
  915:     def test_groupby_apply_timegrouper_with_nat_apply_squeeze(
  916:         self, frame_for_truncated_bingrouper
  917:     ):
  918:         df = frame_for_truncated_bingrouper
  919: 
  920:         # We need to create a GroupBy object with only one non-NaT group,
  921:         #  so use a huge freq so that all non-NaT dates will be grouped together
  922:         tdg = Grouper(key="Date", freq="100YE")
  923:         gb = df.groupby(tdg)
  924: 
  925:         # check that we will go through the singular_series path
  926:         #  in _wrap_applied_output_series
  927:         assert gb.ngroups == 1
  928:         assert gb._selected_obj._get_axis(gb.axis).nlevels == 1
  929: 
  930:         # function that returns a Series
  931:         msg = "DataFrameGroupBy.apply operated on the grouping columns"
  932:         with tm.assert_produces_warning(DeprecationWarning, match=msg):
  933:             res = gb.apply(lambda x: x["Quantity"] * 2)
  934: 
  935:         dti = Index([Timestamp("2013-12-31")], dtype=df["Date"].dtype, name="Date")
  936:         expected = DataFrame(
  937:             [[36, 6, 6, 10, 2]],
  938:             index=dti,
  939:             columns=Index([0, 1, 5, 2, 3], name="Quantity"),
  940:         )
  941:         tm.assert_frame_equal(res, expected)
  942: 
  943:     @pytest.mark.single_cpu
  944:     def test_groupby_agg_numba_timegrouper_with_nat(
  945:         self, groupby_with_truncated_bingrouper
  946:     ):
  947:         pytest.importorskip("numba")
  948: 
  949:         # See discussion in GH#43487
  950:         gb = groupby_with_truncated_bingrouper
  951: 
  952:         result = gb["Quantity"].aggregate(
  953:             lambda values, index: np.nanmean(values), engine="numba"
  954:         )
  955: 
  956:         expected = gb["Quantity"].aggregate("mean")
  957:         tm.assert_series_equal(result, expected)
  958: 
  959:         result_df = gb[["Quantity"]].aggregate(
  960:             lambda values, index: np.nanmean(values), engine="numba"
  961:         )
  962:         expected_df = gb[["Quantity"]].aggregate("mean")
  963:         tm.assert_frame_equal(result_df, expected_df)
