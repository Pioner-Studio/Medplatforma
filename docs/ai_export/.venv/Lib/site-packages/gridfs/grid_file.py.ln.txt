    1: # Copyright 2009-present MongoDB, Inc.
    2: #
    3: # Licensed under the Apache License, Version 2.0 (the "License");
    4: # you may not use this file except in compliance with the License.
    5: # You may obtain a copy of the License at
    6: #
    7: # http://www.apache.org/licenses/LICENSE-2.0
    8: #
    9: # Unless required by applicable law or agreed to in writing, software
   10: # distributed under the License is distributed on an "AS IS" BASIS,
   11: # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   12: # See the License for the specific language governing permissions and
   13: # limitations under the License.
   14: 
   15: """Tools for representing files stored in GridFS."""
   16: from __future__ import annotations
   17: 
   18: import datetime
   19: import io
   20: import math
   21: import os
   22: import warnings
   23: from typing import Any, Iterable, Mapping, NoReturn, Optional
   24: 
   25: from bson.int64 import Int64
   26: from bson.objectid import ObjectId
   27: from gridfs.errors import CorruptGridFile, FileExists, NoFile
   28: from pymongo import ASCENDING
   29: from pymongo.client_session import ClientSession
   30: from pymongo.collection import Collection
   31: from pymongo.common import MAX_MESSAGE_SIZE
   32: from pymongo.cursor import Cursor
   33: from pymongo.errors import (
   34:     BulkWriteError,
   35:     ConfigurationError,
   36:     CursorNotFound,
   37:     DuplicateKeyError,
   38:     InvalidOperation,
   39:     OperationFailure,
   40: )
   41: from pymongo.helpers import _check_write_command_response
   42: from pymongo.read_preferences import ReadPreference
   43: 
   44: _SEEK_SET = os.SEEK_SET
   45: _SEEK_CUR = os.SEEK_CUR
   46: _SEEK_END = os.SEEK_END
   47: 
   48: EMPTY = b""
   49: NEWLN = b"\n"
   50: 
   51: """Default chunk size, in bytes."""
   52: # Slightly under a power of 2, to work well with server's record allocations.
   53: DEFAULT_CHUNK_SIZE = 255 * 1024
   54: # The number of chunked bytes to buffer before calling insert_many.
   55: _UPLOAD_BUFFER_SIZE = MAX_MESSAGE_SIZE
   56: # The number of chunk documents to buffer before calling insert_many.
   57: _UPLOAD_BUFFER_CHUNKS = 100000
   58: # Rough BSON overhead of a chunk document not including the chunk data itself.
   59: # Essentially len(encode({"_id": ObjectId(), "files_id": ObjectId(), "n": 1, "data": ""}))
   60: _CHUNK_OVERHEAD = 60
   61: 
   62: _C_INDEX: dict[str, Any] = {"files_id": ASCENDING, "n": ASCENDING}
   63: _F_INDEX: dict[str, Any] = {"filename": ASCENDING, "uploadDate": ASCENDING}
   64: 
   65: 
   66: def _grid_in_property(
   67:     field_name: str,
   68:     docstring: str,
   69:     read_only: Optional[bool] = False,
   70:     closed_only: Optional[bool] = False,
   71: ) -> Any:
   72:     """Create a GridIn property."""
   73:     warn_str = ""
   74:     if docstring.startswith("DEPRECATED,"):
   75:         warn_str = (
   76:             f"GridIn property '{field_name}' is deprecated and will be removed in PyMongo 5.0"
   77:         )
   78: 
   79:     def getter(self: Any) -> Any:
   80:         if warn_str:
   81:             warnings.warn(warn_str, stacklevel=2, category=DeprecationWarning)
   82:         if closed_only and not self._closed:
   83:             raise AttributeError("can only get %r on a closed file" % field_name)
   84:         # Protect against PHP-237
   85:         if field_name == "length":
   86:             return self._file.get(field_name, 0)
   87:         return self._file.get(field_name, None)
   88: 
   89:     def setter(self: Any, value: Any) -> Any:
   90:         if warn_str:
   91:             warnings.warn(warn_str, stacklevel=2, category=DeprecationWarning)
   92:         if self._closed:
   93:             self._coll.files.update_one({"_id": self._file["_id"]}, {"$set": {field_name: value}})
   94:         self._file[field_name] = value
   95: 
   96:     if read_only:
   97:         docstring += "\n\nThis attribute is read-only."
   98:     elif closed_only:
   99:         docstring = "{}\n\n{}".format(
  100:             docstring,
  101:             "This attribute is read-only and "
  102:             "can only be read after :meth:`close` "
  103:             "has been called.",
  104:         )
  105: 
  106:     if not read_only and not closed_only:
  107:         return property(getter, setter, doc=docstring)
  108:     return property(getter, doc=docstring)
  109: 
  110: 
  111: def _grid_out_property(field_name: str, docstring: str) -> Any:
  112:     """Create a GridOut property."""
  113:     warn_str = ""
  114:     if docstring.startswith("DEPRECATED,"):
  115:         warn_str = (
  116:             f"GridOut property '{field_name}' is deprecated and will be removed in PyMongo 5.0"
  117:         )
  118: 
  119:     def getter(self: Any) -> Any:
  120:         if warn_str:
  121:             warnings.warn(warn_str, stacklevel=2, category=DeprecationWarning)
  122:         self._ensure_file()
  123: 
  124:         # Protect against PHP-237
  125:         if field_name == "length":
  126:             return self._file.get(field_name, 0)
  127:         return self._file.get(field_name, None)
  128: 
  129:     docstring += "\n\nThis attribute is read-only."
  130:     return property(getter, doc=docstring)
  131: 
  132: 
  133: def _clear_entity_type_registry(entity: Any, **kwargs: Any) -> Any:
  134:     """Clear the given database/collection object's type registry."""
  135:     codecopts = entity.codec_options.with_options(type_registry=None)
  136:     return entity.with_options(codec_options=codecopts, **kwargs)
  137: 
  138: 
  139: def _disallow_transactions(session: Optional[ClientSession]) -> None:
  140:     if session and session.in_transaction:
  141:         raise InvalidOperation("GridFS does not support multi-document transactions")
  142: 
  143: 
  144: class GridIn:
  145:     """Class to write data to GridFS."""
  146: 
  147:     def __init__(
  148:         self, root_collection: Collection, session: Optional[ClientSession] = None, **kwargs: Any
  149:     ) -> None:
  150:         """Write a file to GridFS
  151: 
  152:         Application developers should generally not need to
  153:         instantiate this class directly - instead see the methods
  154:         provided by :class:`~gridfs.GridFS`.
  155: 
  156:         Raises :class:`TypeError` if `root_collection` is not an
  157:         instance of :class:`~pymongo.collection.Collection`.
  158: 
  159:         Any of the file level options specified in the `GridFS Spec
  160:         <http://dochub.mongodb.org/core/gridfsspec>`_ may be passed as
  161:         keyword arguments. Any additional keyword arguments will be
  162:         set as additional fields on the file document. Valid keyword
  163:         arguments include:
  164: 
  165:           - ``"_id"``: unique ID for this file (default:
  166:             :class:`~bson.objectid.ObjectId`) - this ``"_id"`` must
  167:             not have already been used for another file
  168: 
  169:           - ``"filename"``: human name for the file
  170: 
  171:           - ``"contentType"`` or ``"content_type"``: valid mime-type
  172:             for the file
  173: 
  174:           - ``"chunkSize"`` or ``"chunk_size"``: size of each of the
  175:             chunks, in bytes (default: 255 kb)
  176: 
  177:           - ``"encoding"``: encoding used for this file. Any :class:`str`
  178:             that is written to the file will be converted to :class:`bytes`.
  179: 
  180:         :param root_collection: root collection to write to
  181:         :param session: a
  182:             :class:`~pymongo.client_session.ClientSession` to use for all
  183:             commands
  184:         :param kwargs: Any: file level options (see above)
  185: 
  186:         .. versionchanged:: 4.0
  187:            Removed the `disable_md5` parameter. See
  188:            :ref:`removed-gridfs-checksum` for details.
  189: 
  190:         .. versionchanged:: 3.7
  191:            Added the `disable_md5` parameter.
  192: 
  193:         .. versionchanged:: 3.6
  194:            Added ``session`` parameter.
  195: 
  196:         .. versionchanged:: 3.0
  197:            `root_collection` must use an acknowledged
  198:            :attr:`~pymongo.collection.Collection.write_concern`
  199:         """
  200:         if not isinstance(root_collection, Collection):
  201:             raise TypeError("root_collection must be an instance of Collection")
  202: 
  203:         if not root_collection.write_concern.acknowledged:
  204:             raise ConfigurationError("root_collection must use acknowledged write_concern")
  205:         _disallow_transactions(session)
  206: 
  207:         # Handle alternative naming
  208:         if "content_type" in kwargs:
  209:             kwargs["contentType"] = kwargs.pop("content_type")
  210:         if "chunk_size" in kwargs:
  211:             kwargs["chunkSize"] = kwargs.pop("chunk_size")
  212: 
  213:         coll = _clear_entity_type_registry(root_collection, read_preference=ReadPreference.PRIMARY)
  214: 
  215:         # Defaults
  216:         kwargs["_id"] = kwargs.get("_id", ObjectId())
  217:         kwargs["chunkSize"] = kwargs.get("chunkSize", DEFAULT_CHUNK_SIZE)
  218:         object.__setattr__(self, "_session", session)
  219:         object.__setattr__(self, "_coll", coll)
  220:         object.__setattr__(self, "_chunks", coll.chunks)
  221:         object.__setattr__(self, "_file", kwargs)
  222:         object.__setattr__(self, "_buffer", io.BytesIO())
  223:         object.__setattr__(self, "_position", 0)
  224:         object.__setattr__(self, "_chunk_number", 0)
  225:         object.__setattr__(self, "_closed", False)
  226:         object.__setattr__(self, "_ensured_index", False)
  227:         object.__setattr__(self, "_buffered_docs", [])
  228:         object.__setattr__(self, "_buffered_docs_size", 0)
  229: 
  230:     def __create_index(self, collection: Collection, index_key: Any, unique: bool) -> None:
  231:         doc = collection.find_one(projection={"_id": 1}, session=self._session)
  232:         if doc is None:
  233:             try:
  234:                 index_keys = [
  235:                     index_spec["key"]
  236:                     for index_spec in collection.list_indexes(session=self._session)
  237:                 ]
  238:             except OperationFailure:
  239:                 index_keys = []
  240:             if index_key not in index_keys:
  241:                 collection.create_index(index_key.items(), unique=unique, session=self._session)
  242: 
  243:     def __ensure_indexes(self) -> None:
  244:         if not object.__getattribute__(self, "_ensured_index"):
  245:             _disallow_transactions(self._session)
  246:             self.__create_index(self._coll.files, _F_INDEX, False)
  247:             self.__create_index(self._coll.chunks, _C_INDEX, True)
  248:             object.__setattr__(self, "_ensured_index", True)
  249: 
  250:     def abort(self) -> None:
  251:         """Remove all chunks/files that may have been uploaded and close."""
  252:         self._coll.chunks.delete_many({"files_id": self._file["_id"]}, session=self._session)
  253:         self._coll.files.delete_one({"_id": self._file["_id"]}, session=self._session)
  254:         object.__setattr__(self, "_closed", True)
  255: 
  256:     @property
  257:     def closed(self) -> bool:
  258:         """Is this file closed?"""
  259:         return self._closed
  260: 
  261:     _id: Any = _grid_in_property("_id", "The ``'_id'`` value for this file.", read_only=True)
  262:     filename: Optional[str] = _grid_in_property("filename", "Name of this file.")
  263:     name: Optional[str] = _grid_in_property("filename", "Alias for `filename`.")
  264:     content_type: Optional[str] = _grid_in_property(
  265:         "contentType", "DEPRECATED, will be removed in PyMongo 5.0. Mime-type for this file."
  266:     )
  267:     length: int = _grid_in_property("length", "Length (in bytes) of this file.", closed_only=True)
  268:     chunk_size: int = _grid_in_property("chunkSize", "Chunk size for this file.", read_only=True)
  269:     upload_date: datetime.datetime = _grid_in_property(
  270:         "uploadDate", "Date that this file was uploaded.", closed_only=True
  271:     )
  272:     md5: Optional[str] = _grid_in_property(
  273:         "md5",
  274:         "DEPRECATED, will be removed in PyMongo 5.0. MD5 of the contents of this file if an md5 sum was created.",
  275:         closed_only=True,
  276:     )
  277: 
  278:     _buffer: io.BytesIO
  279:     _closed: bool
  280:     _buffered_docs: list[dict[str, Any]]
  281:     _buffered_docs_size: int
  282: 
  283:     def __getattr__(self, name: str) -> Any:
  284:         if name in self._file:
  285:             return self._file[name]
  286:         raise AttributeError("GridIn object has no attribute '%s'" % name)
  287: 
  288:     def __setattr__(self, name: str, value: Any) -> None:
  289:         # For properties of this instance like _buffer, or descriptors set on
  290:         # the class like filename, use regular __setattr__
  291:         if name in self.__dict__ or name in self.__class__.__dict__:
  292:             object.__setattr__(self, name, value)
  293:         else:
  294:             # All other attributes are part of the document in db.fs.files.
  295:             # Store them to be sent to server on close() or if closed, send
  296:             # them now.
  297:             self._file[name] = value
  298:             if self._closed:
  299:                 self._coll.files.update_one({"_id": self._file["_id"]}, {"$set": {name: value}})
  300: 
  301:     def __flush_data(self, data: Any, force: bool = False) -> None:
  302:         """Flush `data` to a chunk."""
  303:         self.__ensure_indexes()
  304:         assert len(data) <= self.chunk_size
  305:         if data:
  306:             self._buffered_docs.append(
  307:                 {"files_id": self._file["_id"], "n": self._chunk_number, "data": data}
  308:             )
  309:             self._buffered_docs_size += len(data) + _CHUNK_OVERHEAD
  310:         if not self._buffered_docs:
  311:             return
  312:         # Limit to 100,000 chunks or 32MB (+1 chunk) of data.
  313:         if (
  314:             force
  315:             or self._buffered_docs_size >= _UPLOAD_BUFFER_SIZE
  316:             or len(self._buffered_docs) >= _UPLOAD_BUFFER_CHUNKS
  317:         ):
  318:             try:
  319:                 self._chunks.insert_many(self._buffered_docs, session=self._session)
  320:             except BulkWriteError as exc:
  321:                 # For backwards compatibility, raise an insert_one style exception.
  322:                 write_errors = exc.details["writeErrors"]
  323:                 for err in write_errors:
  324:                     if err.get("code") in (11000, 11001, 12582):  # Duplicate key errors
  325:                         self._raise_file_exists(self._file["_id"])
  326:                 result = {"writeErrors": write_errors}
  327:                 wces = exc.details["writeConcernErrors"]
  328:                 if wces:
  329:                     result["writeConcernError"] = wces[-1]
  330:                 _check_write_command_response(result)
  331:                 raise
  332:             self._buffered_docs = []
  333:             self._buffered_docs_size = 0
  334:         self._chunk_number += 1
  335:         self._position += len(data)
  336: 
  337:     def __flush_buffer(self, force: bool = False) -> None:
  338:         """Flush the buffer contents out to a chunk."""
  339:         self.__flush_data(self._buffer.getvalue(), force=force)
  340:         self._buffer.close()
  341:         self._buffer = io.BytesIO()
  342: 
  343:     def __flush(self) -> Any:
  344:         """Flush the file to the database."""
  345:         try:
  346:             self.__flush_buffer(force=True)
  347:             # The GridFS spec says length SHOULD be an Int64.
  348:             self._file["length"] = Int64(self._position)
  349:             self._file["uploadDate"] = datetime.datetime.now(tz=datetime.timezone.utc)
  350: 
  351:             return self._coll.files.insert_one(self._file, session=self._session)
  352:         except DuplicateKeyError:
  353:             self._raise_file_exists(self._id)
  354: 
  355:     def _raise_file_exists(self, file_id: Any) -> NoReturn:
  356:         """Raise a FileExists exception for the given file_id."""
  357:         raise FileExists("file with _id %r already exists" % file_id)
  358: 
  359:     def close(self) -> None:
  360:         """Flush the file and close it.
  361: 
  362:         A closed file cannot be written any more. Calling
  363:         :meth:`close` more than once is allowed.
  364:         """
  365:         if not self._closed:
  366:             self.__flush()
  367:             object.__setattr__(self, "_closed", True)
  368: 
  369:     def read(self, size: int = -1) -> NoReturn:
  370:         raise io.UnsupportedOperation("read")
  371: 
  372:     def readable(self) -> bool:
  373:         return False
  374: 
  375:     def seekable(self) -> bool:
  376:         return False
  377: 
  378:     def write(self, data: Any) -> None:
  379:         """Write data to the file. There is no return value.
  380: 
  381:         `data` can be either a string of bytes or a file-like object
  382:         (implementing :meth:`read`). If the file has an
  383:         :attr:`encoding` attribute, `data` can also be a
  384:         :class:`str` instance, which will be encoded as
  385:         :attr:`encoding` before being written.
  386: 
  387:         Due to buffering, the data may not actually be written to the
  388:         database until the :meth:`close` method is called. Raises
  389:         :class:`ValueError` if this file is already closed. Raises
  390:         :class:`TypeError` if `data` is not an instance of
  391:         :class:`bytes`, a file-like object, or an instance of :class:`str`.
  392:         Unicode data is only allowed if the file has an :attr:`encoding`
  393:         attribute.
  394: 
  395:         :param data: string of bytes or file-like object to be written
  396:             to the file
  397:         """
  398:         if self._closed:
  399:             raise ValueError("cannot write to a closed file")
  400: 
  401:         try:
  402:             # file-like
  403:             read = data.read
  404:         except AttributeError:
  405:             # string
  406:             if not isinstance(data, (str, bytes)):
  407:                 raise TypeError("can only write strings or file-like objects") from None
  408:             if isinstance(data, str):
  409:                 try:
  410:                     data = data.encode(self.encoding)
  411:                 except AttributeError:
  412:                     raise TypeError(
  413:                         "must specify an encoding for file in order to write str"
  414:                     ) from None
  415:             read = io.BytesIO(data).read
  416: 
  417:         if self._buffer.tell() > 0:
  418:             # Make sure to flush only when _buffer is complete
  419:             space = self.chunk_size - self._buffer.tell()
  420:             if space:
  421:                 try:
  422:                     to_write = read(space)
  423:                 except BaseException:
  424:                     self.abort()
  425:                     raise
  426:                 self._buffer.write(to_write)
  427:                 if len(to_write) < space:
  428:                     return  # EOF or incomplete
  429:             self.__flush_buffer()
  430:         to_write = read(self.chunk_size)
  431:         while to_write and len(to_write) == self.chunk_size:
  432:             self.__flush_data(to_write)
  433:             to_write = read(self.chunk_size)
  434:         self._buffer.write(to_write)
  435: 
  436:     def writelines(self, sequence: Iterable[Any]) -> None:
  437:         """Write a sequence of strings to the file.
  438: 
  439:         Does not add separators.
  440:         """
  441:         for line in sequence:
  442:             self.write(line)
  443: 
  444:     def writeable(self) -> bool:
  445:         return True
  446: 
  447:     def __enter__(self) -> GridIn:
  448:         """Support for the context manager protocol."""
  449:         return self
  450: 
  451:     def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> Any:
  452:         """Support for the context manager protocol.
  453: 
  454:         Close the file if no exceptions occur and allow exceptions to propagate.
  455:         """
  456:         if exc_type is None:
  457:             # No exceptions happened.
  458:             self.close()
  459:         else:
  460:             # Something happened, at minimum mark as closed.
  461:             object.__setattr__(self, "_closed", True)
  462: 
  463:         # propagate exceptions
  464:         return False
  465: 
  466: 
  467: class GridOut(io.IOBase):
  468:     """Class to read data out of GridFS."""
  469: 
  470:     def __init__(
  471:         self,
  472:         root_collection: Collection,
  473:         file_id: Optional[int] = None,
  474:         file_document: Optional[Any] = None,
  475:         session: Optional[ClientSession] = None,
  476:     ) -> None:
  477:         """Read a file from GridFS
  478: 
  479:         Application developers should generally not need to
  480:         instantiate this class directly - instead see the methods
  481:         provided by :class:`~gridfs.GridFS`.
  482: 
  483:         Either `file_id` or `file_document` must be specified,
  484:         `file_document` will be given priority if present. Raises
  485:         :class:`TypeError` if `root_collection` is not an instance of
  486:         :class:`~pymongo.collection.Collection`.
  487: 
  488:         :param root_collection: root collection to read from
  489:         :param file_id: value of ``"_id"`` for the file to read
  490:         :param file_document: file document from
  491:             `root_collection.files`
  492:         :param session: a
  493:             :class:`~pymongo.client_session.ClientSession` to use for all
  494:             commands
  495: 
  496:         .. versionchanged:: 3.8
  497:            For better performance and to better follow the GridFS spec,
  498:            :class:`GridOut` now uses a single cursor to read all the chunks in
  499:            the file.
  500: 
  501:         .. versionchanged:: 3.6
  502:            Added ``session`` parameter.
  503: 
  504:         .. versionchanged:: 3.0
  505:            Creating a GridOut does not immediately retrieve the file metadata
  506:            from the server. Metadata is fetched when first needed.
  507:         """
  508:         if not isinstance(root_collection, Collection):
  509:             raise TypeError("root_collection must be an instance of Collection")
  510:         _disallow_transactions(session)
  511: 
  512:         root_collection = _clear_entity_type_registry(root_collection)
  513: 
  514:         super().__init__()
  515: 
  516:         self.__chunks = root_collection.chunks
  517:         self.__files = root_collection.files
  518:         self.__file_id = file_id
  519:         self.__buffer = EMPTY
  520:         # Start position within the current buffered chunk.
  521:         self.__buffer_pos = 0
  522:         self.__chunk_iter = None
  523:         # Position within the total file.
  524:         self.__position = 0
  525:         self._file = file_document
  526:         self._session = session
  527: 
  528:     _id: Any = _grid_out_property("_id", "The ``'_id'`` value for this file.")
  529:     filename: str = _grid_out_property("filename", "Name of this file.")
  530:     name: str = _grid_out_property("filename", "Alias for `filename`.")
  531:     content_type: Optional[str] = _grid_out_property(
  532:         "contentType", "DEPRECATED, will be removed in PyMongo 5.0. Mime-type for this file."
  533:     )
  534:     length: int = _grid_out_property("length", "Length (in bytes) of this file.")
  535:     chunk_size: int = _grid_out_property("chunkSize", "Chunk size for this file.")
  536:     upload_date: datetime.datetime = _grid_out_property(
  537:         "uploadDate", "Date that this file was first uploaded."
  538:     )
  539:     aliases: Optional[list[str]] = _grid_out_property(
  540:         "aliases", "DEPRECATED, will be removed in PyMongo 5.0. List of aliases for this file."
  541:     )
  542:     metadata: Optional[Mapping[str, Any]] = _grid_out_property(
  543:         "metadata", "Metadata attached to this file."
  544:     )
  545:     md5: Optional[str] = _grid_out_property(
  546:         "md5",
  547:         "DEPRECATED, will be removed in PyMongo 5.0. MD5 of the contents of this file if an md5 sum was created.",
  548:     )
  549: 
  550:     _file: Any
  551:     __chunk_iter: Any
  552: 
  553:     def _ensure_file(self) -> None:
  554:         if not self._file:
  555:             _disallow_transactions(self._session)
  556:             self._file = self.__files.find_one({"_id": self.__file_id}, session=self._session)
  557:             if not self._file:
  558:                 raise NoFile(
  559:                     f"no file in gridfs collection {self.__files!r} with _id {self.__file_id!r}"
  560:                 )
  561: 
  562:     def __getattr__(self, name: str) -> Any:
  563:         self._ensure_file()
  564:         if name in self._file:
  565:             return self._file[name]
  566:         raise AttributeError("GridOut object has no attribute '%s'" % name)
  567: 
  568:     def readable(self) -> bool:
  569:         return True
  570: 
  571:     def readchunk(self) -> bytes:
  572:         """Reads a chunk at a time. If the current position is within a
  573:         chunk the remainder of the chunk is returned.
  574:         """
  575:         received = len(self.__buffer) - self.__buffer_pos
  576:         chunk_data = EMPTY
  577:         chunk_size = int(self.chunk_size)
  578: 
  579:         if received > 0:
  580:             chunk_data = self.__buffer[self.__buffer_pos :]
  581:         elif self.__position < int(self.length):
  582:             chunk_number = int((received + self.__position) / chunk_size)
  583:             if self.__chunk_iter is None:
  584:                 self.__chunk_iter = _GridOutChunkIterator(
  585:                     self, self.__chunks, self._session, chunk_number
  586:                 )
  587: 
  588:             chunk = self.__chunk_iter.next()
  589:             chunk_data = chunk["data"][self.__position % chunk_size :]
  590: 
  591:             if not chunk_data:
  592:                 raise CorruptGridFile("truncated chunk")
  593: 
  594:         self.__position += len(chunk_data)
  595:         self.__buffer = EMPTY
  596:         self.__buffer_pos = 0
  597:         return chunk_data
  598: 
  599:     def _read_size_or_line(self, size: int = -1, line: bool = False) -> bytes:
  600:         """Internal read() and readline() helper."""
  601:         self._ensure_file()
  602:         remainder = int(self.length) - self.__position
  603:         if size < 0 or size > remainder:
  604:             size = remainder
  605: 
  606:         if size == 0:
  607:             return EMPTY
  608: 
  609:         received = 0
  610:         data = []
  611:         while received < size:
  612:             needed = size - received
  613:             if self.__buffer:
  614:                 # Optimization: Read the buffer with zero byte copies.
  615:                 buf = self.__buffer
  616:                 chunk_start = self.__buffer_pos
  617:                 chunk_data = memoryview(buf)[self.__buffer_pos :]
  618:                 self.__buffer = EMPTY
  619:                 self.__buffer_pos = 0
  620:                 self.__position += len(chunk_data)
  621:             else:
  622:                 buf = self.readchunk()
  623:                 chunk_start = 0
  624:                 chunk_data = memoryview(buf)
  625:             if line:
  626:                 pos = buf.find(NEWLN, chunk_start, chunk_start + needed) - chunk_start
  627:                 if pos >= 0:
  628:                     # Decrease size to exit the loop.
  629:                     size = received + pos + 1
  630:                     needed = pos + 1
  631:             if len(chunk_data) > needed:
  632:                 data.append(chunk_data[:needed])
  633:                 # Optimization: Save the buffer with zero byte copies.
  634:                 self.__buffer = buf
  635:                 self.__buffer_pos = chunk_start + needed
  636:                 self.__position -= len(self.__buffer) - self.__buffer_pos
  637:             else:
  638:                 data.append(chunk_data)
  639:             received += len(chunk_data)
  640: 
  641:         # Detect extra chunks after reading the entire file.
  642:         if size == remainder and self.__chunk_iter:
  643:             try:
  644:                 self.__chunk_iter.next()
  645:             except StopIteration:
  646:                 pass
  647: 
  648:         return b"".join(data)
  649: 
  650:     def read(self, size: int = -1) -> bytes:
  651:         """Read at most `size` bytes from the file (less if there
  652:         isn't enough data).
  653: 
  654:         The bytes are returned as an instance of :class:`bytes`
  655:         If `size` is negative or omitted all data is read.
  656: 
  657:         :param size: the number of bytes to read
  658: 
  659:         .. versionchanged:: 3.8
  660:            This method now only checks for extra chunks after reading the
  661:            entire file. Previously, this method would check for extra chunks
  662:            on every call.
  663:         """
  664:         return self._read_size_or_line(size=size)
  665: 
  666:     def readline(self, size: int = -1) -> bytes:  # type: ignore[override]
  667:         """Read one line or up to `size` bytes from the file.
  668: 
  669:         :param size: the maximum number of bytes to read
  670:         """
  671:         return self._read_size_or_line(size=size, line=True)
  672: 
  673:     def tell(self) -> int:
  674:         """Return the current position of this file."""
  675:         return self.__position
  676: 
  677:     def seek(self, pos: int, whence: int = _SEEK_SET) -> int:
  678:         """Set the current position of this file.
  679: 
  680:         :param pos: the position (or offset if using relative
  681:            positioning) to seek to
  682:         :param whence: where to seek
  683:            from. :attr:`os.SEEK_SET` (``0``) for absolute file
  684:            positioning, :attr:`os.SEEK_CUR` (``1``) to seek relative
  685:            to the current position, :attr:`os.SEEK_END` (``2``) to
  686:            seek relative to the file's end.
  687: 
  688:         .. versionchanged:: 4.1
  689:            The method now returns the new position in the file, to
  690:            conform to the behavior of :meth:`io.IOBase.seek`.
  691:         """
  692:         if whence == _SEEK_SET:
  693:             new_pos = pos
  694:         elif whence == _SEEK_CUR:
  695:             new_pos = self.__position + pos
  696:         elif whence == _SEEK_END:
  697:             new_pos = int(self.length) + pos
  698:         else:
  699:             raise OSError(22, "Invalid value for `whence`")
  700: 
  701:         if new_pos < 0:
  702:             raise OSError(22, "Invalid value for `pos` - must be positive")
  703: 
  704:         # Optimization, continue using the same buffer and chunk iterator.
  705:         if new_pos == self.__position:
  706:             return new_pos
  707: 
  708:         self.__position = new_pos
  709:         self.__buffer = EMPTY
  710:         self.__buffer_pos = 0
  711:         if self.__chunk_iter:
  712:             self.__chunk_iter.close()
  713:             self.__chunk_iter = None
  714:         return new_pos
  715: 
  716:     def seekable(self) -> bool:
  717:         return True
  718: 
  719:     def __iter__(self) -> GridOut:
  720:         """Return an iterator over all of this file's data.
  721: 
  722:         The iterator will return lines (delimited by ``b'\\n'``) of
  723:         :class:`bytes`. This can be useful when serving files
  724:         using a webserver that handles such an iterator efficiently.
  725: 
  726:         .. versionchanged:: 3.8
  727:            The iterator now raises :class:`CorruptGridFile` when encountering
  728:            any truncated, missing, or extra chunk in a file. The previous
  729:            behavior was to only raise :class:`CorruptGridFile` on a missing
  730:            chunk.
  731: 
  732:         .. versionchanged:: 4.0
  733:            The iterator now iterates over *lines* in the file, instead
  734:            of chunks, to conform to the base class :py:class:`io.IOBase`.
  735:            Use :meth:`GridOut.readchunk` to read chunk by chunk instead
  736:            of line by line.
  737:         """
  738:         return self
  739: 
  740:     def close(self) -> None:
  741:         """Make GridOut more generically file-like."""
  742:         if self.__chunk_iter:
  743:             self.__chunk_iter.close()
  744:             self.__chunk_iter = None
  745:         super().close()
  746: 
  747:     def write(self, value: Any) -> NoReturn:
  748:         raise io.UnsupportedOperation("write")
  749: 
  750:     def writelines(self, lines: Any) -> NoReturn:
  751:         raise io.UnsupportedOperation("writelines")
  752: 
  753:     def writable(self) -> bool:
  754:         return False
  755: 
  756:     def __enter__(self) -> GridOut:
  757:         """Makes it possible to use :class:`GridOut` files
  758:         with the context manager protocol.
  759:         """
  760:         return self
  761: 
  762:     def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> Any:
  763:         """Makes it possible to use :class:`GridOut` files
  764:         with the context manager protocol.
  765:         """
  766:         self.close()
  767:         return False
  768: 
  769:     def fileno(self) -> NoReturn:
  770:         raise io.UnsupportedOperation("fileno")
  771: 
  772:     def flush(self) -> None:
  773:         # GridOut is read-only, so flush does nothing.
  774:         pass
  775: 
  776:     def isatty(self) -> bool:
  777:         return False
  778: 
  779:     def truncate(self, size: Optional[int] = None) -> NoReturn:
  780:         # See https://docs.python.org/3/library/io.html#io.IOBase.writable
  781:         # for why truncate has to raise.
  782:         raise io.UnsupportedOperation("truncate")
  783: 
  784:     # Override IOBase.__del__ otherwise it will lead to __getattr__ on
  785:     # __IOBase_closed which calls _ensure_file and potentially performs I/O.
  786:     # We cannot do I/O in __del__ since it can lead to a deadlock.
  787:     def __del__(self) -> None:
  788:         pass
  789: 
  790: 
  791: class _GridOutChunkIterator:
  792:     """Iterates over a file's chunks using a single cursor.
  793: 
  794:     Raises CorruptGridFile when encountering any truncated, missing, or extra
  795:     chunk in a file.
  796:     """
  797: 
  798:     def __init__(
  799:         self,
  800:         grid_out: GridOut,
  801:         chunks: Collection,
  802:         session: Optional[ClientSession],
  803:         next_chunk: Any,
  804:     ) -> None:
  805:         self._id = grid_out._id
  806:         self._chunk_size = int(grid_out.chunk_size)
  807:         self._length = int(grid_out.length)
  808:         self._chunks = chunks
  809:         self._session = session
  810:         self._next_chunk = next_chunk
  811:         self._num_chunks = math.ceil(float(self._length) / self._chunk_size)
  812:         self._cursor = None
  813: 
  814:     _cursor: Optional[Cursor]
  815: 
  816:     def expected_chunk_length(self, chunk_n: int) -> int:
  817:         if chunk_n < self._num_chunks - 1:
  818:             return self._chunk_size
  819:         return self._length - (self._chunk_size * (self._num_chunks - 1))
  820: 
  821:     def __iter__(self) -> _GridOutChunkIterator:
  822:         return self
  823: 
  824:     def _create_cursor(self) -> None:
  825:         filter = {"files_id": self._id}
  826:         if self._next_chunk > 0:
  827:             filter["n"] = {"$gte": self._next_chunk}
  828:         _disallow_transactions(self._session)
  829:         self._cursor = self._chunks.find(filter, sort=[("n", 1)], session=self._session)
  830: 
  831:     def _next_with_retry(self) -> Mapping[str, Any]:
  832:         """Return the next chunk and retry once on CursorNotFound.
  833: 
  834:         We retry on CursorNotFound to maintain backwards compatibility in
  835:         cases where two calls to read occur more than 10 minutes apart (the
  836:         server's default cursor timeout).
  837:         """
  838:         if self._cursor is None:
  839:             self._create_cursor()
  840:             assert self._cursor is not None
  841:         try:
  842:             return self._cursor.next()
  843:         except CursorNotFound:
  844:             self._cursor.close()
  845:             self._create_cursor()
  846:             return self._cursor.next()
  847: 
  848:     def next(self) -> Mapping[str, Any]:
  849:         try:
  850:             chunk = self._next_with_retry()
  851:         except StopIteration:
  852:             if self._next_chunk >= self._num_chunks:
  853:                 raise
  854:             raise CorruptGridFile("no chunk #%d" % self._next_chunk) from None
  855: 
  856:         if chunk["n"] != self._next_chunk:
  857:             self.close()
  858:             raise CorruptGridFile(
  859:                 "Missing chunk: expected chunk #%d but found "
  860:                 "chunk with n=%d" % (self._next_chunk, chunk["n"])
  861:             )
  862: 
  863:         if chunk["n"] >= self._num_chunks:
  864:             # According to spec, ignore extra chunks if they are empty.
  865:             if len(chunk["data"]):
  866:                 self.close()
  867:                 raise CorruptGridFile(
  868:                     "Extra chunk found: expected %d chunks but found "
  869:                     "chunk with n=%d" % (self._num_chunks, chunk["n"])
  870:                 )
  871: 
  872:         expected_length = self.expected_chunk_length(chunk["n"])
  873:         if len(chunk["data"]) != expected_length:
  874:             self.close()
  875:             raise CorruptGridFile(
  876:                 "truncated chunk #%d: expected chunk length to be %d but "
  877:                 "found chunk with length %d" % (chunk["n"], expected_length, len(chunk["data"]))
  878:             )
  879: 
  880:         self._next_chunk += 1
  881:         return chunk
  882: 
  883:     __next__ = next
  884: 
  885:     def close(self) -> None:
  886:         if self._cursor:
  887:             self._cursor.close()
  888:             self._cursor = None
  889: 
  890: 
  891: class GridOutIterator:
  892:     def __init__(self, grid_out: GridOut, chunks: Collection, session: ClientSession):
  893:         self.__chunk_iter = _GridOutChunkIterator(grid_out, chunks, session, 0)
  894: 
  895:     def __iter__(self) -> GridOutIterator:
  896:         return self
  897: 
  898:     def next(self) -> bytes:
  899:         chunk = self.__chunk_iter.next()
  900:         return bytes(chunk["data"])
  901: 
  902:     __next__ = next
  903: 
  904: 
  905: class GridOutCursor(Cursor):
  906:     """A cursor / iterator for returning GridOut objects as the result
  907:     of an arbitrary query against the GridFS files collection.
  908:     """
  909: 
  910:     def __init__(
  911:         self,
  912:         collection: Collection,
  913:         filter: Optional[Mapping[str, Any]] = None,
  914:         skip: int = 0,
  915:         limit: int = 0,
  916:         no_cursor_timeout: bool = False,
  917:         sort: Optional[Any] = None,
  918:         batch_size: int = 0,
  919:         session: Optional[ClientSession] = None,
  920:     ) -> None:
  921:         """Create a new cursor, similar to the normal
  922:         :class:`~pymongo.cursor.Cursor`.
  923: 
  924:         Should not be called directly by application developers - see
  925:         the :class:`~gridfs.GridFS` method :meth:`~gridfs.GridFS.find` instead.
  926: 
  927:         .. versionadded 2.7
  928: 
  929:         .. seealso:: The MongoDB documentation on `cursors <https://dochub.mongodb.org/core/cursors>`_.
  930:         """
  931:         _disallow_transactions(session)
  932:         collection = _clear_entity_type_registry(collection)
  933: 
  934:         # Hold on to the base "fs" collection to create GridOut objects later.
  935:         self.__root_collection = collection
  936: 
  937:         super().__init__(
  938:             collection.files,
  939:             filter,
  940:             skip=skip,
  941:             limit=limit,
  942:             no_cursor_timeout=no_cursor_timeout,
  943:             sort=sort,
  944:             batch_size=batch_size,
  945:             session=session,
  946:         )
  947: 
  948:     def next(self) -> GridOut:
  949:         """Get next GridOut object from cursor."""
  950:         _disallow_transactions(self.session)
  951:         next_file = super().next()
  952:         return GridOut(self.__root_collection, file_document=next_file, session=self.session)
  953: 
  954:     __next__ = next
  955: 
  956:     def add_option(self, *args: Any, **kwargs: Any) -> NoReturn:
  957:         raise NotImplementedError("Method does not exist for GridOutCursor")
  958: 
  959:     def remove_option(self, *args: Any, **kwargs: Any) -> NoReturn:
  960:         raise NotImplementedError("Method does not exist for GridOutCursor")
  961: 
  962:     def _clone_base(self, session: Optional[ClientSession]) -> GridOutCursor:
  963:         """Creates an empty GridOutCursor for information to be copied into."""
  964:         return GridOutCursor(self.__root_collection, session=session)
